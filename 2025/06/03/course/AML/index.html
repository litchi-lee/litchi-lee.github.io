<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>NJU2025春季学期-高级机器学习笔记 | Leo的博客</title><meta name="author" content="Leo Sinclair"><meta name="copyright" content="Leo Sinclair"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="关于AML的部分复习笔记整理">
<meta property="og:type" content="article">
<meta property="og:title" content="NJU2025春季学期-高级机器学习笔记">
<meta property="og:url" content="https://litchi-lee.github.io/2025/06/03/course/AML/index.html">
<meta property="og:site_name" content="Leo的博客">
<meta property="og:description" content="关于AML的部分复习笔记整理">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://litchi-lee.github.io/img/bg4.jpg">
<meta property="article:published_time" content="2025-06-02T16:00:00.000Z">
<meta property="article:modified_time" content="2025-06-16T13:36:01.760Z">
<meta property="article:author" content="Leo Sinclair">
<meta property="article:tag" content="课程">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://litchi-lee.github.io/img/bg4.jpg"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "NJU2025春季学期-高级机器学习笔记",
  "url": "https://litchi-lee.github.io/2025/06/03/course/AML/",
  "image": "https://litchi-lee.github.io/img/bg4.jpg",
  "datePublished": "2025-06-02T16:00:00.000Z",
  "dateModified": "2025-06-16T13:36:01.760Z",
  "author": [
    {
      "@type": "Person",
      "name": "Leo Sinclair",
      "url": "https://litchi-lee.github.io/"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://litchi-lee.github.io/2025/06/03/course/AML/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":200,"languages":{"author":"作者: Leo Sinclair","link":"链接: ","source":"来源: Leo的博客","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'NJU2025春季学期-高级机器学习笔记',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="web_bg" style="background-image: url(img/bg0.png);"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/img0.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">19</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">8</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">5</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/galleries/"><i class="fa-fw fas fa-images"></i><span> 图库</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/music/"><i class="fa-fw fas fa-music"></i><span> 听歌日志</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(/img/img3.png);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><img class="site-icon" src="/img/logo0.jpg" alt="Logo"><span class="site-name">Leo的博客</span></a><a class="nav-page-title" href="/"><span class="site-name">NJU2025春季学期-高级机器学习笔记</span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/galleries/"><i class="fa-fw fas fa-images"></i><span> 图库</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/music/"><i class="fa-fw fas fa-music"></i><span> 听歌日志</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">NJU2025春季学期-高级机器学习笔记</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-06-02T16:00:00.000Z" title="发表于 2025-06-03 00:00:00">2025-06-03</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-06-16T13:36:01.760Z" title="更新于 2025-06-16 21:36:01">2025-06-16</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E7%A0%94%E7%A9%B6%E7%94%9F%E8%AF%BE%E7%A8%8B/">研究生课程</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="第一讲绪论">第一讲、绪论</h1>
<div class="note primary no-icon flat"><p><strong>考点</strong>：</p>
<p>（<em>2022，2023</em>）机器学习的定义；</p>
<p>（<em>2021，2023</em>）机器学习的鲁棒性。</p>
</div>
<h2 id="机器学习">机器学习</h2>
<p>机器学习：机器利用数据学习人类经验，不断提高性能的过程</p>
<p>机器学习的鲁棒性：机器学习中的鲁棒性（Robustness）指的是模型在面对数据扰动、异常样本或分布变化时，仍然能够保持良好性能的能力。换句话说，就是模型对“不完美”输入的容忍能力。</p>
<h2 id="人工智能">人工智能</h2>
<p>人工智能的发展阶段：</p>
<ul>
<li>1950年，图灵机；</li>
<li>1956年，达特茅斯（Dartmouth）会议；</li>
<li>60-80年代，<strong>推理期</strong>；</li>
<li>80-90年代，<strong>知识期</strong>；</li>
<li>90年代后，<strong>学习期</strong>。</li>
</ul>
<h2 id="重要术语">重要术语</h2>
<ul>
<li>监督学习与无监督学习；</li>
<li>数据集、训练、测试…</li>
<li>机器学习技术的根本目标是–<strong>使模型具有泛化能力</strong>：应对未见样本的预测能力；</li>
<li>归纳偏置（Inductive Bias）：任何一个有效的机器学习算法必有其偏好；</li>
<li>NFL（No Free Lunch）原则：一个算法 <span class="math inline">𝔏<sub><em>a</em></sub></span> 若在某些问题上比另一个算法 <span class="math inline">𝔏<sub><em>b</em></sub></span> 好，必存在另一些问题 <span class="math inline">𝔏<sub><em>b</em></sub></span> 比 <span class="math inline">𝔏<sub><em>a</em></sub></span> 好；</li>
</ul>
<hr />
<h1 id="第二讲模型评估和选择">第二讲、模型评估和选择</h1>
<div class="note primary no-icon flat"><p><strong>考点</strong>：</p>
<p>（<em>2022</em>）评估方法、性能度量、比较检验各解决什么问题；</p>
<p>（<em>2023</em>）过拟合&amp;欠拟合。</p>
</div>
<p><strong>泛化误差 VS 经验误差</strong></p>
<ul>
<li>泛化误差：在“未来”（unseen）样本上的误差；</li>
<li>经验误差：在训练集上的误差，亦称训练误差；</li>
</ul>
<p>泛化误差越小越好，但是经验误差并非越小越好（会出现过拟合）。</p>
<p><strong>过拟合 VS 欠拟合</strong>：一个是模型还没有学充分，一个是模型学得过于充分</p>
<h2 id="评估方法">评估方法</h2>
<ol type="1">
<li>留出法（hold-out）：将数据集划分为训练和测试集；</li>
<li>交叉验证法（cross-validation）：将数据集分层采样划分为k个大小相似的互斥子集，每次用k-1个子集的并集作为训练集，余下的子集作为测试集，最终返回k个测试结果的均值；</li>
<li>留一法（LOO）：假设数据集D包含m个样本，若令k=m，则得到留一法。</li>
</ol>
<h2 id="性能度量">性能度量</h2>
<p><strong>回归任务</strong>最常用的性能度量为<strong>均方误差</strong>（MSE）：</p>
<p><span class="math display">$$
\begin{split}
    E(f ; D)=\frac{1}{m} \sum_{i=1}^m\left(f\left(\mathbf{x}_i\right)-y_i\right)^2
\end{split}
$$</span></p>
<p><strong>分类任务</strong>最常见的性能度量为<strong>错误率（err）和精度（acc）</strong>：</p>
<p><span class="math display">$$
\begin{split}
    E(f ; D)=\frac{1}{m} \sum_{i=1}^m \mathbf{I}\left(f\left(\mathbf{x}_i\right) \neq y_i\right)
\end{split}
$$</span></p>
<p><span class="math display">$$
\begin{aligned}
    acc(f ; D)&amp;=\frac{1}{m} \sum_{i=1}^m \mathbf{I}\left(f\left(\mathbf{x}_i\right) = y_i\right) \\
    &amp;= 1-E(f ; D)
\end{aligned}
$$</span></p>
<p>信息检索和Web搜索中通常使用<strong>查准率（P）和查全率（R）</strong>来衡量正例比率：</p>
<p><span class="math display">$$
\begin{aligned}
    P &amp;= \frac{TP}{TP+FP} \\
    R &amp;= \frac{TP}{TP+FN}
\end{aligned}
$$</span></p>
<blockquote>
<p>补充：</p>
<p><strong>TP</strong>（真正例，gt-正 &amp; pred-正）</p>
<p><strong>TN</strong>（真负例，gt-负 &amp; pred-负）</p>
<p><strong>FP</strong>（假正例，gt-负 &amp; pred-正）</p>
<p><strong>FN</strong>（假负例，gt-正 &amp; pred-负）</p>
</blockquote>
<p><strong>F1度量</strong>要更为常用：</p>
<p><span class="math display">$$
\begin{split}
    F1 = \frac{2PR}{P+R}
\end{split}
$$</span></p>
<p><span class="math inline"><em>F</em><sub><em>β</em></sub></span>是比F1更一般的形式：</p>
<p><span class="math display">$$
\begin{split}
    F_\beta = \frac{(1+\beta^2)PR}{\beta^2P+R}
\end{split}
$$</span></p>
<p>当<span class="math inline"><em>β</em> = 1</span>时，是标准的F1指标；当<span class="math inline"><em>β</em> &gt; 1</span>时，更偏重查全率（R），此时适用于逃犯信息检索任务；当<span class="math inline"><em>β</em> &lt; 1</span>时，更偏重查准率（P），此时适用于商品推荐任务；</p>
<p><strong>ROC曲线</strong>是常用于二分类任务中的工具，其基于<strong>真正例率</strong>（TPR，又叫做召回率）和<strong>假正例率</strong>（FPR）：</p>
<p><span class="math display">$$
\begin{aligned}
    TPR &amp;= \frac{TP}{TP+FN} \\
    FPR &amp;= \frac{FP}{TN+FP}
\end{aligned}
$$</span></p>
<p>通过遍历不同的阈值，每次将大于阈值的作为正例，小于阈值的作为负例，能得到一组TPR和FPR，最终将这些点按序连接可以得到ROC曲线。而<strong>AUC</strong>的值就是ROC曲线下的面积大小。</p>
<p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/ROC.png" width="50%" /></p>
<h2 id="比较检验">比较检验</h2>
<p>使用T-检验来进行比较检验。</p>
<h2 id="偏差-方差分解">偏差-方差分解</h2>
<p>对于回归任务，其泛化误差可通过偏差-方差分解拆解为：</p>
<p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/generalError.png" width="80%" /></p>
<p>泛化性能是由<strong>学习算法的能力</strong>、<strong>数据的充分性</strong>、<strong>学习任务的难度</strong>共同决定的。</p>
<hr />
<h1 id="第三讲线性模型">第三讲、线性模型</h1>
<div class="note primary no-icon flat"><p><strong>考点</strong>：</p>
<p>（<em>2021，2022</em>）最小二乘法推导；</p>
<p>（<em>2023</em>）多元线性回归求闭式解。</p>
</div>
<h2 id="回归任务模型">3.1.回归任务模型</h2>
<h3 id="一元线性回归">一元线性回归</h3>
<p>对于单一属性的线性回归，其模型可构建为：</p>
<p><span class="math display">$$
\begin{split}
    f\left(x\right)=wx_{i}+b\quad\text{使得}f\left(x_{i}\right)\simeq y_{i}
\end{split}
$$</span></p>
<p>参数估计方法：<strong>最小二乘法</strong></p>
<p><span class="math display">$$
\begin{aligned}
(w^{*},b^{*}) &amp; =\underset{(w,b)}{\operatorname*{\operatorname*{\arg\min}}}\sum_{i=1}^m\left(f\left(x_i\right)-y_i\right)^2 \\
 &amp; =\arg\min_{(w,b)}\sum_{i=1}^m\left(y_i-wx_i-b\right)^2
\end{aligned}
$$</span></p>
<p>求解方法：最小化均方误差，分别对 <span class="math inline"><em>ω</em></span> 和 <span class="math inline"><em>b</em></span> 求导，可得闭式解：</p>
<p><span class="math display">$$
\begin{split}
w=\frac{\sum_{i=1}^my_i\left(x_i-\bar{x}\right)}{\sum_{i=1}^mx_i^2-\frac{1}{m}\left(\sum_{i=1}^mx_i\right)^2}
\end{split}
$$</span></p>
<p><span class="math display">$$
\begin{split}
b=\frac{1}{m}\sum_{i=1}^{m}\left(y_{i}-wx_{i}\right)
\end{split}
$$</span></p>
<h3 id="多元线性回归">多元线性回归</h3>
<p>对于多元属性的线性回归（多元线性回归），其中每个样本属性为 <span class="math inline"><strong>x</strong><sub><em>i</em></sub> = (<em>x</em><sub><em>i</em>1</sub>; <em>x</em><sub><em>i</em>2</sub>; …; <em>x</em><sub><em>i</em><em>d</em></sub>)  <em>y</em><sub><em>i</em></sub> ∈ <strong>R</strong></span> ，其模型可被推广为：</p>
<p><span class="math display">$$
\begin{split}
f\left(\mathbf{x}_i\right)=\mathbf{\omega}^\mathrm{T}\mathbf{x}_i+b\text{ 使得}f\left(\mathbf{x}_i\right)\simeq y_i
\end{split}
$$</span></p>
<p>令 <span class="math inline">$\hat{\mathbf{\omega}} = (\mathbf{\omega};b)$</span> ，并构造：</p>
<p><span class="math display">$$
\mathbf{X}=
\begin{pmatrix}
x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1d} &amp; 1 \\
x_{21} &amp; x_{22} &amp; \cdots &amp; x_{2d} &amp; 1 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots &amp; \vdots \\
x_{m1} &amp; x_{m2} &amp; \cdots &amp; x_{md} &amp; 1
\end{pmatrix}=
\begin{pmatrix}
\mathbf{x}_1^\mathrm{T} &amp; 1 \\
\mathbf{x}_2^\mathrm{T} &amp; 1 \\
\vdots &amp; \vdots \\
\mathbf{x}_m^\mathrm{T} &amp; 1
\end{pmatrix}
$$</span></p>
<p>那么此时使用最小二乘法可以表述为：</p>
<p><span class="math display">$$
\begin{split}
\hat{\mathbf{\omega}}^*=\arg\min_{\hat{\mathbf{\omega}}}\left(\mathbf{y}-\mathbf{X}\hat{\mathbf{\omega}}\right)^\mathrm{T}\left(\mathbf{y}-\mathbf{X}\hat{\mathbf{\omega}}\right).
\end{split}
$$</span></p>
<h3 id="广义线性回归模型">广义线性回归模型</h3>
<p>广义线性回归模型为：</p>
<p><span class="math display">$$
\begin{split}
y=g^{-1}\left(\mathbf{\omega}^\mathrm{T}\mathbf{x}+b\right)
\end{split}
$$</span></p>
<p>其中<span class="math inline"><em>g</em>( ⋅ )</span>被称为<strong>联系函数</strong>，并满足单调可微的性质。可以理解这里为回归模型引入了非线性的成分。</p>
<h2 id="二分类任务模型">3.2.二分类任务模型</h2>
<p>预测值与输出需要找到函数将其联系起来：</p>
<p><span class="math display">$$
\begin{split}
z=\mathbf{\omega}^\mathrm{T}\mathbf{x}+b\quad y\in\{0,1\}
\end{split}
$$</span></p>
<p>最理想的函数是单位跃迁函数，即预测值大于零判为正例、小于零判为负例，但是<strong>单位跃迁函数不连续也不可导</strong>。因此使用替代函数–<strong>对数几率函数</strong>（logistic function）作为联系函数，其单调可微且任意阶可导：</p>
<p><span class="math display">$$
\begin{split}
y=\frac{1}{1+e^{-z}}
\end{split}
$$</span></p>
<p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250604130952.png" width="50%" /></p>
<p>进一步推导可得：</p>
<p><span class="math display">$$
\begin{split}
ln\frac{y}{1-y}=\mathbf{\omega}^\mathrm{T}\mathbf{x}+b
\end{split}
$$</span></p>
<p>这里的 <span class="math inline">$ln\frac{y}{1-y}$</span> 称为几率，反映了 <span class="math inline"><strong>x</strong></span> 作为正例的相对可能性。求解使用极大似然法进行求解：</p>
<p><span class="math display">$$
\begin{split}
\ell\left(\mathbf{\omega},b\right)=\sum_{i=1}^m\ln p\left(y_i\mid\mathbf{x}_i;\mathbf{\omega}_i,b\right)
\end{split}
$$</span></p>
<p><span class="math display">$$
\begin{split}
p\left(y_i\mid\mathbf{x}_i;\mathbf{\omega}_i,b\right)=y_ip_1\left(\hat{\mathbf{x}}_i;\mathbf{\beta}\right)+\left(1-y_i\right)p_0\left(\hat{\mathbf{x}}_i;\mathbf{\beta}\right)
\end{split}
$$</span></p>
<h2 id="多分类任务模型">3.3.多分类任务模型</h2>
<h3 id="一对一ovo">一对一（OvO）</h3>
<p>拆分阶段：N个类别两两配对，各自训练二分类分类器。</p>
<p>测试阶段：新样本提交给所有分类器预测，被预测最多的类别为最终类别。</p>
<h3 id="一对其他ovr">一对其他（OvR）</h3>
<p>拆分阶段：某一类作为正例，其余作为反例，各自训练二分类分类器。</p>
<p>测试阶段：新样本提交给所有分类器，使用置信度最大的类别作为最终类别。</p>
<h3 id="多对多mvm">多对多（MvM）</h3>
<p>若干类作为正类，若干类作为反类，使用纠错输出码（Error Correcting Output Code，ECOC）进行最终预测：</p>
<ul>
<li><strong>编码阶段</strong>：对N个类别做M次划分，每次划分将一部分类别划为正类，一部分划为反类；</li>
<li><strong>解码阶段</strong>：测试样本交给M个分类器预测；</li>
<li>最终距离最小的类别为最终类别。</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/EOOC.png" width="80%" /></p>
<h2 id="线性模型的优缺点">3.4.线性模型的优缺点</h2>
<blockquote>
<p>优点：</p>
<p>形式简单、易于建模；</p>
<p>具有一定的可解释性。</p>
<p>缺点：</p>
<p>难以处理非线性问题。</p>
</blockquote>
<hr />
<h1 id="第四讲支持向量机">第四讲、支持向量机</h1>
<div class="note primary no-icon flat"><p><strong>考点</strong>：</p>
<p>（<em>2022，2023</em>）SVM基本型&amp;对偶型推导；</p>
<p>（<em>2023</em>）核函数。</p>
</div>
<p>将训练样本分开的超平面有很多，但是“正中间”的鲁棒性是最好的，泛化能力也最强。</p>
<p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/SVM.png" width="60%" /></p>
<p>SVM优化目标：寻找最大间隔，求解参数 <span class="math inline"><strong>ω</strong></span> 和 <span class="math inline"><em>b</em></span> 使得 <span class="math inline"><em>γ</em></span> 最大。</p>
<h2 id="基本型和对偶型">4.1.基本型和对偶型</h2>
<p>优化问题：最大间隔。</p>
<p><span class="math display">$$
\begin{aligned}
\underset{\mathbf{\omega}, b}{\arg \max } &amp; \frac{2}{\|\mathbf{\omega}\|} \\
\text { s.t. } &amp; y_i\left(\mathbf{\omega}^{\top} \mathbf{x}_i+b\right) \geq 1, i=1,2, \ldots, m .
\end{aligned}
$$</span></p>
<p>其可转换为等价问题：凸二次规划问题，这也是<strong>支持向量机的基本型</strong>。</p>
<p><span class="math display">$$
\begin{aligned}
\underset{\mathbf{\omega}, b}{\arg \min } &amp; \frac{\|\mathbf{\omega}\|^2}{2} \\
\text { s.t. } &amp; y_i\left(\mathbf{\omega}^{\top} \mathbf{x}_i+b\right) \geq 1, i=1,2, \ldots, m .
\end{aligned}
$$</span></p>
<p>引入拉格朗日乘子 <span class="math inline"><em>α</em><sub><em>i</em></sub> ≥ 0</span> 可以得到拉格朗日函数：</p>
<p><span class="math display">$$
\begin{split}
L(\mathbf{\omega}, b, \mathbf{\alpha})=\frac{1}{2}\|\mathbf{\omega}\|^2+\sum_{i=1}^m \alpha_i\left(1-y_i\left(\mathbf{\omega}^{\mathrm{T}} \mathbf{x}_i+b\right)\right)
\end{split}
$$</span></p>
<p>分别对 <span class="math inline"><strong>ω</strong></span> 和 <span class="math inline"><em>b</em></span> 的偏导为零可得：</p>
<p><span class="math display">$$
\begin{split}
\mathbf{\omega}=\sum_{i=1}^m\alpha_iy_i\mathbf{x}_i,0=\sum_{i=1}^m\alpha_iy_i
\end{split}
$$</span></p>
<p>最后可以得到<strong>支持向量机的对偶型</strong>：</p>
<p><span class="math display">$$
\begin{aligned}
\max_{\alpha} &amp; \sum_{i=1}^{m}\alpha_{i}-\frac{1}{2}\sum_{i=1}^{m}\sum_{j=1}^{m}\alpha_{i}\alpha_{j}y_{i}y_{j}\mathbf{x}_{i}^{\mathrm{T}}\mathbf{x}_{j} \\
\mathrm{s.t.} &amp; \sum_{i=1}^{m}\alpha_{i}y_{i}=0,\alpha_{i}\geqslant0,\quad i=1,2,\ldots,m
\end{aligned}
$$</span></p>
<h2 id="特征空间映射">4.2.特征空间映射</h2>
<p>若不存在一个能正确划分两类样本的超平面，我们可以将样本从原始空间映射到一个更高维的特征空间，使样本在这个特征空间内线性可分。<em>如果原始空间是有限维（特征数有限），那么一定存在一个高维特征空间是样本可分</em>。</p>
<p>设样本 <span class="math inline"><strong>x</strong></span> 映射后的向量为 <span class="math inline"><em>ϕ</em>(<strong>x</strong>)</span> ，划分超平面为：</p>
<p><span class="math display">$$
\begin{split}
    f(\mathbf{x}) = \mathbf{\omega}^{T}\phi(\mathbf{x})+b
\end{split}
$$</span></p>
<p>则原基本型和对偶型可推广为：</p>
<p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250604142808.png" width="60%" /></p>
<p><strong>核函数</strong>：用于绕过显式考虑特征映射、以及计算高维内积的困难。</p>
<p><span class="math display">$$
\begin{split}
    \kappa(\mathbf{x}_i,\mathbf{x}_j)=\phi(\mathbf{x}_i)^\mathrm{T}\phi(\mathbf{x}_j)
\end{split}
$$</span></p>
<blockquote>
<p>Mercer定理：若一个对称函数所对应的核矩阵<strong>半正定</strong>，则它就能作为核函数来使用。</p>
</blockquote>
<p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250604143436.png" width="80%" /></p>
<h2 id="软间隔支持向量机">4.3.软间隔支持向量机</h2>
<p>软间隔SVM不再假设所有样本都可分，而是引入损失函数，计算每个样本的损失，然后再<strong>最大化间隔和最小化整体损失之间做个合理的折中</strong>：</p>
<p><span class="math display">$$
\begin{aligned}
\min_{\mathbf{\omega},b}\frac{1}{2}\|\mathbf{\omega}\|^2+C\sum_{i=1}^ml_{0/1}\left(y_i(\mathbf{\omega}^\top\phi(\mathbf{x}_i)+b)-1\right) \\
l_{0/1}=
\begin{cases}
1 &amp; z&lt;0 \\
0 &amp; otherwise &amp; 
\end{cases}
\end{aligned}
$$</span></p>
<p>存在的问题是0/1损失函数非凸非连续，因此这里使用Hinge损失函数来替代0/1损失函数：</p>
<p><span class="math display">$$
\begin{split}
\mathrm{Hinge}(y,f(x))=\max(0,1-y\cdot f(x))
\end{split}
$$</span></p>
<p>由此可以分别得到软间隔支持向量机的基本型和对偶型：</p>
<ol type="1">
<li>原始问题</li>
</ol>
<p><span class="math display">$$
\begin{split}
\min_{\mathbf{\omega},b}\frac{1}{2}\|\mathbf{\omega}\|^2+C\sum_{i=1}^m\max\left(0,1-y_i(\mathbf{\omega}^\top\phi(\mathbf{x}_i)+b)\right).
\end{split}
$$</span></p>
<ol start="2" type="1">
<li>对偶问题</li>
</ol>
<p><span class="math display">$$
\begin{aligned}
\operatorname*{min}_{\alpha} &amp;\frac{1}{2}\sum_{i=1}^{m}\sum_{j=1}^{m}\alpha_{i}\alpha_{j}y_{i}y_{j}\phi(\mathbf{x}_{i})^{\top}\phi(\mathbf{x}_{j})-\sum_{i=1}^{m}\alpha_{i} \\
\mathrm{s.t.} &amp;\sum_{i=1}^{m}\alpha_{i}y_{i}=0,0\leq\alpha_{i}\leq C,i=1,2,\ldots,m.
\end{aligned}
$$</span></p>
<h2 id="svm拓展正则化">4.4.SVM拓展–正则化</h2>
<p>统计学习模型的更一般形式可以表述为：</p>
<p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250604144936.png" width="60%" /></p>
<p>其中正则化可理解为罚函数，使优化过程趋向于期望目标。</p>
<hr />
<h1 id="第五讲神经网络">第五讲、神经网络</h1>
<div class="note primary no-icon flat"><p><strong>考点</strong>：</p>
<p>（<em>2022，2023</em>）多层前馈网络的计算能力、局限性、解决方法。</p>
</div>
<h2 id="神经网络历史">5.1.神经网络历史</h2>
<ul>
<li>第一阶段（1943~1969）：以感知机为代表；</li>
<li>第二阶段（1982~2000）：反向传播算法为代表，Hopfield网络；</li>
<li>第三阶段（2006~迄今）：深度网络为代表。</li>
</ul>
<h2 id="神经元模型">5.2.神经元模型</h2>
<p>M-P神经元模型 <em>[McCulloch and Pitts, 1943]</em></p>
<ul>
<li>输入：来自其他 n个神经元传递过来的信号</li>
<li>处理：通过带权重连接进行传递, 总值与神经元的阈值比较</li>
<li>输出：通过激活函数得到输出</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250604145756.png" width="60%" /></p>
<blockquote>
<p>常用激活函数：<strong>Sigmoid函数</strong></p>
<p><span class="math display">$$
\operatorname{sigmoid}(x)=\frac{1}{1+e^{-x}}
$$</span></p>
<p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250604150040.png" width="30%" /></p>
</blockquote>
<h2 id="感知机与多层网络">5.3.感知机与多层网络</h2>
<p><strong>单层感知机</strong>：由两层神经元组成，输入层接受外界输入信号传递给输出层M-P神经元。</p>
<p>当两类模式线性可分时，感知机的学习过程一定会收敛，否则其学习过程会发生震荡。因此其无法解决非线性可分问题。</p>
<p><strong>多层感知机</strong>：输入层和输出层中存在多层神经元，称为隐层或隐含层，其中隐层和输出层神经元都是具有激活功能的功能神经元。</p>
<ul>
<li>定义：每两层神经元全互联，不存在同层连接和跨层连接；</li>
<li>前馈：接受外界输入信号，隐含层与输出层神经元对信号进行加工输出；</li>
<li>学习：根据训练数据调整神经元的“连接权”以及功能神经元的“阈值”。</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250604151125.png" width="50%" /></p>
<h2 id="误差逆传播算法">5.4.误差逆传播算法</h2>
<p><strong>误差逆传播算法</strong>（Error BackPropagation，反向传播）是最常用的多层前馈神经网络的学习算法：</p>
<p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250604152717.png" width="50%" /></p>
<ol type="1">
<li>前向计算：</li>
</ol>
<p><span class="math display">$$
\begin{split}
\mathrm{step1:~}b_h=f(\alpha_h-\gamma_h),\alpha_h=\sum_{i=1}^dv_{ih}x_i
\end{split}
$$</span></p>
<p><span class="math display">$$
\begin{split}
\mathrm{step2:}\quad\hat{y}_j^k=f(\beta_j-\theta_j),\beta_j=\sum_{i=q}^dw_{hj}b_h
\end{split}
$$</span></p>
<p><span class="math display">$$
\begin{split}
\mathrm{step3:~}E_k=\frac{1}{2}\sum_{j=1}^l(\hat{y}_j^k-y_j^k)^2
\end{split}
$$</span></p>
<ol start="2" type="1">
<li>参数数量：</li>
</ol>
<ul>
<li><p>对于一层到二层的权重 <span class="math inline"><em>v</em><sub><em>i</em><em>h</em></sub></span> ，总共有 <span class="math inline"><em>d</em> ⋅ <em>q</em></span> 个参数需要优化；</p></li>
<li><p>对于二层到三层的权重 <span class="math inline"><em>w</em><sub><em>h</em><em>j</em></sub></span> ，总共有 <span class="math inline"><em>q</em> ⋅ <em>l</em></span> 个参数需要优化；</p></li>
<li><p>对于二层的阈值 <span class="math inline"><em>θ</em><sub><em>j</em></sub></span> ，总共有 <span class="math inline"><em>q</em></span> 个参数需要优化；</p></li>
<li><p>对于三层的阈值 <span class="math inline"><em>γ</em><sub><em>h</em></sub></span> ，总共有 <span class="math inline"><em>l</em></span> 个参数需要优化。</p></li>
</ul>
<p>因此网络总共有 <span class="math inline">(<em>d</em> + <em>l</em> + 1)<em>q</em> + <em>l</em></span> 个参数需要优化。</p>
<ol start="3" type="1">
<li>参数优化：</li>
</ol>
<p>BP算法基于<strong>梯度下降</strong>策略，以误差率为目标，计算负梯度方向对参数进行调整。</p>
<p>比如根据之前的前向过程，这里可以对二层的权重参数计算反向传播：</p>
<p><span class="math display">$$
\begin{split}
    \Delta w_{hj}=-\eta\frac{\partial E_k}{\partial w_{hj}}
\end{split}
$$</span></p>
<p><span class="math display">$$
\begin{split}
    \frac{\partial E_k}{\partial w_{hj}}=\frac{\partial E_k}{\partial\hat{y}_j^k}\cdot\frac{\partial\hat{y}_j^k}{\partial\beta_j}\cdot\frac{\partial\beta_j}{\partial w_{hj}}
\end{split}
$$</span></p>
<p>其中 <span class="math inline"><em>η</em> ∈ (0, 1)</span> 是学习率，控制着算法每一轮迭代中的更新步长。</p>
<blockquote>
<p>Sigmoid函数在求导时有一个很好的性质：</p>
<p><span class="math display">$$
\begin{split}
f^{\prime}\left(x\right)=f\left(x\right)\left(1-f\left(x\right)\right)
\end{split}
$$</span></p>
</blockquote>
<p><strong>BP的不同实现方式</strong>：</p>
<ul>
<li>标准BP：每次对单个训练样例更新权值和阈值，单次计算开销小但参数更新频繁，迭代次数多；</li>
<li>累计BP：最小化整个训练集上的累积误差，一般是读取整个训练集后更新参数，参数更新频率低但计算开销大；</li>
</ul>
<p><span class="math display">$$
\begin{split}
    E=\frac{1}{m}\sum_{k=1}^mE_k
\end{split}
$$</span></p>
<p>多层前馈网络具有强大的学习能力，包含足够多神经元的隐层，多层前馈神经网络能以任意精度逼近任意复杂度的连续函数。</p>
<p>但其也有其局限性：</p>
<ul>
<li>由于其强大的表达能力，经常遭遇过拟合；</li>
<li>如何设置隐藏神经元个数是个难题，实际应用中常使用试错法。</li>
</ul>
<p>缓解过拟合的一些策略包括：</p>
<ul>
<li>早停：在训练过程中, 若训练误差降低, 但验证误差升高, 则停止训练；</li>
<li>正则化：在误差目标函数中增加一项描述网络复杂程度的成分, 防止模型过于复杂，例如连接权值与阈值的平方和。</li>
</ul>
<p><span class="math display">$$
\begin{split}
    E=\lambda\frac{1}{m}\sum_{k=1}^mE_k+(1-\lambda)\sum_iw_i^2
\end{split}
$$</span></p>
<h2 id="深度学习">5.5.深度学习</h2>
<p>深度学习模型是具有很多个隐层的神经网络。一方面，计算能力的大幅提高缓解了训练效率；另一方面，训练数据的大幅增加降低了过拟合风险。因此，以“深度学习” (deep learning) 为代表的复杂模型成为了合适的选择。</p>
<blockquote>
<p><strong>复杂模型带来的困难</strong>：</p>
<p>深度网络难以直接用经典算法（例如BP算法）进行训练，因为误差在多隐层内传播时会出现梯度消失问题（即梯度迅速为0），难以收敛到稳定状态。</p>
</blockquote>
<p><strong>训练方法</strong>：</p>
<ul>
<li>预训练+微调：将大量参数进行分组，局部先找到较好的设置，然后再基于局部较优的结果进行全局寻优。</li>
<li>权共享：一组神经元使用相同的连接权值，权共享策略在卷积神经网络（CNN）中发挥了重要作用。</li>
</ul>
<hr />
<h1 id="第六讲决策树">第六讲、决策树</h1>
<div class="note primary no-icon flat"><p><strong>考点</strong>：</p>
<p>（<em>2023</em>）决策树最优划分的两个准则及其偏好；</p>
<p>（<em>2022</em>）决策树过拟合原因及其解决方案。</p>
</div>
<h2 id="基本流程">6.1.基本流程</h2>
<p>策略：<strong>分而治之</strong></p>
<p>三种停止条件：</p>
<ul>
<li>当前节点包含的样本全属于同一类别，无需划分；</li>
<li>当前属性集为空，或是所有样本在所有属性上取值相同，无法划分；</li>
<li>当前节点包含的样本集为空，不能划分。</li>
</ul>
<h2 id="划分算法">6.2.划分算法</h2>
<p>直觉上，决策树的分支结点所包含的样本应尽可能属于同一类别，即结点的<strong>纯度</strong>越高越好。</p>
<h3 id="信息增益id3决策树">信息增益（ID3决策树）</h3>
<p>信息熵（information entropy）是度量样本集合纯度最常用的一种指标，假定当前样本集合 <span class="math inline"><em>D</em></span> 中第 <span class="math inline"><em>k</em></span> 类样本所占的比例为 <span class="math inline"><em>p</em><sub><em>k</em></sub>(<em>k</em> = 1, 2, ..., |<em>y</em>|)</span>，则 <span class="math inline"><em>D</em></span> 的信息熵定义为：</p>
<p><span class="math display">$$
\begin{split}
    \mathrm{Ent}(D)=-\sum_{k=1}^{|y|}p_k\log_2p_k
\end{split}
$$</span></p>
<p><strong><span class="math inline"><em>E</em><em>n</em><em>t</em>(<em>D</em>)</span> 的值越小，<span class="math inline"><em>D</em></span> 的纯度越高</strong>。因此我们可以通过计算划分前后信息熵的差值来判断是否是最优划分，即计算<strong>信息增益</strong>：</p>
<p><span class="math display">$$
\begin{split}
    \mathrm{Gain}(D,a)=\mathrm{Ent}(D)-\sum_{v=1}^V\frac{|D^v|}{|D|}\mathrm{Ent}(D^v)
\end{split}
$$</span></p>
<p>这里的 <span class="math inline"><em>v</em></span> 表示目前属性 <span class="math inline"><em>a</em></span> 可能的取值，每一步划分时选择的属性就是最大的信息增益对应的属性，即ID3算法：</p>
<p><span class="math display">$$
\begin{split}
    a_{*}=\underset{a\in A}{\operatorname*{\operatorname*{\arg\max}}}\mathrm{Gain}(D,a)
\end{split}
$$</span></p>
<h3 id="增益率c4.5决策树">增益率（C4.5决策树）</h3>
<p>事实上，信息增益准则对可取值数目较多的属性有所偏好（比如气球的种类可能只有氢气球和氦气球等，但是颜色有很多种，这样计算颜色的信息增益一般都很大，因为平均到每个颜色的样本数相较要少、显得纯度更高），为减少这种偏好可能带来的不良影响，C4.5算法使用<strong>增益率</strong>（gain ratio）来选择最优划分属性：</p>
<p><span class="math display">$$
\begin{split}
    \mathrm{Gain_ratio}(D,a)=\frac{\mathrm{Gain}(D,a)}{\mathrm{IV}(a)}
\end{split}
$$</span></p>
<p><span class="math display">$$
\begin{split}
    \mathrm{IV}(a)=-\sum_{v=1}^V\frac{|D^v|}{|D|}\mathrm{log}_2\frac{|D^v|}{|D|}
\end{split}
$$</span></p>
<p>相当于是给信息增益做了一步规范化，<span class="math inline"><em>I</em><em>V</em>(<em>a</em>)</span> 称为属性 <span class="math inline"><em>a</em></span> 的固有值。一般属性的可能取值数目越多，其固有值越大。</p>
<h2 id="剪枝处理">6.3.剪枝处理</h2>
<p>决策树决策分支过多，以致于把训练集自身的一些特点当做所有数据都具有的一般性质，因此可能会导致<strong>过拟合</strong>。通常的解决办法是预留一部分数据用作“验证集”以进行性能评估，并采取剪枝策略来提升泛化性能。</p>
<h3 id="预剪枝">预剪枝</h3>
<p>思路：<strong>边建树，边剪枝</strong>。决策树生成过程中，对每个结点在划分前先进行估计，若当前结点的划分不能带来决策树泛化性能提升，则停止划分并将当前结点记为叶结点，其类别标记为训练样例数最多的类别。</p>
<blockquote>
<p><em>优缺点</em>：</p>
<p>优点：降低过拟合风险，显著减少训练时间和测试时间开销</p>
<p>缺点：存在欠拟合风险，基于贪心本质，有些分支当前划分虽然不能提升泛化性能，但在其基础上的后续划分可能导致性能提高</p>
</blockquote>
<h3 id="后剪枝">后剪枝</h3>
<p>思路：<strong>先建树，后剪枝</strong>。决策树生成后，自底向上对每个结点进行考察，若当前结点的划分导致在验证集上的精度下降，则将其剪除。</p>
<blockquote>
<p><em>优缺点</em>：</p>
<p>优点：比预剪枝保留了更多分支，欠拟合风险小；</p>
<p>缺点：训练时间开销大，后剪枝过程是在生成完全决策树之后进行的，需要自底向上对所有非叶结点逐一考察；其训练时间要远大于预剪枝决策树。</p>
</blockquote>
<h2 id="多变量决策树">6.4.多变量决策树</h2>
<p>非叶结点不再是仅仅针对某个属性，而是对属性的线性组合。</p>
<p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250604205006093.png" width="50%" /></p>
<hr />
<h1 id="第七讲贝叶斯分类器">第七讲、贝叶斯分类器</h1>
<div class="note primary no-icon flat"><p><strong>考点</strong>：</p>
<p>（<em>2018</em>）最优贝叶斯分类器及贝叶斯风险；</p>
<p>（<em>2019，2022，2023</em>）生成式&amp;判别式模型的区别。</p>
</div>
<h2 id="贝叶斯决策论">7.1.贝叶斯决策论</h2>
<p>给定 <span class="math inline"><em>N</em></span> 个类别，令 <span class="math inline"><em>λ</em><sub><em>i</em><em>j</em></sub></span> 表示将第 <span class="math inline"><em>j</em></span> 类样本误分类为第 <span class="math inline"><em>i</em></span> 类所产生的损失，则基于后验概率将样本 <span class="math inline"><em>x</em></span> 分到第 <span class="math inline"><em>i</em></span> 类的条件风险为：</p>
<p><span class="math display">$$
\begin{split}
    R(c_i\mid\mathbf{x})=\sum_{j=1}^N\lambda_{ij}P(c_j\mid x)
\end{split}
$$</span></p>
<p><strong>贝叶斯判定准则</strong>（Bayes Decision Rule）：</p>
<p><span class="math display">$$
\begin{split}
    h^*(x)=\underset{c\in\mathcal{Y}}{\operatorname*{\arg\min}}R(c\mid\mathbf{x})
\end{split}
$$</span></p>
<p><span class="math inline"><em>h</em><sup>*</sup>(<em>x</em>)</span> 称为贝叶斯最优分类器，其总体风险称为贝叶斯风险，反映了学习性能的理论上限。</p>
<p>机器学习需要实现的是基于有限的训练样本尽可能准确地估计出后验概率：</p>
<p><span class="math display">$$
\begin{split}
    P(c\mid x)=\frac{P(c)P(x\mid c)}{P(x)}
\end{split}
$$</span></p>
<blockquote>
<p><strong>判别式 VS 生成式</strong>：</p>
<p>1.判别式（discriminative）模型</p>
<p>直接对 <span class="math inline"><em>P</em>(<em>c</em> ∣ <em>x</em>)</span> 建模，代表方法：决策树、BP神经网络、SVM</p>
<p>2.生成式（generative）模型</p>
<p>先对联合概率分布 <span class="math inline"><em>P</em>(<em>x</em>, <em>c</em>)</span> 建模，再由此获得 <span class="math inline"><em>P</em>(<em>c</em> ∣ <em>x</em>)</span>，代表方法：贝叶斯分类器</p>
</blockquote>
<h2 id="朴素贝叶斯">7.2.朴素贝叶斯</h2>
<p>主要障碍是所有属性上的联合概率难以从有限训练样本估计获得。因此我们不妨<strong>假定所有属性之间相互独立</strong>，可以得到：</p>
<p><span class="math display">$$
\begin{split}
    P(c\mid\mathbf{x})=\frac{P(c)P(\mathbf{x}\mid c)}{P(\mathbf{x})}=\frac{P(c)}{P(\mathbf{x})}\prod_{i=1}^dP(x_i\mid c)
\end{split}
$$</span></p>
<p>由于 <span class="math inline"><em>P</em>(<strong>x</strong>)</span> 对所有类别相同，由此可以得到：</p>
<p><span class="math display">$$
\begin{split}
    h_{nb}(x)=\arg\max_{c\in\mathcal{Y}}P(c)\prod_{i=1}^dP(x_i\mid c)
\end{split}
$$</span></p>
<p>接下来分两步计算上式：</p>
<ul>
<li>估计 <span class="math inline"><em>P</em>(<em>c</em>)</span></li>
</ul>
<p><span class="math display">$$
\begin{split}
    P(c)=\frac{|D_c|}{|D|}
\end{split}
$$</span></p>
<ul>
<li>估计 <span class="math inline"><em>P</em>(<strong>x</strong> ∣ <em>c</em>)</span></li>
</ul>
<p>对离散属性，有：</p>
<p><span class="math display">$$
\begin{split}
    P(x_i\mid c)=\frac{|D_{c,x_i}|}{|D_c|}
\end{split}
$$</span></p>
<p>对连续属性，使用概率密度函数：</p>
<p><span class="math display">$$
\begin{split}
    p\left(x_i \mid c\right)=\frac{1}{\sqrt{2 \pi} \sigma_{c, i}} \exp \left(-\frac{\left(x_i-\mu_{c, i}\right)^2}{2 \sigma_{c, i}^2}\right)
\end{split}
$$</span></p>
<p>这个思路有一个问题，一旦有一个属性值在训练集中没有在某个类别中出现过，则由频率估计概率，该属性值的概率为0，直接连乘会导致整个 <span class="math inline"><em>P</em>(<strong>x</strong> ∣ <em>c</em>)</span> 为0。因此为了避免这种情况，这里使用<strong>拉普拉斯修正</strong>：</p>
<p><span class="math display">$$
\begin{split}
    \hat{P}(c)=\frac{|D_c|+1}{|D|+N},\quad\hat{P}(x_i\mid c)=\frac{|D_{c,x_i}|+1}{|D_c|+N_i}
\end{split}
$$</span></p>
<p>其中 <span class="math inline"><em>N</em></span> 表示训练集 <span class="math inline"><em>D</em></span> 中可能的类别数，<span class="math inline"><em>N</em><sub><em>i</em></sub></span> 表示第 <span class="math inline"><em>i</em></span> 个属性可能的取值数。</p>
<h2 id="半朴素贝叶斯">7.3.半朴素贝叶斯</h2>
<p>朴素贝叶斯分类器的<strong>属性独立性假设</strong>在现实情况中往往难以成立，因此半朴素贝叶斯分类器（Semi-naïve Bayes Classifier）会适当考虑一部分属性间的相互依赖关系。其中最常用的策略是<strong>独依赖估计</strong>（One-Dependent Estimator，ODE），即假设每个属性在类别之外最多仅依赖一个其他属性：</p>
<p><span class="math display">$$
\begin{split}
    P(c\mid x)\propto P(c)\prod_{i=1}^dP(x_i\mid c,\boxed{pa_i})
\end{split}
$$</span></p>
<p>这个其他属性 <span class="math inline"><em>p</em><em>a</em><sub><em>i</em></sub></span> 也叫做 <span class="math inline"><em>x</em><sub><em>i</em></sub></span> 的<strong>父属性</strong>，那么该如何确定父属性呢？</p>
<ul>
<li>SPODE（Super-Parent ODE）：假设所有属性依赖于同一属性<strong>超父</strong>，通过交叉验证等模型选择方法来确定超父属性；</li>
<li>TAN（Tree Augmented naïve Bayes）：以属性间的条件互信息为边的权重，构建完全图利用最大带权生成树算法，进保留相关属性间的依赖性。</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250605142948145.png" width="70%" /></p>
<p><strong>AODE</strong>（Averaged One-Dependent Estimator）尝试将每个属性作为超父来构建SPODE，最后将拥有足够训练数据支撑的 SPODE集成起来作为最终结果：</p>
<p><span class="math display">$$
\begin{split}
    P(c\mid\mathbf{x})\propto\sum_{
\begin{array}
{c}i=1 \quad |D_{x_i}|\geqslant m^{\prime}
\end{array}}^dP(c,x_i)\prod_{j=1}^dP(x_j\mid c,x_i)
\end{split}
$$</span></p>
<p>其中 <span class="math inline"><em>m</em><sup>′</sup></span> 为阈值常数，用于保留拥有足量训练数据的SPODE。同样AODE在计算时也使用了拉普拉斯修正：</p>
<p><span class="math display">$$
\begin{split}
    \hat{P}(c,x_i)=\frac{|D_{c,x_i}|+1}{|D|+N_i},\quad\hat{P}(x_j\mid c,x_i)=\frac{|D_{c,x_i,x_j}|+1}{|D_{c,x_i}|+N_j}
\end{split}
$$</span></p>
<blockquote>
<p>高阶依赖：ODE <span class="math inline">→</span> kDE</p>
<p>明显障碍：随着 k 的增加，估计 <span class="math inline"><em>P</em>(<em>x</em><sub><em>j</em></sub> ∣ <em>c</em>, <em>p</em><em>a</em><sub><em>i</em></sub>)</span> 所需的样本数将以指数级增加。</p>
</blockquote>
<h2 id="贝叶斯网">7.4.贝叶斯网</h2>
<p>了解即可。</p>
<p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250605144759105.png" width="70%" /></p>
<hr />
<h1 id="第八讲集成学习">第八讲、集成学习</h1>
<h2 id="个体与集成">8.1.个体与集成</h2>
<p>集成学习（ensemble learning）通过多学习器来提升性能：</p>
<p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250605145420227.png" width="50%" /></p>
<p>关键：<strong>好而不同</strong></p>
<p>假设基分类器的错误率相互独立，为 <span class="math inline"><em>P</em>(<em>h</em><sub><em>i</em></sub>(<strong>x</strong>)≠<em>f</em>(<strong>x</strong>)) = <em>ϵ</em></span> ，则由Hoeffding不等式可得，如果我们从一个有限范围内独立采样了 n个样本，那么样本均值偏离期望值超过 <span class="math inline"><em>ϵ</em></span> 的概率会以指数速度下降（关于样本数量n）。即在一定条件下，随着集成分类器数目的增加，集成的错误率将指数级下降，最终趋向于0。</p>
<p>但是这里有一个关键假设，就是基学习器的误差相互独立，然而在现实生活中个体学习器来自同一个问题，显然不可能完全独立。因此如何产生好而不同的个体学习器是集成学习研究的核心。</p>
<p>集成学习大致可分为两大类：串行 VS 并行。</p>
<h2 id="boosting">8.2.Boosting</h2>
<p>特征：</p>
<ul>
<li>每次调整训练数据的样本分布</li>
<li>串行生成</li>
<li>个体学习器间存在强依赖关系</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250605152544770.png" width="70%" /></p>
<p>其中Boosting算法中最重要的是<strong>AdaBoost</strong>算法：</p>
<p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250605153817270.png" width="70%" /></p>
<p>总的来说，其流程可以总结为：</p>
<ul>
<li>初始化训练数据的权值分布 <span class="math inline">$D_1=\frac{1}{m}$</span> ，其中m代表有m个样本数据；</li>
<li>训练弱分类器 <span class="math inline"><em>h</em><sub><em>i</em></sub></span>，如果某个训练样本被弱分类器准确分类，则在构造下一个训练集时其对应的权值会减小；相反，如果某个训练样本被错误分类，其权值则会增大，相当于在下一次训练时加大对其的注意力。最后整个训练过程迭代 <span class="math inline"><em>T</em></span> 次，最终有 <span class="math inline"><em>T</em></span> 个弱分类器；</li>
<li>最终各个弱分类器被组合成一个强分类器，其中分类误差小的分类器其权值被加大，使其起到更大的决定性作用。</li>
</ul>
<h2 id="bagging与随机森林">8.3.Bagging与随机森林</h2>
<h3 id="bagging">Bagging</h3>
<p>Bagging算法首先随机采样原数据集 <span class="math inline"><em>D</em></span> 得到不同分布的数据集 <span class="math inline"><em>D</em><sub>1</sub>, <em>D</em><sub>2</sub>, ..., <em>D</em><sub><em>T</em></sub></span> 用于给不同的基学习器学习（由于不存在依赖关系，这一步可以并行执行），最终得到 <span class="math inline"><em>T</em></span> 个弱学习器，使用<strong>投票法</strong>来结合弱学习器得到最终的强学习器。</p>
<p>特征：</p>
<ul>
<li>个体学习器不存在强依赖关系</li>
<li>并行化生成</li>
<li>自助采样法</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250605162320683.png" width="50%" /></p>
<p>可进一步使用包外估计，即仅考虑那些未使用样本<span class="math inline"><em>x</em></span>训练的基学习器在<span class="math inline"><em>x</em></span>上的预测：</p>
<p><span class="math display">$$
\begin{split}
    H^{oob}(\mathbf{x})=\arg\max_{y\in\mathcal{Y}}\sum_{t=1}^T\mathbb{I}(h_t(\mathbf{x})=y)\cdot\mathbb{I}(\mathbf{x}\notin D_t)
\end{split}
$$</span></p>
<h3 id="随机森林">随机森林</h3>
<p>随机森林（Random Forest）是一个包含多个<strong>决策树</strong>的分类器，其输出类别由个别树输出的类别的<strong>众数</strong>而定。</p>
<p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250605163249.png" width="80%" /></p>
<p>构造一个随机决策树的过程：</p>
<p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250605163737563.png" width="50%" /></p>
<h2 id="结合策略">8.4.结合策略</h2>
<ul>
<li>平均法/加权平均法：最基本的方法</li>
<li>投票法：
<ul>
<li>绝对多数投票法：一个分类结果占据半数优势时才会选择它；</li>
<li>相对多数投票法：选择票数最多的结果；</li>
<li>加权投票法：相对多数的加权版本。</li>
</ul></li>
<li>Stacking学习法</li>
</ul>
<h2 id="多样性">8.5.多样性</h2>
<p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250605165432245.png" width="50%" /></p>
<p>基学习器学得越准确以及越多样性（好而不同），集成学习越好。</p>
<p>常见的增强个体学习器多样性的方法：</p>
<ol type="1">
<li><strong>数据样本扰动</strong>：通常基于采样法的不同，比如Bagging的自助采样、AdaBoost的序列采样</li>
</ol>
<blockquote>
<p>数据样本扰动对“不稳定基学习器”很有效：</p>
<p><strong>不稳定基学习器</strong>包括：决策树、神经网络等；</p>
<p><strong>稳定基学习器</strong>包括：线性学习器、支持向量机、朴素贝叶斯、k近邻等。</p>
</blockquote>
<ol start="2" type="1">
<li><strong>输入属性扰动</strong>：每次随机选择一部分输入数据维度进行学习</li>
</ol>
<p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250605171633933.png" width="50%" /></p>
<ol start="3" type="1">
<li><p><strong>输出属性扰动</strong>：输出表示扰动</p>
<ul>
<li>翻转法：随机改变输入样本的标记</li>
<li>输出调剂法：分类输出改为回归输出得到分类器</li>
<li>ECOC法：多类任务分解为一系列两类任务来求解</li>
</ul></li>
<li><p><strong>算法参数扰动</strong>：采用负相关法，强制要求个体神经网络采用不同的参数</p></li>
</ol>
<hr />
<h1 id="第九讲聚类算法">第九讲、聚类算法</h1>
<div class="note primary no-icon flat"><p><strong>考点</strong>：</p>
<p>（<em>2018</em>）三种算法的举例；</p>
<p>（<em>2018</em>）距离度量的四个性质并证明；</p>
<p>（<em>2022</em>）K-means过程，k值如何选；</p>
<p>（<em>2023</em>）密度聚类和层次聚类的代表算法、关键假设。</p>
</div>
<h2 id="聚类定义">9.1.聚类定义</h2>
<p>目标：将数据样本划分为若干个通常不相交的簇（cluster）</p>
<p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250605172510980.png" width="70%" /></p>
<h2 id="性能度量-1">9.2.性能度量</h2>
<p>聚类性能度量，也称为聚类有效性指标，需要做到<strong>簇内相似度</strong>（intra-cluster similarity）<strong>高且簇间相似度</strong>（inter-cluster similarity）<strong>低</strong>。</p>
<ul>
<li>外部指标：将聚类结果与某个<em>参考模型</em>进行比较</li>
</ul>
<blockquote>
<p><strong>补充</strong>：</p>
<p>定义：TP–同属一个类并被聚在一起的样本对数、FP–不同类但被聚在一起的样本对数、FN–同类但被分在不同簇、TN–不同类且被分开</p>
<p>1.<strong>Jaccard系数</strong>：衡量两个集合的相似度</p>
<p><span class="math display">$$
\begin{split}
  J=\frac{TP}{TP+FP+FN}
\end{split}
$$</span></p>
<p>2.<strong>FM 指数</strong>：P和R的几何平均</p>
<p><span class="math display">$$
\begin{split}
  \mathrm{FM}=\sqrt{\frac{TP}{TP+FP}\cdot\frac{TP}{TP+FN}}
\end{split}
$$</span></p>
<p>3.<strong>Rand指数</strong>：表示聚类结果中有多少比例的样本对被正确分类</p>
<p><span class="math display">$$
\begin{split}
  \mathrm{Rand}=\frac{TP+TN}{TP+TN+FP+FN}
\end{split}
$$</span></p>
</blockquote>
<ul>
<li>内部指标：直接考察聚类结果，无参考模型</li>
</ul>
<blockquote>
<p><strong>补充</strong>：</p>
<p>1.<strong>DB指数</strong>：衡量簇内部和外部距离的指标</p>
<p><span class="math display">$$
\begin{split}
  \mathrm{DB}=\frac{1}{k}\sum_{i=1}^k\max_{j\neq i}\left(\frac{s_i+s_j}{d_{ij}}\right)
\end{split}
$$</span></p>
<p>其中 <span class="math inline"><em>s</em><sub><em>i</em></sub></span> 表示第 <span class="math inline"><em>i</em></span> 个簇的内部平均距离，<span class="math inline"><em>d</em><sub><em>i</em><em>j</em></sub></span> 表示第 <span class="math inline"><em>i</em></span> 和 <span class="math inline"><em>j</em></span> 簇之间的中心距离。该指标越小越好，表示簇内部越紧凑、簇间分离越好。</p>
<p>2.<strong>Dunn指数</strong>：衡量簇内部和外部距离的指标</p>
<p><span class="math display">$$
\begin{split}
  \mathrm{Dunn}=\frac{\min_{i\neq j}d(C_i,C_j)}{\max_k\delta(C_k)}
\end{split}
$$</span></p>
<p>分子表示不同簇之间的最小距离，分母则指代单个簇内部的最大直径，越大越好</p>
</blockquote>
<h2 id="距离计算">9.3.距离计算</h2>
<p>距离度量的意义：聚类来自于分组，分组来自于合理度量，度量来自于距离，因此距离对聚类有很本质的作用。</p>
<p>需满足的基本性质：</p>
<ul>
<li><strong>非负性</strong>：<span class="math inline">dist (<strong>x</strong><sub><em>i</em></sub>, <strong>x</strong><sub><em>j</em></sub>) ≥ 0</span> ；</li>
<li><strong>同一性</strong>：<span class="math inline">dist (<strong>x</strong><sub><em>i</em></sub>, <strong>x</strong><sub><em>j</em></sub>) = 0 当且仅当 <strong>x</strong><sub><em>i</em></sub> = <strong>x</strong><sub><em>j</em></sub></span> ；</li>
<li><strong>对称性</strong>：<span class="math inline">dist (<strong>x</strong><sub><em>i</em></sub>, <strong>x</strong><sub><em>j</em></sub>) = dist (<strong>x</strong><sub><em>j</em></sub>, <strong>x</strong><sub><em>i</em></sub>)</span> ；</li>
<li><strong>直递性</strong>：<span class="math inline">dist (<strong>x</strong><sub><em>i</em></sub>, <strong>x</strong><sub><em>j</em></sub>) &lt; dist (<strong>x</strong><sub><em>i</em></sub>, <strong>x</strong><sub><em>k</em></sub>) + dist (<strong>x</strong><sub><em>k</em></sub>, <strong>x</strong><sub><em>j</em></sub>)</span>.</li>
</ul>
<p>常用的距离形式：<strong>闵可夫斯基距离</strong>（Minkowski Distance）</p>
<p><span class="math display">$$
\begin{split}
    \mathrm{dist}_{\mathrm{mk}}(\boldsymbol{x}_{i},\boldsymbol{x}_{j})=\left(\sum_{u=1}^{n}\left|x_{iu}-x_{ju}\right|^{p}\right)^{\frac{1}{p}}
\end{split}
$$</span></p>
<blockquote>
<p>当 <span class="math inline"><em>p</em> = 2</span> 时，又称为欧氏距离（Euclidean distance）；</p>
<p>当 <span class="math inline"><em>p</em> = 1</span> 时，又称为曼哈顿距离（Manhattan distance）。</p>
</blockquote>
<p>对于无序（non-ordinal）属性，可使用VDM（Value Difference Metric）：</p>
<p><span class="math display">$$
\begin{split}
    VDM_p(a,b)=\sum_{i=1}^k\left|\frac{m_{u,a,i}}{m_{u,a}}-\frac{m_{u,b,i}}{m_{u,b}}\right|^p
\end{split}
$$</span></p>
<p>上式表示属性 <span class="math inline"><em>u</em></span> 上两个离散值 <span class="math inline"><em>a</em></span> 和 <span class="math inline"><em>b</em></span> 之间的VDM距离，其中 <span class="math inline"><em>k</em></span> 为样本簇数，<span class="math inline"><em>m</em><sub><em>u</em>, <em>a</em></sub></span>表示属性<span class="math inline"><em>u</em></span>上取值为<span class="math inline"><em>a</em></span>的样本数，<span class="math inline"><em>m</em><sub><em>u</em>, <em>a</em>, <em>i</em></sub></span>表示第<span class="math inline"><em>i</em></span>个样本簇中在属性<span class="math inline"><em>u</em></span>上取值为<span class="math inline"><em>a</em></span>的样本数。</p>
<p>对于混合属性，可使用MinkovDM：</p>
<p><span class="math display">$$
\begin{split}
    \mathrm{MinkovDM}_p(\boldsymbol{x}_i,\boldsymbol{x}_j)=\left(\sum_{u=1}^{n_c}|x_{iu}-x_{ju}|^p+\sum_{u=n_c+1}^n\mathrm{VDM}_p(x_{iu},x_{ju})\right)^{\frac{1}{p}}
\end{split}
$$</span></p>
<h2 id="聚类算法">9.4.聚类算法</h2>
<p>常见聚类方法可分为以下几类：</p>
<ul>
<li><strong>原型聚类</strong>：有簇中心的聚类方法。先对原型初始化，然后对原型进行迭代更新求解。<em>代表算法：K-means、LVQ、高斯混合</em>；</li>
<li><strong>密度聚类</strong>：划分为多个等价类，未必有簇中心。从样本密度的角度来考察样本之间的可连接性，并基于可连接样本不断扩展聚类簇。<em>代表算法：DBSCAN、OPTICS、DENCLUE</em>；</li>
<li><strong>层次聚类</strong>：在不同层次对数据集进行划分，从而形成树形聚类结构。<em>代表算法：AGNES、DIANA</em>。</li>
</ul>
<h3 id="原型聚类">原型聚类</h3>
<ol type="1">
<li><p><strong>K-means（K均值聚类）</strong>：每个簇中心以该簇中所有样本点的均值表示</p>
<ul>
<li>Step1: 随机选取 k 个样本点作为簇中心；</li>
<li>Step2: 将其他样本点根据其与簇中心的距离，划分给最近的簇；</li>
<li>Step3: 更新各簇的均值向量，将其作为新的簇中心；</li>
<li>Step4: 若所有簇中心未发生改变，则停止；否则执行 Step 2。</li>
</ul></li>
</ol>
<p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250605195805353.png" width="70%" /></p>
<ol start="2" type="1">
<li><strong>学习向量量化（LVQ）</strong>：试图找到一组原型向量来刻画聚类结构，但数据样本带有类别标记，通过聚类来形成类别的子类结构，每个聚类对应于类别的一个子类</li>
</ol>
<p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250605204347631.png" width="50%" /></p>
<ol start="3" type="1">
<li><strong>高斯混合聚类（Gaussian Mixture Clustering, GMM）</strong>：采用高斯概率分布来表达聚类模型，簇中心=均值，簇半径=方差</li>
</ol>
<blockquote>
<p>n维样本空间中的随机向量x若服从高斯分布，则其概率密度函数为：</p>
<p><span class="math display">$$
\begin{split}
    p(\boldsymbol{x})=\frac{1}{(2\pi)^{\frac{n}{2}}|\boldsymbol{\Sigma}|^{\frac{1}{2}}}e^{-\frac{1}{2}(\boldsymbol{x}-\boldsymbol{\mu})^{\mathrm{T}}\boldsymbol{\Sigma}^{-1}(\boldsymbol{x}-\boldsymbol{\mu})}
\end{split}
$$</span></p>
</blockquote>
<p>假设样本由下面这个高斯混合分布生成：</p>
<p><span class="math display">$$
\begin{split}
    p_{\mathcal{M}}(\boldsymbol{x})=\sum_{i=1}^k\alpha_i\cdotp(\boldsymbol{x}\mid\boldsymbol{\mu}_i,\boldsymbol{\Sigma}_i)
\end{split}
$$</span></p>
<p>其中可以根据 <span class="math inline"><em>α</em><sub>1</sub>, <em>α</em><sub>2</sub>, ..., <em>α</em><sub><em>k</em></sub></span> 定义的先验分布选择高斯混合成分，<span class="math inline"><em>α</em><sub><em>i</em></sub></span> 即为选择第 <span class="math inline"><em>i</em></span> 个混合成分的概率。之后根据被选择的混合成分的概率密度函数进行采样，从而生成相应的样本。</p>
<p>由此可以得到，样本 <span class="math inline"><strong>x</strong><sub><strong>j</strong></sub></span> 由第 <span class="math inline"><em>i</em></span> 个高斯混合成分生成的后验概率为：</p>
<p><span class="math display">$$
\begin{split}
    \gamma_{ji} = p_{\mathcal{M}}(z_{j}=i\mid\boldsymbol{x}_{j})=\frac{P(z_{j}=i)\cdot p_{\mathcal{M}}(\boldsymbol{x}_{j}\mid z_{j}=i)}{p_{\mathcal{M}}(\boldsymbol{x}_{j})}=\frac{\alpha_{i}\cdotp(\boldsymbol{x}_{j}\mid\boldsymbol{\mu}_{i},\boldsymbol{\Sigma}_{i})}{\sum_{l=1}^{k}\alpha_{l}\cdotp(\boldsymbol{x}_{j}\mid\boldsymbol{\mu}_{l},\boldsymbol{\Sigma}_{l})}
\end{split}
$$</span></p>
<p>求解时同样使用极大似然估计来进行参数估计：</p>
<p><span class="math display">$$
\begin{split}
    LL(D)=\ln\left(\prod_{j=1}^mp_\mathcal{M}(x_j)\right)=\sum_{j=1}^m\ln\left(\sum_{i=1}^k\alpha_i\cdotp(\boldsymbol{x}_j\mid\boldsymbol{\mu}_i,\boldsymbol{\Sigma}_i)\right)
\end{split}
$$</span></p>
<p>参数估计完毕后，使用<strong>EM算法</strong>类似K-means求解聚类。</p>
<blockquote>
<p><strong>补充：EM算法</strong></p>
<p>EM算法（Expectation-Maximization Algorithm）是一种迭代优化算法，用于含有隐变量的模型中，常用于估计最大似然参数（Maximum Likelihood Estimation, MLE）或最大后验概率（MAP）。</p>
<p><strong>核心思想</strong>：EM 算法在估计过程中交替执行两步</p>
<p>1.<em>E 步（期望步）</em>：在当前参数下，估计隐变量的分布；</p>
<p>2.<em>M 步（最大化步）</em>：在估计出的隐变量分布下，最大化对数似然函数，更新参数。</p>
<p>在这里求解GMM时的场景则是：</p>
<p><strong>E步：</strong>根据当前的参数计算每个样本属于每个高斯成分的后验概率 <span class="math inline"><em>γ</em><sub><em>j</em><em>i</em></sub></span>；（隐变量）</p>
<p><strong>M步：</strong>更新模型参数 <span class="math inline">{(<em>α</em><sub><em>i</em></sub>, <strong>μ</strong><sub><em>i</em></sub>, <strong>Σ</strong><sub><em>i</em></sub>) ∣ 1 ≤ <em>i</em> ≤ <em>k</em>}</span>。（更新参数）</p>
</blockquote>
<h3 id="密度聚类">密度聚类</h3>
<p><strong>DBSCAN</strong>（Density-Based Spatial Clustering of Applications with Noise）是一种基于密度的聚类算法，非常适合处理具有任意形状的簇、噪声点和离群点的数据集。其基本假设是：<strong>簇是由足够密集的点组成的区域</strong>，它通过考察每个点在其邻域中的“密度”来决定是否属于某个簇。</p>
<p><strong>关键定义</strong>：</p>
<ul>
<li><span class="math inline"><em>ϵ</em></span>-邻域（Epsilon Neighborhood）：以点<span class="math inline"><em>p</em></span>为中心、半径为<span class="math inline"><em>ϵ</em></span>的圆形区域；</li>
<li>MinPts（最小点数）：构成一个密集区域所需的最小点数</li>
</ul>
<table>
<thead>
<tr class="header">
<th style="text-align: left;"><strong>类型</strong></th>
<th style="text-align: left;"><strong>定义</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">核心点（core point）</td>
<td style="text-align: left;">自己的<span class="math inline"><em>ϵ</em></span>-邻域内至少包含MinPts个点（包括自己）</td>
</tr>
<tr class="even">
<td style="text-align: left;">边界点（border point）</td>
<td style="text-align: left;">自己不是核心点，但是在某个核心点的<span class="math inline"><em>ϵ</em></span>-邻域内</td>
</tr>
<tr class="odd">
<td style="text-align: left;">噪声点（noise point）</td>
<td style="text-align: left;">既不是核心点，也不是边界点</td>
</tr>
</tbody>
</table>
<p><strong>算法流程</strong>：</p>
<ol type="1">
<li><p>从数据集中任取一个未访问的点 p：</p>
<ul>
<li>如果 p 是核心点，以 p 为起点，扩展一个簇；</li>
<li>如果 p 是边界点或噪声点，则忽略；</li>
</ul></li>
<li><p>对所有点重复此过程，直到所有点都被访问；</p></li>
<li><p>最终将所有密度相连的点归为一类，噪声点单独标记为 -1。</p></li>
</ol>
<p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250605220026372.png" width="60%" /></p>
<p>如上图，虚线表示<span class="math inline"><em>ϵ</em></span>-邻域，MinPts为3，首先我们可以判断 <span class="math inline"><em>x</em><sub>1</sub></span> 是核心点，之后通过迭代过程判断所有点之间的密度相连关系即可构建出当前MinPts对应的聚类结果。</p>
<blockquote>
<p><strong>优缺点</strong>：</p>
<p>优点：能识别任意形状的簇（环形、月牙形）；能识别噪声点（离群点）；不需要向K-means一样指定簇数。</p>
<p>缺点：对参数敏感（<span class="math inline"><em>ϵ</em></span>、MinPts）；需要满足密度假设；如果不同簇的密度有较大差异，表现会很差。</p>
</blockquote>
<h3 id="层次聚类">层次聚类</h3>
<p><strong>AGNES</strong>(AGglomerative NESting)：自底向上，从最细的粒度开始（单个样本），逐渐合并相似的簇，直到最粗的簇（一个簇）。关键假设：能够产生不同粒度的聚类结果。</p>
<p>算法流程：</p>
<ul>
<li>Step1: 将每个样本点作为一个簇；</li>
<li>Step2: 合并最近的两个簇；</li>
<li>Step3: 若所有样本点都存在于一个簇中，则停止；否则转到 Step2。</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250605220904037.png" width="60%" /></p>
<hr />
<h1 id="第十讲降维与度量学习">第十讲、降维与度量学习</h1>
<div class="note primary no-icon flat"><p><strong>考点</strong>：</p>
<p>（<em>2018，2019，2021</em>）PCA的最近重构性和最大可分性；</p>
<p>（<em>2019</em>）马氏距离表达式；</p>
<p>（<em>2022</em>）维度灾难是什么，ISOMAP、LLE；</p>
<p>（<em>2019</em>）特征脸是什么。</p>
</div>
<h2 id="k-近邻学习">10.1.K-近邻学习</h2>
<p>基本思路：近朱者赤，近墨者黑。使用投票法/平均法进行确定样本的类别，<strong>懒惰学习</strong>的代表。</p>
<blockquote>
<p><strong>懒惰学习</strong>：事先没有分类器，输入测试样本才开始准备分类器</p>
</blockquote>
<p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250606125620540.png" width="60%" /></p>
<p>由上述例子可以看出，K-近邻的关键在于<strong>k值的选取</strong>和<strong>距离的计算</strong>。</p>
<p>给定测试样本<span class="math inline"><em>x</em></span>，若其最近邻样本为<span class="math inline"><em>z</em></span>，则最近邻分类器出错的概率为（<span class="math inline"><em>x</em></span>和<span class="math inline"><em>z</em></span>类别标记不同）：</p>
<p><span class="math display">$$
\begin{aligned}
P(err) &amp; =1-\sum_{c\in\mathcal{Y}}P(c\mid\boldsymbol{x})P(c\mid\boldsymbol{z}) \\
 &amp; \simeq1-\sum_{c\in\mathcal{Y}}P^2(c\mid\boldsymbol{x}) \\
 &amp; \leq1-P^2(c^*\mid\boldsymbol{x}) \\
 &amp; =
\begin{pmatrix}
1+P\left(c^{*}\mid\boldsymbol{x}\right)
\end{pmatrix}
\begin{pmatrix}
1-P\left(c^{*}\mid\boldsymbol{x}\right)
\end{pmatrix} \\
 &amp; \leq2\times
\begin{pmatrix}
1-P(c^{*}\mid\boldsymbol{x})
\end{pmatrix}.
\end{aligned}
$$</span></p>
<p><em>最近邻分类器的泛化错误率不会超过贝叶斯最优分类器错误率的两倍！</em></p>
<p>K-近邻的问题：<strong>真实问题中很难准确地找到k-近邻</strong>。</p>
<p>密采样的假设：样本的每个 <span class="math inline"><em>ϵ</em></span>-邻域内都有近邻。</p>
<blockquote>
<p>考虑一个20维的例子，若近邻的距离阈值设为<span class="math inline">10<sup> − 3</sup></span>。单纯考虑一个维度下的k-近邻，那么在这一维的单位空间至少要<span class="math inline">10<sup>3</sup></span>个样本才能满足密采样的假设。当考虑20个维度时，此时单位空间至少需要<span class="math inline">10<sup>3 × 20</sup></span>个样本才能满足密采样条件。</p>
<p>若是一张<span class="math inline">300 × 233</span>的彩色图像，将其按 <span class="math inline"><em>h</em> × <em>w</em> × <em>c</em></span> 的顺序展平，最终得到 <span class="math inline">300 × 233 × 3</span> 的向量，该维度有209700，则更加灾难。</p>
</blockquote>
<p><strong>维度灾难</strong>：高维空间给距离计算带来很大的麻烦。更严重的是，当样本变得稀疏时，k近邻会不准。</p>
<p>数据样本虽是高维的，但与学习任务密切相关的也许仅是某个低维空间，即高维空间中的一个低维嵌入（embedding）。</p>
<h2 id="低维嵌入">10.2.低维嵌入</h2>
<p><strong>MDS</strong>（Multidimensional Scaling，多维尺度分析）是一种降维方法，它的主要目标是：寻找一个低维子空间，使得距离和样本原有距离近似不变。</p>
<p>核心思路：寻找低维子空间尽量保持样本内积不变。已知样本的距离矩阵，使用特征值分解来求解内积矩阵（内积保距）。</p>
<p><strong>算法过程</strong>：</p>
<ol type="1">
<li>从距离矩阵 <span class="math inline"><em>D</em></span> 构造内积矩阵 <span class="math inline"><em>B</em> = <em>X</em><em>X</em><sup>⊤</sup></span>：</li>
</ol>
<p>给定距离矩阵 <span class="math inline"><em>D</em> = [<em>d</em><sub><em>i</em><em>j</em></sub>]</span>，可以先构造平方距离矩阵：</p>
<p><span class="math display">$$
\begin{split}
    D^{(2)} = \|d_{ij}\|^2
\end{split}
$$</span></p>
<p>然后构造<strong>中心化矩阵</strong>：</p>
<p><span class="math display">$$
\begin{split}
    H = I - \frac{1}{n}\mathbf{1}\mathbf{1}^\top
\end{split}
$$</span></p>
<p>其中 <span class="math inline"><em>I</em></span> 是 <span class="math inline"><em>n</em> × <em>n</em></span> 单位矩阵，<span class="math inline"><strong>1</strong></span> 是全1列向量。用它对平方距离矩阵进行双中心化，得到<strong>内积矩阵</strong>：</p>
<p><span class="math display">$$
\begin{split}
    B = - \frac{1}{2}HD^{(2)}H
\end{split}
$$</span></p>
<blockquote>
<p><strong><em>Q：为什么需要中心化？</em></strong></p>
<p>你只有成对的距离信息，但没有坐标。为了推导坐标，就必须：</p>
<p>假设这些点的<strong>整体几何中心是原点</strong>（即数据集中心化）， 否则你推导出的内积会带有“偏移量”误差。</p>
<p><strong><em>Q：而中心化矩阵是如何推导的？</em></strong></p>
<p>对欧几里得距离有：</p>
<p><span class="math display">$$
\begin{split}
  d_{ij}^2=\|x_i-x_j\|^2=x_i^\top x_i-2x_i^\top x_j+x_j^\top x_j
\end{split}
$$</span> <span class="math display">$$
\begin{split}
  B=XX^\top\Rightarrow B_{ij}=x_i^\top x_j
\end{split}
$$</span></p>
<p>设矩阵的对角线元素为 <span class="math inline"><em>b</em><sub><em>i</em><em>i</em></sub> = <em>x</em><sub><em>i</em></sub><sup>⊤</sup><em>x</em><sub><em>i</em></sub></span>，则有：</p>
<p><span class="math display">$$
\begin{split}
  d_{ij}^2=b_{ii}+b_{jj}-2b_{ij}
\end{split}
$$</span></p>
<p>由公式：</p>
<p><span class="math display">$$
\begin{split}
  b_{ij}=-\frac{1}{2}\left(d_{ij}^2-\bar{d}_{i\cdot}^2-\bar{d}_{\cdot j}^2+\bar{d}_{\cdot\cdot}^2\right)
\end{split}
$$</span></p>
<p>这个转换其实就等价于双重中心化操作：</p>
<p><span class="math display">$$
\begin{split}
  B = - \frac{1}{2}HD^{(2)}H
\end{split}
$$</span></p>
</blockquote>
<ol start="2" type="1">
<li>对 <span class="math inline"><em>B</em></span> 作特征值分解：</li>
</ol>
<p><span class="math display">$$
\begin{split}
    B=V\Lambda V^\top
\end{split}
$$</span></p>
<ol start="3" type="1">
<li>选前 k 个最大的特征值和对应的特征向量，计算坐标：</li>
</ol>
<p><span class="math display">$$
\begin{split}
    X=V_k\Lambda_k^{1/2}
\end{split}
$$</span></p>
<p>此时 <span class="math inline"><em>X</em> ∈ ℝ<sup><em>n</em> × <em>k</em></sup></span> 就是降维后每个样本的k-维坐标。</p>
<h2 id="流形学习">10.3.流形学习</h2>
<h3 id="isomap">ISOMAP</h3>
<p><strong>ISOMAP</strong>其实是对 MDS 的非线性扩展版本，适用于流形学习问题。它的目标是：<strong>保留数据在流形上的“测地线距离（Geodesic Distance）”结构，而不是原始空间中的欧几里得距离。</strong>举个不太恰当的例子，比如一张弯曲的纸上的两个点，之前的 MDS 相当于计算的是三维空间中的距离（三维欧氏距离），而ISOMAP考虑的是流形上的路径距离（两个点在纸面上的距离，测地线距离）。</p>
<p>而在算法设计上，ISOMAP使用最短路径算法来确定任意两点的测地线距离：</p>
<ol type="1">
<li><strong>构建邻接图</strong>：对每个点连接它的k-近邻或<span class="math inline"><em>ϵ</em></span>邻域，构建无向图 <span class="math inline"><em>G</em></span> ，边权值为欧氏距离；</li>
<li><strong>使用Dijkstra或Floyd求解任意两点的最短距离</strong>：最终得到测地线距离矩阵 <span class="math inline"><em>D</em><sup><em>g</em><em>e</em><em>o</em></sup></span>；</li>
<li><strong>使用测地线距离执行MDS</strong>：相当于将原距离矩阵替换为测地线距离，其余步骤不变。</li>
</ol>
<h3 id="lle">LLE</h3>
<p><strong>LLE</strong>（Locally Linear Embedding）同样用于非线性降维，该算法假设高维空间中的数据样本在一个低维流形上，且在局部邻域内近似线性。因此我们可以在每个点的邻域中用线性组合重建该点，然后在低维空间中找到映射，使这些“重建关系”得以保留。</p>
<p>算法步骤：</p>
<ol type="1">
<li><p><strong>找到每个点的k近邻</strong>：<span class="math inline">{<em>x</em><sub><em>i</em><sub>1</sub></sub>, <em>x</em><sub><em>i</em><sub>2</sub></sub>, ..., <em>x</em><sub><em>i</em><sub><em>k</em></sub></sub>}</span>；</p></li>
<li><p><strong>学习局部重构权重</strong>：</p>
<p>对每个点 <span class="math inline"><em>x</em><sub><em>i</em></sub></span>，求一组权重<span class="math inline"><em>w</em><sub><em>i</em><em>j</em></sub></span>，使其满足： <span class="math display">$$
 \begin{split}
     \boldsymbol{x}_i\approx\sum_{j\in\mathcal{N}(i)}w_{ij}\boldsymbol{x}_j
 \end{split}
 $$</span></p>
<p>得到<strong>优化目标</strong>： <span class="math display">$$
 \begin{split}
     \min_{w_{ij}}\sum_i\left\|\boldsymbol{x}_i-\sum_{j\in\mathcal{N}(i)}w_{ij}\boldsymbol{x}_j\right\|^2
 \end{split}
 $$</span></p>
<p>同时又约束： <span class="math display">$$
 \begin{split}
     \sum_{j\in\mathcal{N}(i)}w_{ij}=1
 \end{split}
 $$</span></p>
<p>该过程对每个<span class="math inline"><em>x</em><sub><em>i</em></sub></span>独立进行，每个<span class="math inline"><em>x</em><sub><em>i</em></sub></span>有自己的一组权重。</p></li>
<li><p><strong>在低维空间中保留重构权重</strong>：</p>
<p>目标是寻找低维表示<span class="math inline"><em>y</em><sub><em>i</em></sub> ∈ ℝ<sup><em>b</em></sup></span>，使得同样的权重<span class="math inline"><em>w</em><sub><em>i</em><em>j</em></sub></span>仍能重构它： <span class="math display">$$
 \begin{split}
     \min_{\boldsymbol{y}_1,\ldots,\boldsymbol{y}_n}\sum_i\left\|\boldsymbol{y}_i-\sum_{j\in\mathcal{N}(i)}w_{ij}\boldsymbol{y}_j\right\|^2
 \end{split}
 $$</span></p>
<p>这个优化问题通过将其写成矩阵形式，然后通过特征值分解求解，这里就不具体展开了。</p></li>
</ol>
<p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250609205841865.png" width="40%" /></p>
<h2 id="度量学习">10.4.度量学习</h2>
<p>之前介绍的距离度量有以下几种：</p>
<ul>
<li>欧氏距离</li>
<li>曼哈顿距离</li>
<li>测地距离</li>
</ul>
<p>之前说过距离度量对于降维很重要，不同的任务需要选择不同的距离度量，能否直接学习一个合适的距离度量呢？这里使用<strong>马氏距离（Mahalanobis Distance）</strong>来参数化学习距离度量：</p>
<p><span class="math display">$$
\begin{split}
    \mathrm{dist}_{\mathrm{mah}}^2(x_i,x_j)=(x_i-x_j)^\mathrm{T}\mathrm{M}(x_i-x_j)=\|x_i-x_j\|_\mathrm{M}^2
\end{split}
$$</span></p>
<p>其中 <span class="math inline"><em>M</em></span> 称为度量矩阵，它是一个半正定对称矩阵，距离度量学习相当于就是要学习 <span class="math inline"><em>M</em></span>。欧氏距离的一个问题就是–各个方向都同等重要，这里引入度量矩阵则能使降维侧重于某些维度。</p>
<h3 id="距离度量学习nca">距离度量学习–NCA</h3>
<p>NCA（Neighborhood Component Analysis）常用于近邻分类器（KNN），而近邻分类器在进行判别时通常使用多数投票法，这里NCA使用概率投票法（其实本质差不多）。对于任意样本 <span class="math inline"><em>x</em><sub><em>j</em></sub></span>，它对 <span class="math inline"><em>x</em><sub><em>i</em></sub></span> 分类结果影响的概率为：</p>
<p><span class="math display">$$
\begin{split}
    p_{ij}=\frac{\exp\left(-\left\|\boldsymbol{x}_i-\boldsymbol{x}_j\right\|_\mathbf{M}^2\right)}{\sum_l\exp\left(-\left\|\boldsymbol{x}_i-\boldsymbol{x}_l\right\|_\mathbf{M}^2\right)}
\end{split}
$$</span></p>
<p>其实很好理解，对于一个点，离它越近的点越相似，两个点的标签越可能是一样的。那么对于点 <span class="math inline"><em>x</em><sub><em>i</em></sub></span>，其分类正确的概率为：</p>
<p><span class="math display">$$
\begin{split}
    p_i=\sum_{j:y_j=y_i}p_{ij}
\end{split}
$$</span></p>
<p>NCA的目标就是<strong>最大化所有样本的正确分类概率之和</strong>：</p>
<p><span class="math display">$$
\begin{split}
    \mathcal{L}(A)=\sum_ip_i=\sum_i\sum_{j:y_j=y_i}p_{ij}
\end{split}
$$</span></p>
<h3 id="距离度量学习lmnn">距离度量学习–LMNN</h3>
<p><strong>LMNN（Large Margin Nearest Neighbor）</strong> 是另一种监督式度量学习方法，与 NCA 类似，但优化目标和策略不同。LMNN 的核心思想是：学习一个距离度量，使得 K 近邻分类器（KNN）在训练集上分类间隔更大，让同类点靠得近，不同类点被“推远”。</p>
<p>具体来说，其目标函数包含<strong>拉进</strong>和<strong>推远</strong>两项：</p>
<p><span class="math display">$$
\begin{split}
    \mathcal{L}(M)=\sum_{(i,j)\in\mathcal{N}}d_L(x_i,x_j)^2+\mu\sum_{(i,j,l)}\xi_{ijl}
\end{split}
$$</span></p>
<p><span class="math display">$$
\begin{split}
    \xi_{ijl}=
\begin{bmatrix}
1+d_L(x_i,x_j)^2-d_L(x_i,x_l)^2
\end{bmatrix}_+
\end{split}
$$</span></p>
<p>其中第一项中的 <span class="math inline">𝒩</span> 表示所有的<strong>同类K近邻对</strong>（target neighbor），是我们希望拉近的，所以希望距离越小越好； 而第二项中的 <span class="math inline"><em>ξ</em><sub><em>i</em><em>j</em><em>l</em></sub></span> 是<strong>Hinge损失</strong>项，用于惩罚违反<strong>margin规则</strong>的异类点，其中 <span class="math inline"><em>x</em><sub><em>l</em></sub></span> 表示与 <span class="math inline"><em>x</em><sub><em>i</em></sub></span> 标签不同的样本。</p>
<p>那么这里的margin规则是什么意思呢？它表示惩罚那些“离得太近的异类点”，在这里我们希望 <span class="math inline"><em>x</em><sub><em>i</em></sub></span> 与异类点 <span class="math inline"><em>x</em><sub><em>l</em></sub></span> 的距离至少比同类点之间的距离远 1 个单位（margin），如果没有达到这个要求就需要惩罚，所以这里使用Hinge损失。</p>
<p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250606161257888.png" width="90%" /></p>
<h2 id="pca">10.5.PCA</h2>
<p>PCA（Principal Component Analysis，主成分分析）是一种经典的无监督降维方法，其目标是：<strong>在保证尽可能保留原始数据方差信息的前提下，将高维数据投影到低维空间。</strong></p>
<p>正交属性空间中的样本点，如何找到一个超平面使得所有样本都能被恰当表达呢？显然我们希望该超平面具备以下性质：</p>
<ul>
<li><strong>最近重构性</strong>：样本点到这个超平面的距离都足够近；</li>
<li><strong>最大可分性</strong>：样本点在这个超平面上的投影都尽可能分开。</li>
</ul>
<p>那么可以通过以上两种目标进行推导：</p>
<ol type="1">
<li><p>基于最近重构性推导：</p>
<ul>
<li>对样本中心化：<span class="math inline">$\bar{x}=\frac{1}{n}\sum_{i=1}^nx_i,\quad X_{\mathrm{centered}}=X-\bar{x}$</span></li>
<li>假定投影变换后得到的新坐标系为<span class="math inline"><em>w</em><sub><em>i</em></sub></span>， 其中<span class="math inline"><em>w</em><sub><em>i</em></sub></span>是标准正交基向量，即：</li>
</ul></li>
</ol>
<p><span class="math display">$$
\begin{split}
||\boldsymbol{w}_i||_2=1,\boldsymbol{w}_i^\mathrm{T}\boldsymbol{w}_j=0(i\neq j)
\end{split}
$$</span></p>
<ul>
<li>舍弃原坐标系中的部分维度，假设维度降低到<span class="math inline"><em>d</em>′ &lt; <em>d</em></span>，则样本点在低维坐标系中的投影为：</li>
</ul>
<p><span class="math display">$$
\begin{split}
    \boldsymbol{z}_i=(z_{i1};z_{i2};\ldots;z_{id^{\prime}})\quad z_{ij}=\boldsymbol{w}_j^\mathrm{T}\boldsymbol{x}_i
\end{split}
$$</span></p>
<ul>
<li>基于 <span class="math inline"><em>z</em><sub><em>i</em></sub></span> 来重构 <span class="math inline"><em>x</em><sub><em>i</em></sub></span>，可以得到：<span class="math inline">$\hat{\boldsymbol{x}}_i=\sum_{j=1}^{d^{\prime}}z_{ij}\boldsymbol{w}_j$</span>。</li>
<li>由此得到原样本点和基于投影重构的样本点之间的距离为：</li>
</ul>
<p><span class="math display">$$
\begin{aligned}
\sum_{i=1}^m\left\|\sum_{j=1}^{d^{\prime}}z_{ij}\boldsymbol{w}_j-\boldsymbol{x}_i\right\|_2^2 &amp; =\sum_{i=1}^m\boldsymbol{z}_i^\mathrm{T}\boldsymbol{z}_i-2\sum_{i=1}^m\boldsymbol{z}_i^\mathrm{T}\mathbf{W}^\mathrm{T}\boldsymbol{x}_i+\mathrm{const} \\
 &amp; \propto-\mathrm{tr}\left(\mathbf{W}^\mathrm{T}\left(\sum_{i=1}^mx_ix_i^\mathrm{T}\right)\mathbf{W}\right).
\end{aligned}
$$</span></p>
<ul>
<li>由此可得基于最近重构性的优化目标为：</li>
</ul>
<p><span class="math display">$$
\begin{aligned}
 &amp; \min_{\mathbf{W}}\quad-\operatorname{tr}(\mathbf{W}^\mathrm{T}\mathbf{X}\mathbf{X}^\mathrm{T}\mathbf{W}) \\
 &amp; \mathrm{s.t.}\quad\mathbf{W}^\mathrm{T}\mathbf{W}=\mathbf{I}.
\end{aligned}
$$</span></p>
<ol start="2" type="1">
<li>基于最大可分性推导（更好理解）：</li>
</ol>
<p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250606164844991.png" width="50%" /></p>
<ul>
<li>样本点<span class="math inline"><em>x</em><sub><em>i</em></sub></span>在新空间中超平面上的投影是<span class="math inline"><em>W</em><sup>⊤</sup><em>x</em><sub><em>i</em></sub></span>，若所有样本点的投影能尽可能分开，则应该使得投影后样本点的方差最大化<strong>（计算协方差矩阵）</strong>：</li>
</ul>
<p><span class="math display">$$
\begin{split}
    \sum_i\mathbf{W}^\mathrm{T}x_ix_i^\mathrm{T}\mathbf{W}
\end{split}
$$</span></p>
<ul>
<li>由此可以得到基于最大可分性的优化目标为：</li>
</ul>
<p><span class="math display">$$
\begin{split}
    \max_{\mathbf{W}}\quad\mathrm{tr}(\mathbf{W}^\mathrm{T}\mathbf{X}\mathbf{X}^\mathrm{T}\mathbf{W})\mathrm{s.t.}\quad\mathbf{W}^\mathrm{T}\mathbf{W}=\mathbf{I}.
\end{split}
$$</span></p>
<p>可以发现两种思路都是等价的。因此求解思路就是对协方差矩阵做<strong>特征值分解</strong>：</p>
<p><span class="math display">$$
\begin{split}
    C=\frac{1}{n}X^\top X\in\mathbb{R}^{D\times D}
\end{split}
$$</span></p>
<p><span class="math display">$$
\begin{split}
    C=U\Lambda U^\top
\end{split}
$$</span></p>
<p>其中 <span class="math inline"><em>U</em></span> 的列是特征向量，表示主成分方向；<span class="math inline"><em>Λ</em></span> 是对角矩阵，对应的特征值表示方向的方差大小。选取前 <span class="math inline"><em>d</em>′</span> 个最大特征值对应的特征向量，组成<span class="math inline"><em>U</em><sub><em>d</em>′</sub></span>，最后将原始数据投影到这些方向上即可得到降维后的数据：</p>
<p><span class="math display">$$
\begin{split}
    Z=X_{\mathrm{centered}}\cdot U_{d’}
\end{split}
$$</span></p>
<p>PCA是最常用的降维方法，其在不同领域有不同的称谓，例如在人脸识别中该技术称为“特征脸”（eigenface），将前<span class="math inline"><em>d</em>′</span>个特征值对应的特征向量还原为图像可以得到：</p>
<p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250609203020531.png" width="60%" /></p>
<hr />
<h1 id="第十一讲特征选择和稀疏学习">第十一讲、特征选择和稀疏学习</h1>
<div class="note primary no-icon flat"><p><strong>考点</strong>：</p>
<p>（<em>2018，2019，2021，2022，2023</em>）子集搜索和子集评估的三种方法（或者说特征选择的三种方法）；</p>
<p>（<em>2019</em>）L1范数为什么可以得到稀疏解。</p>
</div>
<h2 id="特征选择">11.1.特征选择</h2>
<h3 id="特征">特征</h3>
<p>特征：用于描述物体的属性，分为以下几类：</p>
<ul>
<li>相关特征：对当前学习任务有用的属性；</li>
<li>无关特征：与当前学习任务无关的属性；</li>
<li>冗余特征：其所包含的信息能由其他特征推演出来。</li>
</ul>
<h3 id="特征选择-1">特征选择</h3>
<p>目的：从给定的特征集合中选出任务相关特征的子集，不丢失重要特征。</p>
<ul>
<li>减轻维度灾难：在少量属性上构建模型；</li>
<li>减低学习难度：保留关键信息。</li>
</ul>
<p>可行方法：存在两个关键环节–<strong>子集搜索</strong>和<strong>子集评价</strong>。</p>
<p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250606170706055.png" width="60%" /></p>
<p><strong>子集搜索</strong></p>
<p>用贪心策略选择包含重要信息的特征子集。</p>
<ol type="1">
<li>前向搜索：最优子集初始为空集，特征集合初始时包括所有给定特征；</li>
<li>后向搜索：从完整的特征集合开始，逐渐减少特征；</li>
<li>双向搜索：每一轮逐渐增加相关特征，同时减少无关特征。</li>
</ol>
<p><strong>子集评价</strong></p>
<p>选定的特征子集确定了对数据集的一个划分，样本自身的标签对应着对数据集的真实划分。通过估算这两个划分的差异，就能对特征子集进行评价，与真实划分的差异越小，说明当前特征子集越好。</p>
<p>这里可用信息增益来进行子集评价：特征子集 <span class="math inline"><em>A</em></span> 上的取值将原数据集 <span class="math inline"><em>D</em></span> 分为 <span class="math inline"><em>V</em></span> 份，每一份用 <span class="math inline"><em>D</em><sup><em>v</em></sup></span> 表示，则特征子集 <span class="math inline"><em>A</em></span> 的信息增益为</p>
<p><span class="math display">$$
\begin{split}
    \mathrm{Gain}(A)=\mathrm{Ent}(D)-\sum_{v=1}^V\frac{|D^v|}{|D|}\mathrm{Ent}(D^v)
\end{split}
$$</span></p>
<h4 id="过滤式">过滤式</h4>
<p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250606192533368.png" width="60%" /></p>
<p>先用特征选择过程过滤原始数据，再用过滤后的特征来训练模型，<strong>特征选择过程与后续学习器无关</strong>。</p>
<p><strong>Relief</strong>方法：一个好的特征应当在同类样本之间相似，而在异类样本之间差异大。也就是说，某特征能明显区分一个样本与其“近邻异类”的差异，同时保持“近邻同类”的相似性，那这个特征就是有用的。</p>
<p>相关定义：</p>
<ul>
<li>猜中近邻（near-hit）：<span class="math inline"><em>x</em><sub><em>i</em></sub></span> 的同类样本中的最近邻 <span class="math inline"><em>x</em><sub><em>i</em>, <em>n</em><em>h</em></sub></span>；</li>
<li>猜错近邻（near-miss）：<span class="math inline"><em>x</em><sub><em>i</em></sub></span> 的异类样本中的最近邻 <span class="math inline"><em>x</em><sub><em>i</em>, <em>n</em><em>m</em></sub></span>。</li>
</ul>
<p>算法步骤（以二分类为例）：</p>
<ol type="1">
<li>初始化每个特征的权重为0：</li>
</ol>
<p><span class="math display">$$
\begin{split}
    W_j=0\quad \text{for all } j=1,2,...,d
\end{split}
$$</span></p>
<ol start="2" type="1">
<li><p>对于每个样本 <span class="math inline"><em>x</em><sub><em>i</em></sub></span>：</p>
<ul>
<li>找到该样本的猜中近邻<span class="math inline"><em>H</em><sub><em>i</em></sub></span>和猜错近邻<span class="math inline"><em>M</em><sub><em>i</em></sub></span></li>
<li>对每个特征 <span class="math inline"><em>f</em><sub><em>j</em></sub></span>，更新其权重，其中<span class="math inline"><em>d</em><em>i</em><em>f</em><em>f</em>(<em>a</em>, <em>b</em>) = |<em>a</em> − <em>b</em>|</span>（或其他距离度量），<span class="math inline"><em>x</em><sub><em>i</em></sub><sup><em>j</em></sup></span>表示第<span class="math inline"><em>i</em></span>个样本的第<span class="math inline"><em>j</em></span>个特征值：</li>
</ul></li>
</ol>
<p><span class="math display">$$
\begin{split}
    W_j=0\quad \text{for all } j=1,2,...,d
\end{split}
$$</span></p>
<ol start="3" type="1">
<li>最终得到每个特征的权重（也叫做相关统计量），权重越高说明该特征对分类越有用。</li>
</ol>
<p>Relief方法的时间开销随采样次数以及原始特征数线性增长，运行效率高。</p>
<h4 id="包裹式">包裹式</h4>
<p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250606194820037.png" width="60%" /></p>
<p>包裹式选择把<strong>最终使用的学习器性能</strong>作为特征子集的评价准则。</p>
<p><strong>LVW</strong>（Las Vegas Wrapper）是一种随机化的特征选择方法，属于 wrapper 方法（包装器方法），由 Liu 和 Motoda 在 1998 年提出。随机采样特征子集，使用学习器对其评估准确率，如果新子集更好，就接受它为当前最优解。与 Relief 评分式方法不同，LVW 是基于模型性能来“试验”出优质特征组合的。</p>
<p>算法步骤：重复以下步骤直到cnt达到阈值<span class="math inline"><em>T</em></span></p>
<ol type="1">
<li>随机生成一个特征子集 <span class="math inline"><em>S</em> ⊂ <em>F</em></span>，通常会随机保留一部分特征；</li>
<li>使用分类器L在训练集上评估S的准确率 <span class="math inline"><em>a</em><em>c</em><em>c</em><sub><em>S</em></sub></span>；</li>
<li>若性能更好但子集更小：
<ul>
<li>更新最优子集： <span class="math inline"><em>S</em><sub><em>b</em><em>e</em><em>s</em><em>t</em></sub> ← <em>S</em></span></li>
<li>更新最优准确率： <span class="math inline"><em>a</em><em>c</em><em>c</em><sub><em>b</em><em>e</em><em>s</em><em>t</em></sub> ← <em>a</em><em>c</em><em>c</em><sub><em>S</em></sub></span></li>
<li>计数器归零：cnt = 0</li>
</ul></li>
<li>否则计数器加1： <span class="math inline"><em>c</em><em>n</em><em>t</em> ← <em>c</em><em>n</em><em>t</em> + 1</span></li>
</ol>
<p>从最终学习器性能来看，包裹式特征选择比过滤式特征选择更好，但需多次训练学习器，计算开销通常比过滤式特征选择大得多。</p>
<h4 id="嵌入式">嵌入式</h4>
<p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250606200232624.png" width="60%" /></p>
<p>嵌入式特征选择是将特征选择过程与学习器训练过程融为一体，两者在同一优化过程中完成，<strong>在学习器训练过程中自动地进行特征选择</strong>。</p>
<p>考虑最简单的线性回归–<strong>岭回归</strong>（ridge regression），它在普通的线性回归问题（平方误差损失）上加上了L2正则化项防止过拟合：</p>
<p><span class="math display">$$
\begin{split}
    \operatorname*{min}_{\boldsymbol{w}}\sum_{i=1}^{m}(y_{i}-\boldsymbol{w}^{\top}\boldsymbol{x}_{i})^{2}+\lambda\|\boldsymbol{w}\|_{2}^{2}
\end{split}
$$</span></p>
<p>将L2正则化项替换为L1正则化项，可以得到<strong>LASSO</strong>（Least Absolute Shrinkage and Selection Operator，最小绝对收缩与选择算子）：</p>
<p><span class="math display">$$
\begin{split}
    \operatorname*{min}_{\boldsymbol{w}}\sum_{i=1}^{m}(y_{i}-\boldsymbol{w}^{\top}\boldsymbol{x}_{i})^{2}+\lambda\|\boldsymbol{w}\|_{1}
\end{split}
$$</span></p>
<p>使用L1范式 LASSO 有一个特殊性质，几何上的 L1 范数形成的是一个<strong>菱形约束区域</strong>，它与损失函数的等高线接触时，往往在轴（即某个<span class="math inline"><em>w</em><sub><em>j</em></sub> = 0</span>）上交点，导致某些权重直接为零，从而达到特征选择的目的。</p>
<p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250606200938379.png" width="70%" /></p>
<p>LASSO没有封闭解，常用PGD（Proximal Gradient Descend，近端梯度下降）法求解。</p>
<h2 id="稀疏表示">11.2.稀疏表示</h2>
<p>稀疏表示的优势：文本数据线性可分、存储高效</p>
<p><em>能否将稠密表示的数据集转化为稀疏表示，使其享受上述优势？</em></p>
<p><strong>字典学习</strong>：为普通稠密表达的样本找到合适的字典，将样本转化为稀疏表示，用尽可能少的原子（字典中的“词条”）线性组合来近似表示数据。</p>
<p>给定数据矩阵 <span class="math inline"><strong>X</strong> ∈ ℝ<sup><em>n</em> × <em>d</em></sup></span>，我们需要得到一个字典矩阵 <span class="math inline"><strong>D</strong> ∈ ℝ<sup><em>k</em> × <em>d</em></sup></span>（k为字典的大小），一个稀疏编码矩阵 <span class="math inline"><strong>Z</strong> ∈ ℝ<sup><em>n</em> × <em>k</em></sup></span>，使得：</p>
<p><span class="math display">$$
\begin{split}
    X\approx ZD
\end{split}
$$</span></p>
<p>其中 <span class="math inline"><em>Z</em></span> 中每一行都是稀疏的（多数值为0）。通常其优化形式为：</p>
<p><span class="math display">$$
\begin{split}
    \min_{Z,D}\|X-ZD\|_F^2+\lambda\sum_{i=1}^n\|z_i\|_1
\end{split}
$$</span></p>
<p>其中第一项表示矩阵误差，使用Frobenius范数（也可以用L2范数），第二项表示对编码矩阵非零元素个数的惩罚项。</p>
<p><strong>矩阵补全</strong>：从得到的部分信号, 基于压缩感知的思想恢复出完整信号。其基本目标是，给定一个部分观测的矩阵（很多元素缺失），预测其缺失部分的值。通常假设<strong>矩阵具有低秩（Low-Rank）结构</strong>，即大部分信息可以由少数几个主因子表示。</p>
<p>最基本的优化形式可以表示为：</p>
<p><span class="math display">$$
\begin{aligned}
\min_X\operatorname{rank}(X),\quad\operatorname{s.t.}X_{ij}=M_{ij},\forall(i,j)\in\Omega
\end{aligned}
$$</span></p>
<p>其中<span class="math inline"><em>Ω</em></span>表示观测到的条目集合，优化目标就是恢复出的矩阵中对应元素应该与已观测到的对应元素相同，但是要尽可能保证矩阵的秩越小越好。但这是一个NP难问题，因此使用<strong>核范数</strong>（nuclear norm，即矩阵的奇异值之和）代替秩来进行凸优化：</p>
<p><span class="math display">$$
\begin{aligned}
\min_X\|X\|_*\quad\mathrm{s.t.}X_{ij}=M_{ij},\forall(i,j)\in\Omega
\end{aligned}
$$</span></p>
<p>通常使用半正定规划（SDP，Semi-Definite Programming）求解。</p>
<hr />
<h1 id="第十二讲半监督学习">第十二讲、半监督学习</h1>
<div class="note primary no-icon flat"><p><strong>考点</strong>：</p>
<p>（<em>2018</em>）TSVM；</p>
<p>（<em>2019，2022，2023</em>）图半监督学习推导中的D和能量函数、闭式解。</p>
</div>
<p>监督学习的局限：需要海量的该质量标注样本，现实世界需要获取巨大人力物力。</p>
<p>半监督学习：希望同时使用<strong>有标记样本</strong>和<strong>未标记样本</strong>构建泛化性能良好的模型。</p>
<h2 id="未标记样本">12.1.未标记样本</h2>
<p>未标记样本的潜在假设：</p>
<ul>
<li><strong>聚类假设</strong>（Clustering Assumption）：假设数据存在簇结构，即同一簇的样本属于同一类别；</li>
<li><strong>流形假设</strong>（Manifold Assumption）：假设数据分布在一个流形结构上，邻近的样本具有相似的输出值。</li>
</ul>
<h2 id="生成式方法">12.2.生成式方法</h2>
<p>回顾原型聚类中的GMM，我们使用一个高斯混合模型来建模生成概率分布。这里探讨在半监督下的GMM如何进行：</p>
<p>从最大化后验概率出发：</p>
<p><span class="math display">$$
\begin{aligned}
f(x)=\arg\max_{j\in\mathcal{Y}}p(y=j\mid x)
\end{aligned}
$$</span></p>
<p>由于该项很难求，因此我们引入隐变量<span class="math inline"><em>Θ</em></span>，将其改写为：</p>
<p><span class="math display">$$
\begin{aligned}
p(y=j\mid x)=\sum_{i=1}^kp(y=j,\Theta=i\mid x)=\sum_{i=1}^kp(y=j\mid\Theta=i,x)\cdotp(\Theta=i\mid x)
\end{aligned}
$$</span></p>
<p>最终得到对数似然函数为：</p>
<p><span class="math display">$$
\begin{aligned}
\ln p(D_l\cup D_u) &amp; =\sum_{(x_j,y_j)\in D_l}\ln\left(\sum_{i=1}^k\alpha_ip(x_j\mid\mu_i,\Sigma_i)\cdotp(y_j\mid\Theta=i,x_j)\right) \\
 &amp; +\sum_{x_j\in D_u}\ln\left(\sum_{i=1}^k\alpha_ip(x_j\mid\mu_i,\Sigma_i)\right)
\end{aligned}
$$</span></p>
<p>其中第一项是对有标签数据的对数似然，即使有了标签<span class="math inline"><em>y</em><sub><em>j</em></sub></span>，仍然考虑混合成分<span class="math inline"><em>Θ</em></span>，这是因为<span class="math inline"><em>y</em><sub><em>j</em></sub></span>与<span class="math inline"><em>Θ</em></span>不一定一一对应，尤其当一个类别可能由多个高斯成分生成。第二项则是无标签数据的对数似然，只能使用<span class="math inline"><em>x</em><sub><em>j</em></sub></span>本身。之后和之前介绍GMM一样，使用EM算法求解，将高斯混合模型换成混合专家模型、朴素贝叶斯模型等,，可推出其他生成式半监督方法。</p>
<blockquote>
<p>优缺点：</p>
<p>优点：简单、易于实现</p>
<p>缺点：模型假设必须准确，即假设的生成式模型必须与真实数据分布吻合</p>
</blockquote>
<h2 id="半监督svm">12.3.半监督SVM</h2>
<p><strong>TSVM</strong>（Transductive Support Vector Machine） 是另一种经典的半监督学习方法，但和 GMM 不同，它属于判别式方法，不建模数据分布，而是直接优化分类决策边界。其核心思想是在少量标注样本的基础上，利用未标注数据来帮助调整分类边界，使其落在类簇之间（cluster assumption / low-density separation assumption）。</p>
<p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250606213437535.png" width="50%" /></p>
<p>回顾标准SVM的优化目标，可以总结为：</p>
<p><span class="math display">$$
\begin{split}
\min_{w,b,\xi}\frac{1}{2}\|w\|^2+C\sum_i\xi_i\quad\text{subject to }y_i(w^Tx_i+b)\geq1-\xi_i
\end{split}
$$</span></p>
<p>即在保证分类正确（或有一定松弛）的条件下，最大化间隔<span class="math inline">$\frac{1}{\|w\|}$</span>。而TSVM的想法是，不仅保证有标签数据分类结果良好，<strong>无标签数据还需要远离决策边界</strong>，使得边界落在两个类别的低密度区。具体来说TSVM的目标函数可总结为：</p>
<p><span class="math display">$$
\begin{aligned}
\min_{\boldsymbol{w},b,\hat{\boldsymbol{y}},\boldsymbol{\xi}} &amp; \frac{1}{2}\|\boldsymbol{w}\|_2^2+C_l\sum_{i=1}^l\xi_i+C_u\sum_{i=l+1}^m\xi_i \\
\mathrm{s.t.} &amp; y_{i}(\boldsymbol{w}^{\top}\boldsymbol{x}_{i}+b)\geq1-\xi_{i},i=1,\ldots,l, \\
 &amp; \hat{y}_{i}(\boldsymbol{w}^{\top}\boldsymbol{x}_{i}+b)\geq1-\xi_{i},i=l+1,\ldots,m, \\
 &amp; \xi_{i}\geq0,i=1,\ldots,m,
\end{aligned}
$$</span></p>
<p>其算法过程可总结为：</p>
<ol type="1">
<li>用有标签样本 <span class="math inline"><em>D</em><sub><em>l</em></sub></span> 训练一个标准 SVM；</li>
<li>用 SVM 对无标记样本 D_u 中样本进行预测，得到初始伪标签 <span class="math inline"><em>ȳ</em><sub><em>i</em></sub> ∈ { − 1,  + 1}</span>；</li>
<li>初始化惩罚项权重，<span class="math inline"><em>C</em><sub><em>u</em></sub> &lt; <em>C</em><sub><em>l</em></sub></span>，其中无标签样本的惩罚项很小，以避免伪标签不准时影响模型太大；</li>
<li>之后逐步迭代优化求解TSVM的优化问题（求解TSVM的目标函数），具体来说一般是交替优化SVM；</li>
<li>在这期间会检查为标签是否严重违反了边界合理性，即 <span class="math inline"><em>y</em>̂<sub><em>i</em></sub><em>y</em>̂<sub><em>j</em></sub> &lt; 0, <em>ξ</em><sub><em>i</em></sub> &gt; 0, <em>ξ</em><sub><em>j</em></sub> &gt; 0, <em>ξ</em><sub><em>i</em></sub> + <em>ξ</em><sub><em>j</em></sub> &gt; 2</span>，这表示这些伪标签可能是错的，我们可以尝试交换伪标签看看能否减少目标函数值；</li>
<li>逐步提升无标签样本的惩罚力度：<span class="math inline"><em>C</em><sub><em>u</em></sub> = <em>m</em><em>i</em><em>n</em>(2<em>C</em><sub><em>u</em></sub>, <em>C</em><sub><em>l</em></sub>)</span>。</li>
</ol>
<p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250607104946619.png" width="80%" /></p>
<p>但在未标记样本进行标记指派及调整的过程中，可能会出现类别不平衡问题（某类的样本远多于另一类），为了减轻类别不平衡所造成的不利影响，可将优化目标中的<span class="math inline"><em>C</em><sub><em>u</em></sub></span>拆分为<span class="math inline"><em>C</em><sub><em>u</em></sub><sup>+</sup></span>与<span class="math inline"><em>C</em><sub><em>u</em></sub><sup>−</sup></span>：</p>
<p><span class="math display">$$
\begin{split}
    C_u^+=\frac{u_-}{u_+}C_u^-
\end{split}
$$</span></p>
<h2 id="图半监督学习">12.4.图半监督学习</h2>
<p>给定一个数据集，我们可以将其映射为一个图，数据集中的每个样本对应图中的一个结点，若两个样本之间的相似度很高，则对应结点之间存在一条边，边的强度正比于样本间的相似度。我们将有标记样本对应的结点看作<strong>染过色</strong>，未标记样本所对应的结点<strong>尚未染色</strong>，于是半监督学习可以看作是图染色问题。</p>
<p>算法过程（以二分类为例）：</p>
<ol type="1">
<li>基于数据集 <span class="math inline"><em>D</em><sub><em>l</em></sub> ∪ <em>D</em><sub><em>u</em></sub></span> 构建一个图 <span class="math inline"><em>G</em> = (<em>V</em>, <em>E</em>)</span>。
<ul>
<li>其中结点集为：</li>
</ul></li>
</ol>
<p><span class="math display">$$
\begin{split}
    V=\{x_1,...,x_l,x_{l+1},...,x_{l+u}\}
\end{split}
$$</span></p>
<ul>
<li>边集<span class="math inline"><em>E</em></span>可表示为一个亲和矩阵（affinity matrix），基于高斯函数定义为：</li>
</ul>
<p><span class="math display">$$
\left.\mathbf{W}_{ij}=\left\{
\begin{array}
{cc}\exp\left(\frac{-\|x_i-x_j\|_2^2}{2\sigma^2}\right), &amp; \mathrm{if}\quad i\neq j; \\
 \\
0 ,&amp; \mathrm{otherwise};
\end{array}\right.\right.
$$</span></p>
<ol start="2" type="1">
<li>假定从图 <span class="math inline"><em>G</em> = (<em>V</em>, <em>E</em>)</span> 将学得一个实值函数 <span class="math inline"><em>f</em> : <em>V</em> → ℝ</span>，对每个样本输出一个分类概率得分。直观上相似的样本应该具有相似的标记，于是可以得到关于<span class="math inline"><em>f</em></span>的<strong>能量函数</strong>：</li>
</ol>
<p><span class="math display">$$
\begin{aligned}
 &amp; E(f)={\frac{1}{2}}\sum_{i=1}^{m}\sum_{j=1}^{m}{\boldsymbol{W}_{ij}}(f({\boldsymbol{x}_{i}})-f({\boldsymbol{x}_{j}}))^{2} \\
 &amp; =\frac{1}{2}\left(\sum_{i=1}^{m}d_{i}f^{2}(\boldsymbol{x}_{i})+\sum_{j=1}^{m}d_{j}f^{2}(\boldsymbol{x}_{j})-2\sum_{i=1}^{m}\sum_{j=1}^{m}\boldsymbol{W}_{ij}f(\boldsymbol{x}_{i})f(\boldsymbol{x}_{j})\right) \\
 &amp; =f^{T}(D-W)\boldsymbol{f}
\end{aligned}
$$</span></p>
<p><span class="math display">$$
\begin{split}
    D=\mathrm{diag}(d_i),d_i=\sum_{j=1}^{l+u}(W_{ij})
\end{split}
$$</span></p>
<ol start="3" type="1">
<li>采用分块矩阵的表示方式：</li>
</ol>
<p><span class="math display">$$
\begin{split}
    E(f)=(\boldsymbol{f}_l^\top\boldsymbol{f}_u^\top)\left(
\begin{bmatrix}
\mathbf{D}_{ll} &amp; \mathbf{0}_{lu} \\
\mathbf{0}_{ul} &amp; \mathbf{D}_{uu}
\end{bmatrix}-
\begin{bmatrix}
\mathbf{W}_{ll} &amp; \mathbf{W}_{lu} \\
\mathbf{W}_{ul} &amp; \mathbf{W}_{uu}
\end{bmatrix}\right)
\begin{bmatrix}
f_l \\
\mathbf{f}_u
\end{bmatrix} \\
=\boldsymbol{f}_l^\top(\mathbf{D}_l-\mathbf{W}_{ll})\boldsymbol{f}_l-2\boldsymbol{f}_u^\top\mathbf{W}_{ul}\boldsymbol{f}_l+\boldsymbol{f}_u^\top(\mathbf{D}_{uu}-\mathbf{W}_{uu})\boldsymbol{f}_u.
\end{split}
$$</span></p>
<p>通过<span class="math inline">$\frac{\partial E(f)}{\partial f_u}=0$</span>，可以得到：</p>
<p><span class="math display">$$
\begin{aligned}
f_{u} &amp; =(D_{uu}(\boldsymbol{I}-\boldsymbol{D}_{uu}^{-1}\boldsymbol{W}_{uu}))^{-1}\boldsymbol{W}_{ul}\boldsymbol{f}_{l} \\
 &amp; =(\boldsymbol{I}-\boldsymbol{D}_{uu}^{-1}\boldsymbol{W}_{uu})^{-1}\boldsymbol{D}_{uu}^{-1}\boldsymbol{W}_{ul}\boldsymbol{f}_{l} \\
 &amp; =(\boldsymbol{I}-\boldsymbol{P}_{uu})^{-1}\boldsymbol{P}_{ul}\boldsymbol{f}_l
\end{aligned}
$$</span></p>
<p><span class="math display">$$
\begin{split}
    P=D^{-1}W
\end{split}
$$</span></p>
<blockquote>
<p>优缺点：</p>
<p>图半监督学习方法在概念上相当清晰，易于通过对所涉矩阵运算的分析来探索算法性质</p>
<p>但存储开销高，且构图过程仅能考虑训练样本集，难以判知新样本在图中的位置</p>
</blockquote>
<h2 id="基于分歧的方法">12.5.基于分歧的方法</h2>
<p>基于分歧的方法（disagreement-based methods）使用多学习器，而学习器之间的分歧对未标记数据的利用很重要。协同训练（co-training）是基于分歧方法的重要代表，其最初针对多视图（multi-view）数据设计，</p>
<p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250607134045187.png" width="50%" /></p>
<p>协同训练很好地利用了多视角的相容互补性，但它需要假设<strong>数据拥有两个充分且条件独立的视图</strong>。因此此后出现了一些能在单视图数据上使用的变体算法，可以使用<em>不同的学习算法</em>、或<em>不同的数据采样</em>、或<em>不同的参数设置</em>来产生不同的学习器，也能有效利用未标记样本提升性能。</p>
<p>后续研究发现此类算法事实上无需数据拥有多视图，仅需弱学习器之间具有显著的分歧（差异），即可通过相互提供伪标记样本的方式来提高泛化性能。</p>
<blockquote>
<p>优缺点：</p>
<p>基于分歧的方法只需采用合适的基学习器，就能较少受到模型假设、损失函数非凸性和数据规模等问题的影响，学习方法简单有效、理论基础相对坚实、适用范围广泛。</p>
<p>但为了使用这类方法，需要生成具有显著分歧、性能尚可的多个学习器，但当有标记样本很少、尤其数据不具有多视图时，很难实现这一点。</p>
</blockquote>
<h2 id="半监督聚类">12.6.半监督聚类</h2>
<p>聚类是一种典型的无监督学习任务，但有时我们往往能获得一些额外的监督信息，这时我们可以使用半监督聚类（semi-supervised clustering）来使用监督信息以获得更好的聚类效果。</p>
<p>聚类任务中获得的监督信息大致有两种类型：</p>
<ol type="1">
<li><strong>必连（must-link）与勿连（cannot-link）约束</strong>：前者是指样本必属于同一个簇，后者是指样本必不属于同一个簇；</li>
<li><strong>有少量的有标记样本</strong>。</li>
</ol>
<h3 id="约束k均值">约束k均值</h3>
<p><strong>约束k均值</strong>（constrained k-means）算法是利用第一类监督信息的代表，该算法是k-means算法的扩展，它在聚类过程中要确保必连和勿连关系集合中的约束得以满足，否则将返回错误提示。</p>
<p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250607135958008.png" width="60%" /></p>
<h3 id="约束种子k均值">约束种子k均值</h3>
<p><strong>约束种子k均值</strong>（constrained seed k-means）算法使用少量有标记样本，即假设少量有标记样本属于<span class="math inline"><em>k</em></span>个聚类簇。这样的监督信息利用起来很容易：直接将有标签样本作为种子，用他们初始化k均值算法的k个聚类中心，在聚类粗迭代更新过程中不改变种子样本的簇隶属关系，这样就得到了约束种子k均值算法。</p>
<h1 id="考题回忆">2025考题回忆</h1>
<ol type="1">
<li>什么是机器学习？目前机器学习的稳健性存在问题，请举出一两个例子来说明。</li>
<li>说明什么是过拟合和欠拟合，说明二者产生的原因并以具体模型为例子说明如何避免。</li>
<li>推导带正则化项的线性回归闭式解，并说明线性模型的优缺点。</li>
<li>决策树的两种选择类型，并说明偏好。</li>
<li>SVM的目标？并用基本型推导至对偶型。引入核函数，说明核函数的好处。</li>
<li>PCA有两种性质，选择一种说明，并利用它推导PCA。</li>
<li>生成式和判别式模型的区别？并举例子。</li>
<li>密度聚类和层次聚类的具体代表算法？说明步骤并说明各自依赖什么假设。</li>
<li>多层前馈网络的学习能力？局限与解决办法。</li>
<li>图半监督学习的能量函数？推导闭式解。</li>
<li>LVM是哪种特征选择？说明三种特征选择的区别。</li>
</ol>
<p>简单来说就是基本全是原题。@…@</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://litchi-lee.github.io">Leo Sinclair</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://litchi-lee.github.io/2025/06/03/course/AML/">https://litchi-lee.github.io/2025/06/03/course/AML/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://litchi-lee.github.io" target="_blank">Leo的博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E8%AF%BE%E7%A8%8B/">课程</a></div><div class="post-share"><div class="social-share" data-image="/img/bg4.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2025/05/20/AIGC/MAR/" title="MAR的细节"><img class="cover" src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/bg6.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">MAR的细节</div></div><div class="info-2"><div class="info-item-1">一些细节记录</div></div></div></a><a class="pagination-related" href="/2025/06/20/AI/agent/" title="服务器配置代理"><img class="cover" src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/bg8.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">服务器配置代理</div></div><div class="info-2"><div class="info-item-1">在服务器中配置代理，有时候方便内网配置环境</div></div></div></a></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/img0.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">Leo Sinclair</div><div class="author-info-description">玻璃晴朗，橘子辉煌</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">19</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">8</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">5</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/litchi-lee"><i class="fab fa-github"></i><span>Follow</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/litchi-lee" target="_blank" title="GitHub"><i class="fab fa-github" style="color: #000000;"></i></a><a class="social-icon" href="/lee20020520@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #000000;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">warning：博主精神状态待观察</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E4%B8%80%E8%AE%B2%E7%BB%AA%E8%AE%BA"><span class="toc-number">1.</span> <span class="toc-text">第一讲、绪论</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.1.</span> <span class="toc-text">机器学习</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD"><span class="toc-number">1.2.</span> <span class="toc-text">人工智能</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%87%8D%E8%A6%81%E6%9C%AF%E8%AF%AD"><span class="toc-number">1.3.</span> <span class="toc-text">重要术语</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E4%BA%8C%E8%AE%B2%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E5%92%8C%E9%80%89%E6%8B%A9"><span class="toc-number">2.</span> <span class="toc-text">第二讲、模型评估和选择</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%84%E4%BC%B0%E6%96%B9%E6%B3%95"><span class="toc-number">2.1.</span> <span class="toc-text">评估方法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%80%A7%E8%83%BD%E5%BA%A6%E9%87%8F"><span class="toc-number">2.2.</span> <span class="toc-text">性能度量</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%AF%94%E8%BE%83%E6%A3%80%E9%AA%8C"><span class="toc-number">2.3.</span> <span class="toc-text">比较检验</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%81%8F%E5%B7%AE-%E6%96%B9%E5%B7%AE%E5%88%86%E8%A7%A3"><span class="toc-number">2.4.</span> <span class="toc-text">偏差-方差分解</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E4%B8%89%E8%AE%B2%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B"><span class="toc-number">3.</span> <span class="toc-text">第三讲、线性模型</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9E%E5%BD%92%E4%BB%BB%E5%8A%A1%E6%A8%A1%E5%9E%8B"><span class="toc-number">3.1.</span> <span class="toc-text">3.1.回归任务模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%80%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-number">3.1.1.</span> <span class="toc-text">一元线性回归</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-number">3.1.2.</span> <span class="toc-text">多元线性回归</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B"><span class="toc-number">3.1.3.</span> <span class="toc-text">广义线性回归模型</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E6%A8%A1%E5%9E%8B"><span class="toc-number">3.2.</span> <span class="toc-text">3.2.二分类任务模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%9A%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E6%A8%A1%E5%9E%8B"><span class="toc-number">3.3.</span> <span class="toc-text">3.3.多分类任务模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%80%E5%AF%B9%E4%B8%80ovo"><span class="toc-number">3.3.1.</span> <span class="toc-text">一对一（OvO）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%80%E5%AF%B9%E5%85%B6%E4%BB%96ovr"><span class="toc-number">3.3.2.</span> <span class="toc-text">一对其他（OvR）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E5%AF%B9%E5%A4%9Amvm"><span class="toc-number">3.3.3.</span> <span class="toc-text">多对多（MvM）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BC%98%E7%BC%BA%E7%82%B9"><span class="toc-number">3.4.</span> <span class="toc-text">3.4.线性模型的优缺点</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E5%9B%9B%E8%AE%B2%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA"><span class="toc-number">4.</span> <span class="toc-text">第四讲、支持向量机</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E5%9E%8B%E5%92%8C%E5%AF%B9%E5%81%B6%E5%9E%8B"><span class="toc-number">4.1.</span> <span class="toc-text">4.1.基本型和对偶型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%89%B9%E5%BE%81%E7%A9%BA%E9%97%B4%E6%98%A0%E5%B0%84"><span class="toc-number">4.2.</span> <span class="toc-text">4.2.特征空间映射</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BD%AF%E9%97%B4%E9%9A%94%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA"><span class="toc-number">4.3.</span> <span class="toc-text">4.3.软间隔支持向量机</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#svm%E6%8B%93%E5%B1%95%E6%AD%A3%E5%88%99%E5%8C%96"><span class="toc-number">4.4.</span> <span class="toc-text">4.4.SVM拓展–正则化</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E4%BA%94%E8%AE%B2%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">5.</span> <span class="toc-text">第五讲、神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8E%86%E5%8F%B2"><span class="toc-number">5.1.</span> <span class="toc-text">5.1.神经网络历史</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E5%85%83%E6%A8%A1%E5%9E%8B"><span class="toc-number">5.2.</span> <span class="toc-text">5.2.神经元模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%84%9F%E7%9F%A5%E6%9C%BA%E4%B8%8E%E5%A4%9A%E5%B1%82%E7%BD%91%E7%BB%9C"><span class="toc-number">5.3.</span> <span class="toc-text">5.3.感知机与多层网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%AF%E5%B7%AE%E9%80%86%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95"><span class="toc-number">5.4.</span> <span class="toc-text">5.4.误差逆传播算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0"><span class="toc-number">5.5.</span> <span class="toc-text">5.5.深度学习</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E5%85%AD%E8%AE%B2%E5%86%B3%E7%AD%96%E6%A0%91"><span class="toc-number">6.</span> <span class="toc-text">第六讲、决策树</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E6%B5%81%E7%A8%8B"><span class="toc-number">6.1.</span> <span class="toc-text">6.1.基本流程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%92%E5%88%86%E7%AE%97%E6%B3%95"><span class="toc-number">6.2.</span> <span class="toc-text">6.2.划分算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8Aid3%E5%86%B3%E7%AD%96%E6%A0%91"><span class="toc-number">6.2.1.</span> <span class="toc-text">信息增益（ID3决策树）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A2%9E%E7%9B%8A%E7%8E%87c4.5%E5%86%B3%E7%AD%96%E6%A0%91"><span class="toc-number">6.2.2.</span> <span class="toc-text">增益率（C4.5决策树）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%89%AA%E6%9E%9D%E5%A4%84%E7%90%86"><span class="toc-number">6.3.</span> <span class="toc-text">6.3.剪枝处理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%A2%84%E5%89%AA%E6%9E%9D"><span class="toc-number">6.3.1.</span> <span class="toc-text">预剪枝</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%90%8E%E5%89%AA%E6%9E%9D"><span class="toc-number">6.3.2.</span> <span class="toc-text">后剪枝</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%9A%E5%8F%98%E9%87%8F%E5%86%B3%E7%AD%96%E6%A0%91"><span class="toc-number">6.4.</span> <span class="toc-text">6.4.多变量决策树</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E4%B8%83%E8%AE%B2%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8"><span class="toc-number">7.</span> <span class="toc-text">第七讲、贝叶斯分类器</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%86%B3%E7%AD%96%E8%AE%BA"><span class="toc-number">7.1.</span> <span class="toc-text">7.1.贝叶斯决策论</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF"><span class="toc-number">7.2.</span> <span class="toc-text">7.2.朴素贝叶斯</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%8A%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF"><span class="toc-number">7.3.</span> <span class="toc-text">7.3.半朴素贝叶斯</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BD%91"><span class="toc-number">7.4.</span> <span class="toc-text">7.4.贝叶斯网</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E5%85%AB%E8%AE%B2%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0"><span class="toc-number">8.</span> <span class="toc-text">第八讲、集成学习</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%AA%E4%BD%93%E4%B8%8E%E9%9B%86%E6%88%90"><span class="toc-number">8.1.</span> <span class="toc-text">8.1.个体与集成</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#boosting"><span class="toc-number">8.2.</span> <span class="toc-text">8.2.Boosting</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#bagging%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97"><span class="toc-number">8.3.</span> <span class="toc-text">8.3.Bagging与随机森林</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#bagging"><span class="toc-number">8.3.1.</span> <span class="toc-text">Bagging</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97"><span class="toc-number">8.3.2.</span> <span class="toc-text">随机森林</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%93%E5%90%88%E7%AD%96%E7%95%A5"><span class="toc-number">8.4.</span> <span class="toc-text">8.4.结合策略</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%9A%E6%A0%B7%E6%80%A7"><span class="toc-number">8.5.</span> <span class="toc-text">8.5.多样性</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E4%B9%9D%E8%AE%B2%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95"><span class="toc-number">9.</span> <span class="toc-text">第九讲、聚类算法</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%81%9A%E7%B1%BB%E5%AE%9A%E4%B9%89"><span class="toc-number">9.1.</span> <span class="toc-text">9.1.聚类定义</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%80%A7%E8%83%BD%E5%BA%A6%E9%87%8F-1"><span class="toc-number">9.2.</span> <span class="toc-text">9.2.性能度量</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B7%9D%E7%A6%BB%E8%AE%A1%E7%AE%97"><span class="toc-number">9.3.</span> <span class="toc-text">9.3.距离计算</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95"><span class="toc-number">9.4.</span> <span class="toc-text">9.4.聚类算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E5%9E%8B%E8%81%9A%E7%B1%BB"><span class="toc-number">9.4.1.</span> <span class="toc-text">原型聚类</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AF%86%E5%BA%A6%E8%81%9A%E7%B1%BB"><span class="toc-number">9.4.2.</span> <span class="toc-text">密度聚类</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B1%82%E6%AC%A1%E8%81%9A%E7%B1%BB"><span class="toc-number">9.4.3.</span> <span class="toc-text">层次聚类</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E5%8D%81%E8%AE%B2%E9%99%8D%E7%BB%B4%E4%B8%8E%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0"><span class="toc-number">10.</span> <span class="toc-text">第十讲、降维与度量学习</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#k-%E8%BF%91%E9%82%BB%E5%AD%A6%E4%B9%A0"><span class="toc-number">10.1.</span> <span class="toc-text">10.1.K-近邻学习</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%8E%E7%BB%B4%E5%B5%8C%E5%85%A5"><span class="toc-number">10.2.</span> <span class="toc-text">10.2.低维嵌入</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B5%81%E5%BD%A2%E5%AD%A6%E4%B9%A0"><span class="toc-number">10.3.</span> <span class="toc-text">10.3.流形学习</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#isomap"><span class="toc-number">10.3.1.</span> <span class="toc-text">ISOMAP</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#lle"><span class="toc-number">10.3.2.</span> <span class="toc-text">LLE</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0"><span class="toc-number">10.4.</span> <span class="toc-text">10.4.度量学习</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0nca"><span class="toc-number">10.4.1.</span> <span class="toc-text">距离度量学习–NCA</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0lmnn"><span class="toc-number">10.4.2.</span> <span class="toc-text">距离度量学习–LMNN</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#pca"><span class="toc-number">10.5.</span> <span class="toc-text">10.5.PCA</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E5%8D%81%E4%B8%80%E8%AE%B2%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E5%92%8C%E7%A8%80%E7%96%8F%E5%AD%A6%E4%B9%A0"><span class="toc-number">11.</span> <span class="toc-text">第十一讲、特征选择和稀疏学习</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9"><span class="toc-number">11.1.</span> <span class="toc-text">11.1.特征选择</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%89%B9%E5%BE%81"><span class="toc-number">11.1.1.</span> <span class="toc-text">特征</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9-1"><span class="toc-number">11.1.2.</span> <span class="toc-text">特征选择</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BF%87%E6%BB%A4%E5%BC%8F"><span class="toc-number">11.1.2.1.</span> <span class="toc-text">过滤式</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8C%85%E8%A3%B9%E5%BC%8F"><span class="toc-number">11.1.2.2.</span> <span class="toc-text">包裹式</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B5%8C%E5%85%A5%E5%BC%8F"><span class="toc-number">11.1.2.3.</span> <span class="toc-text">嵌入式</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A8%80%E7%96%8F%E8%A1%A8%E7%A4%BA"><span class="toc-number">11.2.</span> <span class="toc-text">11.2.稀疏表示</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E5%8D%81%E4%BA%8C%E8%AE%B2%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="toc-number">12.</span> <span class="toc-text">第十二讲、半监督学习</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%AA%E6%A0%87%E8%AE%B0%E6%A0%B7%E6%9C%AC"><span class="toc-number">12.1.</span> <span class="toc-text">12.1.未标记样本</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%94%9F%E6%88%90%E5%BC%8F%E6%96%B9%E6%B3%95"><span class="toc-number">12.2.</span> <span class="toc-text">12.2.生成式方法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%8A%E7%9B%91%E7%9D%A3svm"><span class="toc-number">12.3.</span> <span class="toc-text">12.3.半监督SVM</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%BE%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="toc-number">12.4.</span> <span class="toc-text">12.4.图半监督学习</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E5%88%86%E6%AD%A7%E7%9A%84%E6%96%B9%E6%B3%95"><span class="toc-number">12.5.</span> <span class="toc-text">12.5.基于分歧的方法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%8A%E7%9B%91%E7%9D%A3%E8%81%9A%E7%B1%BB"><span class="toc-number">12.6.</span> <span class="toc-text">12.6.半监督聚类</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BA%A6%E6%9D%9Fk%E5%9D%87%E5%80%BC"><span class="toc-number">12.6.1.</span> <span class="toc-text">约束k均值</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BA%A6%E6%9D%9F%E7%A7%8D%E5%AD%90k%E5%9D%87%E5%80%BC"><span class="toc-number">12.6.2.</span> <span class="toc-text">约束种子k均值</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%80%83%E9%A2%98%E5%9B%9E%E5%BF%86"><span class="toc-number">13.</span> <span class="toc-text">2025考题回忆</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/09/19/Algo/cpp_use/" title="C++常用STL结构体"><img src="/img/bg1.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="C++常用STL结构体"/></a><div class="content"><a class="title" href="/2025/09/19/Algo/cpp_use/" title="C++常用STL结构体">C++常用STL结构体</a><time datetime="2025-09-18T16:00:00.000Z" title="发表于 2025-09-19 00:00:00">2025-09-19</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/08/22/AI/RLHF/" title="关于RLHF的一些学习记录"><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/bg6.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="关于RLHF的一些学习记录"/></a><div class="content"><a class="title" href="/2025/08/22/AI/RLHF/" title="关于RLHF的一些学习记录">关于RLHF的一些学习记录</a><time datetime="2025-08-21T16:00:00.000Z" title="发表于 2025-08-22 00:00:00">2025-08-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/06/23/Reconstruction/PerX2CT/" title="PerX2CT记录"><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/bg7.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="PerX2CT记录"/></a><div class="content"><a class="title" href="/2025/06/23/Reconstruction/PerX2CT/" title="PerX2CT记录">PerX2CT记录</a><time datetime="2025-06-22T16:00:00.000Z" title="发表于 2025-06-23 00:00:00">2025-06-23</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/06/20/AI/agent/" title="服务器配置代理"><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/bg8.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="服务器配置代理"/></a><div class="content"><a class="title" href="/2025/06/20/AI/agent/" title="服务器配置代理">服务器配置代理</a><time datetime="2025-06-19T16:00:00.000Z" title="发表于 2025-06-20 00:00:00">2025-06-20</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/06/03/course/AML/" title="NJU2025春季学期-高级机器学习笔记"><img src="/img/bg4.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="NJU2025春季学期-高级机器学习笔记"/></a><div class="content"><a class="title" href="/2025/06/03/course/AML/" title="NJU2025春季学期-高级机器学习笔记">NJU2025春季学期-高级机器学习笔记</a><time datetime="2025-06-02T16:00:00.000Z" title="发表于 2025-06-03 00:00:00">2025-06-03</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url(/img/img3.png);"><div id="footer-wrap"><div class="copyright">&copy;2025 By Leo Sinclair</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.3.5</a></div><div class="footer_custom_text">In case I don't see you, good morning, good afternoon, and good night.</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><div class="js-pjax"><script>(() => {
  const loadMathjax = () => {
    if (!window.MathJax) {
      window.MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          tags: 'none',
        },
        chtml: {
          scale: 1.1
        },
        options: {
          enableMenu: true,
          renderActions: {
            findScript: [10, doc => {
              for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
                const display = !!node.type.match(/; *mode=display/)
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
                const text = document.createTextNode('')
                node.parentNode.replaceChild(text, node)
                math.start = {node: text, delim: '', n: 0}
                math.end = {node: text, delim: '', n: 0}
                doc.math.push(math)
              }
            }, '']
          }
        }
      }

      const script = document.createElement('script')
      script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
      script.id = 'MathJax-script'
      script.async = true
      document.head.appendChild(script)
    } else {
      MathJax.startup.document.state(0)
      MathJax.texReset()
      MathJax.typesetPromise()
    }
  }

  btf.addGlobalFn('encrypt', loadMathjax, 'mathjax')
  window.pjax ? loadMathjax() : window.addEventListener('load', loadMathjax)
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div></body></html>