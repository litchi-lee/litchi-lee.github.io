<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>NJU2025春季学期-高级机器学习笔记</title>
      <link href="/2025/06/03/course/AML/"/>
      <url>/2025/06/03/course/AML/</url>
      
        <content type="html"><![CDATA[<h1 id="第一讲绪论">第一讲、绪论</h1><div class="note primary no-icon flat"><p><strong>考点</strong>：</p><p>（<em>2022，2023</em>）机器学习的定义；</p><p>（<em>2021，2023</em>）机器学习的鲁棒性。</p></div><h2 id="机器学习">机器学习</h2><p>机器学习：机器利用数据学习人类经验，不断提高性能的过程</p><p>机器学习的鲁棒性：机器学习中的鲁棒性（Robustness）指的是模型在面对数据扰动、异常样本或分布变化时，仍然能够保持良好性能的能力。换句话说，就是模型对“不完美”输入的容忍能力。</p><h2 id="人工智能">人工智能</h2><p>人工智能的发展阶段：</p><ul><li>1950年，图灵机；</li><li>1956年，达特茅斯（Dartmouth）会议；</li><li>60-80年代，<strong>推理期</strong>；</li><li>80-90年代，<strong>知识期</strong>；</li><li>90年代后，<strong>学习期</strong>。</li></ul><h2 id="重要术语">重要术语</h2><ul><li>监督学习与无监督学习；</li><li>数据集、训练、测试…</li><li>机器学习技术的根本目标是–<strong>使模型具有泛化能力</strong>：应对未见样本的预测能力；</li><li>归纳偏置（Inductive Bias）：任何一个有效的机器学习算法必有其偏好；</li><li>NFL（No Free Lunch）原则：一个算法 <span class="math inline">𝔏<sub><em>a</em></sub></span> 若在某些问题上比另一个算法 <span class="math inline">𝔏<sub><em>b</em></sub></span> 好，必存在另一些问题 <span class="math inline">𝔏<sub><em>b</em></sub></span> 比 <span class="math inline">𝔏<sub><em>a</em></sub></span> 好；</li></ul><hr /><h1 id="第二讲模型评估和选择">第二讲、模型评估和选择</h1><div class="note primary no-icon flat"><p><strong>考点</strong>：</p><p>（<em>2022</em>）评估方法、性能度量、比较检验各解决什么问题；</p><p>（<em>2023</em>）过拟合&amp;欠拟合。</p></div><p><strong>泛化误差 VS 经验误差</strong></p><ul><li>泛化误差：在“未来”（unseen）样本上的误差；</li><li>经验误差：在训练集上的误差，亦称训练误差；</li></ul><p>泛化误差越小越好，但是经验误差并非越小越好（会出现过拟合）。</p><p><strong>过拟合 VS 欠拟合</strong>：一个是模型还没有学充分，一个是模型学得过于充分</p><h2 id="评估方法">评估方法</h2><ol type="1"><li>留出法（hold-out）：将数据集划分为训练和测试集；</li><li>交叉验证法（cross-validation）：将数据集分层采样划分为k个大小相似的互斥子集，每次用k-1个子集的并集作为训练集，余下的子集作为测试集，最终返回k个测试结果的均值；</li><li>留一法（LOO）：假设数据集D包含m个样本，若令k=m，则得到留一法。</li></ol><h2 id="性能度量">性能度量</h2><p><strong>回归任务</strong>最常用的性能度量为<strong>均方误差</strong>（MSE）：</p><p><span class="math display">$$\begin{split}    E(f ; D)=\frac{1}{m} \sum_{i=1}^m\left(f\left(\mathbf{x}_i\right)-y_i\right)^2\end{split}$$</span></p><p><strong>分类任务</strong>最常见的性能度量为<strong>错误率（err）和精度（acc）</strong>：</p><p><span class="math display">$$\begin{split}    E(f ; D)=\frac{1}{m} \sum_{i=1}^m \mathbf{I}\left(f\left(\mathbf{x}_i\right) \neq y_i\right)\end{split}$$</span></p><p><span class="math display">$$\begin{aligned}    acc(f ; D)&amp;=\frac{1}{m} \sum_{i=1}^m \mathbf{I}\left(f\left(\mathbf{x}_i\right) = y_i\right) \\    &amp;= 1-E(f ; D)\end{aligned}$$</span></p><p>信息检索和Web搜索中通常使用<strong>查准率（P）和查全率（R）</strong>来衡量正例比率：</p><p><span class="math display">$$\begin{aligned}    P &amp;= \frac{TP}{TP+FP} \\    R &amp;= \frac{TP}{TP+FN}\end{aligned}$$</span></p><blockquote><p>补充：</p><p><strong>TP</strong>（真正例，gt-正 &amp; pred-正）</p><p><strong>TN</strong>（真负例，gt-负 &amp; pred-负）</p><p><strong>FP</strong>（假正例，gt-负 &amp; pred-正）</p><p><strong>FN</strong>（假负例，gt-正 &amp; pred-负）</p></blockquote><p><strong>F1度量</strong>要更为常用：</p><p><span class="math display">$$\begin{split}    F1 = \frac{2PR}{P+R}\end{split}$$</span></p><p><span class="math inline"><em>F</em><sub><em>β</em></sub></span>是比F1更一般的形式：</p><p><span class="math display">$$\begin{split}    F_\beta = \frac{(1+\beta^2)PR}{\beta^2P+R}\end{split}$$</span></p><p>当<span class="math inline"><em>β</em> = 1</span>时，是标准的F1指标；当<span class="math inline"><em>β</em> &gt; 1</span>时，更偏重查全率（R），此时适用于逃犯信息检索任务；当<span class="math inline"><em>β</em> &lt; 1</span>时，更偏重查准率（P），此时适用于商品推荐任务；</p><p><strong>ROC曲线</strong>是常用于二分类任务中的工具，其基于<strong>真正例率</strong>（TPR，又叫做召回率）和<strong>假正例率</strong>（FPR）：</p><p><span class="math display">$$\begin{aligned}    TPR &amp;= \frac{TP}{TP+FN} \\    FPR &amp;= \frac{FP}{TN+FP}\end{aligned}$$</span></p><p>通过遍历不同的阈值，每次将大于阈值的作为正例，小于阈值的作为负例，能得到一组TPR和FPR，最终将这些点按序连接可以得到ROC曲线。而<strong>AUC</strong>的值就是ROC曲线下的面积大小。</p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/ROC.png" width="50%" /></p><h2 id="比较检验">比较检验</h2><p>使用T-检验来进行比较检验。</p><h2 id="偏差-方差分解">偏差-方差分解</h2><p>对于回归任务，其泛化误差可通过偏差-方差分解拆解为：</p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/generalError.png" width="80%" /></p><p>泛化性能是由<strong>学习算法的能力</strong>、<strong>数据的充分性</strong>、<strong>学习任务的难度</strong>共同决定的。</p><hr /><h1 id="第三讲线性模型">第三讲、线性模型</h1><div class="note primary no-icon flat"><p><strong>考点</strong>：</p><p>（<em>2021，2022</em>）最小二乘法推导；</p><p>（<em>2023</em>）多元线性回归求闭式解。</p></div><h2 id="回归任务模型">3.1.回归任务模型</h2><h3 id="一元线性回归">一元线性回归</h3><p>对于单一属性的线性回归，其模型可构建为：</p><p><span class="math display">$$\begin{split}    f\left(x\right)=wx_{i}+b\quad\text{使得}f\left(x_{i}\right)\simeq y_{i}\end{split}$$</span></p><p>参数估计方法：<strong>最小二乘法</strong></p><p><span class="math display">$$\begin{aligned}(w^{*},b^{*}) &amp; =\underset{(w,b)}{\operatorname*{\operatorname*{\arg\min}}}\sum_{i=1}^m\left(f\left(x_i\right)-y_i\right)^2 \\ &amp; =\arg\min_{(w,b)}\sum_{i=1}^m\left(y_i-wx_i-b\right)^2\end{aligned}$$</span></p><p>求解方法：最小化均方误差，分别对 <span class="math inline"><em>ω</em></span> 和 <span class="math inline"><em>b</em></span> 求导，可得闭式解：</p><p><span class="math display">$$\begin{split}w=\frac{\sum_{i=1}^my_i\left(x_i-\bar{x}\right)}{\sum_{i=1}^mx_i^2-\frac{1}{m}\left(\sum_{i=1}^mx_i\right)^2}\end{split}$$</span></p><p><span class="math display">$$\begin{split}b=\frac{1}{m}\sum_{i=1}^{m}\left(y_{i}-wx_{i}\right)\end{split}$$</span></p><h3 id="多元线性回归">多元线性回归</h3><p>对于多元属性的线性回归（多元线性回归），其中每个样本属性为 <span class="math inline"><strong>x</strong><sub><em>i</em></sub> = (<em>x</em><sub><em>i</em>1</sub>; <em>x</em><sub><em>i</em>2</sub>; …; <em>x</em><sub><em>i</em><em>d</em></sub>)  <em>y</em><sub><em>i</em></sub> ∈ <strong>R</strong></span> ，其模型可被推广为：</p><p><span class="math display">$$\begin{split}f\left(\mathbf{x}_i\right)=\mathbf{\omega}^\mathrm{T}\mathbf{x}_i+b\text{ 使得}f\left(\mathbf{x}_i\right)\simeq y_i\end{split}$$</span></p><p>令 <span class="math inline">$\hat{\mathbf{\omega}} = (\mathbf{\omega};b)$</span> ，并构造：</p><p><span class="math display">$$\mathbf{X}=\begin{pmatrix}x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1d} &amp; 1 \\x_{21} &amp; x_{22} &amp; \cdots &amp; x_{2d} &amp; 1 \\\vdots &amp; \vdots &amp; \ddots &amp; \vdots &amp; \vdots \\x_{m1} &amp; x_{m2} &amp; \cdots &amp; x_{md} &amp; 1\end{pmatrix}=\begin{pmatrix}\mathbf{x}_1^\mathrm{T} &amp; 1 \\\mathbf{x}_2^\mathrm{T} &amp; 1 \\\vdots &amp; \vdots \\\mathbf{x}_m^\mathrm{T} &amp; 1\end{pmatrix}$$</span></p><p>那么此时使用最小二乘法可以表述为：</p><p><span class="math display">$$\begin{split}\hat{\mathbf{\omega}}^*=\arg\min_{\hat{\mathbf{\omega}}}\left(\mathbf{y}-\mathbf{X}\hat{\mathbf{\omega}}\right)^\mathrm{T}\left(\mathbf{y}-\mathbf{X}\hat{\mathbf{\omega}}\right).\end{split}$$</span></p><h3 id="广义线性回归模型">广义线性回归模型</h3><p>广义线性回归模型为：</p><p><span class="math display">$$\begin{split}y=g^{-1}\left(\mathbf{\omega}^\mathrm{T}\mathbf{x}+b\right)\end{split}$$</span></p><p>其中<span class="math inline"><em>g</em>( ⋅ )</span>被称为<strong>联系函数</strong>，并满足单调可微的性质。可以理解这里为回归模型引入了非线性的成分。</p><h2 id="二分类任务模型">3.2.二分类任务模型</h2><p>预测值与输出需要找到函数将其联系起来：</p><p><span class="math display">$$\begin{split}z=\mathbf{\omega}^\mathrm{T}\mathbf{x}+b\quad y\in\{0,1\}\end{split}$$</span></p><p>最理想的函数是单位跃迁函数，即预测值大于零判为正例、小于零判为负例，但是<strong>单位跃迁函数不连续也不可导</strong>。因此使用替代函数–<strong>对数几率函数</strong>（logistic function）作为联系函数，其单调可微且任意阶可导：</p><p><span class="math display">$$\begin{split}y=\frac{1}{1+e^{-z}}\end{split}$$</span></p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250604130952.png" width="50%" /></p><p>进一步推导可得：</p><p><span class="math display">$$\begin{split}ln\frac{y}{1-y}=\mathbf{\omega}^\mathrm{T}\mathbf{x}+b\end{split}$$</span></p><p>这里的 <span class="math inline">$ln\frac{y}{1-y}$</span> 称为几率，反映了 <span class="math inline"><strong>x</strong></span> 作为正例的相对可能性。求解使用极大似然法进行求解：</p><p><span class="math display">$$\begin{split}\ell\left(\mathbf{\omega},b\right)=\sum_{i=1}^m\ln p\left(y_i\mid\mathbf{x}_i;\mathbf{\omega}_i,b\right)\end{split}$$</span></p><p><span class="math display">$$\begin{split}p\left(y_i\mid\mathbf{x}_i;\mathbf{\omega}_i,b\right)=y_ip_1\left(\hat{\mathbf{x}}_i;\mathbf{\beta}\right)+\left(1-y_i\right)p_0\left(\hat{\mathbf{x}}_i;\mathbf{\beta}\right)\end{split}$$</span></p><h2 id="多分类任务模型">3.3.多分类任务模型</h2><h3 id="一对一ovo">一对一（OvO）</h3><p>拆分阶段：N个类别两两配对，各自训练二分类分类器。</p><p>测试阶段：新样本提交给所有分类器预测，被预测最多的类别为最终类别。</p><h3 id="一对其他ovr">一对其他（OvR）</h3><p>拆分阶段：某一类作为正例，其余作为反例，各自训练二分类分类器。</p><p>测试阶段：新样本提交给所有分类器，使用置信度最大的类别作为最终类别。</p><h3 id="多对多mvm">多对多（MvM）</h3><p>若干类作为正类，若干类作为反类，使用纠错输出码（Error Correcting Output Code，ECOC）进行最终预测：</p><ul><li><strong>编码阶段</strong>：对N个类别做M次划分，每次划分将一部分类别划为正类，一部分划为反类；</li><li><strong>解码阶段</strong>：测试样本交给M个分类器预测；</li><li>最终距离最小的类别为最终类别。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/EOOC.png" width="80%" /></p><h2 id="线性模型的优缺点">3.4.线性模型的优缺点</h2><blockquote><p>优点：</p><p>形式简单、易于建模；</p><p>具有一定的可解释性。</p><p>缺点：</p><p>难以处理非线性问题。</p></blockquote><hr /><h1 id="第四讲支持向量机">第四讲、支持向量机</h1><div class="note primary no-icon flat"><p><strong>考点</strong>：</p><p>（<em>2022，2023</em>）SVM基本型&amp;对偶型推导；</p><p>（<em>2023</em>）核函数。</p></div><p>将训练样本分开的超平面有很多，但是“正中间”的鲁棒性是最好的，泛化能力也最强。</p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/SVM.png" width="60%" /></p><p>SVM优化目标：寻找最大间隔，求解参数 <span class="math inline"><strong>ω</strong></span> 和 <span class="math inline"><em>b</em></span> 使得 <span class="math inline"><em>γ</em></span> 最大。</p><h2 id="基本型和对偶型">4.1.基本型和对偶型</h2><p>优化问题：最大间隔。</p><p><span class="math display">$$\begin{aligned}\underset{\mathbf{\omega}, b}{\arg \max } &amp; \frac{2}{\|\mathbf{\omega}\|} \\\text { s.t. } &amp; y_i\left(\mathbf{\omega}^{\top} \mathbf{x}_i+b\right) \geq 1, i=1,2, \ldots, m .\end{aligned}$$</span></p><p>其可转换为等价问题：凸二次规划问题，这也是<strong>支持向量机的基本型</strong>。</p><p><span class="math display">$$\begin{aligned}\underset{\mathbf{\omega}, b}{\arg \min } &amp; \frac{\|\mathbf{\omega}\|^2}{2} \\\text { s.t. } &amp; y_i\left(\mathbf{\omega}^{\top} \mathbf{x}_i+b\right) \geq 1, i=1,2, \ldots, m .\end{aligned}$$</span></p><p>引入拉格朗日乘子 <span class="math inline"><em>α</em><sub><em>i</em></sub> ≥ 0</span> 可以得到拉格朗日函数：</p><p><span class="math display">$$\begin{split}L(\mathbf{\omega}, b, \mathbf{\alpha})=\frac{1}{2}\|\mathbf{\omega}\|^2+\sum_{i=1}^m \alpha_i\left(1-y_i\left(\mathbf{\omega}^{\mathrm{T}} \mathbf{x}_i+b\right)\right)\end{split}$$</span></p><p>分别对 <span class="math inline"><strong>ω</strong></span> 和 <span class="math inline"><em>b</em></span> 的偏导为零可得：</p><p><span class="math display">$$\begin{split}\mathbf{\omega}=\sum_{i=1}^m\alpha_iy_i\mathbf{x}_i,0=\sum_{i=1}^m\alpha_iy_i\end{split}$$</span></p><p>最后可以得到<strong>支持向量机的对偶型</strong>：</p><p><span class="math display">$$\begin{aligned}\max_{\alpha} &amp; \sum_{i=1}^{m}\alpha_{i}-\frac{1}{2}\sum_{i=1}^{m}\sum_{j=1}^{m}\alpha_{i}\alpha_{j}y_{i}y_{j}\mathbf{x}_{i}^{\mathrm{T}}\mathbf{x}_{j} \\\mathrm{s.t.} &amp; \sum_{i=1}^{m}\alpha_{i}y_{i}=0,\alpha_{i}\geqslant0,\quad i=1,2,\ldots,m\end{aligned}$$</span></p><h2 id="特征空间映射">4.2.特征空间映射</h2><p>若不存在一个能正确划分两类样本的超平面，我们可以将样本从原始空间映射到一个更高维的特征空间，使样本在这个特征空间内线性可分。<em>如果原始空间是有限维（特征数有限），那么一定存在一个高维特征空间是样本可分</em>。</p><p>设样本 <span class="math inline"><strong>x</strong></span> 映射后的向量为 <span class="math inline"><em>ϕ</em>(<strong>x</strong>)</span> ，划分超平面为：</p><p><span class="math display">$$\begin{split}    f(\mathbf{x}) = \mathbf{\omega}^{T}\phi(\mathbf{x})+b\end{split}$$</span></p><p>则原基本型和对偶型可推广为：</p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250604142808.png" width="60%" /></p><p><strong>核函数</strong>：用于绕过显式考虑特征映射、以及计算高维内积的困难。</p><p><span class="math display">$$\begin{split}    \kappa(\mathbf{x}_i,\mathbf{x}_j)=\phi(\mathbf{x}_i)^\mathrm{T}\phi(\mathbf{x}_j)\end{split}$$</span></p><blockquote><p>Mercer定理：若一个对称函数所对应的核矩阵<strong>半正定</strong>，则它就能作为核函数来使用。</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250604143436.png" width="80%" /></p><h2 id="软间隔支持向量机">4.3.软间隔支持向量机</h2><p>软间隔SVM不再假设所有样本都可分，而是引入损失函数，计算每个样本的损失，然后再<strong>最大化间隔和最小化整体损失之间做个合理的折中</strong>：</p><p><span class="math display">$$\begin{aligned}\min_{\mathbf{\omega},b}\frac{1}{2}\|\mathbf{\omega}\|^2+C\sum_{i=1}^ml_{0/1}\left(y_i(\mathbf{\omega}^\top\phi(\mathbf{x}_i)+b)-1\right) \\l_{0/1}=\begin{cases}1 &amp; z&lt;0 \\0 &amp; otherwise &amp; \end{cases}\end{aligned}$$</span></p><p>存在的问题是0/1损失函数非凸非连续，因此这里使用Hinge损失函数来替代0/1损失函数：</p><p><span class="math display">$$\begin{split}\mathrm{Hinge}(y,f(x))=\max(0,1-y\cdot f(x))\end{split}$$</span></p><p>由此可以分别得到软间隔支持向量机的基本型和对偶型：</p><ol type="1"><li>原始问题</li></ol><p><span class="math display">$$\begin{split}\min_{\mathbf{\omega},b}\frac{1}{2}\|\mathbf{\omega}\|^2+C\sum_{i=1}^m\max\left(0,1-y_i(\mathbf{\omega}^\top\phi(\mathbf{x}_i)+b)\right).\end{split}$$</span></p><ol start="2" type="1"><li>对偶问题</li></ol><p><span class="math display">$$\begin{aligned}\operatorname*{min}_{\alpha} &amp;\frac{1}{2}\sum_{i=1}^{m}\sum_{j=1}^{m}\alpha_{i}\alpha_{j}y_{i}y_{j}\phi(\mathbf{x}_{i})^{\top}\phi(\mathbf{x}_{j})-\sum_{i=1}^{m}\alpha_{i} \\\mathrm{s.t.} &amp;\sum_{i=1}^{m}\alpha_{i}y_{i}=0,0\leq\alpha_{i}\leq C,i=1,2,\ldots,m.\end{aligned}$$</span></p><h2 id="svm拓展正则化">4.4.SVM拓展–正则化</h2><p>统计学习模型的更一般形式可以表述为：</p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250604144936.png" width="60%" /></p><p>其中正则化可理解为罚函数，使优化过程趋向于期望目标。</p><hr /><h1 id="第五讲神经网络">第五讲、神经网络</h1><div class="note primary no-icon flat"><p><strong>考点</strong>：</p><p>（<em>2022，2023</em>）多层前馈网络的计算能力、局限性、解决方法。</p></div><h2 id="神经网络历史">5.1.神经网络历史</h2><ul><li>第一阶段（1943~1969）：以感知机为代表；</li><li>第二阶段（1982~2000）：反向传播算法为代表，Hopfield网络；</li><li>第三阶段（2006~迄今）：深度网络为代表。</li></ul><h2 id="神经元模型">5.2.神经元模型</h2><p>M-P神经元模型 <em>[McCulloch and Pitts, 1943]</em></p><ul><li>输入：来自其他 n个神经元传递过来的信号</li><li>处理：通过带权重连接进行传递, 总值与神经元的阈值比较</li><li>输出：通过激活函数得到输出</li></ul><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250604145756.png" width="60%" /></p><blockquote><p>常用激活函数：<strong>Sigmoid函数</strong></p><p><span class="math display">$$\operatorname{sigmoid}(x)=\frac{1}{1+e^{-x}}$$</span></p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250604150040.png" width="30%" /></p></blockquote><h2 id="感知机与多层网络">5.3.感知机与多层网络</h2><p><strong>单层感知机</strong>：由两层神经元组成，输入层接受外界输入信号传递给输出层M-P神经元。</p><p>当两类模式线性可分时，感知机的学习过程一定会收敛，否则其学习过程会发生震荡。因此其无法解决非线性可分问题。</p><p><strong>多层感知机</strong>：输入层和输出层中存在多层神经元，称为隐层或隐含层，其中隐层和输出层神经元都是具有激活功能的功能神经元。</p><ul><li>定义：每两层神经元全互联，不存在同层连接和跨层连接；</li><li>前馈：接受外界输入信号，隐含层与输出层神经元对信号进行加工输出；</li><li>学习：根据训练数据调整神经元的“连接权”以及功能神经元的“阈值”。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250604151125.png" width="50%" /></p><h2 id="误差逆传播算法">5.4.误差逆传播算法</h2><p><strong>误差逆传播算法</strong>（Error BackPropagation，反向传播）是最常用的多层前馈神经网络的学习算法：</p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250604152717.png" width="50%" /></p><ol type="1"><li>前向计算：</li></ol><p><span class="math display">$$\begin{split}\mathrm{step1:~}b_h=f(\alpha_h-\gamma_h),\alpha_h=\sum_{i=1}^dv_{ih}x_i\end{split}$$</span></p><p><span class="math display">$$\begin{split}\mathrm{step2:}\quad\hat{y}_j^k=f(\beta_j-\theta_j),\beta_j=\sum_{i=q}^dw_{hj}b_h\end{split}$$</span></p><p><span class="math display">$$\begin{split}\mathrm{step3:~}E_k=\frac{1}{2}\sum_{j=1}^l(\hat{y}_j^k-y_j^k)^2\end{split}$$</span></p><ol start="2" type="1"><li>参数数量：</li></ol><ul><li><p>对于一层到二层的权重 <span class="math inline"><em>v</em><sub><em>i</em><em>h</em></sub></span> ，总共有 <span class="math inline"><em>d</em> ⋅ <em>q</em></span> 个参数需要优化；</p></li><li><p>对于二层到三层的权重 <span class="math inline"><em>w</em><sub><em>h</em><em>j</em></sub></span> ，总共有 <span class="math inline"><em>q</em> ⋅ <em>l</em></span> 个参数需要优化；</p></li><li><p>对于二层的阈值 <span class="math inline"><em>θ</em><sub><em>j</em></sub></span> ，总共有 <span class="math inline"><em>q</em></span> 个参数需要优化；</p></li><li><p>对于三层的阈值 <span class="math inline"><em>γ</em><sub><em>h</em></sub></span> ，总共有 <span class="math inline"><em>l</em></span> 个参数需要优化。</p></li></ul><p>因此网络总共有 <span class="math inline">(<em>d</em> + <em>l</em> + 1)<em>q</em> + <em>l</em></span> 个参数需要优化。</p><ol start="3" type="1"><li>参数优化：</li></ol><p>BP算法基于<strong>梯度下降</strong>策略，以误差率为目标，计算负梯度方向对参数进行调整。</p><p>比如根据之前的前向过程，这里可以对二层的权重参数计算反向传播：</p><p><span class="math display">$$\begin{split}    \Delta w_{hj}=-\eta\frac{\partial E_k}{\partial w_{hj}}\end{split}$$</span></p><p><span class="math display">$$\begin{split}    \frac{\partial E_k}{\partial w_{hj}}=\frac{\partial E_k}{\partial\hat{y}_j^k}\cdot\frac{\partial\hat{y}_j^k}{\partial\beta_j}\cdot\frac{\partial\beta_j}{\partial w_{hj}}\end{split}$$</span></p><p>其中 <span class="math inline"><em>η</em> ∈ (0, 1)</span> 是学习率，控制着算法每一轮迭代中的更新步长。</p><blockquote><p>Sigmoid函数在求导时有一个很好的性质：</p><p><span class="math display">$$\begin{split}f^{\prime}\left(x\right)=f\left(x\right)\left(1-f\left(x\right)\right)\end{split}$$</span></p></blockquote><p><strong>BP的不同实现方式</strong>：</p><ul><li>标准BP：每次对单个训练样例更新权值和阈值，单次计算开销小但参数更新频繁，迭代次数多；</li><li>累计BP：最小化整个训练集上的累积误差，一般是读取整个训练集后更新参数，参数更新频率低但计算开销大；</li></ul><p><span class="math display">$$\begin{split}    E=\frac{1}{m}\sum_{k=1}^mE_k\end{split}$$</span></p><p>多层前馈网络具有强大的学习能力，包含足够多神经元的隐层，多层前馈神经网络能以任意精度逼近任意复杂度的连续函数。</p><p>但其也有其局限性：</p><ul><li>由于其强大的表达能力，经常遭遇过拟合；</li><li>如何设置隐藏神经元个数是个难题，实际应用中常使用试错法。</li></ul><p>缓解过拟合的一些策略包括：</p><ul><li>早停：在训练过程中, 若训练误差降低, 但验证误差升高, 则停止训练；</li><li>正则化：在误差目标函数中增加一项描述网络复杂程度的成分, 防止模型过于复杂，例如连接权值与阈值的平方和。</li></ul><p><span class="math display">$$\begin{split}    E=\lambda\frac{1}{m}\sum_{k=1}^mE_k+(1-\lambda)\sum_iw_i^2\end{split}$$</span></p><h2 id="深度学习">5.5.深度学习</h2><p>深度学习模型是具有很多个隐层的神经网络。一方面，计算能力的大幅提高缓解了训练效率；另一方面，训练数据的大幅增加降低了过拟合风险。因此，以“深度学习” (deep learning) 为代表的复杂模型成为了合适的选择。</p><blockquote><p><strong>复杂模型带来的困难</strong>：</p><p>深度网络难以直接用经典算法（例如BP算法）进行训练，因为误差在多隐层内传播时会出现梯度消失问题（即梯度迅速为0），难以收敛到稳定状态。</p></blockquote><p><strong>训练方法</strong>：</p><ul><li>预训练+微调：将大量参数进行分组，局部先找到较好的设置，然后再基于局部较优的结果进行全局寻优。</li><li>权共享：一组神经元使用相同的连接权值，权共享策略在卷积神经网络（CNN）中发挥了重要作用。</li></ul><hr /><h1 id="第六讲决策树">第六讲、决策树</h1><div class="note primary no-icon flat"><p><strong>考点</strong>：</p><p>（<em>2023</em>）决策树最优划分的两个准则及其偏好；</p><p>（<em>2022</em>）决策树过拟合原因及其解决方案。</p></div><h2 id="基本流程">6.1.基本流程</h2><p>策略：<strong>分而治之</strong></p><p>三种停止条件：</p><ul><li>当前节点包含的样本全属于同一类别，无需划分；</li><li>当前属性集为空，或是所有样本在所有属性上取值相同，无法划分；</li><li>当前节点包含的样本集为空，不能划分。</li></ul><h2 id="划分算法">6.2.划分算法</h2><p>直觉上，决策树的分支结点所包含的样本应尽可能属于同一类别，即结点的<strong>纯度</strong>越高越好。</p><h3 id="信息增益id3决策树">信息增益（ID3决策树）</h3><p>信息熵（information entropy）是度量样本集合纯度最常用的一种指标，假定当前样本集合 <span class="math inline"><em>D</em></span> 中第 <span class="math inline"><em>k</em></span> 类样本所占的比例为 <span class="math inline"><em>p</em><sub><em>k</em></sub>(<em>k</em> = 1, 2, ..., |<em>y</em>|)</span>，则 <span class="math inline"><em>D</em></span> 的信息熵定义为：</p><p><span class="math display">$$\begin{split}    \mathrm{Ent}(D)=-\sum_{k=1}^{|y|}p_k\log_2p_k\end{split}$$</span></p><p><strong><span class="math inline"><em>E</em><em>n</em><em>t</em>(<em>D</em>)</span> 的值越小，<span class="math inline"><em>D</em></span> 的纯度越高</strong>。因此我们可以通过计算划分前后信息熵的差值来判断是否是最优划分，即计算<strong>信息增益</strong>：</p><p><span class="math display">$$\begin{split}    \mathrm{Gain}(D,a)=\mathrm{Ent}(D)-\sum_{v=1}^V\frac{|D^v|}{|D|}\mathrm{Ent}(D^v)\end{split}$$</span></p><p>这里的 <span class="math inline"><em>v</em></span> 表示目前属性 <span class="math inline"><em>a</em></span> 可能的取值，每一步划分时选择的属性就是最大的信息增益对应的属性，即ID3算法：</p><p><span class="math display">$$\begin{split}    a_{*}=\underset{a\in A}{\operatorname*{\operatorname*{\arg\max}}}\mathrm{Gain}(D,a)\end{split}$$</span></p><h3 id="增益率c4.5决策树">增益率（C4.5决策树）</h3><p>事实上，信息增益准则对可取值数目较多的属性有所偏好（比如气球的种类可能只有氢气球和氦气球等，但是颜色有很多种，这样计算颜色的信息增益一般都很大，因为平均到每个颜色的样本数相较要少、显得纯度更高），为减少这种偏好可能带来的不良影响，C4.5算法使用<strong>增益率</strong>（gain ratio）来选择最优划分属性：</p><p><span class="math display">$$\begin{split}    \mathrm{Gain_ratio}(D,a)=\frac{\mathrm{Gain}(D,a)}{\mathrm{IV}(a)}\end{split}$$</span></p><p><span class="math display">$$\begin{split}    \mathrm{IV}(a)=-\sum_{v=1}^V\frac{|D^v|}{|D|}\mathrm{log}_2\frac{|D^v|}{|D|}\end{split}$$</span></p><p>相当于是给信息增益做了一步规范化，<span class="math inline"><em>I</em><em>V</em>(<em>a</em>)</span> 称为属性 <span class="math inline"><em>a</em></span> 的固有值。一般属性的可能取值数目越多，其固有值越大。</p><h2 id="剪枝处理">6.3.剪枝处理</h2><p>决策树决策分支过多，以致于把训练集自身的一些特点当做所有数据都具有的一般性质，因此可能会导致<strong>过拟合</strong>。通常的解决办法是预留一部分数据用作“验证集”以进行性能评估，并采取剪枝策略来提升泛化性能。</p><h3 id="预剪枝">预剪枝</h3><p>思路：<strong>边建树，边剪枝</strong>。决策树生成过程中，对每个结点在划分前先进行估计，若当前结点的划分不能带来决策树泛化性能提升，则停止划分并将当前结点记为叶结点，其类别标记为训练样例数最多的类别。</p><blockquote><p><em>优缺点</em>：</p><p>优点：降低过拟合风险，显著减少训练时间和测试时间开销</p><p>缺点：存在欠拟合风险，基于贪心本质，有些分支当前划分虽然不能提升泛化性能，但在其基础上的后续划分可能导致性能提高</p></blockquote><h3 id="后剪枝">后剪枝</h3><p>思路：<strong>先建树，后剪枝</strong>。决策树生成后，自底向上对每个结点进行考察，若当前结点的划分导致在验证集上的精度下降，则将其剪除。</p><blockquote><p><em>优缺点</em>：</p><p>优点：比预剪枝保留了更多分支，欠拟合风险小；</p><p>缺点：训练时间开销大，后剪枝过程是在生成完全决策树之后进行的，需要自底向上对所有非叶结点逐一考察；其训练时间要远大于预剪枝决策树。</p></blockquote><h2 id="多变量决策树">6.4.多变量决策树</h2><p>非叶结点不再是仅仅针对某个属性，而是对属性的线性组合。</p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250604205006093.png" width="50%" /></p><hr /><h1 id="第七讲贝叶斯分类器">第七讲、贝叶斯分类器</h1><div class="note primary no-icon flat"><p><strong>考点</strong>：</p><p>（<em>2018</em>）最优贝叶斯分类器及贝叶斯风险；</p><p>（<em>2019，2022，2023</em>）生成式&amp;判别式模型的区别。</p></div><h2 id="贝叶斯决策论">7.1.贝叶斯决策论</h2><p>给定 <span class="math inline"><em>N</em></span> 个类别，令 <span class="math inline"><em>λ</em><sub><em>i</em><em>j</em></sub></span> 表示将第 <span class="math inline"><em>j</em></span> 类样本误分类为第 <span class="math inline"><em>i</em></span> 类所产生的损失，则基于后验概率将样本 <span class="math inline"><em>x</em></span> 分到第 <span class="math inline"><em>i</em></span> 类的条件风险为：</p><p><span class="math display">$$\begin{split}    R(c_i\mid\mathbf{x})=\sum_{j=1}^N\lambda_{ij}P(c_j\mid x)\end{split}$$</span></p><p><strong>贝叶斯判定准则</strong>（Bayes Decision Rule）：</p><p><span class="math display">$$\begin{split}    h^*(x)=\underset{c\in\mathcal{Y}}{\operatorname*{\arg\min}}R(c\mid\mathbf{x})\end{split}$$</span></p><p><span class="math inline"><em>h</em><sup>*</sup>(<em>x</em>)</span> 称为贝叶斯最优分类器，其总体风险称为贝叶斯风险，反映了学习性能的理论上限。</p><p>机器学习需要实现的是基于有限的训练样本尽可能准确地估计出后验概率：</p><p><span class="math display">$$\begin{split}    P(c\mid x)=\frac{P(c)P(x\mid c)}{P(x)}\end{split}$$</span></p><blockquote><p><strong>判别式 VS 生成式</strong>：</p><p>1.判别式（discriminative）模型</p><p>直接对 <span class="math inline"><em>P</em>(<em>c</em> ∣ <em>x</em>)</span> 建模，代表方法：决策树、BP神经网络、SVM</p><p>2.生成式（generative）模型</p><p>先对联合概率分布 <span class="math inline"><em>P</em>(<em>x</em>, <em>c</em>)</span> 建模，再由此获得 <span class="math inline"><em>P</em>(<em>c</em> ∣ <em>x</em>)</span>，代表方法：贝叶斯分类器</p></blockquote><h2 id="朴素贝叶斯">7.2.朴素贝叶斯</h2><p>主要障碍是所有属性上的联合概率难以从有限训练样本估计获得。因此我们不妨<strong>假定所有属性之间相互独立</strong>，可以得到：</p><p><span class="math display">$$\begin{split}    P(c\mid\mathbf{x})=\frac{P(c)P(\mathbf{x}\mid c)}{P(\mathbf{x})}=\frac{P(c)}{P(\mathbf{x})}\prod_{i=1}^dP(x_i\mid c)\end{split}$$</span></p><p>由于 <span class="math inline"><em>P</em>(<strong>x</strong>)</span> 对所有类别相同，由此可以得到：</p><p><span class="math display">$$\begin{split}    h_{nb}(x)=\arg\max_{c\in\mathcal{Y}}P(c)\prod_{i=1}^dP(x_i\mid c)\end{split}$$</span></p><p>接下来分两步计算上式：</p><ul><li>估计 <span class="math inline"><em>P</em>(<em>c</em>)</span></li></ul><p><span class="math display">$$\begin{split}    P(c)=\frac{|D_c|}{|D|}\end{split}$$</span></p><ul><li>估计 <span class="math inline"><em>P</em>(<strong>x</strong> ∣ <em>c</em>)</span></li></ul><p>对离散属性，有：</p><p><span class="math display">$$\begin{split}    P(x_i\mid c)=\frac{|D_{c,x_i}|}{|D_c|}\end{split}$$</span></p><p>对连续属性，使用概率密度函数：</p><p><span class="math display">$$\begin{split}    p\left(x_i \mid c\right)=\frac{1}{\sqrt{2 \pi} \sigma_{c, i}} \exp \left(-\frac{\left(x_i-\mu_{c, i}\right)^2}{2 \sigma_{c, i}^2}\right)\end{split}$$</span></p><p>这个思路有一个问题，一旦有一个属性值在训练集中没有在某个类别中出现过，则由频率估计概率，该属性值的概率为0，直接连乘会导致整个 <span class="math inline"><em>P</em>(<strong>x</strong> ∣ <em>c</em>)</span> 为0。因此为了避免这种情况，这里使用<strong>拉普拉斯修正</strong>：</p><p><span class="math display">$$\begin{split}    \hat{P}(c)=\frac{|D_c|+1}{|D|+N},\quad\hat{P}(x_i\mid c)=\frac{|D_{c,x_i}|+1}{|D_c|+N_i}\end{split}$$</span></p><p>其中 <span class="math inline"><em>N</em></span> 表示训练集 <span class="math inline"><em>D</em></span> 中可能的类别数，<span class="math inline"><em>N</em><sub><em>i</em></sub></span> 表示第 <span class="math inline"><em>i</em></span> 个属性可能的取值数。</p><h2 id="半朴素贝叶斯">7.3.半朴素贝叶斯</h2><p>朴素贝叶斯分类器的<strong>属性独立性假设</strong>在现实情况中往往难以成立，因此半朴素贝叶斯分类器（Semi-naïve Bayes Classifier）会适当考虑一部分属性间的相互依赖关系。其中最常用的策略是<strong>独依赖估计</strong>（One-Dependent Estimator，ODE），即假设每个属性在类别之外最多仅依赖一个其他属性：</p><p><span class="math display">$$\begin{split}    P(c\mid x)\propto P(c)\prod_{i=1}^dP(x_i\mid c,\boxed{pa_i})\end{split}$$</span></p><p>这个其他属性 <span class="math inline"><em>p</em><em>a</em><sub><em>i</em></sub></span> 也叫做 <span class="math inline"><em>x</em><sub><em>i</em></sub></span> 的<strong>父属性</strong>，那么该如何确定父属性呢？</p><ul><li>SPODE（Super-Parent ODE）：假设所有属性依赖于同一属性<strong>超父</strong>，通过交叉验证等模型选择方法来确定超父属性；</li><li>TAN（Tree Augmented naïve Bayes）：以属性间的条件互信息为边的权重，构建完全图利用最大带权生成树算法，进保留相关属性间的依赖性。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250605142948145.png" width="70%" /></p><p><strong>AODE</strong>（Averaged One-Dependent Estimator）尝试将每个属性作为超父来构建SPODE，最后将拥有足够训练数据支撑的 SPODE集成起来作为最终结果：</p><p><span class="math display">$$\begin{split}    P(c\mid\mathbf{x})\propto\sum_{\begin{array}{c}i=1 \quad |D_{x_i}|\geqslant m^{\prime}\end{array}}^dP(c,x_i)\prod_{j=1}^dP(x_j\mid c,x_i)\end{split}$$</span></p><p>其中 <span class="math inline"><em>m</em><sup>′</sup></span> 为阈值常数，用于保留拥有足量训练数据的SPODE。同样AODE在计算时也使用了拉普拉斯修正：</p><p><span class="math display">$$\begin{split}    \hat{P}(c,x_i)=\frac{|D_{c,x_i}|+1}{|D|+N_i},\quad\hat{P}(x_j\mid c,x_i)=\frac{|D_{c,x_i,x_j}|+1}{|D_{c,x_i}|+N_j}\end{split}$$</span></p><blockquote><p>高阶依赖：ODE <span class="math inline">→</span> kDE</p><p>明显障碍：随着 k 的增加，估计 <span class="math inline"><em>P</em>(<em>x</em><sub><em>j</em></sub> ∣ <em>c</em>, <em>p</em><em>a</em><sub><em>i</em></sub>)</span> 所需的样本数将以指数级增加。</p></blockquote><h2 id="贝叶斯网">7.4.贝叶斯网</h2><p>了解即可。</p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250605144759105.png" width="70%" /></p><hr /><h1 id="第八讲集成学习">第八讲、集成学习</h1><h2 id="个体与集成">8.1.个体与集成</h2><p>集成学习（ensemble learning）通过多学习器来提升性能：</p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250605145420227.png" width="50%" /></p><p>关键：<strong>好而不同</strong></p><p>假设基分类器的错误率相互独立，为 <span class="math inline"><em>P</em>(<em>h</em><sub><em>i</em></sub>(<strong>x</strong>)≠<em>f</em>(<strong>x</strong>)) = <em>ϵ</em></span> ，则由Hoeffding不等式可得，如果我们从一个有限范围内独立采样了 n个样本，那么样本均值偏离期望值超过 <span class="math inline"><em>ϵ</em></span> 的概率会以指数速度下降（关于样本数量n）。即在一定条件下，随着集成分类器数目的增加，集成的错误率将指数级下降，最终趋向于0。</p><p>但是这里有一个关键假设，就是基学习器的误差相互独立，然而在现实生活中个体学习器来自同一个问题，显然不可能完全独立。因此如何产生好而不同的个体学习器是集成学习研究的核心。</p><p>集成学习大致可分为两大类：串行 VS 并行。</p><h2 id="boosting">8.2.Boosting</h2><p>特征：</p><ul><li>每次调整训练数据的样本分布</li><li>串行生成</li><li>个体学习器间存在强依赖关系</li></ul><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250605152544770.png" width="70%" /></p><p>其中Boosting算法中最重要的是<strong>AdaBoost</strong>算法：</p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250605153817270.png" width="70%" /></p><p>总的来说，其流程可以总结为：</p><ul><li>初始化训练数据的权值分布 <span class="math inline">$D_1=\frac{1}{m}$</span> ，其中m代表有m个样本数据；</li><li>训练弱分类器 <span class="math inline"><em>h</em><sub><em>i</em></sub></span>，如果某个训练样本被弱分类器准确分类，则在构造下一个训练集时其对应的权值会减小；相反，如果某个训练样本被错误分类，其权值则会增大，相当于在下一次训练时加大对其的注意力。最后整个训练过程迭代 <span class="math inline"><em>T</em></span> 次，最终有 <span class="math inline"><em>T</em></span> 个弱分类器；</li><li>最终各个弱分类器被组合成一个强分类器，其中分类误差小的分类器其权值被加大，使其起到更大的决定性作用。</li></ul><h2 id="bagging与随机森林">8.3.Bagging与随机森林</h2><h3 id="bagging">Bagging</h3><p>Bagging算法首先随机采样原数据集 <span class="math inline"><em>D</em></span> 得到不同分布的数据集 <span class="math inline"><em>D</em><sub>1</sub>, <em>D</em><sub>2</sub>, ..., <em>D</em><sub><em>T</em></sub></span> 用于给不同的基学习器学习（由于不存在依赖关系，这一步可以并行执行），最终得到 <span class="math inline"><em>T</em></span> 个弱学习器，使用<strong>投票法</strong>来结合弱学习器得到最终的强学习器。</p><p>特征：</p><ul><li>个体学习器不存在强依赖关系</li><li>并行化生成</li><li>自助采样法</li></ul><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250605162320683.png" width="50%" /></p><p>可进一步使用包外估计，即仅考虑那些未使用样本<span class="math inline"><em>x</em></span>训练的基学习器在<span class="math inline"><em>x</em></span>上的预测：</p><p><span class="math display">$$\begin{split}    H^{oob}(\mathbf{x})=\arg\max_{y\in\mathcal{Y}}\sum_{t=1}^T\mathbb{I}(h_t(\mathbf{x})=y)\cdot\mathbb{I}(\mathbf{x}\notin D_t)\end{split}$$</span></p><h3 id="随机森林">随机森林</h3><p>随机森林（Random Forest）是一个包含多个<strong>决策树</strong>的分类器，其输出类别由个别树输出的类别的<strong>众数</strong>而定。</p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250605163249.png" width="80%" /></p><p>构造一个随机决策树的过程：</p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250605163737563.png" width="50%" /></p><h2 id="结合策略">8.4.结合策略</h2><ul><li>平均法/加权平均法：最基本的方法</li><li>投票法：<ul><li>绝对多数投票法：一个基学习器占据半数优势时才会选择它；</li><li>相对多数投票法：选择票数最多的结果</li><li>加权投票法：相对多数的加权版本</li></ul></li><li>Stacking学习法</li></ul><h2 id="多样性">8.5.多样性</h2><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250605165432245.png" width="50%" /></p><p>基学习器学得越准确以及越多样性（好而不同），集成学习越好。</p><p>常见的增强个体学习器多样性的方法：</p><ol type="1"><li><strong>数据样本扰动</strong>：通常基于采样法的不同，比如Bagging的自助采样、AdaBoost的序列采样</li></ol><blockquote><p>数据样本扰动对“不稳定基学习器”很有效：</p><p><strong>不稳定基学习器</strong>包括：决策树、神经网络等；</p><p><strong>稳定基学习器</strong>包括：线性学习器、支持向量机、朴素贝叶斯、k近邻等。</p></blockquote><ol start="2" type="1"><li><strong>输入属性扰动</strong>：每次随机选择一部分输入数据维度进行学习</li></ol><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250605171633933.png" width="50%" /></p><ol start="3" type="1"><li><p><strong>输出属性扰动</strong>：输出表示扰动</p><ul><li>翻转法：随机改变输入样本的标记</li><li>输出调剂法：分类输出改为回归输出得到分类器</li><li>ECOC法：多类任务分解为一系列两类任务来求解</li></ul></li><li><p><strong>算法参数扰动</strong>：采用负相关法，强制要求个体神经网络采用不同的参数</p></li></ol><hr /><h1 id="第九讲聚类算法">第九讲、聚类算法</h1><div class="note primary no-icon flat"><p><strong>考点</strong>：</p><p>（<em>2018</em>）三种算法的举例；</p><p>（<em>2018</em>）距离度量的四个性质并证明；</p><p>（<em>2022</em>）K-means过程，k值如何选；</p><p>（<em>2023</em>）密度聚类和层次聚类的代表算法、关键假设。</p></div><h2 id="聚类定义">9.1.聚类定义</h2><p>目标：将数据样本划分为若干个通常不相交的簇（cluster）</p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250605172510980.png" width="70%" /></p><h2 id="性能度量-1">9.2.性能度量</h2><p>聚类性能度量，也称为聚类有效性指标，需要做到<strong>簇内相似度</strong>（intra-cluster similarity）<strong>高且簇间相似度</strong>（inter-cluster similarity）<strong>低</strong>。</p><ul><li>外部指标：将聚类结果与某个<em>参考模型</em>进行比较</li></ul><blockquote><p><strong>补充</strong>：</p><p>定义：TP–同属一个类并被聚在一起的样本对数、FP–不同类但被聚在一起的样本对数、FN–同类但被分在不同簇、TN–不同类且被分开</p><p>1.<strong>Jaccard系数</strong>：衡量两个集合的相似度</p><p><span class="math display">$$\begin{split}  J=\frac{TP}{TP+FP+FN}\end{split}$$</span></p><p>2.<strong>FM 指数</strong>：P和R的几何平均</p><p><span class="math display">$$\begin{split}  \mathrm{FM}=\sqrt{\frac{TP}{TP+FP}\cdot\frac{TP}{TP+FN}}\end{split}$$</span></p><p>3.<strong>Rand指数</strong>：表示聚类结果中有多少比例的样本对被正确分类</p><p><span class="math display">$$\begin{split}  \mathrm{Rand}=\frac{TP+TN}{TP+TN+FP+FN}\end{split}$$</span></p></blockquote><ul><li>内部指标：直接考察聚类结果，无参考模型</li></ul><blockquote><p><strong>补充</strong>：</p><p>1.<strong>DB指数</strong>：衡量簇内部和外部距离的指标</p><p><span class="math display">$$\begin{split}  \mathrm{DB}=\frac{1}{k}\sum_{i=1}^k\max_{j\neq i}\left(\frac{s_i+s_j}{d_{ij}}\right)\end{split}$$</span></p><p>其中 <span class="math inline"><em>s</em><sub><em>i</em></sub></span> 表示第 <span class="math inline"><em>i</em></span> 个簇的内部平均距离，<span class="math inline"><em>d</em><sub><em>i</em><em>j</em></sub></span> 表示第 <span class="math inline"><em>i</em></span> 和 <span class="math inline"><em>j</em></span> 簇之间的中心距离。该指标越小越好，表示簇内部越紧凑、簇间分离越好。</p><p>2.<strong>Dunn指数</strong>：衡量簇内部和外部距离的指标</p><p><span class="math display">$$\begin{split}  \mathrm{Dunn}=\frac{\min_{i\neq j}d(C_i,C_j)}{\max_k\delta(C_k)}\end{split}$$</span></p><p>分子表示不同簇之间的最小距离，分母则指代单个簇内部的最大直径，越大越好</p></blockquote><h2 id="距离计算">9.3.距离计算</h2><p>距离度量的意义：聚类来自于分组，分组来自于合理度量，度量来自于距离，因此距离对聚类有很本质的作用。</p><p>需满足的基本性质：</p><ul><li><strong>非负性</strong>：<span class="math inline">dist (<strong>x</strong><sub><em>i</em></sub>, <strong>x</strong><sub><em>j</em></sub>) ≥ 0</span> ；</li><li><strong>同一性</strong>：<span class="math inline">dist (<strong>x</strong><sub><em>i</em></sub>, <strong>x</strong><sub><em>j</em></sub>) = 0 当且仅当 <strong>x</strong><sub><em>i</em></sub> = <strong>x</strong><sub><em>j</em></sub></span> ；</li><li><strong>对称性</strong>：<span class="math inline">dist (<strong>x</strong><sub><em>i</em></sub>, <strong>x</strong><sub><em>j</em></sub>) = dist (<strong>x</strong><sub><em>j</em></sub>, <strong>x</strong><sub><em>i</em></sub>)</span> ；</li><li><strong>直递性</strong>：<span class="math inline">dist (<strong>x</strong><sub><em>i</em></sub>, <strong>x</strong><sub><em>j</em></sub>) &lt; dist (<strong>x</strong><sub><em>i</em></sub>, <strong>x</strong><sub><em>k</em></sub>) + dist (<strong>x</strong><sub><em>k</em></sub>, <strong>x</strong><sub><em>j</em></sub>)</span>.</li></ul><p>常用的距离形式：<strong>闵可夫斯基距离</strong>（Minkowski Distance）</p><p><span class="math display">$$\begin{split}    \mathrm{dist}_{\mathrm{mk}}(\boldsymbol{x}_{i},\boldsymbol{x}_{j})=\left(\sum_{u=1}^{n}\left|x_{iu}-x_{ju}\right|^{p}\right)^{\frac{1}{p}}\end{split}$$</span></p><blockquote><p>当 <span class="math inline"><em>p</em> = 2</span> 时，又称为欧氏距离（Euclidean distance）；</p><p>当 <span class="math inline"><em>p</em> = 1</span> 时，又称为曼哈顿距离（Manhattan distance）。</p></blockquote><p>对于无序（non-ordinal）属性，可使用VDM（Value Difference Metric）：</p><p><span class="math display">$$\begin{split}    VDM_p(a,b)=\sum_{i=1}^k\left|\frac{m_{u,a,i}}{m_{u,a}}-\frac{m_{u,b,i}}{m_{u,b}}\right|^p\end{split}$$</span></p><p>上式表示属性 <span class="math inline"><em>u</em></span> 上两个离散值 <span class="math inline"><em>a</em></span> 和 <span class="math inline"><em>b</em></span> 之间的VDM距离，其中 <span class="math inline"><em>k</em></span> 为样本簇数，<span class="math inline"><em>m</em><sub><em>u</em>, <em>a</em></sub></span>表示属性<span class="math inline"><em>u</em></span>上取值为<span class="math inline"><em>a</em></span>的样本数，<span class="math inline"><em>m</em><sub><em>u</em>, <em>a</em>, <em>i</em></sub></span>表示第<span class="math inline"><em>i</em></span>个样本簇中在属性<span class="math inline"><em>u</em></span>上取值为<span class="math inline"><em>a</em></span>的样本数。</p><p>对于混合属性，可使用MinkovDM：</p><p><span class="math display">$$\begin{split}    \mathrm{MinkovDM}_p(\boldsymbol{x}_i,\boldsymbol{x}_j)=\left(\sum_{u=1}^{n_c}|x_{iu}-x_{ju}|^p+\sum_{u=n_c+1}^n\mathrm{VDM}_p(x_{iu},x_{ju})\right)^{\frac{1}{p}}\end{split}$$</span></p><h2 id="聚类算法">9.4.聚类算法</h2><p>常见聚类方法可分为以下几类：</p><ul><li><strong>原型聚类</strong>：有簇中心的聚类方法。先对圆形初始化，然后对原型进行迭代更新求解。<em>代表算法：K-means、LVQ、高斯混合</em>；</li><li><strong>密度聚类</strong>：划分为多个等价类，未必有簇中心。从样本密度的角度来考察样本之间的可连接性，并基于可连接样本不断扩展聚类簇。<em>代表算法：DBSCAN、OPTICS、DENCLUE</em>；</li><li><strong>层次聚类</strong>：在不同层次对数据集进行划分，从而形成树形聚类结构。<em>代表算法：AGNES、DIANA</em>。</li></ul><h3 id="原型聚类">原型聚类</h3><ol type="1"><li><p><strong>K-means（K均值聚类）</strong>：每个簇中心以该簇中所有样本点的均值表示</p><ul><li>Step1: 随机选取 k 个样本点作为簇中心；</li><li>Step2: 将其他样本点根据其与簇中心的距离，划分给最近的簇；</li><li>Step3: 更新各簇的均值向量，将其作为新的簇中心；</li><li>Step4: 若所有簇中心未发生改变，则停止；否则执行 Step 2。</li></ul></li></ol><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250605195805353.png" width="70%" /></p><ol start="2" type="1"><li><strong>学习向量量化（LVQ）</strong>：试图找到一组原型向量来刻画聚类结构，但数据样本带有类别标记，通过聚类来形成类别的子类结构，每个聚类对应于类别的一个子类</li></ol><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250605204347631.png" width="50%" /></p><ol start="3" type="1"><li><strong>高斯混合聚类（Gaussian Mixture Clustering, GMM）</strong>：采用高斯概率分布来表达聚类模型，簇中心=均值，簇半径=方差</li></ol><blockquote><p>n维样本空间中的随机向量x若服从高斯分布，则其概率密度函数为：</p><p><span class="math display">$$\begin{split}    p(\boldsymbol{x})=\frac{1}{(2\pi)^{\frac{n}{2}}|\boldsymbol{\Sigma}|^{\frac{1}{2}}}e^{-\frac{1}{2}(\boldsymbol{x}-\boldsymbol{\mu})^{\mathrm{T}}\boldsymbol{\Sigma}^{-1}(\boldsymbol{x}-\boldsymbol{\mu})}\end{split}$$</span></p></blockquote><p>假设样本由下面这个高斯混合分布生成：</p><p><span class="math display">$$\begin{split}    p_{\mathcal{M}}(\boldsymbol{x})=\sum_{i=1}^k\alpha_i\cdotp(\boldsymbol{x}\mid\boldsymbol{\mu}_i,\boldsymbol{\Sigma}_i)\end{split}$$</span></p><p>其中可以根据 <span class="math inline"><em>α</em><sub>1</sub>, <em>α</em><sub>2</sub>, ..., <em>α</em><sub><em>k</em></sub></span> 定义的先验分布选择高斯混合成分，<span class="math inline"><em>α</em><sub><em>i</em></sub></span> 即为选择第 <span class="math inline"><em>i</em></span> 个混合成分的概率。之后根据被选择的混合成分的概率密度函数进行采样，从而生成相应的样本。</p><p>由此可以得到，样本 <span class="math inline"><strong>x</strong><sub><strong>j</strong></sub></span> 由第 <span class="math inline"><em>i</em></span> 个高斯混合成分生成的后验概率为：</p><p><span class="math display">$$\begin{split}    \gamma_{ji} = p_{\mathcal{M}}(z_{j}=i\mid\boldsymbol{x}_{j})=\frac{P(z_{j}=i)\cdotp_{\mathcal{M}}(\boldsymbol{x}_{j}\mid z_{j}=i)}{p_{\mathcal{M}}(\boldsymbol{x}_{j})}=\frac{\alpha_{i}\cdotp(\boldsymbol{x}_{j}\mid\boldsymbol{\mu}_{i},\boldsymbol{\Sigma}_{i})}{\sum_{l=1}^{k}\alpha_{l}\cdotp(\boldsymbol{x}_{j}\mid\boldsymbol{\mu}_{l},\boldsymbol{\Sigma}_{l})}\end{split}$$</span></p><p>求解时同样使用极大似然估计来进行参数估计：</p><p><span class="math display">$$\begin{split}    LL(D)=\ln\left(\prod_{j=1}^mp_\mathcal{M}(x_j)\right)=\sum_{j=1}^m\ln\left(\sum_{i=1}^k\alpha_i\cdotp(\boldsymbol{x}_j\mid\boldsymbol{\mu}_i,\boldsymbol{\Sigma}_i)\right)\end{split}$$</span></p><p>参数估计完毕后，使用<strong>EM算法</strong>类似K-means求解聚类。</p><blockquote><p><strong>补充：EM算法</strong></p><p>EM算法（Expectation-Maximization Algorithm）是一种迭代优化算法，用于含有隐变量的模型中，常用于估计最大似然参数（Maximum Likelihood Estimation, MLE）或最大后验概率（MAP）。</p><p><strong>核心思想</strong>：EM 算法在估计过程中交替执行两步</p><p>1.<em>E 步（期望步）</em>：在当前参数下，估计隐变量的分布；</p><p>2.<em>M 步（最大化步）</em>：在估计出的隐变量分布下，最大化对数似然函数，更新参数。</p><p>在这里求解GMM时的场景则是：</p><p><strong>E步：</strong>根据当前的参数计算每个样本属于每个高斯成分的后验概率 <span class="math inline"><em>γ</em><sub><em>j</em><em>i</em></sub></span>；（隐变量）</p><p><strong>M步：</strong>更新模型参数 <span class="math inline">{(<em>α</em><sub><em>i</em></sub>, <strong>μ</strong><sub><em>i</em></sub>, <strong>Σ</strong><sub><em>i</em></sub>) ∣ 1 ≤ <em>i</em> ≤ <em>k</em>}</span>。（更新参数）</p></blockquote><h3 id="密度聚类">密度聚类</h3><p><strong>DBSCAN</strong>（Density-Based Spatial Clustering of Applications with Noise）是一种基于密度的聚类算法，非常适合处理具有任意形状的簇、噪声点和离群点的数据集。其基本假设是：<strong>簇是由足够密集的点组成的区域</strong>，它通过考察每个点在其邻域中的“密度”来决定是否属于某个簇。</p><p><strong>关键定义</strong>：</p><ul><li><span class="math inline"><em>ϵ</em></span>-邻域（Epsilon Neighborhood）：以点<span class="math inline"><em>p</em></span>为中心、半径为<span class="math inline"><em>ϵ</em></span>的圆形区域；</li><li>MinPts（最小点数）：构成一个密集区域所需的最小点数</li></ul><table><thead><tr class="header"><th style="text-align: left;"><strong>类型</strong></th><th style="text-align: left;"><strong>定义</strong></th></tr></thead><tbody><tr class="odd"><td style="text-align: left;">核心点（core point）</td><td style="text-align: left;">自己的<span class="math inline"><em>ϵ</em></span>-邻域内至少包含MinPts个点（包括自己）</td></tr><tr class="even"><td style="text-align: left;">边界点（border point）</td><td style="text-align: left;">自己不是核心点，但是在某个核心点的<span class="math inline"><em>ϵ</em></span>-邻域内</td></tr><tr class="odd"><td style="text-align: left;">噪声点（noise point）</td><td style="text-align: left;">既不是核心点，也不是边界点</td></tr></tbody></table><p><strong>算法流程</strong>：</p><ol type="1"><li><p>从数据集中任取一个未访问的点 p：</p><ul><li>如果 p 是核心点，以 p 为起点，扩展一个簇；</li><li>如果 p 是边界点或噪声点，则忽略；</li></ul></li><li><p>对所有点重复此过程，直到所有点都被访问；</p></li><li><p>最终将所有密度相连的点归为一类，噪声点单独标记为 -1。</p></li></ol><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250605220026372.png" width="60%" /></p><p>如上图，虚线表示<span class="math inline"><em>ϵ</em></span>-邻域，MinPts为3，首先我们可以判断 <span class="math inline"><em>x</em><sub>1</sub></span> 是核心点，之后通过迭代过程判断所有点之间的密度相连关系即可构建出当前MinPts对应的聚类结果。</p><blockquote><p><strong>优缺点</strong>：</p><p>优点：能识别任意形状的簇（环形、月牙形）；能识别噪声点（离群点）；不需要向K-means一样指定簇数。</p><p>缺点：对参数敏感（<span class="math inline"><em>ϵ</em></span>、MinPts）；需要满足密度假设；如果不同簇的密度有较大差异，表现会很差。</p></blockquote><h3 id="层次聚类">层次聚类</h3><p><strong>AGNES</strong>(AGglomerative NESting)：自底向上，从最细的粒度开始（单个样本），逐渐合并相似的簇，直到最粗的簇（一个簇）。关键假设：能够产生不同粒度的聚类结果。</p><p>算法流程：</p><ul><li>Step1: 将每个样本点作为一个簇；</li><li>Step2: 合并最近的两个簇；</li><li>Step3: 若所有样本点都存在于一个簇中，则停止；否则转到 Step2。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250605220904037.png" width="60%" /></p><hr /><h1 id="第十讲降维与度量学习">第十讲、降维与度量学习</h1><div class="note primary no-icon flat"><p><strong>考点</strong>：</p><p>（<em>2018，2019，2021</em>）PCA的最近重构性和最大可分性；</p><p>（<em>2019</em>）马氏距离表达式；</p><p>（<em>2022</em>）维度灾难是什么，ISOMAP、LLE；</p><p>（<em>2019</em>）特征脸是什么。</p></div><h2 id="k-近邻学习">10.1.K-近邻学习</h2><p>基本思路：近朱者赤，近墨者黑。使用投票法/平均法进行确定样本的类别，<strong>懒惰学习</strong>的代表。</p><blockquote><p><strong>懒惰学习</strong>：事先没有分类器，输入测试样本才开始准备分类器</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250606125620540.png" width="60%" /></p><p>由上述例子可以看出，K-近邻的关键在于<strong>k值的选取</strong>和<strong>距离的计算</strong>。</p><p>给定测试样本<span class="math inline"><em>x</em></span>，若其最近邻样本为<span class="math inline"><em>z</em></span>，则最近邻分类器出错的概率为（<span class="math inline"><em>x</em></span>和<span class="math inline"><em>z</em></span>类别标记不同）：</p><p><span class="math display">$$\begin{aligned}P(err) &amp; =1-\sum_{c\in\mathcal{Y}}P(c\mid\boldsymbol{x})P(c\mid\boldsymbol{z}) \\ &amp; \simeq1-\sum_{c\in\mathcal{Y}}P^2(c\mid\boldsymbol{x}) \\ &amp; \leq1-P^2(c^*\mid\boldsymbol{x}) \\ &amp; =\begin{pmatrix}1+P\left(c^{*}\mid\boldsymbol{x}\right)\end{pmatrix}\begin{pmatrix}1-P\left(c^{*}\mid\boldsymbol{x}\right)\end{pmatrix} \\ &amp; \leq2\times\begin{pmatrix}1-P(c^{*}\mid\boldsymbol{x})\end{pmatrix}.\end{aligned}$$</span></p><p><em>最近邻分类器的泛化错误率不会超过贝叶斯最优分类器错误率的两倍！</em></p><p>K-近邻的问题：<strong>真实问题中很难准确地找到k-近邻</strong>。</p><p>密采样的假设：样本的每个 <span class="math inline"><em>ϵ</em></span>-邻域内都有近邻。</p><blockquote><p>考虑一个20维的例子，若近邻的距离阈值设为<span class="math inline">10<sup> − 3</sup></span>。单纯考虑一个维度下的k-近邻，那么在这一维的单位空间至少要<span class="math inline">10<sup>3</sup></span>个样本才能满足密采样的假设。当考虑20个维度时，此时单位空间至少需要<span class="math inline">10<sup>3 × 20</sup></span>个样本才能满足密采样条件。</p><p>若是一张<span class="math inline">300 × 233</span>的彩色图像，将其按 <span class="math inline"><em>h</em> × <em>w</em> × <em>c</em></span> 的顺序展平，最终得到 <span class="math inline">300 × 233 × 3</span> 的向量，该维度有209700，则更加灾难。</p></blockquote><p><strong>维度灾难</strong>：高维空间给距离计算带来很大的麻烦。更严重的是，当样本变得稀疏时，k近邻会不准。</p><p>数据样本虽是高维的，但与学习任务密切相关的也许仅是某个低维空间，即高维空间中的一个低维嵌入（embedding）。</p><h2 id="低维嵌入">10.2.低维嵌入</h2><p><strong>MDS</strong>（Multidimensional Scaling，多维尺度分析）是一种降维方法，它的主要目标是：寻找一个低维子空间，使得距离和样本原有距离近似不变。</p><p>核心思路：寻找低维子空间尽量保持样本内积不变。已知样本的距离矩阵，使用特征值分解来求解内积矩阵（内积保距）。</p><p><strong>算法过程</strong>：</p><ol type="1"><li>从距离矩阵 <span class="math inline"><em>D</em></span> 构造内积矩阵 <span class="math inline"><em>B</em> = <em>X</em><em>X</em><sup>⊤</sup></span>：</li></ol><p>给定距离矩阵 <span class="math inline"><em>D</em> = [<em>d</em><sub><em>i</em><em>j</em></sub>]</span>，可以先构造平方距离矩阵：</p><p><span class="math display">$$\begin{split}    D^{(2)} = [d_{ij}]\end{split}$$</span></p><p>然后构造<strong>中心化矩阵</strong>：</p><p><span class="math display">$$\begin{split}    H = I - \frac{1}{n}\mathbf{1}\mathbf{1}^\top\end{split}$$</span></p><p>其中 <span class="math inline"><em>I</em></span> 是 <span class="math inline"><em>n</em> × <em>n</em></span> 单位矩阵，<span class="math inline"><strong>1</strong></span> 是全1列向量。用它对平方距离矩阵进行双中心化，得到<strong>内积矩阵</strong>：</p><p><span class="math display">$$\begin{split}    B = - \frac{1}{2}HD^{(2)}H\end{split}$$</span></p><blockquote><p><strong><em>Q：为什么需要中心化？</em></strong></p><p>你只有成对的距离信息，但没有坐标。为了推导坐标，就必须：</p><p>假设这些点的<strong>整体几何中心是原点</strong>（即数据集中心化）， 否则你推导出的内积会带有“偏移量”误差。</p><p><strong><em>Q：而中心化矩阵是如何推导的？</em></strong></p><p>对欧几里得距离有：</p><p><span class="math display">$$\begin{split}  d_{ij}^2=\|x_i-x_j\|^2=x_i^\top x_i-2x_i^\top x_j+x_j^\top x_j\end{split}$$</span> <span class="math display">$$\begin{split}  B=XX^\top\Rightarrow B_{ij}=x_i^\top x_j\end{split}$$</span></p><p>设矩阵的对角线元素为 <span class="math inline"><em>b</em><sub><em>i</em><em>i</em></sub> = <em>x</em><sub><em>i</em></sub><sup>⊤</sup><em>x</em><sub><em>i</em></sub></span>，则有：</p><p><span class="math display">$$\begin{split}  d_{ij}^2=b_{ii}+b_{jj}-2b_{ij}\end{split}$$</span></p><p>由公式：</p><p><span class="math display">$$\begin{split}  b_{ij}=-\frac{1}{2}\left(d_{ij}^2-\bar{d}_{i\cdot}^2-\bar{d}_{\cdot j}^2+\bar{d}_{\cdot\cdot}^2\right)\end{split}$$</span></p><p>这个转换其实就等价于双重中心化操作：</p><p><span class="math display">$$\begin{split}  B = - \frac{1}{2}HD^{(2)}H\end{split}$$</span></p></blockquote><ol start="2" type="1"><li>对 <span class="math inline"><em>B</em></span> 作特征值分解：</li></ol><p><span class="math display">$$\begin{split}    B=V\Lambda V^\top\end{split}$$</span></p><ol start="3" type="1"><li>选前 k 个最大的特征值和对应的特征向量，计算坐标：</li></ol><p><span class="math display">$$\begin{split}    X=V_k\Lambda_k^{1/2}\end{split}$$</span></p><p>此时 <span class="math inline"><em>X</em> ∈ ℝ<sup><em>n</em> × <em>k</em></sup></span> 就是降维后每个样本的k-维坐标。</p><h2 id="流形学习">10.3.流形学习</h2><h3 id="isomap">ISOMAP</h3><p><strong>ISOMAP</strong>其实是对 MDS 的非线性扩展版本，适用于流形学习问题。它的目标是：<strong>保留数据在流形上的“测地线距离（Geodesic Distance）”结构，而不是原始空间中的欧几里得距离。</strong>举个不太恰当的例子，比如一张弯曲的纸上的两个点，之前的 MDS 相当于计算的是三维空间中的距离（三维欧氏距离），而ISOMAP考虑的是流形上的路径距离（两个点在纸面上的距离，测地线距离）。</p><p>而在算法设计上，ISOMAP使用最短路径算法来确定任意两点的测地线距离：</p><ol type="1"><li><strong>构建邻接图</strong>：对每个点连接它的k-近邻或<span class="math inline"><em>ϵ</em></span>邻域，构建无向图 <span class="math inline"><em>G</em></span> ，边权值为欧氏距离；</li><li><strong>使用Dijkstra或Floyd求解任意两点的最短距离</strong>：最终得到测地线距离矩阵 <span class="math inline"><em>D</em><sup><em>g</em><em>e</em><em>o</em></sup></span>；</li><li><strong>使用测地线距离执行MDS</strong>：相当于将原距离矩阵替换为测地线距离，其余步骤不变。</li></ol><h3 id="lle">LLE</h3><p><strong>LLE</strong>（Locally Linear Embedding）同样用于非线性降维，该算法假设高维空间中的数据样本在一个低维流形上，且在局部邻域内近似线性。因此我们可以在每个点的邻域中用线性组合重建该点，然后在低维空间中找到映射，使这些“重建关系”得以保留。</p><p>算法步骤：</p><ol type="1"><li><p><strong>找到每个点的k近邻</strong>：<span class="math inline">{<em>x</em><sub><em>i</em><sub>1</sub></sub>, <em>x</em><sub><em>i</em><sub>2</sub></sub>, ..., <em>x</em><sub><em>i</em><sub><em>k</em></sub></sub>}</span>；</p></li><li><p><strong>学习局部重构权重</strong>：</p><p>对每个点 <span class="math inline"><em>x</em><sub><em>i</em></sub></span>，求一组权重<span class="math inline"><em>w</em><sub><em>i</em><em>j</em></sub></span>，使其满足： <span class="math display">$$ \begin{split}     \boldsymbol{x}_i\approx\sum_{j\in\mathcal{N}(i)}w_{ij}\boldsymbol{x}_j \end{split} $$</span></p><p>得到<strong>优化目标</strong>： <span class="math display">$$ \begin{split}     \min_{w_{ij}}\sum_i\left\|\boldsymbol{x}_i-\sum_{j\in\mathcal{N}(i)}w_{ij}\boldsymbol{x}_j\right\|^2 \end{split} $$</span></p><p>同时又约束： <span class="math display">$$ \begin{split}     \sum_{j\in\mathcal{N}(i)}w_{ij}=1 \end{split} $$</span></p><p>该过程对每个<span class="math inline"><em>x</em><sub><em>i</em></sub></span>独立进行，每个<span class="math inline"><em>x</em><sub><em>i</em></sub></span>有自己的一组权重。</p></li><li><p><strong>在低维空间中保留重构权重</strong>：</p><p>目标是寻找低维表示<span class="math inline"><em>y</em><sub><em>i</em></sub> ∈ ℝ<sup><em>b</em></sup></span>，使得同样的权重<span class="math inline"><em>w</em><sub><em>i</em><em>j</em></sub></span>仍能重构它： <span class="math display">$$ \begin{split}     \min_{\boldsymbol{y}_1,\ldots,\boldsymbol{y}_n}\sum_i\left\|\boldsymbol{y}_i-\sum_{j\in\mathcal{N}(i)}w_{ij}\boldsymbol{y}_j\right\|^2 \end{split} $$</span></p><p>这个优化问题通过将其写成矩阵形式，然后通过特征值分解求解，这里就不具体展开了。</p></li></ol><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250609205841865.png" width="40%" /></p><h2 id="度量学习">10.4.度量学习</h2><p>之前介绍的距离度量有以下几种：</p><ul><li>欧氏距离</li><li>曼哈顿距离</li><li>测地距离</li></ul><p>之前说过距离度量对于降维很重要，不同的任务需要选择不同的距离度量，能否直接学习一个合适的距离度量呢？这里使用<strong>马氏距离（Mahalanobis Distance）</strong>来参数化学习距离度量：</p><p><span class="math display">$$\begin{split}    \mathrm{dist}_{\mathrm{mah}}^2(x_i,x_j)=(x_i-x_j)^\mathrm{T}\mathrm{M}(x_i-x_j)=\|x_i-x_j\|_\mathrm{M}^2\end{split}$$</span></p><p>其中 <span class="math inline"><em>M</em></span> 称为度量矩阵，它是一个半正定对称矩阵，距离度量学习相当于就是要学习 <span class="math inline"><em>M</em></span>。欧氏距离的一个问题就是–各个方向都同等重要，这里引入度量矩阵则能使降维侧重于某些维度。</p><h3 id="距离度量学习nca">距离度量学习–NCA</h3><p>NCA（Neighborhood Component Analysis）常用于近邻分类器（KNN），而近邻分类器在进行判别时通常使用多数投票法，这里NCA使用概率投票法（其实本质差不多）。对于任意样本 <span class="math inline"><em>x</em><sub><em>j</em></sub></span>，它对 <span class="math inline"><em>x</em><sub><em>i</em></sub></span> 分类结果影响的概率为：</p><p><span class="math display">$$\begin{split}    p_{ij}=\frac{\exp\left(-\left\|\boldsymbol{x}_i-\boldsymbol{x}_j\right\|_\mathbf{M}^2\right)}{\sum_l\exp\left(-\left\|\boldsymbol{x}_i-\boldsymbol{x}_l\right\|_\mathbf{M}^2\right)}\end{split}$$</span></p><p>其实很好理解，对于一个点，离它越近的点越相似，两个点的标签越可能是一样的。那么对于点 <span class="math inline"><em>x</em><sub><em>i</em></sub></span>，其分类正确的概率为：</p><p><span class="math display">$$\begin{split}    p_i=\sum_{j:y_j=y_i}p_{ij}\end{split}$$</span></p><p>NCA的目标就是<strong>最大化所有样本的正确分类概率之和</strong>：</p><p><span class="math display">$$\begin{split}    \mathcal{L}(A)=\sum_ip_i=\sum_i\sum_{j:y_j=y_i}p_{ij}\end{split}$$</span></p><h3 id="距离度量学习lmnn">距离度量学习–LMNN</h3><p><strong>LMNN（Large Margin Nearest Neighbor）</strong> 是另一种监督式度量学习方法，与 NCA 类似，但优化目标和策略不同。LMNN 的核心思想是：学习一个距离度量，使得 K 近邻分类器（KNN）在训练集上分类间隔更大，让同类点靠得近，不同类点被“推远”。</p><p>具体来说，其目标函数包含<strong>拉进</strong>和<strong>推远</strong>两项：</p><p><span class="math display">$$\begin{split}    \mathcal{L}(M)=\sum_{(i,j)\in\mathcal{N}}d_L(x_i,x_j)^2+\mu\sum_{(i,j,l)}\xi_{ijl}\end{split}$$</span></p><p><span class="math display">$$\begin{split}    \xi_{ijl}=\begin{bmatrix}1+d_L(x_i,x_j)^2-d_L(x_i,x_l)^2\end{bmatrix}_+\end{split}$$</span></p><p>其中第一项中的 <span class="math inline">𝒩</span> 表示所有的<strong>同类K近邻对</strong>（target neighbor），是我们希望拉近的，所以希望距离越小越好； 而第二项中的 <span class="math inline"><em>ξ</em><sub><em>i</em><em>j</em><em>l</em></sub></span> 是<strong>Hinge损失</strong>项，用于惩罚违反<strong>margin规则</strong>的异类点，其中 <span class="math inline"><em>x</em><sub><em>l</em></sub></span> 表示与 <span class="math inline"><em>x</em><sub><em>i</em></sub></span> 标签不同的样本。</p><p>那么这里的margin规则是什么意思呢？它表示惩罚那些“离得太近的异类点”，在这里我们希望 <span class="math inline"><em>x</em><sub><em>i</em></sub></span> 与异类点 <span class="math inline"><em>x</em><sub><em>l</em></sub></span> 的距离至少比同类点之间的距离远 1 个单位（margin），如果没有达到这个要求就需要惩罚，所以这里使用Hinge损失。</p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250606161257888.png" width="90%" /></p><h2 id="pca">10.5.PCA</h2><p>PCA（Principal Component Analysis，主成分分析）是一种经典的无监督降维方法，其目标是：<strong>在保证尽可能保留原始数据方差信息的前提下，将高维数据投影到低维空间。</strong></p><p>正交属性空间中的样本点，如何找到一个超平面使得所有样本都能被恰当表达呢？显然我们希望该超平面具备以下性质：</p><ul><li><strong>最近重构性</strong>：样本点到这个超平面的距离都足够近；</li><li><strong>最大可分性</strong>：样本点在这个超平面上的投影都尽可能分开。</li></ul><p>那么可以通过以上两种目标进行推导：</p><ol type="1"><li><p>基于最近重构性推导：</p><ul><li>对样本中心化：<span class="math inline">$\bar{x}=\frac{1}{n}\sum_{i=1}^nx_i,\quad X_{\mathrm{centered}}=X-\bar{x}$</span></li><li>假定投影变换后得到的新坐标系为<span class="math inline"><em>w</em><sub><em>i</em></sub></span>， 其中<span class="math inline"><em>w</em><sub><em>i</em></sub></span>是标准正交基向量，即：</li></ul></li></ol><p><span class="math display">$$\begin{split}||\boldsymbol{w}_i||_2=1,\boldsymbol{w}_i^\mathrm{T}\boldsymbol{w}_j=0(i\neq j)\end{split}$$</span></p><ul><li>舍弃原坐标系中的部分维度，假设维度降低到<span class="math inline"><em>d</em>′ &lt; <em>d</em></span>，则样本点在低维坐标系中的投影为：</li></ul><p><span class="math display">$$\begin{split}    \boldsymbol{z}_i=(z_{i1};z_{i2};\ldots;z_{id^{\prime}})\quad z_{ij}=\boldsymbol{w}_j^\mathrm{T}\boldsymbol{x}_i\end{split}$$</span></p><ul><li>基于 <span class="math inline"><em>z</em><sub><em>i</em></sub></span> 来重构 <span class="math inline"><em>x</em><sub><em>i</em></sub></span>，可以得到：<span class="math inline">$\hat{\boldsymbol{x}}_i=\sum_{j=1}^{d^{\prime}}z_{ij}\boldsymbol{w}_j$</span>。</li><li>由此得到原样本点和基于投影重构的样本点之间的距离为：</li></ul><p><span class="math display">$$\begin{aligned}\sum_{i=1}^m\left\|\sum_{j=1}^{d^{\prime}}z_{ij}\boldsymbol{w}_j-\boldsymbol{x}_i\right\|_2^2 &amp; =\sum_{i=1}^m\boldsymbol{z}_i^\mathrm{T}\boldsymbol{z}_i-2\sum_{i=1}^m\boldsymbol{z}_i^\mathrm{T}\mathbf{W}^\mathrm{T}\boldsymbol{x}_i+\mathrm{const} \\ &amp; \propto-\mathrm{tr}\left(\mathbf{W}^\mathrm{T}\left(\sum_{i=1}^mx_ix_i^\mathrm{T}\right)\mathbf{W}\right).\end{aligned}$$</span></p><ul><li>由此可得基于最近重构性的优化目标为：</li></ul><p><span class="math display">$$\begin{aligned} &amp; \min_{\mathbf{W}}\quad-\operatorname{tr}(\mathbf{W}^\mathrm{T}\mathbf{X}\mathbf{X}^\mathrm{T}\mathbf{W}) \\ &amp; \mathrm{s.t.}\quad\mathbf{W}^\mathrm{T}\mathbf{W}=\mathbf{I}.\end{aligned}$$</span></p><ol start="2" type="1"><li>基于最大可分性推导（更好理解）：</li></ol><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250606164844991.png" width="50%" /></p><ul><li>样本点<span class="math inline"><em>x</em><sub><em>i</em></sub></span>在新空间中超平面上的投影是<span class="math inline"><em>W</em><sup>⊤</sup><em>x</em><sub><em>i</em></sub></span>，若所有样本点的投影能尽可能分开，则应该使得投影后样本点的方差最大化<strong>（计算协方差矩阵）</strong>：</li></ul><p><span class="math display">$$\begin{split}    \sum_i\mathbf{W}^\mathrm{T}x_ix_i^\mathrm{T}\mathbf{W}\end{split}$$</span></p><ul><li>由此可以得到基于最大可分性的优化目标为：</li></ul><p><span class="math display">$$\begin{split}    \max_{\mathbf{W}}\quad\mathrm{tr}(\mathbf{W}^\mathrm{T}\mathbf{X}\mathbf{X}^\mathrm{T}\mathbf{W})\mathrm{s.t.}\quad\mathbf{W}^\mathrm{T}\mathbf{W}=\mathbf{I}.\end{split}$$</span></p><p>可以发现两种思路都是等价的。因此求解思路就是对协方差矩阵做<strong>特征值分解</strong>：</p><p><span class="math display">$$\begin{split}    C=\frac{1}{n}X^\top X\in\mathbb{R}^{D\times D}\end{split}$$</span></p><p><span class="math display">$$\begin{split}    C=U\Lambda U^\top\end{split}$$</span></p><p>其中 <span class="math inline"><em>U</em></span> 的列是特征向量，表示主成分方向；<span class="math inline"><em>Λ</em></span> 是对角矩阵，对应的特征值表示方向的方差大小。选取前 <span class="math inline"><em>d</em>′</span> 个最大特征值对应的特征向量，组成<span class="math inline"><em>U</em><sub><em>d</em>′</sub></span>，最后将原始数据投影到这些方向上即可得到降维后的数据：</p><p><span class="math display">$$\begin{split}    Z=X_{\mathrm{centered}}\cdot U_{d’}\end{split}$$</span></p><p>PCA是最常用的降维方法，其在不同领域有不同的称谓，例如在人脸识别中该技术称为“特征脸”（eigenface），将前<span class="math inline"><em>d</em>′</span>个特征值对应的特征向量还原为图像可以得到：</p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250609203020531.png" width="60%" /></p><hr /><h1 id="第十一讲特征选择和稀疏学习">第十一讲、特征选择和稀疏学习</h1><div class="note primary no-icon flat"><p><strong>考点</strong>：</p><p>（<em>2018，2019，2021，2022，2023</em>）子集搜索和子集评估的三种方法（或者说特征选择的三种方法）；</p><p>（<em>2019</em>）L1范数为什么可以得到稀疏解。</p></div><h2 id="特征选择">11.1.特征选择</h2><h3 id="特征">特征</h3><p>特征：用于描述物体的属性，分为以下几类：</p><ul><li>相关特征：对当前学习任务有用的属性；</li><li>无关特征：与当前学习任务无关的属性；</li><li>冗余特征：其所包含的信息能由其他特征推演出来。</li></ul><h3 id="特征选择-1">特征选择</h3><p>目的：从给定的特征集合中选出任务相关特征的子集，不丢失重要特征。</p><ul><li>减轻维度灾难：在少量属性上构建模型；</li><li>减低学习难度：保留关键信息。</li></ul><p>可行方法：存在两个关键环节–<strong>子集搜索</strong>和<strong>子集评价</strong>。</p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250606170706055.png" width="60%" /></p><p><strong>子集搜索</strong></p><p>用贪心策略选择包含重要信息的特征子集。</p><ol type="1"><li>前向搜索：最优子集初始为空集，特征集合初始时包括所有给定特征；</li><li>后向搜索：从完整的特征集合开始，逐渐减少特征；</li><li>双向搜索：每一轮逐渐增加相关特征，同时减少无关特征。</li></ol><p><strong>子集评价</strong></p><p>选定的特征子集确定了对数据集的一个划分，样本自身的标签对应着对数据集的真实划分。通过估算这两个划分的差异，就能对特征子集进行评价，与真实划分的差异越小，说明当前特征子集越好。</p><p>这里可用信息增益来进行子集评价：特征子集 <span class="math inline"><em>A</em></span> 上的取值将原数据集 <span class="math inline"><em>D</em></span> 分为 <span class="math inline"><em>V</em></span> 份，每一份用 <span class="math inline"><em>D</em><sup><em>v</em></sup></span> 表示，则特征子集 <span class="math inline"><em>A</em></span> 的信息增益为</p><p><span class="math display">$$\begin{split}    \mathrm{Gain}(A)=\mathrm{Ent}(D)-\sum_{v=1}^V\frac{|D^v|}{|D|}\mathrm{Ent}(D^v)\end{split}$$</span></p><h4 id="过滤式">过滤式</h4><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250606192533368.png" width="60%" /></p><p>先用特征选择过程过滤原始数据，再用过滤后的特征来训练模型，<strong>特征选择过程与后续学习器无关</strong>。</p><p><strong>Relief</strong>方法：一个好的特征应当在同类样本之间相似，而在异类样本之间差异大。也就是说，某特征能明显区分一个样本与其“近邻异类”的差异，同时保持“近邻同类”的相似性，那这个特征就是有用的。</p><p>相关定义：</p><ul><li>猜中近邻（near-hit）：<span class="math inline"><em>x</em><sub><em>i</em></sub></span> 的同类样本中的最近邻 <span class="math inline"><em>x</em><sub><em>i</em>, <em>n</em><em>h</em></sub></span>；</li><li>猜错近邻（near-miss）：<span class="math inline"><em>x</em><sub><em>i</em></sub></span> 的异类样本中的最近邻 <span class="math inline"><em>x</em><sub><em>i</em>, <em>n</em><em>m</em></sub></span>。</li></ul><p>算法步骤（以二分类为例）：</p><ol type="1"><li>初始化每个特征的权重为0：</li></ol><p><span class="math display">$$\begin{split}    W_j=0\quad \text{for all } j=1,2,...,d\end{split}$$</span></p><ol start="2" type="1"><li><p>对于每个样本 <span class="math inline"><em>x</em><sub><em>i</em></sub></span>：</p><ul><li>找到该样本的猜中近邻<span class="math inline"><em>H</em><sub><em>i</em></sub></span>和猜错近邻<span class="math inline"><em>M</em><sub><em>i</em></sub></span></li><li>对每个特征 <span class="math inline"><em>f</em><sub><em>j</em></sub></span>，更新其权重，其中<span class="math inline"><em>d</em><em>i</em><em>f</em><em>f</em>(<em>a</em>, <em>b</em>) = |<em>a</em> − <em>b</em>|</span>（或其他距离度量），<span class="math inline"><em>x</em><sub><em>i</em></sub><sup><em>j</em></sup></span>表示第<span class="math inline"><em>i</em></span>个样本的第<span class="math inline"><em>j</em></span>个特征值：</li></ul></li></ol><p><span class="math display">$$\begin{split}    W_j=0\quad \text{for all } j=1,2,...,d\end{split}$$</span></p><ol start="3" type="1"><li>最终得到每个特征的权重（也叫做相关统计量），权重越高说明该特征对分类越有用。</li></ol><p>Relief方法的时间开销随采样次数以及原始特征数线性增长，运行效率高。</p><h4 id="包裹式">包裹式</h4><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250606194820037.png" width="60%" /></p><p>包裹式选择把<strong>最终使用的学习器性能</strong>作为特征子集的评价准则。</p><p><strong>LVW</strong>（Las Vegas Wrapper）是一种随机化的特征选择方法，属于 wrapper 方法（包装器方法），由 Liu 和 Motoda 在 1998 年提出。随机采样特征子集，使用学习器对其评估准确率，如果新子集更好，就接受它为当前最优解。与 Relief 评分式方法不同，LVW 是基于模型性能来“试验”出优质特征组合的。</p><p>算法步骤：重复以下步骤直到cnt达到阈值<span class="math inline"><em>T</em></span></p><ol type="1"><li>随机生成一个特征子集 <span class="math inline"><em>S</em> ⊂ <em>F</em></span>，通常会随机保留一部分特征；</li><li>使用分类器L在训练集上评估S的准确率 <span class="math inline"><em>a</em><em>c</em><em>c</em><sub><em>S</em></sub></span>；</li><li>若性能更好但子集更小：<ul><li>更新最优子集： <span class="math inline"><em>S</em><sub><em>b</em><em>e</em><em>s</em><em>t</em></sub> ← <em>S</em></span></li><li>更新最优准确率： <span class="math inline"><em>a</em><em>c</em><em>c</em><sub><em>b</em><em>e</em><em>s</em><em>t</em></sub> ← <em>a</em><em>c</em><em>c</em><sub><em>S</em></sub></span></li><li>计数器归零：cnt = 0</li></ul></li><li>否则计数器加1： <span class="math inline"><em>c</em><em>n</em><em>t</em> ← <em>c</em><em>n</em><em>t</em> + 1</span></li></ol><p>从最终学习器性能来看，包裹式特征选择比过滤式特征选择更好，但需多次训练学习器，计算开销通常比过滤式特征选择大得多。</p><h4 id="嵌入式">嵌入式</h4><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250606200232624.png" width="60%" /></p><p>嵌入式特征选择是将特征选择过程与学习器训练过程融为一体，两者在同一优化过程中完成，<strong>在学习器训练过程中自动地进行特征选择</strong>。</p><p>考虑最简单的线性回归–<strong>岭回归</strong>（ridge regression），它在普通的线性回归问题（平方误差损失）上加上了L2正则化项防止过拟合：</p><p><span class="math display">$$\begin{split}    \operatorname*{min}_{\boldsymbol{w}}\sum_{i=1}^{m}(y_{i}-\boldsymbol{w}^{\top}\boldsymbol{x}_{i})^{2}+\lambda\|\boldsymbol{w}\|_{2}^{2}\end{split}$$</span></p><p>将L2正则化项替换为L1正则化项，可以得到<strong>LASSO</strong>（Least Absolute Shrinkage and Selection Operator，最小绝对收缩与选择算子）：</p><p><span class="math display">$$\begin{split}    \operatorname*{min}_{\boldsymbol{w}}\sum_{i=1}^{m}(y_{i}-\boldsymbol{w}^{\top}\boldsymbol{x}_{i})^{2}+\lambda\|\boldsymbol{w}\|_{1}\end{split}$$</span></p><p>使用L1范式 LASSO 有一个特殊性质，几何上的 L1 范数形成的是一个<strong>菱形约束区域</strong>，它与损失函数的等高线接触时，往往在轴（即某个<span class="math inline"><em>w</em><sub><em>j</em></sub> = 0</span>）上交点，导致某些权重直接为零，从而达到特征选择的目的。</p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250606200938379.png" width="70%" /></p><p>LASSO没有封闭解，常用PGD（Proximal Gradient Descend，近端梯度下降）法求解。</p><h2 id="稀疏表示">11.2.稀疏表示</h2><p>稀疏表示的优势：文本数据线性可分、存储高效</p><p><em>能否将稠密表示的数据集转化为稀疏表示，使其享受上述优势？</em></p><p><strong>字典学习</strong>：为普通稠密表达的样本找到合适的字典，将样本转化为稀疏表示，用尽可能少的原子（字典中的“词条”）线性组合来近似表示数据。</p><p>给定数据矩阵 <span class="math inline"><strong>X</strong> ∈ ℝ<sup><em>n</em> × <em>d</em></sup></span>，我们需要得到一个字典矩阵 <span class="math inline"><strong>D</strong> ∈ ℝ<sup><em>k</em> × <em>d</em></sup></span>（k为字典的大小），一个稀疏编码矩阵 <span class="math inline"><strong>Z</strong> ∈ ℝ<sup><em>n</em> × <em>k</em></sup></span>，使得：</p><p><span class="math display">$$\begin{split}    X\approx ZD\end{split}$$</span></p><p>其中 <span class="math inline"><em>Z</em></span> 中每一行都是稀疏的（多数值为0）。通常其优化形式为：</p><p><span class="math display">$$\begin{split}    \min_{Z,D}\|X-ZD\|_F^2+\lambda\sum_{i=1}^n\|z_i\|_1\end{split}$$</span></p><p>其中第一项表示矩阵误差，使用Frobenius范数（也可以用L2范数），第二项表示对编码矩阵非零元素个数的惩罚项。</p><p><strong>矩阵补全</strong>：从得到的部分信号, 基于压缩感知的思想恢复出完整信号。其基本目标是，给定一个部分观测的矩阵（很多元素缺失），预测其缺失部分的值。通常假设<strong>矩阵具有低秩（Low-Rank）结构</strong>，即大部分信息可以由少数几个主因子表示。</p><p>最基本的优化形式可以表示为：</p><p><span class="math display">$$\begin{aligned}\min_X\operatorname{rank}(X),\quad\operatorname{s.t.}X_{ij}=M_{ij},\forall(i,j)\in\Omega\end{aligned}$$</span></p><p>其中<span class="math inline"><em>Ω</em></span>表示观测到的条目集合，优化目标就是恢复出的矩阵中对应元素应该与已观测到的对应元素相同，但是要尽可能保证矩阵的秩越小越好。但这是一个NP难问题，因此使用<strong>核范数</strong>（nuclear norm，即矩阵的奇异值之和）代替秩来进行凸优化：</p><p><span class="math display">$$\begin{aligned}\min_X\|X\|_*\quad\mathrm{s.t.}X_{ij}=M_{ij},\forall(i,j)\in\Omega\end{aligned}$$</span></p><p>通常使用半正定规划（SDP，Semi-Definite Programming）求解。</p><hr /><h1 id="第十二讲半监督学习">第十二讲、半监督学习</h1><div class="note primary no-icon flat"><p><strong>考点</strong>：</p><p>（<em>2018</em>）TSVM；</p><p>（<em>2019，2022，2023</em>）图半监督学习推导中的D和能量函数、闭式解。</p></div><p>监督学习的局限：需要海量的该质量标注样本，现实世界需要获取巨大人力物力。</p><p>半监督学习：希望同时使用<strong>有标记样本</strong>和<strong>未标记样本</strong>构建泛化性能良好的模型。</p><h2 id="未标记样本">12.1.未标记样本</h2><p>未标记样本的潜在假设：</p><ul><li><strong>聚类假设</strong>（Clustering Assumption）：假设数据存在簇结构，即同一簇的样本属于同一类别；</li><li><strong>流形假设</strong>（Manifold Assumption）：假设数据分布在一个流形结构上，邻近的样本具有相似的输出值。</li></ul><h2 id="生成式方法">12.2.生成式方法</h2><p>回顾原型聚类中的GMM，我们使用一个高斯混合模型来建模生成概率分布。这里探讨在半监督下的GMM如何进行：</p><p>从最大化后验概率出发：</p><p><span class="math display">$$\begin{aligned}f(x)=\arg\max_{j\in\mathcal{Y}}p(y=j\mid x)\end{aligned}$$</span></p><p>由于该项很难求，因此我们引入隐变量<span class="math inline"><em>Θ</em></span>，将其改写为：</p><p><span class="math display">$$\begin{aligned}p(y=j\mid x)=\sum_{i=1}^kp(y=j,\Theta=i\mid x)=\sum_{i=1}^kp(y=j\mid\Theta=i,x)\cdotp(\Theta=i\mid x)\end{aligned}$$</span></p><p>最终得到对数似然函数为：</p><p><span class="math display">$$\begin{aligned}\ln p(D_l\cup D_u) &amp; =\sum_{(x_j,y_j)\in D_l}\ln\left(\sum_{i=1}^k\alpha_ip(x_j\mid\mu_i,\Sigma_i)\cdotp(y_j\mid\Theta=i,x_j)\right) \\ &amp; +\sum_{x_j\in D_u}\ln\left(\sum_{i=1}^k\alpha_ip(x_j\mid\mu_i,\Sigma_i)\right)\end{aligned}$$</span></p><p>其中第一项是对有标签数据的对数似然，即使有了标签<span class="math inline"><em>y</em><sub><em>j</em></sub></span>，仍然考虑混合成分<span class="math inline"><em>Θ</em></span>，这是因为<span class="math inline"><em>y</em><sub><em>j</em></sub></span>与<span class="math inline"><em>Θ</em></span>不一定一一对应，尤其当一个类别可能由多个高斯成分生成。第二项则是无标签数据的对数似然，只能使用<span class="math inline"><em>x</em><sub><em>j</em></sub></span>本身。之后和之前介绍GMM一样，使用EM算法求解，将高斯混合模型换成混合专家模型、朴素贝叶斯模型等,，可推出其他生成式半监督方法。</p><blockquote><p>优缺点：</p><p>优点：简单、易于实现</p><p>缺点：模型假设必须准确，即假设的生成式模型必须与真实数据分布吻合</p></blockquote><h2 id="半监督svm">12.3.半监督SVM</h2><p><strong>TSVM</strong>（Transductive Support Vector Machine） 是另一种经典的半监督学习方法，但和 GMM 不同，它属于判别式方法，不建模数据分布，而是直接优化分类决策边界。其核心思想是在少量标注样本的基础上，利用未标注数据来帮助调整分类边界，使其落在类簇之间（cluster assumption / low-density separation assumption）。</p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250606213437535.png" width="50%" /></p><p>回顾标准SVM的优化目标，可以总结为：</p><p><span class="math display">$$\begin{split}\min_{w,b,\xi}\frac{1}{2}\|w\|^2+C\sum_i\xi_i\quad\text{subject to }y_i(w^Tx_i+b)\geq1-\xi_i\end{split}$$</span></p><p>即在保证分类正确（或有一定松弛）的条件下，最大化间隔<span class="math inline">$\frac{1}{\|w\|}$</span>。而TSVM的想法是，不仅保证有标签数据分类结果良好，<strong>无标签数据还需要远离决策边界</strong>，使得边界落在两个类别的低密度区。具体来说TSVM的目标函数可总结为：</p><p><span class="math display">$$\begin{aligned}\min_{\boldsymbol{w},b,\hat{\boldsymbol{y}},\boldsymbol{\xi}} &amp; \frac{1}{2}\|\boldsymbol{w}\|_2^2+C_l\sum_{i=1}^l\xi_i+C_u\sum_{i=l+1}^m\xi_i \\\mathrm{s.t.} &amp; y_{i}(\boldsymbol{w}^{\top}\boldsymbol{x}_{i}+b)\geq1-\xi_{i},i=1,\ldots,l, \\ &amp; \hat{y}_{i}(\boldsymbol{w}^{\top}\boldsymbol{x}_{i}+b)\geq1-\xi_{i},i=l+1,\ldots,m, \\ &amp; \xi_{i}\geq0,i=1,\ldots,m,\end{aligned}$$</span></p><p>其算法过程可总结为：</p><ol type="1"><li>用有标签样本 <span class="math inline"><em>D</em><sub><em>l</em></sub></span> 训练一个标准 SVM；</li><li>用 SVM 对无标记样本 D_u 中样本进行预测，得到初始伪标签 <span class="math inline"><em>ȳ</em><sub><em>i</em></sub> ∈ { − 1,  + 1}</span>；</li><li>初始化惩罚项权重，<span class="math inline"><em>C</em><sub><em>u</em></sub> &lt; <em>C</em><sub><em>l</em></sub></span>，其中无标签样本的惩罚项很小，以避免伪标签不准时影响模型太大；</li><li>之后逐步迭代优化求解TSVM的优化问题（求解TSVM的目标函数），具体来说一般是交替优化SVM；</li><li>在这期间会检查为标签是否严重违反了边界合理性，即 <span class="math inline"><em>y</em>̂<sub><em>i</em></sub><em>y</em>̂<sub><em>j</em></sub> &lt; 0, <em>ξ</em><sub><em>i</em></sub> &gt; 0, <em>ξ</em><sub><em>j</em></sub> &gt; 0, <em>ξ</em><sub><em>i</em></sub> + <em>ξ</em><sub><em>j</em></sub> &gt; 2</span>，这表示这些伪标签可能是错的，我们可以尝试交换伪标签看看能否减少目标函数值；</li><li>逐步提升无标签样本的惩罚力度：<span class="math inline"><em>C</em><sub><em>u</em></sub> = <em>m</em><em>i</em><em>n</em>(2<em>C</em><sub><em>u</em></sub>, <em>C</em><sub><em>l</em></sub>)</span>。</li></ol><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250607104946619.png" width="80%" /></p><p>但在未标记样本进行标记指派及调整的过程中，可能会出现类别不平衡问题（某类的样本远多于另一类），为了减轻类别不平衡所造成的不利影响，可将优化目标中的<span class="math inline"><em>C</em><sub><em>u</em></sub></span>拆分为<span class="math inline"><em>C</em><sub><em>u</em></sub><sup>+</sup></span>与<span class="math inline"><em>C</em><sub><em>u</em></sub><sup>−</sup></span>：</p><p><span class="math display">$$\begin{split}    C_u^+=\frac{u_-}{u_+}C_u^-\end{split}$$</span></p><h2 id="图半监督学习">12.4.图半监督学习</h2><p>给定一个数据集，我们可以将其映射为一个图，数据集中的每个样本对应图中的一个结点，若两个样本之间的相似度很高，则对应结点之间存在一条边，边的强度正比于样本间的相似度。我们将有标记样本对应的结点看作<strong>染过色</strong>，未标记样本所对应的结点<strong>尚未染色</strong>，于是半监督学习可以看作是图染色问题。</p><p>算法过程（以二分类为例）：</p><ol type="1"><li>基于数据集 <span class="math inline"><em>D</em><sub><em>l</em></sub> ∪ <em>D</em><sub><em>u</em></sub></span> 构建一个图 <span class="math inline"><em>G</em> = (<em>V</em>, <em>E</em>)</span>。<ul><li>其中结点集为：</li></ul></li></ol><p><span class="math display">$$\begin{split}    V=\{x_1,...,x_l,x_{l+1},...,x_{l+u}\}\end{split}$$</span></p><ul><li>边集<span class="math inline"><em>E</em></span>可表示为一个亲和矩阵（affinity matrix），基于高斯函数定义为：</li></ul><p><span class="math display">$$\left.\mathbf{W}_{ij}=\left\{\begin{array}{cc}\exp\left(\frac{-\|x_i-x_j\|_2^2}{2\sigma^2}\right), &amp; \mathrm{if}\quad i\neq j; \\ \\0 ,&amp; \mathrm{otherwise};\end{array}\right.\right.$$</span></p><ol start="2" type="1"><li>假定从图 <span class="math inline"><em>G</em> = (<em>V</em>, <em>E</em>)</span> 将学得一个实值函数 <span class="math inline"><em>f</em> : <em>V</em> → ℝ</span>，对每个样本输出一个分类概率得分。直观上相似的样本应该具有相似的标记，于是可以得到关于<span class="math inline"><em>f</em></span>的<strong>能量函数</strong>：</li></ol><p><span class="math display">$$\begin{aligned} &amp; E(f)={\frac{1}{2}}\sum_{i=1}^{m}\sum_{j=1}^{m}{\boldsymbol{W}_{ij}}(f({\boldsymbol{x}_{i}})-f({\boldsymbol{x}_{j}}))^{2} \\ &amp; =\frac{1}{2}\left(\sum_{i=1}^{m}d_{i}f^{2}(\boldsymbol{x}_{i})+\sum_{j=1}^{m}d_{j}f^{2}(\boldsymbol{x}_{j})-2\sum_{i=1}^{m}\sum_{j=1}^{m}\boldsymbol{W}_{ij}f(\boldsymbol{x}_{i})f(\boldsymbol{x}_{j})\right) \\ &amp; =f^{T}(D-W)\boldsymbol{f}\end{aligned}$$</span></p><p><span class="math display">$$\begin{split}    D=\mathrm{diag}(d_i),d_i=\sum_{j=1}^{l+u}(W_{ij})\end{split}$$</span></p><ol start="3" type="1"><li>采用分块矩阵的表示方式：</li></ol><p><span class="math display">$$\begin{split}    E(f)=(\boldsymbol{f}_l^\top\boldsymbol{f}_u^\top)\left(\begin{bmatrix}\mathbf{D}_{ll} &amp; \mathbf{0}_{lu} \\\mathbf{0}_{ul} &amp; \mathbf{D}_{uu}\end{bmatrix}-\begin{bmatrix}\mathbf{W}_{ll} &amp; \mathbf{W}_{lu} \\\mathbf{W}_{ul} &amp; \mathbf{W}_{uu}\end{bmatrix}\right)\begin{bmatrix}f_l \\\mathbf{f}_u\end{bmatrix} \\=\boldsymbol{f}_l^\top(\mathbf{D}_l-\mathbf{W}_{ll})\boldsymbol{f}_l-2\boldsymbol{f}_u^\top\mathbf{W}_{ul}\boldsymbol{f}_l+\boldsymbol{f}_u^\top(\mathbf{D}_{uu}-\mathbf{W}_{uu})\boldsymbol{f}_u.\end{split}$$</span></p><p>通过<span class="math inline">$\frac{\partial E(f)}{\partial f_u}=0$</span>，可以得到：</p><p><span class="math display">$$\begin{aligned}f_{u} &amp; =(D_{uu}(\boldsymbol{I}-\boldsymbol{D}_{uu}^{-1}\boldsymbol{W}_{uu}))^{-1}\boldsymbol{W}_{ul}\boldsymbol{f}_{l} \\ &amp; =(\boldsymbol{I}-\boldsymbol{D}_{uu}^{-1}\boldsymbol{W}_{uu})^{-1}\boldsymbol{D}_{uu}^{-1}\boldsymbol{W}_{ul}\boldsymbol{f}_{l} \\ &amp; =(\boldsymbol{I}-\boldsymbol{P}_{uu})^{-1}\boldsymbol{P}_{ul}\boldsymbol{f}_l\end{aligned}$$</span></p><p><span class="math display">$$\begin{split}    P=D^{-1}W\end{split}$$</span></p><blockquote><p>优缺点：</p><p>图半监督学习方法在概念上相当清晰，易于通过对所涉矩阵运算的分析来探索算法性质</p><p>但存储开销高，且构图过程仅能考虑训练样本集，难以判知新样本在图中的位置</p></blockquote><h2 id="基于分歧的方法">12.5.基于分歧的方法</h2><p>基于分歧的方法（disagreement-based methods）使用多学习器，而学习器之间的分歧对未标记数据的利用很重要。协同训练（co-training）是基于分歧方法的重要代表，其最初针对多视图（multi-view）数据设计，</p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250607134045187.png" width="50%" /></p><p>协同训练很好地利用了多视角的相容互补性，但它需要假设<strong>数据拥有两个充分且条件独立的视图</strong>。因此此后出现了一些能在单视图数据上使用的变体算法，可以使用<em>不同的学习算法</em>、或<em>不同的数据采样</em>、或<em>不同的参数设置</em>来产生不同的学习器，也能有效利用未标记样本提升性能。</p><p>后续研究发现此类算法事实上无需数据拥有多视图，仅需弱学习器之间具有显著的分歧（差异），即可通过相互提供伪标记样本的方式来提高泛化性能。</p><blockquote><p>优缺点：</p><p>基于分歧的方法只需采用合适的基学习器，就能较少受到模型假设、损失函数非凸性和数据规模等问题的影响，学习方法简单有效、理论基础相对坚实、适用范围广泛。</p><p>但为了使用这类方法，需要生成具有显著分歧、性能尚可的多个学习器，但当有标记样本很少、尤其数据不具有多视图时，很难实现这一点。</p></blockquote><h2 id="半监督聚类">12.6.半监督聚类</h2><p>聚类是一种典型的无监督学习任务，但有时我们往往能获得一些额外的监督信息，这时我们可以使用半监督聚类（semi-supervised clustering）来使用监督信息以获得更好的聚类效果。</p><p>聚类任务中获得的监督信息大致有两种类型：</p><ol type="1"><li><strong>必连（must-link）与勿连（cannot-link）约束</strong>：前者是指样本必属于同一个簇，后者是指样本必不属于同一个簇；</li><li><strong>有少量的有标记样本</strong>。</li></ol><h3 id="约束k均值">约束k均值</h3><p><strong>约束k均值</strong>（constrained k-means）算法是利用第一类监督信息的代表，该算法是k-means算法的扩展，它在聚类过程中要确保必连和勿连关系集合中的约束得以满足，否则将返回错误提示。</p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250607135958008.png" width="60%" /></p><h3 id="约束种子k均值">约束种子k均值</h3><p><strong>约束种子k均值</strong>（constrained seed k-means）算法使用少量有标记样本，即假设少量有标记样本属于<span class="math inline"><em>k</em></span>个聚类簇。这样的监督信息利用起来很容易：直接将有标签样本作为种子，用他们初始化k均值算法的k个聚类中心，在聚类粗迭代更新过程中不改变种子样本的簇隶属关系，这样就得到了约束种子k均值算法。</p>]]></content>
      
      
      <categories>
          
          <category> 研究生课程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 课程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MAR的细节</title>
      <link href="/2025/05/20/AIGC/MAR/"/>
      <url>/2025/05/20/AIGC/MAR/</url>
      
        <content type="html"><![CDATA[<p><strong>系列文章：</strong></p><ol class="series-items"><li><a href="/2025/05/20/AIGC/MAR/" title="MAR的细节">MAR的细节</a></li></ol><blockquote><p>原论文：<a href="https://proceedings.neurips.cc/paper_files/paper/2024/hash/66e226469f20625aaebddbe47f0ca997-Abstract-Conference.html">Autoregressive Image Generation without Vector Quantization</a></p><p>源码：<a href="https://github.com/LTH14/mar">https://github.com/LTH14/mar</a></p></blockquote><p>同时参考了<a href="https://zhuanlan.zhihu.com/p/711930343">知乎大佬的博客</a>辅助理解。</p><h1 id="背景">1.背景</h1><p>自回归模型是目前自然语言处理中生成模型的基础，这些模型使用输入的前一个词预测序列中下一个词，考虑到语言的离散性，模型的输入和输出都表示在离散空间中，这种流行的做法使得大家普遍认为自回归模型就一定要使用到离散表征（其中在图像生成中通常使用VQ）。因此本工作旨在解决这个问题，即<strong>自回归模型是否一定要与向量量化（VQ）相结合</strong>？</p><p>本文注意到，其实自回归强调的“predicting next tokens based on previous ones”跟值是离散还是连续无关，真正重要的其实是得到每个token的概率分布，使用离散值表示能很好使用类别分布（Categories Distribution）建模得到，但是这并不代表这就是必要的，如果能找到另一种建模方式，就可以不使用VQ来进行自回归生成。</p><blockquote><p><strong>补充</strong>：为什么要代替VQ呢</p><p>VQ Tokenizer训练不稳定，在训练过程中由于使用了<strong>argmin</strong>这种不可导的计算，导致在梯度回传的过程中是直接将量化向量梯度复制给encode向量的，这个梯度不准确，因此往往导致不准确的训练。</p></blockquote><p>本文尝试在连续值域上使用扩散模型来为每个token的概率分布建模，为每个token预测一个向量<span class="math inline"><em>z</em></span>，作为去噪网络的条件嵌入，输出下一个token的概率分布<span class="math inline"><em>p</em>(<em>x</em>|<em>z</em>)</span>。不仅如此，本文进一步将AR模型和掩码生成模型结合在一起，提出了掩码自回归模型（MAR）。</p><h1 id="method">2.Method</h1><h2 id="回顾离散tokens的做法">2.1.回顾离散tokens的做法</h2><p>在自回归生成任务中，如果采用一个离散的tokenizer来预测下一个<span class="math inline"><em>x</em></span>，那么<span class="math inline"><em>x</em></span>将被表示为一个索引 <span class="math inline">0 ≤ <em>x</em> &lt; <em>K</em></span> 用于在词汇表（大小为<span class="math inline"><em>K</em></span>）中查找对应的表示。此时自回归模型将生成一个<span class="math inline"><em>D</em></span>维的向量<span class="math inline"><em>z</em></span> ，之后将通过与<span class="math inline"><em>K</em></span>分类矩阵 <span class="math inline"><em>W</em> ∈ ℝ<sup><em>K</em> × <em>D</em></sup></span> 相乘得到属于每个分类的概率 <span class="math inline"><em>p</em>(<em>x</em>|<em>z</em>) = <em>s</em><em>o</em><em>f</em><em>t</em><em>m</em><em>a</em><em>x</em>(<em>W</em><em>z</em>)</span> ，取最大概率的词汇得到 <span class="math inline"><em>x</em></span> 。</p><p>回顾这个做法，其中有两点很重要：</p><ul><li>需要有一个能衡量预测分布和真实分布之间差异的<strong>损失函数</strong>；</li><li>在推理时有一个能从分布 <span class="math inline"><em>x</em> ∼ <em>p</em>(<em>x</em>|<em>z</em>)</span> 中采样的<strong>采样器</strong>。</li></ul><p>由此可以发现，其实自回归模型不一定要用离散表征（VQ），对概率分布的建模才是最重要的。</p><h2 id="diffusion-loss">2.2.Diffusion Loss</h2><h3 id="loss-function">2.2.1.Loss Function</h3><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250521143240.png" width="50%"/></p><p>本文使用了扩散模型来建模每个token的概率分布 <span class="math inline"><em>p</em>(<em>x</em>|<em>z</em>)</span> ，而用自回归模型来生成每个token的条件向量 <span class="math inline"><em>z</em></span>（如上图），具体来说本文使用的Diffusion Loss可表述为：</p><p><span class="math display">$$\begin{split}    \mathcal{L}(z,x)=\mathbb{E}_{\varepsilon,t}\left[\left\|\varepsilon-\varepsilon_\theta(x_t|t,z)\right\|^2\right].\end{split}$$</span></p><p>其中需要注意的是，这里的噪声预测模型<span class="math inline"><em>ε</em><sub><em>θ</em></sub></span>使用的是一个小型的MLP，所以计算开销不会过大。这里的条件<span class="math inline"><em>z</em></span>是用自回归网络 <span class="math inline"><em>z</em> = <em>f</em>( ⋅ )</span> 生成的，并且由于使用连续表示，这里 <span class="math inline"><em>z</em></span> 的梯度也是可以通过上面的损失函数进行传播的，所以使用Diffusion Loss也能同步训练自回归网络 <span class="math inline"><em>f</em>( ⋅ )</span> 。</p><p>在训练时，作者提到了一个小trick，Diffusion Loss中的期望值 <span class="math inline">𝔼<sub><em>ε</em>, <em>t</em></sub></span> 是对时间步<span class="math inline"><em>t</em></span>的期望，因此为了训练的稳定，需要在同一个给定的<span class="math inline"><em>z</em></span>下对时间步<span class="math inline"><em>t</em></span>采样多次，而不需要重新计算<span class="math inline"><em>z</em></span>，在实验中设置<span class="math inline"><em>t</em></span>地重复采样次数为4。</p><h3 id="采样">2.2.2.采样</h3><p>这里的采样就是扩散模型中的采样过程，即：</p><p><span class="math display">$$\begin{split}    x_{t-1}=\frac{1}{\sqrt{\alpha_t}}\left(x_t-\frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}_t}}\varepsilon_\theta(x_t|t,z)\right)+\sigma_t\delta.\end{split}$$</span></p><p>同时，参考openAI的CG（2021，Diffusion Beats Gan）中提出的温度控制，这里引入了温度因子<span class="math inline"><em>τ</em></span>来控制采样的随机性（多样性）。具体来说，本文采用的方式是对噪声方差<span class="math inline"><em>σ</em><sub><em>t</em></sub><em>δ</em></span>乘上<span class="math inline"><em>τ</em></span>来控制采样的多样性。</p><h2 id="引入自回归模型">2.3.引入自回归模型</h2><p>对于一个tokens序列<span class="math inline">{<em>x</em><sup>1</sup>, <em>x</em><sup>2</sup>, ..., <em>x</em><sup><em>n</em></sup>}</span>（以某种规定的顺序排序），自回归模型的建模可以表示为：</p><p><span class="math display">$$\begin{split}    p\left(x^1, \ldots, x^n\right)=\prod_{i=1}^n p\left(x^i \mid x^1, \ldots, x^{i-1}\right)\end{split}$$</span></p><p>这里定义的<span class="math inline"><em>x</em><sup><em>i</em></sup></span>可以定义在连续空间中，并且其概率分布是用扩散模型来建模的。而条件向量<span class="math inline"><em>z</em><sup><em>i</em></sup></span>则是通过一个自回归网络生成的：<span class="math inline"><em>z</em><sup><em>i</em></sup> = <em>f</em>(<em>x</em><sup>1</sup>, …, <em>x</em><sup><em>i</em> − 1</sup>)</span>。</p><h2 id="masked-generative-models">2.4.Masked Generative Models</h2><h3 id="mae">2.4.1.MAE</h3><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/bi-attention.png" width="50%"/></p><p>目前大多数的自回归生成采用<strong>因果注意力</strong>（Casual Attention）作为自回归架构（即用前面的预测后面的），但是这种单向的约束其实并不完全符合自回归的定义（即用已知tokens预测未知tokens），因此作者认为应该选择<strong>双向注意力</strong>（Bidirectional Attention）作为自回归架构。进一步说，作者选择了<strong>MAE</strong>（Masked Autoencoder）实现双向注意力，基于未Masked的tokens预测一批masked的tokens。</p><h3 id="随机排序">2.4.2.随机排序</h3><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/MAR_random_order.png" width="50%"/></p><p>本文使用<strong>随机排序</strong>定义token序列，这其实也符合MAE的思想，每个样本对应的tokens序列都是随机的。但在解码阶段，MAE添加了额外的positional embedding来指定要预测的token。</p><p>并且在掩码生成建模中，模型会基于已知的tokens生成一系列tokens而不是仅仅预测一个token，即 <span class="math inline"><em>p</em>({<em>x</em><sup><em>i</em></sup>, <em>x</em><sup><em>i</em> + 1</sup>..., <em>x</em><sup><em>j</em></sup>} ∣ <em>x</em><sup>1</sup>, ..., <em>x</em><sup><em>i</em> − 1</sup>)</span> ，其中 <span class="math inline"><em>i</em> ≤ <em>j</em></span>。相当于每步预测时同步预测好几个tokens，这样可以加速生成过程，作者称其为“next set-of-tokens prediction”。</p><h1 id="实现细节">3.实现细节</h1><h2 id="diffusion-loss-1">3.1.Diffusion Loss</h2><p>作者使用的扩散模型基于iDDPM，使用余弦噪声调度方案，并可以引入<span class="math inline">ℒ<sub><em>v</em><em>l</em><em>b</em></sub></span>，训练过程使用CFG（Classifier-free Guidance）进行训练。噪声预测网络使用多层ResBlock进行构建，条件向量<span class="math inline"><em>z</em></span>和<span class="math inline"><em>t</em></span>是通过AdaLN嵌入（具体可参考DiT）来进行条件控制的。</p><h2 id="ar-and-mar">3.2.AR and MAR</h2><p>作者使用的tokenizer来自于LDM，包括VQ-16和KL-16，Transformer的架构参考ViT，因果注意力的实现来自于GPT。</p><h2 id="总结">3.3.总结</h2><p>具体来说，MAR的实现细节可分为两个阶段：</p><h3 id="训练阶段">3.3.1.训练阶段</h3><ul><li><strong>From pixel to latent</strong>：首先对于输入图像，其实需要先过一层预训练的VAE从像素空间（pixel）转换为隐空间（latent）；</li><li><strong>Patchify</strong>：接着和ViT类似将其划分为patches，从<span class="math inline">ℝ<sup><em>b</em>, <em>c</em>, <em>h</em>, <em>w</em></sup></span>reshape为<span class="math inline">ℝ<sup><em>b</em>, <em>l</em>, <em>d</em></sup></span>，这就是定义在<strong>连续空间上的image tokens</strong>；</li><li><strong>Masking</strong>：得到tokens后，需要mask一部分tokens进行训练，这里作者定义的masking ratio范围为 <span class="math inline">[0.7, 1]</span>。需要注意的是，在同一个batch中的masking ratio是相同的，但是采样的masked tokens是不同的；</li><li><strong>MAE</strong>：在编码解码之前，需要为tokens加上<strong>位置编码</strong>，masked tokens表示为<code>[m]</code>，同时为了避免unmasked的tokens序列过短，作者在tokens序列开头扩充了64个<code>[cls]</code>tokens，并且这个类别tokens也可以适应CFG的训练；</li><li><strong>Diffusion</strong>：根据输入的gt latent <span class="math inline"><em>x</em><sub>0</sub></span> 和MAE解码出来的条件 <span class="math inline"><em>z</em></span> 可以训练扩散模型，具体来说采样一个时间步<span class="math inline"><em>t</em></span>和一个噪音<span class="math inline"><em>ε</em></span>，可以得到<span class="math inline"><em>x</em><sub><em>t</em></sub></span>，通过噪声预测网络得到预测噪声<span class="math inline"><em>ε</em><sub><em>θ</em></sub></span>，通过计算<span class="math inline"><em>ε</em></span>和<span class="math inline"><em>ε</em><sub><em>θ</em></sub></span>的MSE损失进行训练。</li></ul><h3 id="推理阶段">3.3.2.推理阶段</h3><ul><li><strong>Mask</strong>：在推理阶段，作者设置了<strong>余弦衰减的mask ratio调度曲线</strong>（从1.0到0），并设置64步采样。i.e.假设指定的随机token顺序为<span class="math inline">[10, 18, 23, 8, 39, 28, 30, 60...]</span>，那么生成的顺序可以是第一步根据<span class="math inline">[[<em>c</em><em>l</em><em>s</em>], ..., [<em>c</em><em>l</em><em>s</em>]]</span>生成<span class="math inline">[10, 18, 23]</span>，第二步根据<span class="math inline">[[<em>c</em><em>l</em><em>s</em>], ..., [<em>c</em><em>l</em><em>s</em>], 10, 18, 23]</span>生成<span class="math inline">[8, 39, 28, 30]</span>，之后mask放开得越来越多，生成的token也越来越多；</li><li><strong>CFG</strong>：由于在训练时使用Classifier-free Guidance进行训练，因此MAR可以实现无条件生成和条件生成。在条件生成时，给定条件类别label，会生成<strong>class embedding</strong>，同时与<strong>fake latent</strong>进行拼接，这是因为CFG在进行条件生成时需要同时预测条件噪声与无条件噪声（<span class="math inline">∇<sub><em>x</em><sub><em>t</em></sub></sub>log <em>P</em>(<em>x</em><sub><em>t</em></sub>|<em>y</em>) = <em>λ</em>∇<sub><em>x</em><sub><em>t</em></sub></sub>log <em>P</em>(<em>x</em><sub><em>t</em></sub>|<em>y</em>) + (1 − <em>λ</em>)∇<sub><em>x</em><sub><em>t</em></sub></sub>log <em>P</em>(<em>x</em><sub><em>t</em></sub>)</span>），然后作为64个<code>[cls]</code>喂给MAE Encoder；</li><li><strong>Diffusion Sample</strong>：每步MAE生成后，根据对应要生成tokens的条件信息<span class="math inline"><em>z</em></span>，使用Diffusion的逆向过程进行采样即可；</li><li><strong>From latent to pixel</strong>：生成所有tokens后，此时其维度为<span class="math inline">ℝ<sup><em>b</em> × <em>l</em> × <em>d</em></sup></span>，首先将其unpatchify回图像维度，然后通过预训练VAE的Decoder回到原Pixel空间。</li></ul><h1 id="源码">4.源码</h1><p>mar的模型定义在<code>models\mar.py</code>中，从<code>forward</code>函数中入手：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MAR</span>(nn.Module):</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, imgs, labels</span>):</span><br><span class="line">        <span class="comment"># class embed</span></span><br><span class="line">        class_embedding = <span class="variable language_">self</span>.class_emb(labels)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># patchify and mask (drop) tokens</span></span><br><span class="line">        x = <span class="variable language_">self</span>.patchify(imgs)</span><br><span class="line">        gt_latents = x.clone().detach()</span><br><span class="line">        orders = <span class="variable language_">self</span>.sample_orders(bsz=x.size(<span class="number">0</span>))</span><br><span class="line">        mask = <span class="variable language_">self</span>.random_masking(x, orders)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># mae encoder</span></span><br><span class="line">        x = <span class="variable language_">self</span>.forward_mae_encoder(x, mask, class_embedding)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># mae decoder</span></span><br><span class="line">        z = <span class="variable language_">self</span>.forward_mae_decoder(x, mask)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># diffloss</span></span><br><span class="line">        loss = <span class="variable language_">self</span>.forward_loss(z=z, target=gt_latents, mask=mask)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><h2 id="patchify-mask">4.1.Patchify &amp; Mask</h2><p>第一步对类别的embedding，源码直接使用<code>nn.embedding</code>来处理，即<code>self.class_emb = nn.Embedding(class_num, encoder_embed_dim)</code>。第二步<code>patchify</code>的做法很直观，就是和ViT一样将其划分为patches：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">patchify</span>(<span class="params">self, x</span>):</span><br><span class="line">    bsz, c, h, w = x.shape</span><br><span class="line">    p = <span class="variable language_">self</span>.patch_size</span><br><span class="line">    h_, w_ = h // p, w // p</span><br><span class="line"></span><br><span class="line">    x = x.reshape(bsz, c, h_, p, w_, p)</span><br><span class="line">    x = torch.einsum(<span class="string">&#x27;nchpwq-&gt;nhwcpq&#x27;</span>, x)</span><br><span class="line">    x = x.reshape(bsz, h_ * w_, c * p ** <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> x  <span class="comment"># [n, l, d]</span></span><br></pre></td></tr></table></figure><p>接着后续的mask部分分两步，首先使用<code>sample_orders()</code>打乱顺序，然后使用<code>random_masking()</code>来生成token mask，这里的mask ratio是通过截断正态分布生成的，范围在<span class="math inline">[0.75, 1]</span>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">sample_orders</span>(<span class="params">self, bsz</span>):</span><br><span class="line">    <span class="comment"># generate a batch of random generation orders</span></span><br><span class="line">    orders = []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(bsz):</span><br><span class="line">        order = np.array(<span class="built_in">list</span>(<span class="built_in">range</span>(<span class="variable language_">self</span>.seq_len)))</span><br><span class="line">        np.random.shuffle(order)</span><br><span class="line">        orders.append(order)</span><br><span class="line">    orders = torch.Tensor(np.array(orders)).cuda().long()</span><br><span class="line">    <span class="keyword">return</span> orders</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">random_masking</span>(<span class="params">self, x, orders</span>):</span><br><span class="line">    <span class="comment"># generate token mask</span></span><br><span class="line">    bsz, seq_len, embed_dim = x.shape</span><br><span class="line">    mask_rate = <span class="variable language_">self</span>.mask_ratio_generator.rvs(<span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">    num_masked_tokens = <span class="built_in">int</span>(np.ceil(seq_len * mask_rate))</span><br><span class="line">    mask = torch.zeros(bsz, seq_len, device=x.device)</span><br><span class="line">    mask = torch.scatter(mask, dim=-<span class="number">1</span>, index=orders[:, :num_masked_tokens],</span><br><span class="line">                         src=torch.ones(bsz, seq_len, device=x.device))</span><br><span class="line">    <span class="keyword">return</span> mask</span><br></pre></td></tr></table></figure><h2 id="mae-encode-decode">4.2.MAE Encode &amp; Decode</h2><p>首先是Encoder部分：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward_mae_encoder</span>(<span class="params">self, x, mask, class_embedding</span>):</span><br><span class="line">    x = <span class="variable language_">self</span>.z_proj(x)</span><br><span class="line">    bsz, seq_len, embed_dim = x.shape</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 提前预留出64个[cls]tokens的位置，并且也为mask向量预留出同样的位置</span></span><br><span class="line">    x = torch.cat([torch.zeros(bsz, <span class="variable language_">self</span>.buffer_size, embed_dim, device=x.device), x], dim=<span class="number">1</span>)</span><br><span class="line">    mask_with_buffer = torch.cat([torch.zeros(x.size(<span class="number">0</span>), <span class="variable language_">self</span>.buffer_size, device=x.device), mask], dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># CFG训练 此处的fake_latent表示无条件生成</span></span><br><span class="line">    <span class="keyword">if</span> <span class="variable language_">self</span>.training:</span><br><span class="line">        drop_latent_mask = torch.rand(bsz) &lt; <span class="variable language_">self</span>.label_drop_prob</span><br><span class="line">        drop_latent_mask = drop_latent_mask.unsqueeze(-<span class="number">1</span>).cuda().to(x.dtype)</span><br><span class="line">        class_embedding = drop_latent_mask * <span class="variable language_">self</span>.fake_latent + (<span class="number">1</span> - drop_latent_mask) * class_embedding</span><br><span class="line"></span><br><span class="line">    x[:, :<span class="variable language_">self</span>.buffer_size] = class_embedding.unsqueeze(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 引入可学习的位置编码</span></span><br><span class="line">    x = x + <span class="variable language_">self</span>.encoder_pos_embed_learned</span><br><span class="line">    x = <span class="variable language_">self</span>.z_proj_ln(x)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 只保留unmask的部分</span></span><br><span class="line">    x = x[(<span class="number">1</span>-mask_with_buffer).nonzero(as_tuple=<span class="literal">True</span>)].reshape(bsz, -<span class="number">1</span>, embed_dim)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># transformer encoder</span></span><br><span class="line">    <span class="keyword">if</span> <span class="variable language_">self</span>.grad_checkpointing <span class="keyword">and</span> <span class="keyword">not</span> torch.jit.is_scripting():</span><br><span class="line">        <span class="keyword">for</span> block <span class="keyword">in</span> <span class="variable language_">self</span>.encoder_blocks:</span><br><span class="line">            x = checkpoint(block, x)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">for</span> block <span class="keyword">in</span> <span class="variable language_">self</span>.encoder_blocks:</span><br><span class="line">            x = block(x)</span><br><span class="line">    x = <span class="variable language_">self</span>.encoder_norm(x)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><p>接着是Decoder部分：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward_mae_decoder</span>(<span class="params">self, x, mask</span>):</span><br><span class="line">    x = <span class="variable language_">self</span>.decoder_embed(x)</span><br><span class="line">    mask_with_buffer = torch.cat([torch.zeros(x.size(<span class="number">0</span>), <span class="variable language_">self</span>.buffer_size, device=x.device), mask], dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 和Encode一样填充前面的[cls]</span></span><br><span class="line">    mask_tokens = <span class="variable language_">self</span>.mask_token.repeat(mask_with_buffer.shape[<span class="number">0</span>], mask_with_buffer.shape[<span class="number">1</span>], <span class="number">1</span>).to(x.dtype)</span><br><span class="line">    x_after_pad = mask_tokens.clone()</span><br><span class="line">    x_after_pad[(<span class="number">1</span> - mask_with_buffer).nonzero(as_tuple=<span class="literal">True</span>)] = x.reshape(x.shape[<span class="number">0</span>] * x.shape[<span class="number">1</span>], x.shape[<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># decoder position embedding</span></span><br><span class="line">    x = x_after_pad + <span class="variable language_">self</span>.decoder_pos_embed_learned</span><br><span class="line"></span><br><span class="line">    <span class="comment"># apply Transformer blocks</span></span><br><span class="line">    <span class="keyword">if</span> <span class="variable language_">self</span>.grad_checkpointing <span class="keyword">and</span> <span class="keyword">not</span> torch.jit.is_scripting():</span><br><span class="line">        <span class="keyword">for</span> block <span class="keyword">in</span> <span class="variable language_">self</span>.decoder_blocks:</span><br><span class="line">            x = checkpoint(block, x)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">for</span> block <span class="keyword">in</span> <span class="variable language_">self</span>.decoder_blocks:</span><br><span class="line">            x = block(x)</span><br><span class="line">    x = <span class="variable language_">self</span>.decoder_norm(x)</span><br><span class="line"></span><br><span class="line">    x = x[:, <span class="variable language_">self</span>.buffer_size:]</span><br><span class="line">    x = x + <span class="variable language_">self</span>.diffusion_pos_embed_learned</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><h2 id="diffusion-loss-2">4.3.Diffusion Loss</h2><p>关于Diffusion Loss的计算，这里使用的是iDDPM的计算，即需要加上<span class="math inline">ℒ<sub><em>v</em><em>l</em><em>b</em></sub></span>，并使用预测方差，具体的diffusion部分可参考<a href="https://litchi-lee.github.io/2025/05/08/AIGC/iDDPM/">iDDPM</a>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MAR</span>(nn.Module):</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward_loss</span>(<span class="params">self, z, target, mask</span>):</span><br><span class="line">        bsz, seq_len, _ = target.shape</span><br><span class="line">        target = target.reshape(bsz * seq_len, -<span class="number">1</span>).repeat(<span class="variable language_">self</span>.diffusion_batch_mul, <span class="number">1</span>)</span><br><span class="line">        z = z.reshape(bsz*seq_len, -<span class="number">1</span>).repeat(<span class="variable language_">self</span>.diffusion_batch_mul, <span class="number">1</span>)</span><br><span class="line">        mask = mask.reshape(bsz*seq_len).repeat(<span class="variable language_">self</span>.diffusion_batch_mul)</span><br><span class="line">        loss = <span class="variable language_">self</span>.diffloss(z=z, target=target, mask=mask)</span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DiffLoss</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, target, z, mask=<span class="literal">None</span></span>):</span><br><span class="line">        t = torch.randint(<span class="number">0</span>, <span class="variable language_">self</span>.train_diffusion.num_timesteps, (target.shape[<span class="number">0</span>],), device=target.device)</span><br><span class="line">        model_kwargs = <span class="built_in">dict</span>(c=z)</span><br><span class="line">        loss_dict = <span class="variable language_">self</span>.train_diffusion.training_losses(<span class="variable language_">self</span>.net, target, t, model_kwargs)</span><br><span class="line">        loss = loss_dict[<span class="string">&quot;loss&quot;</span>]</span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            loss = (loss * mask).<span class="built_in">sum</span>() / mask.<span class="built_in">sum</span>()</span><br><span class="line">        <span class="keyword">return</span> loss.mean()</span><br></pre></td></tr></table></figure><p>这里不太一样的是噪声预测网络的实现，本文使用的是一个简单的MLP，条件则是通过AdaLN进行控制的（类似于<a href="https://litchi-lee.github.io/2025/05/08/AIGC/DIT/">DiT</a>）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleMLPAdaLN</span>(nn.Module):</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, t, c</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Apply the model to an input batch.</span></span><br><span class="line"><span class="string">        :param x: an [N x C] Tensor of inputs.</span></span><br><span class="line"><span class="string">        :param t: a 1-D batch of timesteps.</span></span><br><span class="line"><span class="string">        :param c: conditioning from AR transformer.</span></span><br><span class="line"><span class="string">        :return: an [N x C] Tensor of outputs.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        x = <span class="variable language_">self</span>.input_proj(x)</span><br><span class="line">        t = <span class="variable language_">self</span>.time_embed(t)</span><br><span class="line">        c = <span class="variable language_">self</span>.cond_embed(c)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将时间步信息和条件信息融合</span></span><br><span class="line">        y = t + c</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.grad_checkpointing <span class="keyword">and</span> <span class="keyword">not</span> torch.jit.is_scripting():</span><br><span class="line">            <span class="keyword">for</span> block <span class="keyword">in</span> <span class="variable language_">self</span>.res_blocks:</span><br><span class="line">                x = checkpoint(block, x, y)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">for</span> block <span class="keyword">in</span> <span class="variable language_">self</span>.res_blocks:</span><br><span class="line">                x = block(x, y)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.final_layer(x, y)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ResBlock</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        channels</span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.channels = channels</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.in_ln = nn.LayerNorm(channels, eps=<span class="number">1e-6</span>)</span><br><span class="line">        <span class="variable language_">self</span>.mlp = nn.Sequential(</span><br><span class="line">            nn.Linear(channels, channels, bias=<span class="literal">True</span>),</span><br><span class="line">            nn.SiLU(),</span><br><span class="line">            nn.Linear(channels, channels, bias=<span class="literal">True</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.adaLN_modulation = nn.Sequential(</span><br><span class="line">            nn.SiLU(),</span><br><span class="line">            nn.Linear(channels, <span class="number">3</span> * channels, bias=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, y</span>):</span><br><span class="line">        shift_mlp, scale_mlp, gate_mlp = <span class="variable language_">self</span>.adaLN_modulation(y).chunk(<span class="number">3</span>, dim=-<span class="number">1</span>)</span><br><span class="line">        h = modulate(<span class="variable language_">self</span>.in_ln(x), shift_mlp, scale_mlp)</span><br><span class="line">        h = <span class="variable language_">self</span>.mlp(h)</span><br><span class="line">        <span class="keyword">return</span> x + gate_mlp * h</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AIGC </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>随手记录一些常用知识点</title>
      <link href="/2025/05/10/AI/tricks/"/>
      <url>/2025/05/10/AI/tricks/</url>
      
        <content type="html"><![CDATA[<h1 id="正向kl散度-vs-反向kl散度">1.正向KL散度 VS 反向KL散度</h1><p>正向KL散度（Forward KL divergence）和反向KL散度（Reverse KL divergence）是衡量两个概率分布差异的常用方法，这两个散度在使用时的侧重优化目标是有些许差异的。</p><p>设有两个概率分布<span class="math inline"><em>P</em>(<em>x</em>), <em>Q</em>(<em>x</em>)</span>，其中前者为真实分布，后者为近似分布。则<strong>正向KL散度</strong>的定义为：</p><p><span class="math display">$$\begin{split}    D_{KL}(P\|Q) = \sum_x P(x) \log\frac{P(x)}{Q(x)}\end{split}$$</span></p><blockquote><p>正向KL的特点是：<strong>模式覆盖</strong>（mode covering）</p><p>把真实分布<span class="math inline"><em>P</em>(<em>x</em>)</span>作为基准，衡量Q与P的差异，换句话说其目标是让近似分布 <span class="math inline"><em>Q</em>(<em>x</em>)</span> 逼近真实分布 <span class="math inline"><em>P</em>(<em>x</em>)</span> 。 正向KL会惩罚P很大但是Q很小的情况，所以它会更<strong>倾向于让Q覆盖P的所有区域</strong>，所以正向KL也叫做模式覆盖，其更容易产生更分散的分布。</p></blockquote><p><strong>反向KL散度</strong>的定义为：</p><p><span class="math display">$$\begin{split}    D_{KL}(Q\|P) = \sum_x Q(x) \log\frac{Q(x)}{P(x)}\end{split}$$</span></p><blockquote><p>反向KL的特点是：<strong>模式检索</strong>（mode seeking）</p><p>它把近似分布<span class="math inline"><em>Q</em>(<em>x</em>)</span>作为基准，惩罚P很小但是Q很大的情况，更<strong>倾向于让Q集中在P的高概率区域</strong>，但是不一定能覆盖P的全部区域，更为激进会更容易产生模式坍塌（mode collapse）。</p></blockquote><p>举个例子来说，假设P是一个双峰分布，而Q是一个单峰分布：</p><ul><li>正向KL：为了减小KL散度，Q会尽量去覆盖两个峰；</li><li>反向KL：Q更有可能选择只拟合其中一个概率更大的单峰，而忽略另一个。</li></ul><h1 id="ema">2.EMA</h1><p>EMA（指数移动平均）是<strong>Exponential Moving Average</strong>的缩写，是一种对数据序列进行平滑的统计方法，它给予最近的数据更大的权重，而旧数据的权重指数级衰减。常用于模型参数平滑、损失平滑等，给定某个变量（比如损失、权重）在时间<span class="math inline"><em>t</em></span>的值为<span class="math inline"><em>x</em><sub><em>t</em></sub></span>，则EMA的更新公式为：</p><p><span class="math display">$$\begin{split}    EMA_t = \alpha \cdot x_t + (1-\alpha) \cdot EMA_{t-1}\end{split}$$</span></p><p>其中 <span class="math inline"><em>α</em> ∈ (0, 1)</span> 是平滑系数（也叫做衰减率），<span class="math inline"><em>α</em></span> 越小EMA更新越平稳，相反<span class="math inline"><em>α</em></span>越大EMA更新对当前值更敏感。在实际使用中，通常设为0.99或0.999。</p>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DiT的细节</title>
      <link href="/2025/05/08/AIGC/DIT/"/>
      <url>/2025/05/08/AIGC/DIT/</url>
      
        <content type="html"><![CDATA[<p><strong>系列文章：</strong></p><ol class="series-items"><li><a href="/2025/05/08/AIGC/DIT/" title="DiT的细节">DiT的细节</a></li></ol><blockquote><p>原论文： <a href="http://openaccess.thecvf.com/content/ICCV2023/html/Peebles_Scalable_Diffusion_Models_with_Transformers_ICCV_2023_paper.html">Scalable diffusion models with transformers</a></p><p>源码： <a href="https://github.com/facebookresearch/DiT">https://github.com/facebookresearch/DiT</a></p></blockquote><h1 id="背景介绍">1.背景介绍</h1><p>DiT使用LDM的架构来生成图片，但是它<strong>将通常使用的UNet架构替换为了Transformer架构</strong>，作者发现这样做不仅可以实现高质量的生成，并且由于使用了Transformer作为噪声预测网络，该方法的Scaling Laws也拟合得很好。</p><p>DDPM首次提出使用UNet作为DM的骨架，是因为UNet在像素级自回归模型和条件GAN中表现都很出色，后续的很多工作也延续了这一架构。DiT通过一系列实验表明其实UNet的归纳偏置对扩散模型的性能其实并不重要，因此扩散模型完全有能力从最近的Transformer架构中获益，通过继承其他领域的实践经验和训练方法，保留可扩展性、鲁棒性和效率等特性。</p><h1 id="method">2.Method</h1><h2 id="序言">2.1.序言</h2><h3 id="diffusion">2.1.1.Diffusion</h3><p>对于DDPM的前向过程，具体过程可参考<a href="https://litchi-lee.github.io/2025/04/15/AIGC/DDPM/">DDPM</a>，可以总结为：</p><p><span class="math display">$$\begin{split}    q(x_t \mid x_0) = \mathcal{N}\bigl(x_t;\;\sqrt{\bar\alpha_t}\,x_0,\;(1-\bar\alpha_t)\,I\bigr).\end{split}$$</span></p><p>Diffusion Model的训练目标是要拟合出逆向过程的概率分布：</p><p><span class="math display">$$\begin{split}    p_\theta(x_{t-1} \mid x_t) = \mathcal{N}\bigl(\mu_\theta(x_t), \Sigma_\theta(x_t)\bigr).\end{split}$$</span></p><p>逆向过程的训练则是通过最小化 <span class="math inline"><em>x</em><sub>0</sub></span> 的对数似然的变分下界进行，其中第一项表示<strong>重建项（reconstruction term）</strong>，第二项表示<strong>一致项（consistency term）</strong>：</p><p><span class="math display">$$\begin{split}    \mathcal{L}(\theta)=-p(x_0|x_1)+\sum_t\mathcal{D}_{KL}(q^*(x_{t-1}|x_t,x_0)||p_\theta(x_{t-1}|x_t))\end{split}$$</span></p><p>通过进一步重参数化推导，并使用噪声预测网络<span class="math inline"><em>ϵ</em><sub><em>t</em></sub><em>h</em><em>e</em><em>t</em><em>a</em></span>，最终上述损失可以重新组织为对预测噪声和真实采样噪声间的均方误差：</p><p><span class="math display">$$\begin{split}    L_{\mathrm{simple}}=E_{t,x_0,\epsilon}\left[||\epsilon-\epsilon_\theta(x_t,t)||^2\right]\end{split}$$</span></p><p>在DDPM中使用不可学习的方差 <span class="math inline"><em>Σ</em><sub><em>θ</em></sub>(<em>x</em><sub><em>t</em></sub>)</span> ，但是后续iDDPM对这一部分进行了优化，为了训练可学习的方差，额外引入了 <span class="math inline">ℒ<sub><em>v</em><em>l</em><em>b</em></sub></span> ，具体可参考<a href="https://litchi-lee.github.io/2025/05/08/AIGC/iDDPM/">iDDPM</a>：</p><p><span class="math display">$$\begin{split}    L_{\mathrm{hybrid}}=L_{\mathrm{simple}}+\lambda L_{\mathrm{vlb}}\end{split}$$</span></p><h3 id="cfg">2.1.2.CFG</h3><p>条件扩散模型需要输入额外的条件信息作为输入（比如类标签<span class="math inline"><em>c</em></span>），通常使用<strong>Classifier-Free Guidance</strong>的方式进行训练。具体来说，由贝叶斯公式可以推导：<span class="math inline">∇<sub><em>x</em></sub>log <em>p</em>(<em>c</em> ∣ <em>x</em>) ∝ ∇<sub><em>x</em></sub>log <em>p</em>(<em>x</em> ∣ <em>c</em>) − ∇<sub><em>x</em></sub>log <em>p</em>(<em>x</em>)</span>，代入Classifier Guidance中公式：<span class="math inline">∇<sub><em>x</em></sub>log <em>p</em>(<em>x</em> ∣ <em>c</em>) = <em>s</em> ⋅ ∇<sub><em>x</em></sub>log <em>p</em>(<em>c</em> ∣ <em>x</em>) + ∇<sub><em>x</em></sub>log <em>p</em>(<em>x</em>)</span>，即可得到CFG的计算公式：</p><p><span class="math display">$$\begin{gather}\begin{split}    \hat{\epsilon}_{\theta}(x_{t},c)&amp;=\epsilon_{\theta}(x_{t},\emptyset)+ s\cdot\nabla_{x}\log p(x|c) \\    &amp;\propto \epsilon_{\theta}(x_{t},\emptyset)+s\cdot(\epsilon_{\theta}(x_{t},c)-\epsilon_{\theta}(x_{t},\emptyset))\end{split}\end{gather}$$</span></p><p>其中当 <span class="math inline"><em>c</em> = ∅</span> 时表示无条件生成，通过调节超参数<span class="math inline"><em>s</em></span>的值，可以控制条件引导的强度。</p><h3 id="ldm">2.1.3.LDM</h3><p>在高分辨率像素空间（Pixel Space）中训练扩散模型计算开销是非常大的，Latent Diffusion Models通过两阶段处理来解决这个问题：首先学习一个自编码器将图像压缩到潜在空间（Latent Space）表征中，之后在这个潜在空间中训练扩散模型，具体可参考<a href="https://litchi-lee.github.io/2025/04/27/AIGC/LDM/">LDM</a>。</p><p>在这里DiT也使用LDM的思路，即在潜在空间中训练扩散模型。</p><h2 id="diffusion-transformer">2.2.Diffusion Transformer</h2><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/Patchify.png" width="50%" /></p><p>如上图所示，DiT基于ViT来构建Transformer，即通过对图像进行<strong>Patchify</strong>来生成图像tokens，对于DiT的输入 <span class="math inline"><em>z</em> ∈ ℝ<sup><em>b</em>, <em>c</em>, <em>h</em>, <em>w</em></sup></span> ，经过Patchify后输出 <span class="math inline"><em>z</em>′ ∈ ℝ<sup><em>b</em>, <em>T</em>, <em>d</em></sup></span> 。其中patch size参数<span class="math inline"><em>p</em></span>用于调节patch的大小，可以用于控制Patchify精度。</p><p>Patchify得到tokens后，就可以送入transformer block中进行序列生成了（在这里相当于是生成噪音tokens）。但是还有一个问题，条件信息<span class="math inline"><em>y</em></span>和时间步信息<span class="math inline"><em>t</em></span>该如何嵌入到transformer中控制条件生成呢？</p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/DiT_0.png" /></p><p>如上图所示，这里DiT进行了四种架构的尝试。</p><h3 id="in-context-conditioning">2.2.1.In-Context Conditioning</h3><p>直接将条件信息<span class="math inline"><em>c</em></span>和<span class="math inline"><em>t</em></span>的embedding作为两个额外的tokens添加到输入tokens中，这和原ViT使用<code>[cls]</code>的方法是一样的，因此都不用对ViT的结构进行修改。</p><h3 id="cross-attention-block">2.2.2.Cross-Attention Block</h3><p>使用Cross-Attention需要额外增加一个多头交叉注意力模块来嵌入条件，条件输入时直接将两个条件信息的embedding拼接起来输入，但是这种做法会增加很多计算量。</p><h3 id="adaln-block">2.2.3.AdaLN Block</h3><blockquote><p><strong>补充：AdaLN</strong></p><p>首先对于普通的LN，通常的做法是：</p><p><span class="math display">$$ LayerNorm(x) = \frac{x-\mu}{\sigma}\cdot\gamma + \beta $$</span></p><p>其中<span class="math inline"><em>μ</em>, <em>σ</em></span>是输入<span class="math inline"><em>x</em></span>的均值和标准差，<span class="math inline"><em>γ</em>, <em>β</em></span>则分别是缩放因子和偏移因子，表示缩放和偏移。</p><p>而AdaLN中，模型不再使用固定的<span class="math inline"><em>γ</em></span>和<span class="math inline"><em>β</em></span>，而是通过外部输入条件<span class="math inline"><em>y</em></span>来生成与输入<span class="math inline"><em>x</em></span>相同维度的缩放参数和偏移参数。具体来说，通常的做法是让<span class="math inline"><em>y</em></span>通过一层Linear网络得到<strong>三个参数<code>shift</code>、<code>scale</code>和<code>gate</code></strong>，其中<code>shift</code>和<code>scale</code>分别用于代替<span class="math inline"><em>β</em></span>和<span class="math inline"><em>γ</em></span>，而<code>gate</code>是一个可学习的残差门控参数，用于在ResBlock中控制多大程度保留这里的隐层输出<span class="math inline"><em>h</em></span>，即：</p><p><span class="math display">$$ h=\frac{x-\mu}{\sigma}\cdot scale + shift $$</span></p><p>接着使用门控参数<code>gate</code>控制多大程度保留隐层输出：</p><p><span class="math display">$$\begin{split}x'=x+gate\cdot h \end{split}$$</span></p></blockquote><p>DiT将原Transformer Block中的LN层替换为了adaLN层，并通过MLP将输入条件embedding转换为AdaLN层的缩放和偏移参数<span class="math inline"><em>γ</em>, <em>β</em></span>。在前三个方法中，AdaLN方法所带来的额外计算量是最小的。</p><h3 id="adaln-zero-block">2.2.4.AdaLN-Zero Block</h3><p>原来的AdaLN方法可以表述为：</p><p><span class="math display">$$\begin{split}    \text{AdaLN}(x)=\gamma(c)\cdot\frac{x-\mu}{\sigma}+\beta(c)\end{split}$$</span></p><p>但是后续的研究发现使用Zero-Initializing能加速模型的训练，该设计的含义即初始化时让调节参数 <span class="math inline">(<em>γ</em>, <em>β</em>)</span> 为零向量，使模型在无条件控制下初始行为等价于不使用条件信息。这种做法的好处是在训练初期不会因为条件噪声干扰模型主干网络的训练，其过程可以表述为：</p><p><span class="math display">$$\begin{split}    \text{AdaLN-Zero}(x)=(1+\Delta\gamma(c))\cdot\frac{x-\mu}{\sigma}+\Delta\beta(c)\end{split}$$</span></p><p>除此之外，DiT还使用了额外的一个缩放参数 <span class="math inline"><em>α</em></span> 对残差连接进行处理。经过实验，作者发现该方法的效果是最好的，因此DiT最后使用<strong>AdaLN-Zero</strong>来进行条件控制。</p><h1 id="源码">3.源码</h1><h2 id="训练">3.1.训练</h2><p>训练时的workflow主要定义在<code>train.py</code>中，每个epoch需要完成的流程为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(args.epochs):</span><br><span class="line">    sampler.set_epoch(epoch)</span><br><span class="line">    logger.info(<span class="string">f&quot;Beginning epoch <span class="subst">&#123;epoch&#125;</span>...&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> loader:</span><br><span class="line">        x = x.to(device)</span><br><span class="line">        y = y.to(device)</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            <span class="comment"># Map input images to latent space + normalize latents:</span></span><br><span class="line">            x = vae.encode(x).latent_dist.sample().mul_(<span class="number">0.18215</span>)</span><br><span class="line">        t = torch.randint(<span class="number">0</span>, diffusion.num_timesteps, (x.shape[<span class="number">0</span>],), device=device)</span><br><span class="line">        model_kwargs = <span class="built_in">dict</span>(y=y)</span><br><span class="line">        loss_dict = diffusion.training_losses(model, x, t, model_kwargs)</span><br><span class="line">        loss = loss_dict[<span class="string">&quot;loss&quot;</span>].mean()</span><br><span class="line">        opt.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        opt.step()</span><br><span class="line">        update_ema(ema, model.module)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Log loss values:</span></span><br><span class="line">        running_loss += loss.item()</span><br><span class="line">        log_steps += <span class="number">1</span></span><br><span class="line">        train_steps += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> train_steps % args.log_every == <span class="number">0</span>:</span><br><span class="line">            <span class="comment"># Measure training speed:</span></span><br><span class="line">            torch.cuda.synchronize()</span><br><span class="line">            end_time = time()</span><br><span class="line">            steps_per_sec = log_steps / (end_time - start_time)</span><br><span class="line">            <span class="comment"># Reduce loss history over all processes:</span></span><br><span class="line">            avg_loss = torch.tensor(running_loss / log_steps, device=device)</span><br><span class="line">            dist.all_reduce(avg_loss, op=dist.ReduceOp.SUM)</span><br><span class="line">            avg_loss = avg_loss.item() / dist.get_world_size()</span><br><span class="line">            logger.info(<span class="string">f&quot;(step=<span class="subst">&#123;train_steps:07d&#125;</span>) Train Loss: <span class="subst">&#123;avg_loss:<span class="number">.4</span>f&#125;</span>, Train Steps/Sec: <span class="subst">&#123;steps_per_sec:<span class="number">.2</span>f&#125;</span>&quot;</span>)</span><br><span class="line">            <span class="comment"># Reset monitoring variables:</span></span><br><span class="line">            running_loss = <span class="number">0</span></span><br><span class="line">            log_steps = <span class="number">0</span></span><br><span class="line">            start_time = time()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Save DiT checkpoint:</span></span><br><span class="line">        <span class="keyword">if</span> train_steps % args.ckpt_every == <span class="number">0</span> <span class="keyword">and</span> train_steps &gt; <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">if</span> rank == <span class="number">0</span>:</span><br><span class="line">                checkpoint = &#123;</span><br><span class="line">                    <span class="string">&quot;model&quot;</span>: model.module.state_dict(),</span><br><span class="line">                    <span class="string">&quot;ema&quot;</span>: ema.state_dict(),</span><br><span class="line">                    <span class="string">&quot;opt&quot;</span>: opt.state_dict(),</span><br><span class="line">                    <span class="string">&quot;args&quot;</span>: args</span><br><span class="line">                &#125;</span><br><span class="line">                checkpoint_path = <span class="string">f&quot;<span class="subst">&#123;checkpoint_dir&#125;</span>/<span class="subst">&#123;train_steps:07d&#125;</span>.pt&quot;</span></span><br><span class="line">                torch.save(checkpoint, checkpoint_path)</span><br><span class="line">                logger.info(<span class="string">f&quot;Saved checkpoint to <span class="subst">&#123;checkpoint_path&#125;</span>&quot;</span>)</span><br><span class="line">            dist.barrier()</span><br></pre></td></tr></table></figure><p>其中最重要的逻辑在中间部分：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> x, y <span class="keyword">in</span> loader:</span><br><span class="line">    x = x.to(device)</span><br><span class="line">    y = y.to(device)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="comment"># Map input images to latent space + normalize latents:</span></span><br><span class="line">        x = vae.encode(x).latent_dist.sample().mul_(<span class="number">0.18215</span>)</span><br><span class="line">    t = torch.randint(<span class="number">0</span>, diffusion.num_timesteps, (x.shape[<span class="number">0</span>],), device=device)</span><br><span class="line">    model_kwargs = <span class="built_in">dict</span>(y=y)</span><br><span class="line">    loss_dict = diffusion.training_losses(model, x, t, model_kwargs)</span><br></pre></td></tr></table></figure><p>其主要workflow可以总结为以下几步：</p><ul><li>得到输入图像<span class="math inline"><em>x</em></span>和条件<span class="math inline"><em>y</em></span>，对<span class="math inline"><em>x</em></span>进行VAE编码将其变换到latent空间中，并额外引入了一个<strong>经验性缩放常数</strong>（这里是SD训练时用到的经验性常数0.18215，因为这里使用的是SD中的VAE），将输入图像在latent空间中的值变换到<span class="math inline">[ − 1, 1]</span>；</li><li>随机采样一个时间步<span class="math inline"><em>t</em></span>，<span class="math inline"><em>t</em> ∈ [0, <em>n</em><em>u</em><em>m</em>_<em>s</em><em>t</em><em>e</em><em>p</em><em>s</em>]</span>；</li><li>将输入图像<span class="math inline"><em>x</em></span>、条件<span class="math inline"><em>y</em></span>和时间步<span class="math inline"><em>t</em></span>输入扩散模型进行训练。</li></ul><h3 id="diffusion-1">3.1.1.Diffusion</h3><p>由于作者这里使用的是可学习方差，因此这一部分的代码主要来自iDDPM，我们主要关注这里的loss是如何计算出来的。这里使用的是<strong>混合损失</strong>，即：</p><p><span class="math display">$$\begin{split}    L_{\mathrm{hybrid}}=L_{\mathrm{mse}}+\lambda L_{\mathrm{vlb}}\end{split}$$</span></p><p>其中<span class="math inline"><em>λ</em> = 0.001</span>，用于防止<span class="math inline"><em>L</em><sub><em>v</em><em>l</em><em>b</em></sub></span>过大影响MSE损失。同时，在预测模型输出的时候也会同步输出预测方差，<code>model_output, model_var_values = th.split(model_output, C, dim=1)</code>，这里的输出会多一层。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">GaussianDiffusion</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">training_losses</span>(<span class="params">self, model, x_start, t, model_kwargs=<span class="literal">None</span>, noise=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="keyword">if</span> model_kwargs <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            model_kwargs = &#123;&#125;  <span class="comment"># 无条件</span></span><br><span class="line">        <span class="keyword">if</span> noise <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            noise = th.randn_like(x_start)  <span class="comment"># 采样噪声</span></span><br><span class="line">        <span class="comment"># 前向过程得到x_t</span></span><br><span class="line">        x_t = <span class="variable language_">self</span>.q_sample(x_start, t, noise=noise)</span><br><span class="line"></span><br><span class="line">        terms = &#123;&#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment"># KL散度损失</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.loss_type == LossType.KL <span class="keyword">or</span> <span class="variable language_">self</span>.loss_type == LossType.RESCALED_KL:</span><br><span class="line">            terms[<span class="string">&quot;loss&quot;</span>] = <span class="variable language_">self</span>._vb_terms_bpd(</span><br><span class="line">                model=model,</span><br><span class="line">                x_start=x_start,</span><br><span class="line">                x_t=x_t,</span><br><span class="line">                t=t,</span><br><span class="line">                clip_denoised=<span class="literal">False</span>,</span><br><span class="line">                model_kwargs=model_kwargs,</span><br><span class="line">            )[<span class="string">&quot;output&quot;</span>]</span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>.loss_type == LossType.RESCALED_KL:</span><br><span class="line">                terms[<span class="string">&quot;loss&quot;</span>] *= <span class="variable language_">self</span>.num_timesteps</span><br><span class="line">        <span class="comment"># MSE损失</span></span><br><span class="line">        <span class="keyword">elif</span> <span class="variable language_">self</span>.loss_type == LossType.MSE <span class="keyword">or</span> <span class="variable language_">self</span>.loss_type == LossType.RESCALED_MSE:</span><br><span class="line">            model_output = model(x_t, t, **model_kwargs)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 可学习方差</span></span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>.model_var_type <span class="keyword">in</span> [</span><br><span class="line">                ModelVarType.LEARNED,</span><br><span class="line">                ModelVarType.LEARNED_RANGE,</span><br><span class="line">            ]:</span><br><span class="line">                B, C = x_t.shape[:<span class="number">2</span>]</span><br><span class="line">                <span class="keyword">assert</span> model_output.shape == (B, C * <span class="number">2</span>, *x_t.shape[<span class="number">2</span>:])</span><br><span class="line">                model_output, model_var_values = th.split(model_output, C, dim=<span class="number">1</span>)</span><br><span class="line">                frozen_out = th.cat([model_output.detach(), model_var_values], dim=<span class="number">1</span>)</span><br><span class="line">                terms[<span class="string">&quot;vb&quot;</span>] = <span class="variable language_">self</span>._vb_terms_bpd(</span><br><span class="line">                    model=<span class="keyword">lambda</span> *args, r=frozen_out: r,</span><br><span class="line">                    x_start=x_start,</span><br><span class="line">                    x_t=x_t,</span><br><span class="line">                    t=t,</span><br><span class="line">                    clip_denoised=<span class="literal">False</span>,</span><br><span class="line">                )[<span class="string">&quot;output&quot;</span>]</span><br><span class="line">                <span class="keyword">if</span> <span class="variable language_">self</span>.loss_type == LossType.RESCALED_MSE:</span><br><span class="line">                    <span class="comment"># 乘上一个很小的系数防止影响MSE损失</span></span><br><span class="line">                    terms[<span class="string">&quot;vb&quot;</span>] *= <span class="variable language_">self</span>.num_timesteps / <span class="number">1000.0</span></span><br><span class="line"></span><br><span class="line">            target = &#123;</span><br><span class="line">                ModelMeanType.PREVIOUS_X: <span class="variable language_">self</span>.q_posterior_mean_variance(</span><br><span class="line">                    x_start=x_start, x_t=x_t, t=t</span><br><span class="line">                )[<span class="number">0</span>],</span><br><span class="line">                ModelMeanType.START_X: x_start,</span><br><span class="line">                ModelMeanType.EPSILON: noise,</span><br><span class="line">            &#125;[<span class="variable language_">self</span>.model_mean_type]</span><br><span class="line">            <span class="keyword">assert</span> model_output.shape == target.shape == x_start.shape</span><br><span class="line">            <span class="comment"># 计算噪声的MSE损失</span></span><br><span class="line">            terms[<span class="string">&quot;mse&quot;</span>] = mean_flat((target - model_output) ** <span class="number">2</span>)</span><br><span class="line">            <span class="keyword">if</span> <span class="string">&quot;vb&quot;</span> <span class="keyword">in</span> terms:</span><br><span class="line">                terms[<span class="string">&quot;loss&quot;</span>] = terms[<span class="string">&quot;mse&quot;</span>] + terms[<span class="string">&quot;vb&quot;</span>]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                terms[<span class="string">&quot;loss&quot;</span>] = terms[<span class="string">&quot;mse&quot;</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> NotImplementedError(<span class="variable language_">self</span>.loss_type)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> terms</span><br></pre></td></tr></table></figure><h3 id="dit">3.1.2.DiT</h3><p>DiT中噪声预测网络定义在<code>DiT</code>类中，对于训练过程，输入图像 <span class="math inline"><em>x</em> ∈ ℝ<sup><em>b</em>, <em>c</em>, <em>h</em>, <em>w</em></sup></span> 、时间步条件 <span class="math inline"><em>t</em></span> 和类别条件 <span class="math inline"><em>c</em></span>，输出预测噪声<span class="math inline"><em>ϵ</em><sub><em>θ</em></sub></span>（对于iDDPM还会输出预测方差<span class="math inline"><em>Σ</em><sub><em>θ</em></sub></span>）。三个输入首先都会通过一层embedder转换成相应的嵌入向量，然后将两个条件向量相加送入DiT Block进行噪声预测，最后将输出unpatchify为原形状。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DiT</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, t, y</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Forward pass of DiT.</span></span><br><span class="line"><span class="string">        x: (N, C, H, W) tensor of spatial inputs (images or latent representations of images)</span></span><br><span class="line"><span class="string">        t: (N,) tensor of diffusion timesteps</span></span><br><span class="line"><span class="string">        y: (N,) tensor of class labels</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        x = (</span><br><span class="line">            <span class="variable language_">self</span>.x_embedder(x) + <span class="variable language_">self</span>.pos_embed</span><br><span class="line">        )  <span class="comment"># (N, T, D), where T = H * W / patch_size ** 2</span></span><br><span class="line">        t = <span class="variable language_">self</span>.t_embedder(t)  <span class="comment"># (N, D)</span></span><br><span class="line">        y = <span class="variable language_">self</span>.y_embedder(y, <span class="variable language_">self</span>.training)  <span class="comment"># (N, D)</span></span><br><span class="line">        <span class="comment"># 直接将两个嵌入相加</span></span><br><span class="line">        c = t + y  <span class="comment"># (N, D)</span></span><br><span class="line">        <span class="keyword">for</span> block <span class="keyword">in</span> <span class="variable language_">self</span>.blocks:</span><br><span class="line">            x = block(x, c)  <span class="comment"># (N, T, D)</span></span><br><span class="line">        x = <span class="variable language_">self</span>.final_layer(x, c)  <span class="comment"># (N, T, patch_size ** 2 * out_channels)</span></span><br><span class="line">        x = <span class="variable language_">self</span>.unpatchify(x)  <span class="comment"># (N, out_channels, H, W)</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><p>下面我们将具体看一看各自的<code>embedder</code>和<code>DiT Block</code>是如何实现的。</p><h4 id="timestepembedder">TimestepEmbedder</h4><p>这个类用于将时间步 <span class="math inline"><em>t</em></span> 编码成为向量表示。对于一个标量输入<span class="math inline"><em>t</em></span>，首先该embedder会对该时间步计算<strong>正余弦位置编码</strong>（类似于传统transformer中的位置编码），即：</p><p><span class="math display">$$\begin{split}    \omega_i = \frac{1}{(\text{max_period})^{\frac{i}{[D / 2]}}}, \quad i=0,1,\ldots,\left\lfloor \frac{D}{2} \right\rfloor - 1 \end{split}$$</span></p><p><span class="math display">$$\begin{split}    \theta_i = t \cdot \omega_i\end{split}$$</span></p><p><span class="math display">$$\begin{split}    \text{embedding}(t) = \left[ \cos(\theta_0), \cos(\theta_1), \ldots, \cos(\theta_{H-1}), \sin(\theta_0), \sin(\theta_1), \ldots, \sin(\theta_{H-1}) \right]\end{split}$$</span></p><p>之后再经过一层MLP将其转换为输出的维度：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TimestepEmbedder</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, hidden_size, frequency_embedding_size=<span class="number">256</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.mlp = nn.Sequential(</span><br><span class="line">            nn.Linear(frequency_embedding_size, hidden_size, bias=<span class="literal">True</span>),</span><br><span class="line">            nn.SiLU(),</span><br><span class="line">            nn.Linear(hidden_size, hidden_size, bias=<span class="literal">True</span>),</span><br><span class="line">        )</span><br><span class="line">        <span class="variable language_">self</span>.frequency_embedding_size = frequency_embedding_size</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">timestep_embedding</span>(<span class="params">t, dim, max_period=<span class="number">10000</span></span>):</span><br><span class="line">        half = dim // <span class="number">2</span></span><br><span class="line">        freqs = torch.exp(</span><br><span class="line">            -math.log(max_period)</span><br><span class="line">            * torch.arange(start=<span class="number">0</span>, end=half, dtype=torch.float32)</span><br><span class="line">            / half</span><br><span class="line">        ).to(device=t.device)</span><br><span class="line">        args = t[:, <span class="literal">None</span>].<span class="built_in">float</span>() * freqs[<span class="literal">None</span>]</span><br><span class="line">        embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> dim % <span class="number">2</span>:</span><br><span class="line">            embedding = torch.cat(</span><br><span class="line">                [embedding, torch.zeros_like(embedding[:, :<span class="number">1</span>])], dim=-<span class="number">1</span></span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">return</span> embedding</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, t</span>):</span><br><span class="line">        t_freq = <span class="variable language_">self</span>.timestep_embedding(t, <span class="variable language_">self</span>.frequency_embedding_size)</span><br><span class="line">        t_emb = <span class="variable language_">self</span>.mlp(t_freq)</span><br><span class="line">        <span class="keyword">return</span> t_emb</span><br></pre></td></tr></table></figure><h3 id="labelembedder">LabelEmbedder</h3><p>这一部分包含了训练时CFG的逻辑，即在训练时有一定概率会扔掉类别信息：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LabelEmbedder</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_classes, hidden_size, dropout_prob</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        use_cfg_embedding = dropout_prob &gt; <span class="number">0</span></span><br><span class="line">        <span class="variable language_">self</span>.embedding_table = nn.Embedding(</span><br><span class="line">            num_classes + use_cfg_embedding, hidden_size</span><br><span class="line">        )</span><br><span class="line">        <span class="variable language_">self</span>.num_classes = num_classes</span><br><span class="line">        <span class="variable language_">self</span>.dropout_prob = dropout_prob</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">token_drop</span>(<span class="params">self, labels, force_drop_ids=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Drops labels to enable classifier-free guidance.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> force_drop_ids <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            drop_ids = (</span><br><span class="line">                torch.rand(labels.shape[<span class="number">0</span>], device=labels.device) &lt; <span class="variable language_">self</span>.dropout_prob</span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            drop_ids = force_drop_ids == <span class="number">1</span></span><br><span class="line">        labels = torch.where(drop_ids, <span class="variable language_">self</span>.num_classes, labels)</span><br><span class="line">        <span class="keyword">return</span> labels</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, labels, train, force_drop_ids=<span class="literal">None</span></span>):</span><br><span class="line">        use_dropout = <span class="variable language_">self</span>.dropout_prob &gt; <span class="number">0</span></span><br><span class="line">        <span class="keyword">if</span> (train <span class="keyword">and</span> use_dropout) <span class="keyword">or</span> (force_drop_ids <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>):</span><br><span class="line">            labels = <span class="variable language_">self</span>.token_drop(labels, force_drop_ids)</span><br><span class="line">        embeddings = <span class="variable language_">self</span>.embedding_table(labels)</span><br><span class="line">        <span class="keyword">return</span> embeddings</span><br></pre></td></tr></table></figure><h3 id="dit-block">DiT Block</h3><p>这一部分是DiT模型的核心模块，首先条件<span class="math inline"><em>c</em></span>会通过线性层得到<span class="math inline"><em>γ</em><sub>1</sub>, <em>β</em><sub>1</sub>, <em>α</em><sub>1</sub>, <em>γ</em><sub>2</sub>, <em>β</em><sub>2</sub>, <em>α</em><sub>2</sub></span>，然后通过AdaLN-Zero对中间层进行归一化，分别经过Attention层和Forward层：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">modulate</span>(<span class="params">x, shift, scale</span>):</span><br><span class="line">    <span class="keyword">return</span> x * (<span class="number">1</span> + scale.unsqueeze(<span class="number">1</span>)) + shift.unsqueeze(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DiTBlock</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, hidden_size, num_heads, mlp_ratio=<span class="number">4.0</span>, **block_kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.norm1 = nn.LayerNorm(hidden_size, elementwise_affine=<span class="literal">False</span>, eps=<span class="number">1e-6</span>)</span><br><span class="line">        <span class="variable language_">self</span>.attn = Attention(</span><br><span class="line">            hidden_size, num_heads=num_heads, qkv_bias=<span class="literal">True</span>, **block_kwargs</span><br><span class="line">        )</span><br><span class="line">        <span class="variable language_">self</span>.norm2 = nn.LayerNorm(hidden_size, elementwise_affine=<span class="literal">False</span>, eps=<span class="number">1e-6</span>)</span><br><span class="line">        mlp_hidden_dim = <span class="built_in">int</span>(hidden_size * mlp_ratio)</span><br><span class="line">        approx_gelu = <span class="keyword">lambda</span>: nn.GELU(approximate=<span class="string">&quot;tanh&quot;</span>)</span><br><span class="line">        <span class="variable language_">self</span>.mlp = Mlp(</span><br><span class="line">            in_features=hidden_size,</span><br><span class="line">            hidden_features=mlp_hidden_dim,</span><br><span class="line">            act_layer=approx_gelu,</span><br><span class="line">            drop=<span class="number">0</span>,</span><br><span class="line">        )</span><br><span class="line">        <span class="variable language_">self</span>.adaLN_modulation = nn.Sequential(</span><br><span class="line">            nn.SiLU(), nn.Linear(hidden_size, <span class="number">6</span> * hidden_size, bias=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, c</span>):</span><br><span class="line">        <span class="comment"># 得到 \gamma_1, \beta_1, \alpha_1, \gamma_2, \beta_2, \alpha_2</span></span><br><span class="line">        shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp = (</span><br><span class="line">            <span class="variable language_">self</span>.adaLN_modulation(c).chunk(<span class="number">6</span>, dim=<span class="number">1</span>)</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># Attention层</span></span><br><span class="line">        x = x + gate_msa.unsqueeze(<span class="number">1</span>) * <span class="variable language_">self</span>.attn(</span><br><span class="line">            modulate(<span class="variable language_">self</span>.norm1(x), shift_msa, scale_msa)</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># forward层</span></span><br><span class="line">        x = x + gate_mlp.unsqueeze(<span class="number">1</span>) * <span class="variable language_">self</span>.mlp(</span><br><span class="line">            modulate(<span class="variable language_">self</span>.norm2(x), shift_mlp, scale_mlp)</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><h2 id="推理">3.2.推理</h2><p>模型训练完毕后，采样一张图像的主要逻辑如下：</p><ul><li>生成在latent空间下（经过VAE encode后）的高斯噪声<code>z = torch.randn(n, 4, latent_size, latent_size, device=device)</code>，输入条件类别信息 <span class="math inline"><em>y</em></span> 用于条件生成；</li><li>生成同样大小的无条件输入用于CFG，并将其与原条件 <span class="math inline"><em>z</em></span> 和 <span class="math inline"><em>y</em></span> 拼接在一起：<code>z = torch.cat([z, z], 0), y = torch.cat([y, y_null], 0)</code>；</li><li>将输入 <span class="math inline"><em>z</em></span> 和条件 <span class="math inline"><em>c</em></span> 送入Diffusion采样；</li><li>最终将输出通过VAE Decode重新转换为像素空间。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Load model:</span></span><br><span class="line">latent_size = args.image_size // <span class="number">8</span></span><br><span class="line">model = DiT_models[args.model](</span><br><span class="line">    input_size=latent_size,</span><br><span class="line">    num_classes=args.num_classes</span><br><span class="line">).to(device)</span><br><span class="line"><span class="comment"># Auto-download a pre-trained model or load a custom DiT checkpoint from train.py:</span></span><br><span class="line">ckpt_path = args.ckpt <span class="keyword">or</span> <span class="string">f&quot;DiT-XL-2-<span class="subst">&#123;args.image_size&#125;</span>x<span class="subst">&#123;args.image_size&#125;</span>.pt&quot;</span></span><br><span class="line">state_dict = find_model(ckpt_path)</span><br><span class="line">model.load_state_dict(state_dict)</span><br><span class="line">model.<span class="built_in">eval</span>()  <span class="comment"># important!</span></span><br><span class="line">diffusion = create_diffusion(<span class="built_in">str</span>(args.num_sampling_steps))</span><br><span class="line">vae = AutoencoderKL.from_pretrained(<span class="string">f&quot;stabilityai/sd-vae-ft-<span class="subst">&#123;args.vae&#125;</span>&quot;</span>).to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Labels to condition the model with (feel free to change):</span></span><br><span class="line">class_labels = [<span class="number">207</span>, <span class="number">360</span>, <span class="number">387</span>, <span class="number">974</span>, <span class="number">88</span>, <span class="number">979</span>, <span class="number">417</span>, <span class="number">279</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create sampling noise:</span></span><br><span class="line">n = <span class="built_in">len</span>(class_labels)</span><br><span class="line">z = torch.randn(n, <span class="number">4</span>, latent_size, latent_size, device=device) <span class="comment"># [N, C, H, W]</span></span><br><span class="line">y = torch.tensor(class_labels, device=device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Setup classifier-free guidance:</span></span><br><span class="line">z = torch.cat([z, z], <span class="number">0</span>)</span><br><span class="line">y_null = torch.tensor([<span class="number">1000</span>] * n, device=device)</span><br><span class="line">y = torch.cat([y, y_null], <span class="number">0</span>)</span><br><span class="line">model_kwargs = <span class="built_in">dict</span>(y=y, cfg_scale=args.cfg_scale)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sample images:</span></span><br><span class="line">samples = diffusion.p_sample_loop(</span><br><span class="line">    model.forward_with_cfg, z.shape, z, clip_denoised=<span class="literal">False</span>, model_kwargs=model_kwargs, progress=<span class="literal">True</span>, device=device</span><br><span class="line">)</span><br><span class="line">samples, _ = samples.chunk(<span class="number">2</span>, dim=<span class="number">0</span>)  <span class="comment"># Remove null class samples</span></span><br><span class="line">samples = vae.decode(samples / <span class="number">0.18215</span>).sample</span><br></pre></td></tr></table></figure><p>以下是DiT内部专门用于推理时CFG的forward：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DiT</span>(nn.Module):</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward_with_cfg</span>(<span class="params">self, x, t, y, cfg_scale</span>):</span><br><span class="line">        half = x[: <span class="built_in">len</span>(x) // <span class="number">2</span>]</span><br><span class="line">        combined = torch.cat([half, half], dim=<span class="number">0</span>)</span><br><span class="line">        model_out = <span class="variable language_">self</span>.forward(combined, t, y)</span><br><span class="line"></span><br><span class="line">        eps, rest = model_out[:, :<span class="number">3</span>], model_out[:, <span class="number">3</span>:]</span><br><span class="line">        cond_eps, uncond_eps = torch.split(eps, <span class="built_in">len</span>(eps) // <span class="number">2</span>, dim=<span class="number">0</span>)</span><br><span class="line">        half_eps = uncond_eps + cfg_scale * (cond_eps - uncond_eps)</span><br><span class="line">        eps = torch.cat([half_eps, half_eps], dim=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> torch.cat([eps, rest], dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AIGC </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>iDDPM总结</title>
      <link href="/2025/05/08/AIGC/iDDPM/"/>
      <url>/2025/05/08/AIGC/iDDPM/</url>
      
        <content type="html"><![CDATA[<p><strong>系列文章：</strong></p><ol class="series-items"><li><a href="/2025/05/08/AIGC/iDDPM/" title="iDDPM总结">iDDPM总结</a></li></ol><blockquote><p>原论文： <a href="https://proceedings.mlr.press/v139/nichol21a.html">Improved denoising diffusion probabilistic models</a></p><p>源码： <a href="https://github.com/openai/improved-diffusion">https://github.com/openai/improved-diffusion</a></p></blockquote><h1 id="背景介绍">背景介绍</h1><p>虽然DDPMs可以生成很高质量的图像，但是在log似然指标上还是无法超过自回归模型（VAEs），DDPMs是否能真正学习到数据分布还有待验证。同时DDPM在采样效率上也存在大问题，该如何减少采样所需的步长也有待研究。</p><p>本文提出了结合原目标函数和variational lower-bound（VLB）目标函数的<strong>混合目标函数</strong>，能实现更好的log似然指标，并使用<strong>重要性采样</strong>实现更平滑的训练。并且本文发现使用预训练好的混合损失模型能使用更少的步数实现高质量的生成。除此之外，还提出了一些其他的改进，进一步提升DDPM的生成质量。</p><h1 id="一些改进ddpms的tricks">一些改进DDPMs的tricks</h1><h2 id="可学习方差">可学习方差</h2><p>在DDPM中，对于逆向过程的后验概率可以用神经网络估计为：</p><p><span class="math display">$$\begin{split}    p_\theta\left(x_{t-1} \mid x_t\right):=\mathcal{N}\left(x_{t-1} ; \mu_\theta\left(x_t, t\right), \Sigma_\theta\left(x_t, t\right)\right)\end{split}$$</span></p><p>其中方差部分DDPM设定为固定方差（不可学习），即 <span class="math inline"><em>Σ</em><sub><em>θ</em></sub>(<em>x</em><sub><em>t</em></sub>,<em>t</em>) = <em>σ</em><sub><em>t</em></sub><sup>2</sup><strong>I</strong></span> 。并且DDPM发现当 <span class="math inline"><em>σ</em><sub><em>t</em></sub><sup>2</sup></span> 设定为 <span class="math inline"><em>β</em><sub><em>t</em></sub></span> 和 <span class="math inline">$\tilde{\beta_t}$</span> 时效果其实相差不大，其中 <span class="math inline"><em>β</em><sub><em>t</em></sub> := 1 − <em>α</em><sub><em>t</em></sub></span>，<span class="math inline">$\tilde{\beta}_t:=\frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t} \beta_t$</span>。但实际这两个参数在时间步逐渐增长的过程中也在逐渐逼近（如下图），最终基本重合。</p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/iDDPM_beta.png" width="50%"/></p><blockquote><p><strong>补充</strong>：</p><p>为什么这里要讨论 <span class="math inline"><em>β</em><sub><em>t</em></sub></span> 和 <span class="math inline"><em>β̃</em><sub><em>t</em></sub></span> 呢，这两个值有什么特殊意义吗？事实上，这里的 <span class="math inline"><em>β</em><sub><em>t</em></sub></span> 和 <span class="math inline"><em>β̃</em><sub><em>t</em></sub></span> 分别是<strong>扩散模型采样方差的上下界</strong>，详细证明出自<a href="http://proceedings.mlr.press/v37/sohl-dickstein15.html">DPM原论文</a></p></blockquote><p>因此既然方差的上下界最终都相差不大，所以其实 <span class="math inline"><em>σ</em><sub><em>t</em></sub></span> 无论选择 <span class="math inline"><em>β</em><sub><em>t</em></sub></span> 还是 <span class="math inline"><em>β̃</em><sub><em>t</em></sub></span> 其实对采样的质量没有多大的影响，这使得模型预测时会更多的考虑均值 <span class="math inline"><em>μ</em><sub><em>θ</em></sub>(<em>x</em><sub><em>t</em></sub>,<em>t</em>)</span> 而不是方差 <span class="math inline"><em>Σ</em><sub><em>θ</em></sub>(<em>x</em><sub><em>t</em></sub>,<em>t</em>)</span>，这也许也是DDPM直接固定方差的原因。</p><p>既然我们都得到了方差的上下界，就可以直接使用简单的插值预测方差，即：</p><p><span class="math display">$$\begin{split}\Sigma_\theta\left(x_t, t\right)=\exp \left(v \log \beta_t+(1-v) \log \tilde{\beta}_t\right)\end{split}$$</span></p><p>这里使用log形式是为了进一步拉大上下界的差距（<span class="math inline"><em>β</em><sub><em>t</em></sub></span> 和 <span class="math inline"><em>β̃</em><sub><em>t</em></sub></span> 都处于0-1），方便模型更好的预测。这里没有对<span class="math inline"><em>v</em></span>进一步约束，按理来说为保证上下界应该约束为0-1，但是在实验中表明学习的方差还是能处在上下界之间的。</p><p>此时由于原DDPM的损失函数是不包含方差的，因此本文还引入了变分下界损失函数 <span class="math inline">ℒ<sub><em>v</em><em>l</em><em>b</em></sub></span>，并使用 <span class="math inline"><em>λ</em></span> 来进一步约束。具体来说，所有损失可以表达为：</p><p><span class="math display">$$\begin{gather}\begin{split}L_{\mathrm{simple}}&amp;=E_{t,x_0,\epsilon}\left[||\epsilon-\epsilon_\theta(x_t,t)||^2\right] \\\\L_{\mathrm{vlb}} &amp;:=L_0+L_1+...+L_{T-1}+L_T \\L_{0} &amp;:=-\log p_\theta(x_0|x_1) \\L_{t-1} &amp;:=D_{KL}(q(x_{t-1}|x_t,x_0)\parallel p_\theta(x_{t-1}|x_t)) \\L_T &amp;:=D_{KL}(q(x_T|x_0)\parallel p(x_T)) \\\\L_{\mathrm{hybrid}}&amp;=L_{\mathrm{simple}}+\lambda L_{\mathrm{vlb}}\end{split}\end{gather}$$</span></p><h2 id="改进噪声强度曲线">改进噪声强度曲线</h2><p>在DDPM中，噪声强度调度是<strong>线性变化</strong>的，但是这会导致前向过程中的后段图像变得过于加噪（如下图，第一排是原DDPM线性噪声强度，第二排是iDDPM），并且 <span class="math inline"><em>ᾱ</em><sub><em>t</em></sub></span> 过快的趋于0导致图片信息损失得太快。</p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/iDDPM_noise.png" width="80%" /></p><p>因此iDDPM设计了<strong>余弦噪声强度调度</strong>方案：</p><p><span class="math display">$$\begin{split}\bar{\alpha}_t=\frac{f(t)}{f(0)},f(t)=\cos\left(\frac{t/T+s}{1+s}\cdot\frac{\pi}{2}\right)^2\end{split}$$</span></p><p>其中极小偏移量<span class="math inline"><em>s</em></span>的引入是为了防止 <span class="math inline"><em>β</em><sub><em>t</em></sub></span> 在 <span class="math inline"><em>t</em> = 0</span> 附近过小，在实验中设置 <span class="math inline"><em>s</em> = 0.008</span>。最终噪声调度变化曲线如下图，可以做到基本拟合线性下降，并且在初始和结束阶段变化幅度较小。</p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/iDDPM_loss.png" width="50%" /></p><h2 id="基于重要性的采样">基于重要性的采样</h2><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/iDDPM_noise_schedule.png" width="50%" /></p><p>iDDPM本来是想直接优化 <span class="math inline"><em>L</em><sub><em>v</em><em>l</em><em>b</em></sub></span> 来实现最好的log似然指标，但是在实际实验中发现 <span class="math inline"><em>L</em><sub><em>v</em><em>l</em><em>b</em></sub></span> 很难直接优化（如上图），作者推测可能是由于 <span class="math inline"><em>L</em><sub><em>v</em><em>l</em><em>b</em></sub></span> 的梯度比 <span class="math inline"><em>L</em><sub><em>h</em><em>y</em><em>b</em><em>r</em><em>i</em><em>d</em></sub></span> 更为 noisier。通过实验注意到不同时期的 <span class="math inline"><em>L</em><sub><em>v</em><em>l</em><em>b</em></sub></span> 有很大区别（如下图），在 <span class="math inline"><em>t</em></span> 比较小的时候loss下降得更快，这也说明这时候的网络更“需要”训练，使用简单的均匀时间步采样可能不太合适。</p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/iDDPM_log.png" width="50%" /></p><p>因此作者提出了<strong>重要性采样</strong>方法，看下面的公式可能一时之间有些难以理解，其实定性理解就是高损失的时间步（重要的去噪阶段）被赋予更高的采样概率，而低损失的时间步被减少采样。</p><p><span class="math display">$$\begin{split}L_{\mathrm{vlb}}=E_{t\sim p_{t}}\left[\frac{L_{t}}{p_{t}}\right],\mathrm{where~}p_{t}\propto\sqrt{E[L_{t}^{2}]}\mathrm{~and~}\sum p_{t}=1\end{split}$$</span></p><h2 id="加速采样">加速采样</h2><p>这一部分和DDIM都差不多使用子序列采样，不同主要在于 <span class="math inline"><em>σ</em><sub><em>t</em></sub><sup>2</sup> = <em>β̃</em><sub><em>t</em></sub></span>，具体可以看看<a href="https://litchi-lee.github.io/2025/04/17/AIGC/DDIM/">DDIM的工作</a>。</p><h1 id="源码">源码</h1><h2 id="噪声强度调度">噪声强度调度</h2><p>对于iDDPM使用的余弦噪声强度调度，这里源码中使用<code>improved_diffusion\gaussian_diffusion.py</code>中的<code>get_named_beta_schedule</code>方法得到<span class="math inline"><em>β</em><sub><em>t</em></sub></span>，对于余弦策略，代码定义的<code>lambda t: math.cos((t + 0.008) / 1.008 * math.pi / 2) ** 2</code>和原论文中的公式保持一致。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_named_beta_schedule</span>(<span class="params">schedule_name, num_diffusion_timesteps</span>):</span><br><span class="line">    <span class="keyword">if</span> schedule_name == <span class="string">&quot;linear&quot;</span>:</span><br><span class="line">        scale = <span class="number">1000</span> / num_diffusion_timesteps</span><br><span class="line">        beta_start = scale * <span class="number">0.0001</span></span><br><span class="line">        beta_end = scale * <span class="number">0.02</span></span><br><span class="line">        <span class="keyword">return</span> np.linspace(</span><br><span class="line">            beta_start, beta_end, num_diffusion_timesteps, dtype=np.float64</span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">elif</span> schedule_name == <span class="string">&quot;cosine&quot;</span>:</span><br><span class="line">        <span class="keyword">return</span> betas_for_alpha_bar(</span><br><span class="line">            num_diffusion_timesteps,</span><br><span class="line">            <span class="keyword">lambda</span> t: math.cos((t + <span class="number">0.008</span>) / <span class="number">1.008</span> * math.pi / <span class="number">2</span>) ** <span class="number">2</span>,</span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError(<span class="string">f&quot;unknown beta schedule: <span class="subst">&#123;schedule_name&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">betas_for_alpha_bar</span>(<span class="params">num_diffusion_timesteps, alpha_bar, max_beta=<span class="number">0.999</span></span>):</span><br><span class="line">    betas = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_diffusion_timesteps):</span><br><span class="line">        t1 = i / num_diffusion_timesteps</span><br><span class="line">        t2 = (i + <span class="number">1</span>) / num_diffusion_timesteps</span><br><span class="line">        betas.append(<span class="built_in">min</span>(<span class="number">1</span> - alpha_bar(t2) / alpha_bar(t1), max_beta))</span><br><span class="line">    <span class="keyword">return</span> np.array(betas)</span><br></pre></td></tr></table></figure><h2 id="重要性采样">重要性采样</h2><p>这里源码实现了两种采样，一个就是原DDPM的均匀采样，另一种就是iDDPM使用的重要性采样。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">create_named_schedule_sampler</span>(<span class="params">name, diffusion</span>):</span><br><span class="line">    <span class="keyword">if</span> name == <span class="string">&quot;uniform&quot;</span>:</span><br><span class="line">        <span class="keyword">return</span> UniformSampler(diffusion)</span><br><span class="line">    <span class="keyword">elif</span> name == <span class="string">&quot;loss-second-moment&quot;</span>:</span><br><span class="line">        <span class="keyword">return</span> LossSecondMomentResampler(diffusion)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError(<span class="string">f&quot;unknown schedule sampler: <span class="subst">&#123;name&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p>这两种方式都继承自<code>ScheduleSampler</code>类，需要采样时间步时使用<code>ScheduleSampler.sample</code>方法采样时间步，需要实现<code>ScheduleSampler.weights</code>方法计算每个时间步的重要性权重，进而计算出每个时间步的采样概率，根据概率采样时间步。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ScheduleSampler</span>(<span class="title class_ inherited__">ABC</span>):</span><br><span class="line"><span class="meta">    @abstractmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">weights</span>(<span class="params">self</span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">sample</span>(<span class="params">self, batch_size, device</span>):</span><br><span class="line">        <span class="comment"># 得到时间步权重，计算采样概率</span></span><br><span class="line">        w = <span class="variable language_">self</span>.weights()</span><br><span class="line">        p = w / np.<span class="built_in">sum</span>(w)</span><br><span class="line"></span><br><span class="line">        indices_np = np.random.choice(<span class="built_in">len</span>(p), size=(batch_size,), p=p)</span><br><span class="line">        indices = th.from_numpy(indices_np).long().to(device)</span><br><span class="line">        weights_np = <span class="number">1</span> / (<span class="built_in">len</span>(p) * p[indices_np])</span><br><span class="line">        weights = th.from_numpy(weights_np).<span class="built_in">float</span>().to(device)</span><br><span class="line">        <span class="keyword">return</span> indices, weights</span><br></pre></td></tr></table></figure><h3 id="均匀采样实现">均匀采样实现</h3><p>均匀采样就很简单了，直接每个时间步权重设置为1即可。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">UniformSampler</span>(<span class="title class_ inherited__">ScheduleSampler</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, diffusion</span>):</span><br><span class="line">        <span class="variable language_">self</span>.diffusion = diffusion</span><br><span class="line">        <span class="variable language_">self</span>._weights = np.ones([diffusion.num_timesteps])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">weights</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>._weights</span><br></pre></td></tr></table></figure><h3 id="重要性采样实现">重要性采样实现</h3><p>主要需要关注 <code>LossSecondMomentResampler</code> 类的实现，其中有两个变量需要关注，分别是记录时间步 <span class="math inline"><em>t</em></span> 最近 <code>history_per_term</code> 次的 loss 列表 <code>self._loss_history</code> 和记录时间步 <span class="math inline"><em>t</em></span> 时的历史记录次数 <code>self._loss_counts</code>。在更新时，调用 <code>update_with_all_losses</code> 方法，采用FIFO 的更新方式，记录最新的 <code>history_per_term</code> 次loss。而在计算重要性权重时，额外引入了 <code>self.uniform_prob</code> 以防止某些时间步概率为0，增强鲁棒性。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LossAwareSampler</span>(<span class="title class_ inherited__">ScheduleSampler</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">update_with_local_losses</span>(<span class="params">self, local_ts, local_losses</span>):</span><br><span class="line">        batch_sizes = [</span><br><span class="line">            th.tensor([<span class="number">0</span>], dtype=th.int32, device=local_ts.device)</span><br><span class="line">            <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(dist.get_world_size())</span><br><span class="line">        ]</span><br><span class="line">        dist.all_gather(</span><br><span class="line">            batch_sizes,</span><br><span class="line">            th.tensor([<span class="built_in">len</span>(local_ts)], dtype=th.int32, device=local_ts.device),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Pad all_gather batches to be the maximum batch size.</span></span><br><span class="line">        batch_sizes = [x.item() <span class="keyword">for</span> x <span class="keyword">in</span> batch_sizes]</span><br><span class="line">        max_bs = <span class="built_in">max</span>(batch_sizes)</span><br><span class="line"></span><br><span class="line">        timestep_batches = [th.zeros(max_bs).to(local_ts) <span class="keyword">for</span> bs <span class="keyword">in</span> batch_sizes]</span><br><span class="line">        loss_batches = [th.zeros(max_bs).to(local_losses) <span class="keyword">for</span> bs <span class="keyword">in</span> batch_sizes]</span><br><span class="line">        dist.all_gather(timestep_batches, local_ts)</span><br><span class="line">        dist.all_gather(loss_batches, local_losses)</span><br><span class="line">        timesteps = [</span><br><span class="line">            x.item() <span class="keyword">for</span> y, bs <span class="keyword">in</span> <span class="built_in">zip</span>(timestep_batches, batch_sizes) <span class="keyword">for</span> x <span class="keyword">in</span> y[:bs]</span><br><span class="line">        ]</span><br><span class="line">        losses = [x.item() <span class="keyword">for</span> y, bs <span class="keyword">in</span> <span class="built_in">zip</span>(loss_batches, batch_sizes) <span class="keyword">for</span> x <span class="keyword">in</span> y[:bs]]</span><br><span class="line">        <span class="variable language_">self</span>.update_with_all_losses(timesteps, losses)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @abstractmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">update_with_all_losses</span>(<span class="params">self, ts, losses</span>):</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LossSecondMomentResampler</span>(<span class="title class_ inherited__">LossAwareSampler</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, diffusion, history_per_term=<span class="number">10</span>, uniform_prob=<span class="number">0.001</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.diffusion = diffusion</span><br><span class="line">        <span class="variable language_">self</span>.history_per_term = history_per_term</span><br><span class="line">        <span class="variable language_">self</span>.uniform_prob = uniform_prob</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 初始化时间步 t 最近 history_per_term 次的loss</span></span><br><span class="line">        <span class="variable language_">self</span>._loss_history = np.zeros(</span><br><span class="line">            [diffusion.num_timesteps, history_per_term], dtype=np.float64</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># 初始化当前时间步 t 的历史记录次数</span></span><br><span class="line">        <span class="variable language_">self</span>._loss_counts = np.zeros([diffusion.num_timesteps], dtype=np.<span class="built_in">int</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">weights</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="variable language_">self</span>._warmed_up():</span><br><span class="line">            <span class="keyword">return</span> np.ones([<span class="variable language_">self</span>.diffusion.num_timesteps], dtype=np.float64)</span><br><span class="line">        weights = np.sqrt(np.mean(<span class="variable language_">self</span>._loss_history**<span class="number">2</span>, axis=-<span class="number">1</span>))</span><br><span class="line">        weights /= np.<span class="built_in">sum</span>(weights)</span><br><span class="line">        weights *= <span class="number">1</span> - <span class="variable language_">self</span>.uniform_prob</span><br><span class="line">        weights += <span class="variable language_">self</span>.uniform_prob / <span class="built_in">len</span>(weights)</span><br><span class="line">        <span class="keyword">return</span> weights</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">update_with_all_losses</span>(<span class="params">self, ts, losses</span>):</span><br><span class="line">        <span class="keyword">for</span> t, loss <span class="keyword">in</span> <span class="built_in">zip</span>(ts, losses):</span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>._loss_counts[t] == <span class="variable language_">self</span>.history_per_term:</span><br><span class="line">                <span class="comment"># Shift out the oldest loss term.</span></span><br><span class="line">                <span class="variable language_">self</span>._loss_history[t, :-<span class="number">1</span>] = <span class="variable language_">self</span>._loss_history[t, <span class="number">1</span>:]</span><br><span class="line">                <span class="variable language_">self</span>._loss_history[t, -<span class="number">1</span>] = loss</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="variable language_">self</span>._loss_history[t, <span class="variable language_">self</span>._loss_counts[t]] = loss</span><br><span class="line">                <span class="variable language_">self</span>._loss_counts[t] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_warmed_up</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> (<span class="variable language_">self</span>._loss_counts == <span class="variable language_">self</span>.history_per_term).<span class="built_in">all</span>()</span><br></pre></td></tr></table></figure><h3 id="可学习方差-1">可学习方差</h3><p>具体代码细节可在 <code>GaussianDiffusion.p_mean_variance</code> 方法中查看，可以看到若指定方差是可学习的，则此时模型输出两部分，分别预测噪声和方差<code>assert model_output.shape == (B, C * 2, *x.shape[2:])</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">GaussianDiffusion</span>:</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">p_mean_variance</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self, model, x, t, clip_denoised=<span class="literal">True</span>, denoised_fn=<span class="literal">None</span>, model_kwargs=<span class="literal">None</span></span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="keyword">if</span> model_kwargs <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            model_kwargs = &#123;&#125;</span><br><span class="line"></span><br><span class="line">        B, C = x.shape[:<span class="number">2</span>]</span><br><span class="line">        <span class="keyword">assert</span> t.shape == (B,)</span><br><span class="line">        model_output = model(x, <span class="variable language_">self</span>._scale_timesteps(t), **model_kwargs)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.model_var_type <span class="keyword">in</span> [ModelVarType.LEARNED, ModelVarType.LEARNED_RANGE]:</span><br><span class="line">            <span class="comment"># 模型输出预测噪声和预测方差两部分</span></span><br><span class="line">            <span class="keyword">assert</span> model_output.shape == (B, C * <span class="number">2</span>, *x.shape[<span class="number">2</span>:])</span><br><span class="line">            model_output, model_var_values = th.split(model_output, C, dim=<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>.model_var_type == ModelVarType.LEARNED:</span><br><span class="line">                model_log_variance = model_var_values</span><br><span class="line">                model_variance = th.exp(model_log_variance)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                min_log = _extract_into_tensor(</span><br><span class="line">                    <span class="variable language_">self</span>.posterior_log_variance_clipped, t, x.shape</span><br><span class="line">                )</span><br><span class="line">                max_log = _extract_into_tensor(np.log(<span class="variable language_">self</span>.betas), t, x.shape)</span><br><span class="line">                <span class="comment"># The model_var_values is [-1, 1] for [min_var, max_var].</span></span><br><span class="line">                frac = (model_var_values + <span class="number">1</span>) / <span class="number">2</span></span><br><span class="line">                model_log_variance = frac * max_log + (<span class="number">1</span> - frac) * min_log</span><br><span class="line">                model_variance = th.exp(model_log_variance)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            model_variance, model_log_variance = &#123;</span><br><span class="line">                <span class="comment"># for fixedlarge, we set the initial (log-)variance like so</span></span><br><span class="line">                <span class="comment"># to get a better decoder log likelihood.</span></span><br><span class="line">                ModelVarType.FIXED_LARGE: (</span><br><span class="line">                    np.append(<span class="variable language_">self</span>.posterior_variance[<span class="number">1</span>], <span class="variable language_">self</span>.betas[<span class="number">1</span>:]),</span><br><span class="line">                    np.log(np.append(<span class="variable language_">self</span>.posterior_variance[<span class="number">1</span>], <span class="variable language_">self</span>.betas[<span class="number">1</span>:])),</span><br><span class="line">                ),</span><br><span class="line">                ModelVarType.FIXED_SMALL: (</span><br><span class="line">                    <span class="variable language_">self</span>.posterior_variance,</span><br><span class="line">                    <span class="variable language_">self</span>.posterior_log_variance_clipped,</span><br><span class="line">                ),</span><br><span class="line">            &#125;[<span class="variable language_">self</span>.model_var_type]</span><br><span class="line">            model_variance = _extract_into_tensor(model_variance, t, x.shape)</span><br><span class="line">            model_log_variance = _extract_into_tensor(model_log_variance, t, x.shape)</span><br><span class="line">        ...</span><br></pre></td></tr></table></figure><p>更深地扒一扒，可以发现 UNet 的输出随是否可学习方差而改变（<code>out_channels=(3 if not learn_sigma else 6)</code>）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">create_model</span>(<span class="params">...</span>)</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> UNetModel(</span><br><span class="line">        in_channels=<span class="number">3</span>,</span><br><span class="line">        model_channels=num_channels,</span><br><span class="line">        out_channels=(<span class="number">3</span> <span class="keyword">if</span> <span class="keyword">not</span> learn_sigma <span class="keyword">else</span> <span class="number">6</span>),</span><br><span class="line">        ...</span><br><span class="line">    )</span><br></pre></td></tr></table></figure><p>关于损失的计算，这里需要额外计算 <span class="math inline"><em>L</em><sub><em>v</em><em>l</em><em>b</em></sub></span> ，其中<code>_vb_terms_bpd</code>方法用于计算 <span class="math inline"><em>K</em><em>L</em>(<em>q</em>(<em>x</em><sub><em>t</em> − 1</sub>|<em>x</em><sub><em>t</em></sub>, <em>x</em><sub>0</sub>)||<em>p</em>(<em>x</em><sub><em>t</em> − 1</sub>|<em>x</em><sub><em>t</em></sub>))</span>，并且实验中为了减少对MSE损失的影响，对KL损失乘上了0.001的权重（可以说是影响非常小了）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">GaussianDiffusion</span>:</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_vb_terms_bpd</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self, model, x_start, x_t, t, clip_denoised=<span class="literal">True</span>, model_kwargs=<span class="literal">None</span></span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        true_mean, _, true_log_variance_clipped = <span class="variable language_">self</span>.q_posterior_mean_variance(</span><br><span class="line">            x_start=x_start, x_t=x_t, t=t</span><br><span class="line">        )</span><br><span class="line">        out = <span class="variable language_">self</span>.p_mean_variance(</span><br><span class="line">            model, x_t, t, clip_denoised=clip_denoised, model_kwargs=model_kwargs</span><br><span class="line">        )</span><br><span class="line">        kl = normal_kl(</span><br><span class="line">            true_mean, true_log_variance_clipped, out[<span class="string">&quot;mean&quot;</span>], out[<span class="string">&quot;log_variance&quot;</span>]</span><br><span class="line">        )</span><br><span class="line">        kl = mean_flat(kl) / np.log(<span class="number">2.0</span>)</span><br><span class="line"></span><br><span class="line">        decoder_nll = -discretized_gaussian_log_likelihood(</span><br><span class="line">            x_start, means=out[<span class="string">&quot;mean&quot;</span>], log_scales=<span class="number">0.5</span> * out[<span class="string">&quot;log_variance&quot;</span>]</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">assert</span> decoder_nll.shape == x_start.shape</span><br><span class="line">        decoder_nll = mean_flat(decoder_nll) / np.log(<span class="number">2.0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># At the first timestep return the decoder NLL,</span></span><br><span class="line">        <span class="comment"># otherwise return KL(q(x_&#123;t-1&#125;|x_t,x_0) || p(x_&#123;t-1&#125;|x_t))</span></span><br><span class="line">        output = th.where((t == <span class="number">0</span>), decoder_nll, kl)</span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">&quot;output&quot;</span>: output, <span class="string">&quot;pred_xstart&quot;</span>: out[<span class="string">&quot;pred_xstart&quot;</span>]&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">training_losses</span>(<span class="params">self, model, x_start, t, model_kwargs=<span class="literal">None</span>, noise=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="keyword">if</span> model_kwargs <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            model_kwargs = &#123;&#125;</span><br><span class="line">        <span class="keyword">if</span> noise <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            noise = th.randn_like(x_start)</span><br><span class="line">        x_t = <span class="variable language_">self</span>.q_sample(x_start, t, noise=noise)</span><br><span class="line"></span><br><span class="line">        terms = &#123;&#125;</span><br><span class="line"></span><br><span class="line">        ...</span><br><span class="line">        <span class="keyword">elif</span> <span class="variable language_">self</span>.loss_type == LossType.MSE <span class="keyword">or</span> <span class="variable language_">self</span>.loss_type == LossType.RESCALED_MSE:</span><br><span class="line">            model_output = model(x_t, <span class="variable language_">self</span>._scale_timesteps(t), **model_kwargs)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>.model_var_type <span class="keyword">in</span> [</span><br><span class="line">                ModelVarType.LEARNED,</span><br><span class="line">                ModelVarType.LEARNED_RANGE,</span><br><span class="line">            ]:</span><br><span class="line">                B, C = x_t.shape[:<span class="number">2</span>]</span><br><span class="line">                <span class="keyword">assert</span> model_output.shape == (B, C * <span class="number">2</span>, *x_t.shape[<span class="number">2</span>:])</span><br><span class="line">                model_output, model_var_values = th.split(model_output, C, dim=<span class="number">1</span>)</span><br><span class="line">                <span class="comment"># Learn the variance using the variational bound, but don&#x27;t let</span></span><br><span class="line">                <span class="comment"># it affect our mean prediction.</span></span><br><span class="line">                frozen_out = th.cat([model_output.detach(), model_var_values], dim=<span class="number">1</span>)</span><br><span class="line">                terms[<span class="string">&quot;vb&quot;</span>] = <span class="variable language_">self</span>._vb_terms_bpd(</span><br><span class="line">                    model=<span class="keyword">lambda</span> *args, r=frozen_out: r,</span><br><span class="line">                    x_start=x_start,</span><br><span class="line">                    x_t=x_t,</span><br><span class="line">                    t=t,</span><br><span class="line">                    clip_denoised=<span class="literal">False</span>,</span><br><span class="line">                )[<span class="string">&quot;output&quot;</span>]</span><br><span class="line">                <span class="keyword">if</span> <span class="variable language_">self</span>.loss_type == LossType.RESCALED_MSE:</span><br><span class="line">                    <span class="comment"># Divide by 1000 for equivalence with initial implementation.</span></span><br><span class="line">                    <span class="comment"># Without a factor of 1/1000, the VB term hurts the MSE term.</span></span><br><span class="line">                    terms[<span class="string">&quot;vb&quot;</span>] *= <span class="variable language_">self</span>.num_timesteps / <span class="number">1000.0</span></span><br><span class="line"></span><br><span class="line">            target = &#123;</span><br><span class="line">                ModelMeanType.PREVIOUS_X: <span class="variable language_">self</span>.q_posterior_mean_variance(</span><br><span class="line">                    x_start=x_start, x_t=x_t, t=t</span><br><span class="line">                )[<span class="number">0</span>],</span><br><span class="line">                ModelMeanType.START_X: x_start,</span><br><span class="line">                ModelMeanType.EPSILON: noise,</span><br><span class="line">            &#125;[<span class="variable language_">self</span>.model_mean_type]</span><br><span class="line">            <span class="keyword">assert</span> model_output.shape == target.shape == x_start.shape</span><br><span class="line">            terms[<span class="string">&quot;mse&quot;</span>] = mean_flat((target - model_output) ** <span class="number">2</span>)</span><br><span class="line">            <span class="keyword">if</span> <span class="string">&quot;vb&quot;</span> <span class="keyword">in</span> terms:</span><br><span class="line">                terms[<span class="string">&quot;loss&quot;</span>] = terms[<span class="string">&quot;mse&quot;</span>] + terms[<span class="string">&quot;vb&quot;</span>]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                terms[<span class="string">&quot;loss&quot;</span>] = terms[<span class="string">&quot;mse&quot;</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> NotImplementedError(<span class="variable language_">self</span>.loss_type)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> terms</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AIGC </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LDM的细节</title>
      <link href="/2025/04/27/AIGC/LDM/"/>
      <url>/2025/04/27/AIGC/LDM/</url>
      
        <content type="html"><![CDATA[<p><strong>系列文章：</strong></p><ol class="series-items"><li><a href="/2025/04/27/AIGC/LDM/" title="LDM的细节">LDM的细节</a></li></ol><blockquote><p>原论文：<a href="http://openaccess.thecvf.com/content/CVPR2022/html/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.html">High-resolution image synthesis with latent diffusion models</a></p><p>源码：<a href="https://github.com/CompVis/latent-diffusion">https://github.com/CompVis/latent-diffusion</a></p></blockquote><h1 id="背景介绍">1.背景介绍</h1><p>影像生成是当时计算需求最大的领域之一，尤其是复杂场景的高分辨率合成。这对于自回归架构（AR）来说可能需要数十亿参数，同时GANs所取得的成果大多局限于有限变化的数据，不容易扩展到复杂的多模态分布建模。扩散模型的进步在多个任务下取得了SOTA，却不会像GANs一样表现出模式崩溃或者训练不稳定性，也无需像AR模型那样涉及数十亿个参数。即便如此，DMs仍是计算高需求的。</p><blockquote><p>likelihood-based model学习过程可分为两个阶段：</p><ul><li>感知压缩阶段：消除高频细节，学习语义信息</li><li>生成模型阶段：学习该语义压缩信息</li></ul></blockquote><p>本文的目标就是找到一个合适的表示空间，保持相同的语义信息但是更计算高效。</p><h2 id="两阶段图像生成">1.1.两阶段图像生成</h2><h3 id="vqvae">1.1.1.VQVAE</h3><p><strong>VQVAE</strong>使用自回归模型学习离散潜空间的先验表达，它与AE最大的不同在于AE编码的特征向量是连续的，但是VQVAE编码的特征向量是离散的，即视觉码本（Visual Codebook）或视觉字典（Visual Dictionary）。</p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/屏幕截图 2025-04-27 142255.png" width="80%"/></p><h3 id="vqgan">1.1.2.VQGAN</h3><p><strong>VQGAN</strong>是一个改良版的VQVAE，相比于VQVAE，其有3点改进：</p><ul><li>将传统CNN改为Transformer来捕捉较远像素之间的依赖关系</li><li>额外增加了一个PatchGAN作为判别器，并在训练时加入判别损失</li><li>使用感知损失代替传统的L2损失</li></ul><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/屏幕截图 2025-04-27 143922.png" width="60%"/></p><h1 id="method">2.Method</h1><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/LDM_0.png" width="60%"/></p><h2 id="感知图像压缩">2.1.感知图像压缩</h2><p>Perceptual Image Compression模块使用一个预训练好的VQGAN将原始输入图像进行感知压缩。具体来说，对于给定的输入空间 <span class="math inline"><em>x</em> ∈ ℝ<sup><em>H</em> × <em>W</em> × 3</sup></span> ，编码器 <span class="math inline">ℰ</span> 将 <span class="math inline"><em>x</em></span> 编码到隐空间表示 <span class="math inline"><em>z</em> = ℰ(<em>x</em>)</span> ，其中 <span class="math inline"><em>z</em> ∈ ℝ<sup><em>h</em> × <em>w</em> × <em>c</em></sup></span>。</p><p>为了避免潜空间的方差过大，LDM使用了两种不同的正则化方法：KL-reg和VQ-reg。其中KL-reg相当于在潜空间上施加了KL惩罚，VQ-reg的作用则是使用向量量化层（Vector Quantization Layer）将特征归一化为码本中最近的那个特征。</p><p>对于VQ-reg的计算可以分解为以下步骤：</p><ol type="1"><li>特征reshape：将输入数据 <span class="math inline"><em>z</em><sub><em>e</em></sub> ∈ ℝ<sup><em>n</em> × <em>h</em> × <em>w</em> × <em>d</em></sup></span> 进行维度合并，得到 <span class="math inline"><em>n</em> × <em>h</em> × <em>w</em></span> 个长度为 <span class="math inline"><em>d</em></span> 的特征向量；</li><li>映射码本：对于每个特征向量，计算其与码本中<span class="math inline"><em>k</em></span>个向量的距离，选择距离最近的特征作为索引，最终得到经过码本查找的<span class="math inline"><em>n</em> × <em>h</em> × <em>w</em></span> 个长度为 <span class="math inline"><em>d</em></span> 的特征向量；</li><li>还原reshape：对特征再次进行reshape，得到 <span class="math inline"><em>z</em><sub><em>q</em></sub> ∈ ℝ<sup><em>n</em> × <em>h</em> × <em>w</em> × <em>d</em></sup></span>；</li><li><strong>复制梯度</strong>：由于在进行选择码本索引时使用了 <span class="math inline"><em>a</em><em>r</em><em>g</em><em>m</em><em>i</em><em>n</em></span>，因此造成了梯度回传不连续，因此这里直接将 <span class="math inline"><em>z</em><sub><em>q</em></sub></span> 的导数复制到 <span class="math inline"><em>z</em><sub><em>e</em></sub></span>。</li></ol><h2 id="ldm">2.2.LDM</h2><p>得到压缩后的图像潜空间表示后，可以直接在隐空间上进行前向过程和去噪过程了，由于隐空间特征的大小要比图像空间小很多，因此LDM的推理速度要快很多。</p><p><span class="math display">$$\begin{split}    L_{L D M}:=\mathbb{E}_{\mathcal{E}(x), \epsilon \sim \mathcal{N}(0,1), t}\left[\left\|\epsilon-\epsilon_\theta\left(z_t, t\right)\right\|_2^2\right]\end{split}$$</span></p><h2 id="条件控制">2.3.条件控制</h2><p>去噪过程的条件引入往往是将条件信息 <span class="math inline"><em>y</em></span> 和时间步条件 <span class="math inline"><em>t</em></span> 一同加入噪声预测器 <span class="math inline"><em>ϵ</em><sub><em>θ</em></sub>(<em>z</em><sub><em>t</em></sub>, <em>t</em>, <em>y</em>)</span>，针对不同的生成任务，条件<span class="math inline"><em>y</em></span>可能是文本信息、语义图等。</p><p>对于文本信息，LDM引入了一个条件编码器 <span class="math inline"><em>τ</em><sub><em>θ</em></sub></span> 来将条件信息 <span class="math inline"><em>y</em></span> 统一编码为中间特征 <span class="math inline"><em>τ</em><sub><em>θ</em></sub>(<em>y</em>) ∈ ℝ<sup><em>M</em> × <em>d</em><sub><em>r</em></sub></sup></span> ，之后通过交叉注意力映射到UNet的中间层。</p><p><span class="math display">$$\begin{split}    L_{L D M}:=\mathbb{E}_{\mathcal{E}(x), \epsilon \sim \mathcal{N}(0,1), t}\left[\left\|\epsilon-\epsilon_\theta\left(z_t, t, \tau_\theta(y)\right)\right\|_2^2\right]\end{split}$$</span></p><p>对于语义图等条件，则可直接将特征图拼接到UNet中完成。</p><h1 id="rdm">3.RDM</h1><blockquote><p>原论文：<a href="http://arxiv.org/abs/2204.11824">Semi-Parametric Neural Image Synthesis</a></p></blockquote><p>这里简单介绍了一下RDM训练范式，因为LDM提供的源码可通过RDM进行推理。</p><h2 id="背景">3.1.背景</h2><p>当时的训练范式需要巨量的计算资源和训练时间，受到<strong>检索增强NLP（RAG）</strong>发展的启发，RDM质疑目前将不同训练样例直接转化为巨量可训练参数的方式，并提出为一个小的生成模型配备一个大型图像数据库的方式。</p><pre><code>注：RAG（Retrieval-Augmented Generation）是一种使LLM在生成回答时读取外部信息库的技术，可以理解为在生成内容之前，先从外部数据库中检索出相关信息作为参考。</code></pre><p>在训练过程中，它会通过近邻查找访问数据库，无需从头学习数据，而是通过检索到的视觉特征合成新的场景。这一方法不仅提升了生成性能，也大幅度降低了参数数量和训练开销。并且这种方式也独立于所使用的生成模型，允许我们使用retrieval-augmented diffusion（RDM），也能使用retrieval-augmented autoregressive（RARM）。</p><h2 id="方法">3.2.方法</h2><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/RDM.png" width="80%"/></p><h3 id="总体介绍">3.2.1.总体介绍</h3><p>和传统全参数生成模型不同，我们定义了一个半参数生成模型 <span class="math inline"><em>p</em><sub><em>θ</em>, 𝒟, <em>ξ</em><sub><em>k</em></sub></sub>(<em>x</em>)</span> ，其中 <span class="math inline"><em>θ</em></span> 是可训练参数而 <span class="math inline">𝒟, <em>ξ</em><sub><em>k</em></sub></span> 是不可训练的模型模块，<span class="math inline">𝒟 = {<em>y</em><sub><em>i</em></sub>}<sub><em>i</em> = 1</sub><sup><em>N</em></sup></span> 是一个大小为 <span class="math inline"><em>N</em></span> 的固定图像数据库，与训练图像 <span class="math inline">𝒳</span>不相关， <span class="math inline"><em>ξ</em><sub><em>k</em></sub></span> 则表示基于查询 <span class="math inline"><em>x</em></span> 的采样策略，即 <span class="math inline"><em>ξ</em><sub><em>k</em></sub> : <em>x</em>, 𝒟 ↦ ℳ<sub>𝒟</sub><sup>(<em>k</em>)</sup></span>。</p><p>由此可以发现，<span class="math inline"><em>ξ</em><sub><em>k</em></sub>(<em>x</em>, 𝒟)</span> 的选择很重要，以便模型能从检索数据库中提取到有用的信息。通常的做法是，考虑一个查询图像 <span class="math inline"><em>x</em> ∈ ℝ<sup><em>H</em><sub><em>x</em></sub> × <em>W</em><sub><em>x</em></sub> × 3</sup></span> ，<span class="math inline"><em>ξ</em><sub><em>k</em></sub>(<em>x</em>, 𝒟)</span> 通过给定距离函数 <span class="math inline"><em>d</em>{<em>x</em>,  ⋅ }</span> ，返回 <span class="math inline"><em>k</em></span> 个最近邻的集合。得到检索的图像样本 <span class="math inline"><em>y</em> ∈ ℳ<sub>𝒟</sub><sup>(<em>k</em>)</sup></span> 后，再通过一个固定的预训练图像编码器 <span class="math inline"><em>ϕ</em></span> 将高维度图像投射到低维流形。即：</p><p><span class="math display">$$\begin{split}    p_{\theta,\mathcal D,\xi_k}(x) = p_\theta(x|\{\phi(y)|y\in\xi_k(x,\mathcal D)\}).\end{split}$$</span></p><h3 id="半参数图像生成模型的两种实例">3.2.2.半参数图像生成模型的两种实例</h3><p>在训练过程中，假设训练数据集 <span class="math inline">𝒳 = {<em>x</em><sub><em>i</em></sub>}<sub><em>i</em> = 1</sub><sup><em>M</em></sup></span> 满足分布 <span class="math inline"><em>p</em>(<em>x</em>)</span> ，我们使用的采样策略是使用CLIP在图像特征空间中计算余弦相似度来选取 <span class="math inline"><em>k</em></span> -近邻，同样后续也通过CLIP编码器 <span class="math inline"><em>ϕ</em> = <em>ϕ</em><sub><em>C</em><em>L</em><em>I</em><em>P</em></sub></span> 得到图像表征。</p><ol type="1"><li>Retrieval-Augmented Diffusion Models（RDM）使用LDMs作为底层图像生成模型，即：</li></ol><p><span class="math display">$$\begin{split}    \min_\theta\mathcal{L}=\mathbb{E}_{p(x),z\thicksim E(x),\epsilon\thicksim\mathcal{N}(0,1),t}\left[\|\epsilon-\epsilon_\theta(z_t,t,\left.\{\phi_{\mathbf{CLIP}}(y)\mid y\in\xi_k(x,\mathcal{D})\}\right)\|_2^2\right].\end{split}$$</span></p><ol start="2" type="1"><li>Retrieval-Augmented Autoregressive Models（RARM）使用VQGAN作为底层图像生成模型，即：</li></ol><p><span class="math display">$$\begin{split}    \min_\theta\mathcal{L}=-\mathbb{E}_{p(x),z_q\sim E(x)}{\left[\sum_i\log p(z_q^{(i)}\mid z_q^{(&lt;i)},\mathrm{~}\{\phi_{\mathbf{CLIP}}(y)\mid y\in\xi_k(x,\mathcal{D})\})\right]}\end{split}$$</span></p><h3 id="推理过程">3.2.3.推理过程</h3><p>在推理过程中，<span class="math inline">𝒟, <em>ξ</em><sub><em>k</em></sub></span> 是可以随着下游任务的要求而改变的，比如根据应用需求扩大或者缩减数据库 <span class="math inline">𝒟</span> 、或是直接跳过检索提供条件集 <span class="math inline"><em>ϕ</em><sub><strong>C</strong><strong>L</strong><strong>I</strong><strong>P</strong></sub>(<em>y</em>)</span>。这也允许模型完成训练过程中未涉及到的其他任务，比如text-prompt或是class-labeled。</p><p>比如对于text-to-image任务，既可以通过text prompt <span class="math inline"><em>c</em><sub><em>t</em><em>e</em><em>x</em><em>t</em></sub></span> 使用CLIP生成特征图，然后通过该特征图检索<span class="math inline"><em>k</em></span>-近邻作为条件生成；也可以直接通过 <span class="math inline"><em>c</em><sub><em>t</em><em>e</em><em>x</em><em>t</em></sub></span> 生成的特征图作为条件生成。</p><h1 id="源码">4.源码</h1><h2 id="dm模型">4.1.DM模型</h2><p>LDM定义的模型在 <code>ldm\models\diffusion\ddpm.py</code> 中，因为使用pytorch-lighting进行训练，所以看起来很复杂，其实只需要关注模型结构部分。其中在模型<code>forward</code>部分，先对每个batch随机选取一个时间步进行训练，然后对条件输入进行处理，将其转为对应的中间特征，最终返回的是<code>p_losses</code>得到的损失。</p><p>而在<code>p_losses</code>中，则先随机初始化一个高斯噪声，通过前向过程<code>q_sample</code>得到 <span class="math inline"><em>x</em><sub><em>t</em></sub></span> ，接着将 <span class="math inline"><em>x</em><sub><em>t</em></sub></span> 、时间步 <span class="math inline"><em>t</em></span> 、条件 <span class="math inline"><em>c</em></span> 一同输入噪声预测网络中，得到预测噪声，之后通过l2损失计算loss。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, c, *args, **kwargs</span>):</span><br><span class="line">    t = torch.randint(<span class="number">0</span>, <span class="variable language_">self</span>.num_timesteps, (x.shape[<span class="number">0</span>],), device=<span class="variable language_">self</span>.device).long()</span><br><span class="line">    <span class="keyword">if</span> <span class="variable language_">self</span>.model.conditioning_key <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">assert</span> c <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.cond_stage_trainable:</span><br><span class="line">            c = <span class="variable language_">self</span>.get_learned_conditioning(c)</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.shorten_cond_schedule:  <span class="comment"># <span class="doctag">TODO:</span> drop this option</span></span><br><span class="line">            tc = <span class="variable language_">self</span>.cond_ids[t].to(<span class="variable language_">self</span>.device)</span><br><span class="line">            c = <span class="variable language_">self</span>.q_sample(x_start=c, t=tc, noise=torch.randn_like(c.<span class="built_in">float</span>()))</span><br><span class="line">    <span class="keyword">return</span> <span class="variable language_">self</span>.p_losses(x, c, t, *args, **kwargs)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">p_losses</span>(<span class="params">self, x_start, cond, t, noise=<span class="literal">None</span></span>):</span><br><span class="line">    noise = default(noise, <span class="keyword">lambda</span>: torch.randn_like(x_start))</span><br><span class="line">    x_noisy = <span class="variable language_">self</span>.q_sample(x_start=x_start, t=t, noise=noise)</span><br><span class="line">    model_output = <span class="variable language_">self</span>.apply_model(x_noisy, t, cond)</span><br><span class="line"></span><br><span class="line">    ...</span><br><span class="line">    loss = <span class="variable language_">self</span>.get_loss(model_out, target, mean=<span class="literal">False</span>).mean(dim=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss, loss_dict</span><br></pre></td></tr></table></figure><h3 id="前向过程">4.1.1.前向过程</h3><p>这里的前向过程与DDPM是保持一致的：</p><p><span class="math display">$$\begin{split}    q(x_t \mid x_0) = \mathcal{N}\bigl(x_t;\;\sqrt{\bar\alpha_t}\,x_0,\;(1-\bar\alpha_t)\,I\bigr).\end{split}$$</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">q_sample</span>(<span class="params">self, x_start, t, noise=<span class="literal">None</span></span>):</span><br><span class="line">    noise = default(noise, <span class="keyword">lambda</span>: torch.randn_like(x_start))</span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">        extract_into_tensor(<span class="variable language_">self</span>.sqrt_alphas_cumprod, t, x_start.shape) * x_start</span><br><span class="line">        + extract_into_tensor(<span class="variable language_">self</span>.sqrt_one_minus_alphas_cumprod, t, x_start.shape)</span><br><span class="line">        * noise</span><br><span class="line">    )</span><br></pre></td></tr></table></figure><h3 id="条件控制噪声预测">4.1.2.条件控制噪声预测</h3><p>这里噪声预测和DDPM一样还是使用UNet，不同在于为了引入条件控制加入了交叉注意力来嵌入条件信息。需要注意的是，时间步信息和条件信息的嵌入方式是不同的，时间步信息 <span class="math inline"><em>t</em></span> 是直接拼接到UNet每一层中的特征图，而条件信息 <span class="math inline"><em>c</em></span> 则是通过每一层的交叉注意力进行嵌入的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, timesteps=<span class="literal">None</span>, context=<span class="literal">None</span>, y=<span class="literal">None</span>,**kwargs</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Apply the model to an input batch.</span></span><br><span class="line"><span class="string">    :param x: an [N x C x ...] Tensor of inputs.</span></span><br><span class="line"><span class="string">    :param timesteps: a 1-D batch of timesteps.</span></span><br><span class="line"><span class="string">    :param context: conditioning plugged in via crossattn</span></span><br><span class="line"><span class="string">    :param y: an [N] Tensor of labels, if class-conditional.</span></span><br><span class="line"><span class="string">    :return: an [N x C x ...] Tensor of outputs.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">assert</span> (y <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>) == (</span><br><span class="line">        <span class="variable language_">self</span>.num_classes <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line">    ), <span class="string">&quot;must specify y if and only if the model is class-conditional&quot;</span></span><br><span class="line">    hs = []</span><br><span class="line">    t_emb = timestep_embedding(timesteps, <span class="variable language_">self</span>.model_channels, repeat_only=<span class="literal">False</span>)</span><br><span class="line">    emb = <span class="variable language_">self</span>.time_embed(t_emb)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="variable language_">self</span>.num_classes <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">assert</span> y.shape == (x.shape[<span class="number">0</span>],)</span><br><span class="line">        emb = emb + <span class="variable language_">self</span>.label_emb(y)</span><br><span class="line"></span><br><span class="line">    h = x.<span class="built_in">type</span>(<span class="variable language_">self</span>.dtype)</span><br><span class="line">    <span class="keyword">for</span> module <span class="keyword">in</span> <span class="variable language_">self</span>.input_blocks:</span><br><span class="line">        h = module(h, emb, context)</span><br><span class="line">        hs.append(h)</span><br><span class="line">    h = <span class="variable language_">self</span>.middle_block(h, emb, context)</span><br><span class="line">    <span class="keyword">for</span> module <span class="keyword">in</span> <span class="variable language_">self</span>.output_blocks:</span><br><span class="line">        h = th.cat([h, hs.pop()], dim=<span class="number">1</span>)</span><br><span class="line">        h = module(h, emb, context)</span><br><span class="line">    h = h.<span class="built_in">type</span>(x.dtype)</span><br><span class="line">    <span class="keyword">if</span> <span class="variable language_">self</span>.predict_codebook_ids:</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.id_predictor(h)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.out(h)</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/unet_trans.png"/></p><h3 id="采样过程推理">4.1.3.采样过程（推理）</h3><p>可以直接用 DDIM 进行采样，大大减少推理所需时间。这部分源码可参考<a href="https://litchi-lee.github.io/2025/04/17/AIGC/DDIM/">DDIM的细节</a>。</p><h2 id="感知图像压缩-1">4.2.感知图像压缩</h2><p>在<code>get_input</code>函数中，输入batch，将返回压缩后的图像和处理后的条件信息，这里我们先关注输入是如何进行压缩的，条件信息的处理将在后续重点讲解。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LatentDiffusion</span>(<span class="title class_ inherited__">DDPM</span>):</span><br><span class="line">    ...</span><br><span class="line"><span class="meta">    @torch.no_grad()</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_input</span>(<span class="params">self, batch, k, return_first_stage_outputs=<span class="literal">False</span>, force_c_encode=<span class="literal">False</span>, cond_key=<span class="literal">None</span>, return_original_cond=<span class="literal">False</span>, bs=<span class="literal">None</span></span>):</span><br><span class="line">        x = <span class="built_in">super</span>().get_input(batch, k)</span><br><span class="line">        <span class="keyword">if</span> bs <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            x = x[:bs]</span><br><span class="line">        x = x.to(<span class="variable language_">self</span>.device)</span><br><span class="line">        <span class="comment"># 感知图像压缩</span></span><br><span class="line">        encoder_posterior = <span class="variable language_">self</span>.encode_first_stage(x)</span><br><span class="line">        z = <span class="variable language_">self</span>.get_first_stage_encoding(encoder_posterior).detach()</span><br><span class="line">        ...</span><br><span class="line">        out = [z, c]</span><br><span class="line">        ...</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line">    </span><br><span class="line"><span class="meta">    @torch.no_grad()</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">encode_first_stage</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">hasattr</span>(<span class="variable language_">self</span>, <span class="string">&quot;split_input_params&quot;</span>):</span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>.split_input_params[<span class="string">&quot;patch_distributed_vq&quot;</span>]:</span><br><span class="line">            ...</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">return</span> <span class="variable language_">self</span>.first_stage_model.encode(x)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="variable language_">self</span>.first_stage_model.encode(x)</span><br></pre></td></tr></table></figure><p>进一步深入，可以发现输入是通过<code>self.first_stage_model.encode(x)</code>这个接口进行压缩的，源码中实现了两种自编码器用于encode，分别是VQVAE和VAE（在<code>ldm\models\autoencoder.py</code>中），但是由于原论文使用VQVAE，因此这里主要扒一扒VQVAE部分的源码。</p><h3 id="encode部分">4.2.1.encode部分</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">VQModel</span>(pl.LightningModule):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,...</span>)</span><br><span class="line">        ...</span><br><span class="line">        <span class="variable language_">self</span>.encoder = Encoder(**ddconfig)</span><br><span class="line">        <span class="variable language_">self</span>.quant_conv = torch.nn.Conv2d(ddconfig[<span class="string">&quot;z_channels&quot;</span>], embed_dim, <span class="number">1</span>)</span><br><span class="line">        <span class="variable language_">self</span>.quantize = VectorQuantizer(n_embed, embed_dim, beta=<span class="number">0.25</span>, remap=remap, sane_index_shape=sane_index_shape)</span><br><span class="line">    </span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">encode</span>(<span class="params">self, x</span>):</span><br><span class="line">        h = <span class="variable language_">self</span>.encoder(x)</span><br><span class="line">        h = <span class="variable language_">self</span>.quant_conv(h)</span><br><span class="line">        quant, emb_loss, info = <span class="variable language_">self</span>.quantize(h)</span><br><span class="line">        <span class="keyword">return</span> quant, emb_loss, info</span><br></pre></td></tr></table></figure><p>首先是第一部分Encoder部分，这一部分主要是一系列的下采样，最终得到中间特征图<span class="math inline"><em>h</em></span>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Encoder</span>(nn.Module):</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">    <span class="comment"># timestep embedding</span></span><br><span class="line">    temb = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># downsampling</span></span><br><span class="line">    hs = [<span class="variable language_">self</span>.conv_in(x)]</span><br><span class="line">    <span class="keyword">for</span> i_level <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.num_resolutions):</span><br><span class="line">        <span class="keyword">for</span> i_block <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.num_res_blocks):</span><br><span class="line">            h = <span class="variable language_">self</span>.down[i_level].block[i_block](hs[-<span class="number">1</span>], temb)</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.down[i_level].attn) &gt; <span class="number">0</span>:</span><br><span class="line">                h = <span class="variable language_">self</span>.down[i_level].attn[i_block](h)</span><br><span class="line">            hs.append(h)</span><br><span class="line">        <span class="keyword">if</span> i_level != <span class="variable language_">self</span>.num_resolutions-<span class="number">1</span>:</span><br><span class="line">            hs.append(<span class="variable language_">self</span>.down[i_level].downsample(hs[-<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># middle</span></span><br><span class="line">    h = hs[-<span class="number">1</span>]</span><br><span class="line">    h = <span class="variable language_">self</span>.mid.block_1(h, temb)</span><br><span class="line">    h = <span class="variable language_">self</span>.mid.attn_1(h)</span><br><span class="line">    h = <span class="variable language_">self</span>.mid.block_2(h, temb)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># end</span></span><br><span class="line">    h = <span class="variable language_">self</span>.norm_out(h)</span><br><span class="line">    h = nonlinearity(h)</span><br><span class="line">    h = <span class="variable language_">self</span>.conv_out(h)</span><br><span class="line">    <span class="keyword">return</span> h</span><br></pre></td></tr></table></figure><p>之后经过<code>quant_conv</code>将<span class="math inline"><em>h</em></span>的维度转为<code>embed_dim</code>，经过VectorQuantizer转为离散化量化向量。具体来说，先将原特征图reshape为<span class="math inline">(<em>b</em> * <em>h</em> * <em>w</em> * <em>c</em>, <em>d</em>)</span>的向量，方便与码本中的向量计算欧氏距离，然后选择最小距离的向量作为量化向量。之后计算量化损失（codebook loss+commitment loss）：</p><p><span class="math display">$$\begin{split}    \mathcal L_{vq} = \beta\|z_q - z.detach\| + \|z_q.detach - z\|\end{split}$$</span></p><p>由于中间操作涉及到取最小值，会阻断梯度的回传，因此额外加了一步<code>z_q = z + (z_q - z).detach()</code>用于保留梯度。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">VectorQuantizer2</span>(nn.Module):</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, z, temp=<span class="literal">None</span>, rescale_logits=<span class="literal">False</span>, return_logits=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="keyword">assert</span> temp <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> temp==<span class="number">1.0</span>, <span class="string">&quot;Only for interface compatible with Gumbel&quot;</span></span><br><span class="line">        <span class="keyword">assert</span> rescale_logits==<span class="literal">False</span>, <span class="string">&quot;Only for interface compatible with Gumbel&quot;</span></span><br><span class="line">        <span class="keyword">assert</span> return_logits==<span class="literal">False</span>, <span class="string">&quot;Only for interface compatible with Gumbel&quot;</span></span><br><span class="line">        <span class="comment"># reshape z -&gt; (batch, height, width, channel) and flatten</span></span><br><span class="line">        z = rearrange(z, <span class="string">&#x27;b c h w d -&gt; b h w d c&#x27;</span>).contiguous()</span><br><span class="line">        <span class="comment"># 展平为 (b * h * w * c, d) 的二维矩阵</span></span><br><span class="line">        z_flattened = z.view(-<span class="number">1</span>, <span class="variable language_">self</span>.e_dim)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算每个向量与codebook中所有向量的欧式距离平方 </span></span><br><span class="line">        <span class="comment"># (z - e)^2 = z^2 + e^2 - 2ze</span></span><br><span class="line">        <span class="comment"># 输出形状为 (b * h * w * c, n_codes)</span></span><br><span class="line">        d = torch.<span class="built_in">sum</span>(z_flattened ** <span class="number">2</span>, dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>) + \</span><br><span class="line">            torch.<span class="built_in">sum</span>(<span class="variable language_">self</span>.embedding.weight**<span class="number">2</span>, dim=<span class="number">1</span>) - <span class="number">2</span> * \</span><br><span class="line">            torch.einsum(<span class="string">&#x27;bd,dn-&gt;bn&#x27;</span>, z_flattened, rearrange(<span class="variable language_">self</span>.embedding.weight, <span class="string">&#x27;n d -&gt; d n&#x27;</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 为每个输入向量选出距离最小的codebook向量的下标</span></span><br><span class="line">        min_encoding_indices = torch.argmin(d, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 得到量化后的向量</span></span><br><span class="line">        z_q = <span class="variable language_">self</span>.embedding(min_encoding_indices).view(z.shape)</span><br><span class="line">        perplexity = <span class="literal">None</span></span><br><span class="line">        min_encodings = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 量化损失 = commit loss + codebook loss</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="variable language_">self</span>.legacy:</span><br><span class="line">            loss = <span class="variable language_">self</span>.beta * torch.mean((z_q.detach()-z)**<span class="number">2</span>) + \</span><br><span class="line">                   torch.mean((z_q - z.detach()) ** <span class="number">2</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> z_q.<span class="built_in">min</span>() &lt; <span class="variable language_">self</span>.lower <span class="keyword">or</span> z_q.<span class="built_in">max</span>() &gt; <span class="variable language_">self</span>.upper:</span><br><span class="line">                loss = torch.mean((z_q.detach() - z) ** <span class="number">2</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                loss = torch.mean((z_q.detach()-z)**<span class="number">2</span>) + <span class="variable language_">self</span>.beta * \</span><br><span class="line">                           torch.mean((z_q - z.detach()) ** <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># preserve gradients</span></span><br><span class="line">        z_q = z + (z_q - z).detach()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># reshape back to match original input shape</span></span><br><span class="line">        z_q = rearrange(z_q, <span class="string">&#x27;b h w d c -&gt; b c h w d&#x27;</span>).contiguous()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.remap <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            min_encoding_indices = min_encoding_indices.reshape(z.shape[<span class="number">0</span>],-<span class="number">1</span>) <span class="comment"># add batch axis</span></span><br><span class="line">            min_encoding_indices = <span class="variable language_">self</span>.remap_to_used(min_encoding_indices)</span><br><span class="line">            min_encoding_indices = min_encoding_indices.reshape(-<span class="number">1</span>,<span class="number">1</span>) <span class="comment"># flatten</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.sane_index_shape:</span><br><span class="line">            min_encoding_indices = min_encoding_indices.reshape(</span><br><span class="line">                z_q.shape[<span class="number">0</span>], z_q.shape[<span class="number">2</span>], z_q.shape[<span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> z_q, loss, (perplexity, min_encodings, min_encoding_indices)</span><br></pre></td></tr></table></figure><h3 id="decode部分">4.2.2.decode部分</h3><p>decode部分和encode一样，也是直接调用定义好的自编码器类内部的<code>decode</code>方法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LatentDiffusion</span>(<span class="title class_ inherited__">DDPM</span>):</span><br><span class="line">    ...</span><br><span class="line"><span class="meta">    @torch.no_grad()</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">decode_first_stage</span>(<span class="params">self, z, predict_cids=<span class="literal">False</span>, force_not_quantize=<span class="literal">False</span></span>):</span><br><span class="line">        ...</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(<span class="variable language_">self</span>.first_stage_model, VQModelInterface):</span><br><span class="line">                <span class="keyword">return</span> <span class="variable language_">self</span>.first_stage_model.decode(</span><br><span class="line">                    z, force_not_quantize=predict_cids <span class="keyword">or</span> force_not_quantize</span><br><span class="line">                )</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">return</span> <span class="variable language_">self</span>.first_stage_model.decode(z)</span><br></pre></td></tr></table></figure><p>和encode过程类似，不过相当于是反过来了，先过一层<code>post_quant_conv</code>恢复原特征维度，然后通过若干层上采样恢复原输入图像的尺寸。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">VQModel</span>(pl.LightningModule):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,...</span>)</span><br><span class="line">        ...</span><br><span class="line">        <span class="variable language_">self</span>.post_quant_conv = torch.nn.Conv2d(embed_dim, ddconfig[<span class="string">&quot;z_channels&quot;</span>], <span class="number">1</span>)</span><br><span class="line">        <span class="variable language_">self</span>.decoder = Decoder(**ddconfig)</span><br><span class="line"></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">decode</span>(<span class="params">self, quant</span>):</span><br><span class="line">        quant = <span class="variable language_">self</span>.post_quant_conv(quant)</span><br><span class="line">        dec = <span class="variable language_">self</span>.decoder(quant)</span><br><span class="line">        <span class="keyword">return</span> dec</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Decoder</span>(nn.Module):</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, z</span>):</span><br><span class="line">        <span class="comment">#assert z.shape[1:] == self.z_shape[1:]</span></span><br><span class="line">        <span class="variable language_">self</span>.last_z_shape = z.shape</span><br><span class="line"></span><br><span class="line">        <span class="comment"># timestep embedding</span></span><br><span class="line">        temb = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># z to block_in</span></span><br><span class="line">        h = <span class="variable language_">self</span>.conv_in(z)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># middle</span></span><br><span class="line">        h = <span class="variable language_">self</span>.mid.block_1(h, temb)</span><br><span class="line">        h = <span class="variable language_">self</span>.mid.attn_1(h)</span><br><span class="line">        h = <span class="variable language_">self</span>.mid.block_2(h, temb)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># upsampling</span></span><br><span class="line">        <span class="keyword">for</span> i_level <span class="keyword">in</span> <span class="built_in">reversed</span>(<span class="built_in">range</span>(<span class="variable language_">self</span>.num_resolutions)):</span><br><span class="line">            <span class="keyword">for</span> i_block <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.num_res_blocks+<span class="number">1</span>):</span><br><span class="line">                h = <span class="variable language_">self</span>.up[i_level].block[i_block](h, temb)</span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.up[i_level].attn) &gt; <span class="number">0</span>:</span><br><span class="line">                    h = <span class="variable language_">self</span>.up[i_level].attn[i_block](h)</span><br><span class="line">            <span class="keyword">if</span> i_level != <span class="number">0</span>:</span><br><span class="line">                h = <span class="variable language_">self</span>.up[i_level].upsample(h)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># end</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.give_pre_end:</span><br><span class="line">            <span class="keyword">return</span> h</span><br><span class="line"></span><br><span class="line">        h = <span class="variable language_">self</span>.norm_out(h)</span><br><span class="line">        h = nonlinearity(h)</span><br><span class="line">        h = <span class="variable language_">self</span>.conv_out(h)</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.tanh_out:</span><br><span class="line">            h = torch.tanh(h)</span><br><span class="line">        <span class="keyword">return</span> h</span><br></pre></td></tr></table></figure><h2 id="条件信息控制">4.3.条件信息控制</h2><p>之前在DM部分其实已经讲过条件特征是如何嵌入到扩散模型中的，这里来扒一扒在源码中是如何处理各类条件信息的，以及如何将其编码为统一的条件特征。可以看到条件信息都是通过<code>self.get_learned_conditioning</code>这个方法进行编码的，而在<code>self.get_learned_conditioning</code>中条件信息又是统一调用<code>self.cond_stage_model</code>中的方法进行编码的，因此我们将重点看一看各类条件信息编码器模型（定义在<code>ldm\modules\encoders\modules.py</code>中）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LatentDiffusion</span>(<span class="title class_ inherited__">DDPM</span>):</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, c, *args, **kwargs</span>):</span><br><span class="line">        ...</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.model.conditioning_key <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">assert</span> c <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>.cond_stage_trainable:</span><br><span class="line">                c = <span class="variable language_">self</span>.get_learned_conditioning(c)</span><br><span class="line">            ...</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.p_losses(x, c, t, *args, **kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_learned_conditioning</span>(<span class="params">self, c</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.cond_stage_forward <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">hasattr</span>(<span class="variable language_">self</span>.cond_stage_model, <span class="string">&#x27;encode&#x27;</span>) <span class="keyword">and</span> <span class="built_in">callable</span>(</span><br><span class="line">                <span class="variable language_">self</span>.cond_stage_model.encode</span><br><span class="line">            ):</span><br><span class="line">                c = <span class="variable language_">self</span>.cond_stage_model.encode(c)</span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">isinstance</span>(c, DiagonalGaussianDistribution):</span><br><span class="line">                    c = c.mode()</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                c = <span class="variable language_">self</span>.cond_stage_model(c)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">assert</span> <span class="built_in">hasattr</span>(<span class="variable language_">self</span>.cond_stage_model, <span class="variable language_">self</span>.cond_stage_forward)</span><br><span class="line">            c = <span class="built_in">getattr</span>(<span class="variable language_">self</span>.cond_stage_model, <span class="variable language_">self</span>.cond_stage_forward)(c)</span><br><span class="line">        <span class="keyword">return</span> c</span><br></pre></td></tr></table></figure><h3 id="类别编码器">4.3.1.类别编码器</h3><p>用于将<strong>类别信息</strong>转换为embedding向量，适用于分类条件生成任务。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ClassEmbedder</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, embed_dim, n_classes=<span class="number">1000</span>, key=<span class="string">&#x27;class&#x27;</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.key = key</span><br><span class="line">        <span class="variable language_">self</span>.embedding = nn.Embedding(n_classes, embed_dim)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, batch, key=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="keyword">if</span> key <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            key = <span class="variable language_">self</span>.key</span><br><span class="line">        <span class="comment"># this is for use in crossattn</span></span><br><span class="line">        c = batch[key][:, <span class="literal">None</span>]</span><br><span class="line">        c = <span class="variable language_">self</span>.embedding(c)</span><br><span class="line">        <span class="keyword">return</span> c</span><br></pre></td></tr></table></figure><h3 id="文本编码器">4.3.2.文本编码器</h3><h4 id="transformer">transformer</h4><p>普通的transformer编码器，输入<strong>Token IDs</strong>，输出其embedding向量表示。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TransformerEmbedder</span>(<span class="title class_ inherited__">AbstractEncoder</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Some transformer encoder layers&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n_embed, n_layer, vocab_size, max_seq_len=<span class="number">77</span>, device=<span class="string">&quot;cuda&quot;</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.device = device</span><br><span class="line">        <span class="variable language_">self</span>.transformer = TransformerWrapper(num_tokens=vocab_size, max_seq_len=max_seq_len,</span><br><span class="line">                                              attn_layers=Encoder(dim=n_embed, depth=n_layer))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, tokens</span>):</span><br><span class="line">        tokens = tokens.to(<span class="variable language_">self</span>.device)  <span class="comment"># meh</span></span><br><span class="line">        z = <span class="variable language_">self</span>.transformer(tokens, return_embeddings=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> z</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">encode</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>(x)</span><br></pre></td></tr></table></figure><h4 id="bert">Bert</h4><p>输入<strong>原始文本</strong>，先使用BERT的Tokenizer将其转为Token IDs，然后再调用<code>BertEmbedder</code>将其转为embedding向量。其中Tokenizer来自hugging face无法训练，后者可训练。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BERTTokenizer</span>(<span class="title class_ inherited__">AbstractEncoder</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; Uses a pretrained BERT tokenizer by huggingface. Vocab size: 30522 (?)&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, device=<span class="string">&quot;cuda&quot;</span>, vq_interface=<span class="literal">True</span>, max_length=<span class="number">77</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizerFast  <span class="comment"># <span class="doctag">TODO:</span> add to reuquirements</span></span><br><span class="line">        <span class="variable language_">self</span>.tokenizer = BertTokenizerFast.from_pretrained(<span class="string">&quot;bert-base-uncased&quot;</span>)</span><br><span class="line">        <span class="variable language_">self</span>.device = device</span><br><span class="line">        <span class="variable language_">self</span>.vq_interface = vq_interface</span><br><span class="line">        <span class="variable language_">self</span>.max_length = max_length</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, text</span>):</span><br><span class="line">        batch_encoding = <span class="variable language_">self</span>.tokenizer(text, truncation=<span class="literal">True</span>, max_length=<span class="variable language_">self</span>.max_length, return_length=<span class="literal">True</span>,</span><br><span class="line">                                        return_overflowing_tokens=<span class="literal">False</span>, padding=<span class="string">&quot;max_length&quot;</span>, return_tensors=<span class="string">&quot;pt&quot;</span>)</span><br><span class="line">        tokens = batch_encoding[<span class="string">&quot;input_ids&quot;</span>].to(<span class="variable language_">self</span>.device)</span><br><span class="line">        <span class="keyword">return</span> tokens</span><br><span class="line"></span><br><span class="line"><span class="meta">    @torch.no_grad()</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">encode</span>(<span class="params">self, text</span>):</span><br><span class="line">        tokens = <span class="variable language_">self</span>(text)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="variable language_">self</span>.vq_interface:</span><br><span class="line">            <span class="keyword">return</span> tokens</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span>, <span class="literal">None</span>, [<span class="literal">None</span>, <span class="literal">None</span>, tokens]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">decode</span>(<span class="params">self, text</span>):</span><br><span class="line">        <span class="keyword">return</span> text</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BERTEmbedder</span>(<span class="title class_ inherited__">AbstractEncoder</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Uses the BERT tokenizr model and add some transformer encoder layers&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n_embed, n_layer, vocab_size=<span class="number">30522</span>, max_seq_len=<span class="number">77</span>,</span></span><br><span class="line"><span class="params">                 device=<span class="string">&quot;cuda&quot;</span>,use_tokenizer=<span class="literal">True</span>, embedding_dropout=<span class="number">0.0</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.use_tknz_fn = use_tokenizer</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.use_tknz_fn:</span><br><span class="line">            <span class="variable language_">self</span>.tknz_fn = BERTTokenizer(vq_interface=<span class="literal">False</span>, max_length=max_seq_len)</span><br><span class="line">        <span class="variable language_">self</span>.device = device</span><br><span class="line">        <span class="variable language_">self</span>.transformer = TransformerWrapper(num_tokens=vocab_size, max_seq_len=max_seq_len,</span><br><span class="line">                                              attn_layers=Encoder(dim=n_embed, depth=n_layer),</span><br><span class="line">                                              emb_dropout=embedding_dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, text</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.use_tknz_fn:</span><br><span class="line">            tokens = <span class="variable language_">self</span>.tknz_fn(text)<span class="comment">#.to(self.device)</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            tokens = text</span><br><span class="line">        z = <span class="variable language_">self</span>.transformer(tokens, return_embeddings=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> z</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">encode</span>(<span class="params">self, text</span>):</span><br><span class="line">        <span class="comment"># output of length 77</span></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>(text)</span><br></pre></td></tr></table></figure><h4 id="clip">CLIP</h4><p>输入<strong>原始文本</strong>，使用OpenAI CLIP的tokenizer以及文本编码器，生成文本的embedding表示，参数冻结无法训练。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">FrozenCLIPTextEmbedder</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Uses the CLIP transformer encoder for text.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, version=<span class="string">&#x27;ViT-L/14&#x27;</span>, device=<span class="string">&quot;cuda&quot;</span>, max_length=<span class="number">77</span>, n_repeat=<span class="number">1</span>, normalize=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.model, _ = clip.load(version, jit=<span class="literal">False</span>, device=<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">        <span class="variable language_">self</span>.device = device</span><br><span class="line">        <span class="variable language_">self</span>.max_length = max_length</span><br><span class="line">        <span class="variable language_">self</span>.n_repeat = n_repeat</span><br><span class="line">        <span class="variable language_">self</span>.normalize = normalize</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">freeze</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="variable language_">self</span>.model = <span class="variable language_">self</span>.model.<span class="built_in">eval</span>()</span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> <span class="variable language_">self</span>.parameters():</span><br><span class="line">            param.requires_grad = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, text</span>):</span><br><span class="line">        tokens = clip.tokenize(text).to(<span class="variable language_">self</span>.device)</span><br><span class="line">        z = <span class="variable language_">self</span>.model.encode_text(tokens)</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.normalize:</span><br><span class="line">            z = z / torch.linalg.norm(z, dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> z</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">encode</span>(<span class="params">self, text</span>):</span><br><span class="line">        z = <span class="variable language_">self</span>(text)</span><br><span class="line">        <span class="keyword">if</span> z.ndim==<span class="number">2</span>:</span><br><span class="line">            z = z[:, <span class="literal">None</span>, :]</span><br><span class="line">        z = repeat(z, <span class="string">&#x27;b 1 d -&gt; b k d&#x27;</span>, k=<span class="variable language_">self</span>.n_repeat)</span><br><span class="line">        <span class="keyword">return</span> z</span><br></pre></td></tr></table></figure><h3 id="图像编码器">4.3.3.图像编码器</h3><h4 id="图像缩放器">图像缩放器</h4><p>输入<strong>图像特征图</strong>，对其进行空间尺寸的调整（下采样或上采样以及维度映射），输出调整尺寸后的图像特征图。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SpatialRescaler</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                 n_stages=<span class="number">1</span>,</span></span><br><span class="line"><span class="params">                 method=<span class="string">&#x27;bilinear&#x27;</span>,</span></span><br><span class="line"><span class="params">                 multiplier=<span class="number">0.5</span>,</span></span><br><span class="line"><span class="params">                 in_channels=<span class="number">3</span>,</span></span><br><span class="line"><span class="params">                 out_channels=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 bias=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.n_stages = n_stages</span><br><span class="line">        <span class="keyword">assert</span> <span class="variable language_">self</span>.n_stages &gt;= <span class="number">0</span></span><br><span class="line">        <span class="keyword">assert</span> method <span class="keyword">in</span> [<span class="string">&#x27;nearest&#x27;</span>,<span class="string">&#x27;linear&#x27;</span>,<span class="string">&#x27;bilinear&#x27;</span>,<span class="string">&#x27;trilinear&#x27;</span>,<span class="string">&#x27;bicubic&#x27;</span>,<span class="string">&#x27;area&#x27;</span>]</span><br><span class="line">        <span class="variable language_">self</span>.multiplier = multiplier</span><br><span class="line">        <span class="variable language_">self</span>.interpolator = partial(torch.nn.functional.interpolate, mode=method)</span><br><span class="line">        <span class="variable language_">self</span>.remap_output = out_channels <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.remap_output:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&#x27;Spatial Rescaler mapping from <span class="subst">&#123;in_channels&#125;</span> to <span class="subst">&#123;out_channels&#125;</span> channels after resizing.&#x27;</span>)</span><br><span class="line">            <span class="variable language_">self</span>.channel_mapper = nn.Conv2d(in_channels,out_channels,<span class="number">1</span>,bias=bias)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        <span class="keyword">for</span> stage <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.n_stages):</span><br><span class="line">            x = <span class="variable language_">self</span>.interpolator(x, scale_factor=<span class="variable language_">self</span>.multiplier)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.remap_output:</span><br><span class="line">            x = <span class="variable language_">self</span>.channel_mapper(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">encode</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>(x)</span><br></pre></td></tr></table></figure><h4 id="clip-1">CLIP</h4><p>输入<strong>图像特征</strong>，使用OpenAI CLIP图像编码器，将其转为图像对应的embedding向量，参数冻结同样无法训练。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">FrozenClipImageEmbedder</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Uses the CLIP image encoder.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">            self,</span></span><br><span class="line"><span class="params">            model,</span></span><br><span class="line"><span class="params">            jit=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">            device=<span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available(<span class="params"></span>) <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>,</span></span><br><span class="line"><span class="params">            antialias=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">        </span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.model, _ = clip.load(name=model, device=device, jit=jit)</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.antialias = antialias</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.register_buffer(<span class="string">&#x27;mean&#x27;</span>, torch.Tensor([<span class="number">0.48145466</span>, <span class="number">0.4578275</span>, <span class="number">0.40821073</span>]), persistent=<span class="literal">False</span>)</span><br><span class="line">        <span class="variable language_">self</span>.register_buffer(<span class="string">&#x27;std&#x27;</span>, torch.Tensor([<span class="number">0.26862954</span>, <span class="number">0.26130258</span>, <span class="number">0.27577711</span>]), persistent=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">preprocess</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># normalize to [0,1]</span></span><br><span class="line">        x = kornia.geometry.resize(x, (<span class="number">224</span>, <span class="number">224</span>),</span><br><span class="line">                                   interpolation=<span class="string">&#x27;bicubic&#x27;</span>,align_corners=<span class="literal">True</span>,</span><br><span class="line">                                   antialias=<span class="variable language_">self</span>.antialias)</span><br><span class="line">        x = (x + <span class="number">1.</span>) / <span class="number">2.</span></span><br><span class="line">        <span class="comment"># renormalize according to clip</span></span><br><span class="line">        x = kornia.enhance.normalize(x, <span class="variable language_">self</span>.mean, <span class="variable language_">self</span>.std)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># x is assumed to be in range [-1,1]</span></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.model.encode_image(<span class="variable language_">self</span>.preprocess(x))</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AIGC </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>重建任务常用指标</title>
      <link href="/2025/04/26/AIGC/metrics/"/>
      <url>/2025/04/26/AIGC/metrics/</url>
      
        <content type="html"><![CDATA[<h1 id="psnr">PSNR</h1><p>峰值信噪比（Peak Signal-to-Noise Ratio，PSNR）是<strong>像素级误差指标</strong>，用于衡量重建图像与参考图像之间的像素误差，值越大表示图像越接近参考图像，单位dB。</p><p><span class="math display">$$  \begin{split}     PSNR = 10 \cdot log_{10}(\frac{MAX^2}{MSE})  \end{split}$$</span></p><p>其中 <span class="math inline">$MSE = \frac{1}{n} \sum (x-\hat{x})^2$</span> 。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">psnr = <span class="number">20</span> * np.log10(PIXEL_MAX / np.sqrt(np.power(img1 - img2, <span class="number">2</span>)))</span><br></pre></td></tr></table></figure><p>或者在pytorch中，可以写作：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_psnr</span>(<span class="params">pred, target, max_val=<span class="number">1.0</span></span>):</span><br><span class="line">    mse = F.mse_loss(pred, target)</span><br><span class="line">    psnr = <span class="number">10</span> * torch.log10(max_val ** <span class="number">2</span> / mse)</span><br><span class="line">    <span class="keyword">return</span> psnr.item()</span><br></pre></td></tr></table></figure><h1 id="ssim">SSIM</h1><p>结构相似性（Structure Similarity Index，SSIM），衡量图像在亮度、对比度、结构上的相似性，范围为 <span class="math inline">[ − 1, 1]</span> ，越接近1表示越相似。</p><p><span class="math display">$$\begin{split}    \mathrm{SSIM}(x,y)=\frac{(2\mu_x\mu_y+C_1)(2\sigma_{xy}+C_2)}{(\mu_x^2+\mu_y^2+C_1)(\sigma_x^2+\sigma_y^2+C_2)}\end{split}$$</span></p><p>在pytorch中，直接调用：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> piq <span class="keyword">import</span> ssim</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_ssim</span>(<span class="params">pred, target</span>):</span><br><span class="line">    <span class="keyword">return</span> ssim(pred, target, data_range=<span class="number">1.0</span>).item()</span><br></pre></td></tr></table></figure><p><strong>MS-SSIM</strong>（Multi-Scale SSIM）是SSIM的扩展版本，更鲁棒细致。在pytorch中同样可以直接调包：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> piq <span class="keyword">import</span> multi_scale_ssim</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_ms_ssim</span>(<span class="params">pred, target</span>):</span><br><span class="line">    <span class="keyword">return</span> multi_scale_ssim(pred, target, data_range=<span class="number">1.0</span>).item()</span><br></pre></td></tr></table></figure><h1 id="lpips">LPIPS</h1><p><strong>LPIPS</strong>(Learned Perceptual Image Patch Similarity)基于深度网络（如Alexnet或者VGG）提取特征后计算相似性，更强调感知上的差距，范围为 <span class="math inline">[ − 1, 1]</span> ，越小越好。在pytorch中可直接调用：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> lpips</span><br><span class="line">loss_fn = lpips.LPIPS(net=<span class="string">&#x27;alex&#x27;</span>)  <span class="comment"># 可选: alex / vgg / squeeze</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_lpips</span>(<span class="params">pred, target</span>):</span><br><span class="line">    <span class="keyword">return</span> loss_fn(pred, target).item()</span><br></pre></td></tr></table></figure><h1 id="hfen">HFEN</h1><p><strong>HFEN</strong>（High-Frequency Error Norm）用于评估图像中的高频信息是否被重建出来。具体来说，它将图像通过<strong>LoG</strong>（Laplacian of Gaussian）滤波器提取高频信息，然后计算误差。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">hfen</span>(<span class="params">pred, target</span>):</span><br><span class="line">    <span class="comment"># 使用 LoG 近似（可以用更精确的方法替代）</span></span><br><span class="line">    laplacian_kernel = torch.tensor([[[[<span class="number">0</span>,  <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">                                       [<span class="number">1</span>, -<span class="number">4</span>, <span class="number">1</span>],</span><br><span class="line">                                       [<span class="number">0</span>,  <span class="number">1</span>, <span class="number">0</span>]]]], dtype=pred.dtype, device=pred.device)</span><br><span class="line">    pred_hf = F.conv2d(pred, laplacian_kernel, padding=<span class="number">1</span>)</span><br><span class="line">    target_hf = F.conv2d(target, laplacian_kernel, padding=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> F.mse_loss(pred_hf, target_hf).item()</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AIGC </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DDIM总结</title>
      <link href="/2025/04/17/AIGC/DDIM/"/>
      <url>/2025/04/17/AIGC/DDIM/</url>
      
        <content type="html"><![CDATA[<p><strong>系列文章：</strong></p><ol class="series-items"><li><a href="/2025/04/15/AIGC/DDPM/" title="DDPM总结">DDPM总结</a></li><li><a href="/2025/04/17/AIGC/DDIM/" title="DDIM总结">DDIM总结</a></li><li><a href="/2025/05/08/AIGC/DIT/" title="DiT的细节">DiT的细节</a></li><li><a href="/2025/05/08/AIGC/iDDPM/" title="iDDPM总结">iDDPM总结</a></li></ol><h1 id="论文回顾">论文回顾</h1><blockquote><p>原论文：<a href="http://arxiv.org/abs/2010.02502">Denoising Diffusion Implicit Models</a></p></blockquote><h2 id="背景">背景</h2><p>近期的降噪过程要么是基于朗之万动力学（NCSN）或是基于加噪过程的逆过程（DDPM），但是这些方法都有很严重的缺陷–需要很多次迭代来生成高质量的图像。本文提出了DDIM（Denoising Diffusion Implicit Models），将<strong>原基于马尔可夫假设的DDPM推广到了非马尔科夫过程的DDPM</strong>，加速采样过程。</p><h2 id="非马尔可夫前向过程的变分推理">非马尔可夫前向过程的变分推理</h2><p>回顾DDPM中的目标函数，可以将其写作（其中<span class="math inline"><em>γ</em><sub><em>t</em></sub></span>是一些常数项）：</p><p><span class="math display">$$    \begin{split}        L_{\gamma} :=  \sum_{t=1}^T \gamma_t \mathbb E_{q(x_t|x_0)}[ \left\lVert\epsilon_t  - {\hat\epsilon}_{\theta}(x_t, t)\right\rVert_2^2 ], \epsilon_t \sim \mathcal{N}(0,\textit{I})    \end{split}$$</span></p><p>可以发现，其实<span class="math inline"><em>L</em><sub><em>γ</em></sub></span>只依赖于边缘分布<span class="math inline"><em>q</em>(<em>x</em><sub><em>t</em></sub>|<em>x</em><sub>0</sub>)</span>而不直接依赖于联合分布<span class="math inline"><em>q</em>(<em>x</em><sub>1 : <em>T</em></sub>|<em>x</em><sub>0</sub>)</span>，换句话说只要<span class="math inline"><em>q</em>(<em>x</em><sub><em>t</em></sub>|<em>x</em><sub>0</sub>)</span>已知并且满足高斯分布的形式，那么就可以用DDPM预测噪音的目标函数训练模型。进一步说，只要<strong>我们保证<span class="math inline"><em>q</em>(<em>x</em><sub><em>t</em></sub>|<em>x</em><sub>0</sub>)</span>的形式不变，那么我们就可以复用训好的DDPM，然后定义新的采样过程</strong>。</p><h3 id="非马尔可夫前向过程">非马尔可夫前向过程</h3><p>（1）考虑一个非马尔可夫过程，定义为（与DDPM不同）：</p><p><span class="math display">$$\begin{split}q_\sigma(\mathbf{x}_{1:T}|\mathbf{x}_0):=q_\sigma(\mathbf{x}_T|\mathbf{x}_0)\prod_{t=2}^Tq_\sigma(\mathbf{x}_{t-1}|\mathbf{x}_t,\mathbf{x}_0)\end{split}$$</span></p><blockquote><p>注意在DDPM中，马尔可夫前向过程为： <span class="math display">$$\begin{split}q(\mathbf{x}_{1:T}|\mathbf{x}_0):=\prod_{t=1}^Tq(\mathbf{x}_{t}|\mathbf{x}_{t-1})\end{split}$$</span></p></blockquote><p>（2）并且定义（与DDPM一致）：</p><p><span class="math display">$$\begin{split}q_\sigma(\mathbf{x}_T|\mathbf{x}_0)=\mathcal{N}(\sqrt{\alpha_T}\mathbf{x}_0,(1-\alpha_T)\mathbf{I})\end{split}$$</span></p><div class="note warning flat"><p>要注意这里的符号和DDPM中不太一样，这里的<span class="math inline"><em>α</em><sub><em>T</em></sub></span>相当于DDPM中的<span class="math inline"><em>ᾱ</em><sub><em>T</em></sub></span></p></div><p>（3）对<span class="math inline"><em>t</em> &gt; 1</span>定义：</p><p><span class="math display">$$\begin{split}q_\sigma(\mathbf{x}_{t-1}|\mathbf{x}_t,\mathbf{x}_0)=\mathcal{N}\left(\sqrt{\alpha_{t-1}}\mathbf{x}_0+\sqrt{1-\alpha_{t-1}-\sigma_t^2}\cdot\frac{\mathbf{x}_t-\sqrt{\alpha_t}\mathbf{x}_0}{\sqrt{1-\alpha_t}},\sigma_t^2\mathbf{I}\right).\end{split}$$</span></p><p>只要满足上述三个定义，就可以确保之前所说的<span class="math inline"><em>q</em>(<em>x</em><sub><em>t</em></sub>|<em>x</em><sub>0</sub>)</span>的形式不改变（原论文附录证明），即对任意<span class="math inline"><em>t</em></span>，满足：</p><p><span class="math display">$$\begin{split}q_\sigma(\mathbf{x}_{t}|\mathbf{x}_0)=\mathcal{N}\left(\sqrt{\alpha_t}\mathbf{x}_0,(1-\alpha_t)\mathbf{I}\right).\end{split}$$</span></p><p>由此可以根据贝叶斯公式得到：</p><p><span class="math display">$$\begin{split}q_\sigma(\mathbf{x}_t|\mathbf{x}_{t-1},\mathbf{x}_0)=\frac{q_\sigma(\mathbf{x}_{t-1}|\mathbf{x}_t,\mathbf{x}_0)q_\sigma(\mathbf{x}_t|\mathbf{x}_0)}{q_\sigma(\mathbf{x}_{t-1}|\mathbf{x}_0)}\end{split}$$</span></p><p>此时这里的前向过程不再是马尔可夫过程，<span class="math inline"><em>σ</em></span>的大小控制着前向过程的随机性，当<span class="math inline"><em>σ</em> → 0</span>时，就是一个确定性过程，即对<span class="math inline"><em>t</em></span>一旦确定<span class="math inline"><em>x</em><sub>0</sub></span>和<span class="math inline"><em>x</em><sub><em>t</em></sub></span>，<span class="math inline"><em>x</em><sub><em>t</em> − 1</sub></span>也随之确定。</p><h3 id="采样过程">采样过程</h3><p>回顾DDPM的采样过程，我们使用一个噪声预测模型<span class="math inline"><em>ϵ</em><sub><em>θ</em></sub></span>来预测<span class="math inline"><em>ϵ</em><sub><em>t</em></sub></span>，即<span class="math inline">$\mathbf{x}_t=\sqrt{\alpha_t}\mathbf{x}_0+\sqrt{1-\alpha_t}\epsilon_\theta$</span>，由此我们可以根据<span class="math inline"><em>x</em><sub><em>t</em></sub></span>预测<span class="math inline"><em>x</em><sub>0</sub></span>：</p><p><span class="math display">$$\begin{split}f_\theta^{(t)}(\mathbf{x}_t):=(\mathbf{x}_t-\sqrt{1-\alpha_t}\cdot\epsilon_\theta^{(t)}(\mathbf{x}_t))/\sqrt{\alpha_t}.\end{split}$$</span></p><p>进一步得到采样过程的后验公式：</p><p><span class="math display">$$p_\theta^{(t)}(\mathbf{x}_{t-1}|\mathbf{x}_t)=\begin{cases}\mathcal{N}(f_\theta^{(1)}(\mathbf{x}_1),\sigma_1^2\mathbf{I}) &amp; \mathrm{if~}t=1 \\q_\sigma(\mathbf{x}_{t-1}|\mathbf{x}_t,f_\theta^{(t)}(\mathbf{x}_t)) &amp; \text{otherwise,}\end{cases}$$</span></p><p>由此可以得到非马尔可夫过程的扩散过程。</p><h2 id="从广义生成过程中采样">从广义生成过程中采样</h2><h3 id="扩散隐模型的降噪过程">扩散隐模型的降噪过程</h3><p>进一步带入公式求解，可以<strong>从<span class="math inline"><em>x</em><sub><em>t</em></sub></span>得到<span class="math inline"><em>x</em><sub><em>t</em> − 1</sub></span>的求解</strong>：</p><p><span class="math display">$$\begin{split}x_{t-1}=\sqrt{\alpha_{t-1}}\underbrace{\left(\frac{x_t-\sqrt{1-\alpha_t}\epsilon_\theta^{(t)}(x_t)}{\sqrt{\alpha_t}}\right)}_{\text{“predicted }\mathbf{x}_0\text{”}}+\underbrace{\sqrt{1-\alpha_{t-1}-\sigma_t^2}\cdot\epsilon_\theta^{(t)}(\mathbf{x}_t)}_{\text{“direction}\,\text{pointing}\,\text{to}\,\mathbf{x}_t\text{”}}+\underbrace{\sigma_t\epsilon_t}_{\text{random}\,\text{noise}}\end{split}$$</span></p><p>由上述公式可以发现，在给定<span class="math inline"><em>x</em><sub><em>t</em></sub></span>和<span class="math inline"><em>ϵ</em><sub><em>θ</em></sub></span>时，此时只有<span class="math inline"><em>σ</em><sub><em>t</em></sub></span>会导致随机性，构成不同的生成过程。我们不妨讨论一下这里<span class="math inline"><em>σ</em><sub><em>t</em></sub></span>的取值，原文考虑了2种情况：</p><ol type="1"><li>当<span class="math inline">$\sigma_t=\sqrt{(1-\alpha_{t-1})/(1-\alpha_t)}\sqrt{1-\alpha_t/\alpha_{t-1}}$</span>时，此时生成过程退化为<strong>DDPM</strong>（推导略）；</li><li>当<span class="math inline"><em>σ</em><sub><em>t</em></sub> = 0</span>时，此时该生成过程为确定性过程，我们将这个生成过程命名为<strong>DDIM</strong>。</li></ol><p>在实验过程中，使用参数<span class="math inline"><em>η</em></span>来进行控制随机性生成：</p><p><span class="math display">$$\begin{split}\sigma_{\tau_i}(\eta)=\eta\sqrt{(1-\alpha_{\tau_{i-1}})/(1-\alpha_{\tau_i})}\sqrt{1-\alpha_{\tau_i}/\alpha_{\tau_{i-1}}}\end{split}$$</span></p><h3 id="加速采样过程">加速采样过程</h3><p>由于只要满足<span class="math inline"><em>q</em>(<em>x</em><sub><em>t</em></sub>|<em>x</em><sub>0</sub>)</span>的形式不改变，我们就可以复用DDPM训好的模型，而DDPM在前向和反向过程中都需要迭代<span class="math inline"><em>T</em></span>步，我们不妨来考虑一下步长小于<span class="math inline"><em>T</em></span>的前向过程。</p><p>考虑一个从原始序列 <span class="math inline">[1, …, <em>T</em>]</span> 采样得到的长度为 <span class="math inline"><em>S</em></span> 的子序列 <span class="math inline">[<em>τ</em><sub>1</sub>, …, <em>τ</em><sub><em>S</em></sub>]</span> ，此时前向过程 <span class="math inline">[<em>x</em><sub><em>τ</em><sub>1</sub></sub>, …, <em>x</em><sub><em>τ</em><sub><em>S</em></sub></sub>]</span> 同样满足 <span class="math inline">$q({x}_{\tau_i}|{x}_0)=\mathcal{N}({x}_t;\sqrt{\alpha_{\tau_i}}{x}_0,(1-\alpha_{\tau_i}){I})$</span> 。此时对于采样过程便可以同样使用子序列进行采样达到加速采样的效果。</p><h1 id="源码">源码</h1><p>来源于LDM（<a href="https://github.com/CompVis/latent-diffusion">https://github.com/CompVis/latent-diffusion</a>），和原论文可能有出入，但是核心实现是一样的。</p><p>DDIM代码主要逻辑在<code>ldm\models\diffusion\ddim.py</code>中，在训练时使用DDPM进行训练，推理时使用DDIM进行采样来加速。在DDIM中使用<code>sample</code>函数作为接口进行采样，其中比较重要的参数包括<strong>DDIM采样步数<span class="math inline"><em>S</em></span>、随机化生成参数<span class="math inline"><em>η</em></span></strong>、条件信息<span class="math inline"><em>c</em></span>（适配条件生成）、无条件引导参数（适配Classifier-free guidance）等，我们其实主要关注前两个参数即可，这里实验设置为<span class="math inline"><em>S</em> = 200, <em>η</em> = 0</span>，即论文所说的确定性<strong>DDIM采样</strong>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sample</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">           S,</span></span><br><span class="line"><span class="params">           batch_size,</span></span><br><span class="line"><span class="params">           shape,</span></span><br><span class="line"><span class="params">           conditioning=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">           callback=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">           normals_sequence=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">           img_callback=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">           quantize_x0=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">           eta=<span class="number">0.</span>,</span></span><br><span class="line"><span class="params">           mask=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">           x0=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">           temperature=<span class="number">1.</span>,</span></span><br><span class="line"><span class="params">           noise_dropout=<span class="number">0.</span>,</span></span><br><span class="line"><span class="params">           score_corrector=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">           corrector_kwargs=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">           verbose=<span class="literal">True</span>,</span></span><br><span class="line"><span class="params">           x_T=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">           log_every_t=<span class="number">100</span>,</span></span><br><span class="line"><span class="params">           unconditional_guidance_scale=<span class="number">1.</span>,</span></span><br><span class="line"><span class="params">           unconditional_conditioning=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">           <span class="comment"># this has to come in the same format as the conditioning, # e.g. as encoded tokens, ...</span></span></span><br><span class="line"><span class="params">           **kwargs</span></span><br><span class="line"><span class="params">           </span>):</span><br><span class="line">    <span class="keyword">if</span> conditioning <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(conditioning, <span class="built_in">dict</span>):</span><br><span class="line">            cbs = conditioning[<span class="built_in">list</span>(conditioning.keys())[<span class="number">0</span>]].shape[<span class="number">0</span>]</span><br><span class="line">            <span class="keyword">if</span> cbs != batch_size:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;Warning: Got <span class="subst">&#123;cbs&#125;</span> conditionings but batch-size is <span class="subst">&#123;batch_size&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> conditioning.shape[<span class="number">0</span>] != batch_size:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;Warning: Got <span class="subst">&#123;conditioning.shape[<span class="number">0</span>]&#125;</span> conditionings but batch-size is <span class="subst">&#123;batch_size&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="variable language_">self</span>.make_schedule(ddim_num_steps=S, ddim_eta=eta, verbose=verbose)</span><br><span class="line">    <span class="comment"># sampling</span></span><br><span class="line">    C, H, W = shape</span><br><span class="line">    size = (batch_size, C, H, W)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;Data shape for DDIM sampling is <span class="subst">&#123;size&#125;</span>, eta <span class="subst">&#123;eta&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    samples, intermediates = <span class="variable language_">self</span>.ddim_sampling(conditioning, size,</span><br><span class="line">                                                callback=callback,</span><br><span class="line">                                                 img_callback=img_callback,</span><br><span class="line">                                                 quantize_denoised=quantize_x0,</span><br><span class="line">                                                 mask=mask, x0=x0,</span><br><span class="line">                                                 ddim_use_original_steps=<span class="literal">False</span>,</span><br><span class="line">                                                 noise_dropout=noise_dropout,</span><br><span class="line">                                                 temperature=temperature,</span><br><span class="line">                                                 score_corrector=score_corrector,</span><br><span class="line">                                                 corrector_kwargs=corrector_kwargs ,</span><br><span class="line">                                                 x_T=x_T,</span><br><span class="line">                                                 log_every_t=log_every_t,</span><br><span class="line">                                                 unconditional_guidance_scale=unconditional_guidance_scale,</span><br><span class="line">                                                unconditional_conditioning=unconditional_conditioning,</span><br><span class="line">                                                )</span><br><span class="line">    <span class="keyword">return</span> samples, intermediates</span><br></pre></td></tr></table></figure><h2 id="参数计算">参数计算</h2><p>在<code>make_schedule</code>函数中，定义了绝大多数需要用到的中间计算参数。</p><p>首先是DDIM所用到的时间步 <span class="math inline">[<em>τ</em><sub>1</sub>, …, <em>τ</em><sub><em>S</em></sub>]</span> ，可以以两种方式进行生成：<code>uniform</code>方法和<code>quad</code>方法。<code>uniform</code>方法用于生成均匀分布的时间步，时间间隔为<code>c = num_ddpm_timesteps // num_ddim_timesteps</code>，这也是常用的方式；而<code>quad</code>方法用于生成二次函数分布的非均匀时间步，首先在 <span class="math inline">$[0,\sqrt{0.8*T}]$</span> 中生成均匀分布的点，然后再对这些点进行平方。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">make_schedule</span>(<span class="params">self, ddim_num_steps, ddim_discretize=<span class="string">&quot;uniform&quot;</span>, ddim_eta=<span class="number">0.</span>, verbose=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="variable language_">self</span>.ddim_timesteps = make_ddim_timesteps(ddim_discr_method=ddim_discretize, num_ddim_timesteps=ddim_num_steps, num_ddpm_timesteps=<span class="variable language_">self</span>.ddpm_num_timesteps,verbose=verbose)</span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">make_ddim_timesteps</span>(<span class="params">ddim_discr_method, num_ddim_timesteps, num_ddpm_timesteps, verbose=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="keyword">if</span> ddim_discr_method == <span class="string">&#x27;uniform&#x27;</span>:</span><br><span class="line">        c = num_ddpm_timesteps // num_ddim_timesteps</span><br><span class="line">        ddim_timesteps = np.asarray(<span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">0</span>, num_ddpm_timesteps, c)))</span><br><span class="line">    <span class="keyword">elif</span> ddim_discr_method == <span class="string">&#x27;quad&#x27;</span>:</span><br><span class="line">        ddim_timesteps = ((np.linspace(<span class="number">0</span>, np.sqrt(num_ddpm_timesteps * <span class="number">.8</span>), num_ddim_timesteps)) ** <span class="number">2</span>).astype(<span class="built_in">int</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError(<span class="string">f&#x27;There is no ddim discretization method called &quot;<span class="subst">&#123;ddim_discr_method&#125;</span>&quot;&#x27;</span>)</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> steps_out</span><br></pre></td></tr></table></figure><p>其次主要是 <span class="math inline"><em>α</em><sub><em>τ</em><sub><em>i</em></sub></sub></span> 和 <span class="math inline"><em>σ</em><sub><em>τ</em><sub><em>i</em></sub></sub></span> 的计算，<span class="math inline"><em>α</em><sub><em>τ</em><sub><em>i</em></sub></sub></span>（<code>ddim_alphas</code>） 相当于是在原来DDPM的 <span class="math inline"><em>α</em><sub><em>t</em></sub></span> 上在新的子序列时间步上采样得到，这里为了方便后续计算也提前定义好了 <span class="math inline"><em>α</em><sub><em>τ</em><sub><em>i</em> − 1</sub></sub></span>（<code>ddim_alphas_prev</code>） 和 <span class="math inline">$\sqrt{1-\alpha_{\tau_i}}$</span>（<code>ddim_sqrt_one_minus_alphas</code>）， <span class="math inline"><em>σ</em><sub><em>τ</em><sub><em>i</em></sub></sub></span>（<code>ddim_sigmas</code>）的计算则是直接使用公式 <span class="math inline">$\sigma_{\tau_i}(\eta)=\eta\sqrt{(1-\alpha_{\tau_{i-1}})/(1-\alpha_{\tau_i})}\sqrt{1-\alpha_{\tau_i}/\alpha_{\tau_{i-1}}}$</span> 得到。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">make_schedule</span>(<span class="params">self, ddim_num_steps, ddim_discretize=<span class="string">&quot;uniform&quot;</span>, ddim_eta=<span class="number">0.</span>, verbose=<span class="literal">True</span></span>):</span><br><span class="line">    ...</span><br><span class="line">    ddim_sigmas, ddim_alphas, ddim_alphas_prev = make_ddim_sampling_parameters(alphacums=alphas_cumprod.cpu(), ddim_timesteps=<span class="variable language_">self</span>.ddim_timesteps, eta=ddim_eta,verbose=verbose)</span><br><span class="line">    <span class="variable language_">self</span>.register_buffer(<span class="string">&#x27;ddim_sigmas&#x27;</span>, ddim_sigmas)</span><br><span class="line">    <span class="variable language_">self</span>.register_buffer(<span class="string">&#x27;ddim_alphas&#x27;</span>, ddim_alphas)</span><br><span class="line">    <span class="variable language_">self</span>.register_buffer(<span class="string">&#x27;ddim_alphas_prev&#x27;</span>, ddim_alphas_prev)</span><br><span class="line">    <span class="variable language_">self</span>.register_buffer(<span class="string">&#x27;ddim_sqrt_one_minus_alphas&#x27;</span>, np.sqrt(<span class="number">1.</span> - ddim_alphas))</span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">make_ddim_sampling_parameters</span>(<span class="params">alphacums, ddim_timesteps, eta, verbose=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="comment"># alpha_t的计算</span></span><br><span class="line">    alphas = alphacums[ddim_timesteps]</span><br><span class="line">    <span class="comment"># alpha_&#123;t-1&#125;的计算</span></span><br><span class="line">    alphas_prev = np.asarray([alphacums[<span class="number">0</span>]] + alphacums[ddim_timesteps[:-<span class="number">1</span>]].tolist())</span><br><span class="line">    <span class="comment"># sigma_t的计算</span></span><br><span class="line">    sigmas = eta * np.sqrt((<span class="number">1</span> - alphas_prev) / (<span class="number">1</span> - alphas) * (<span class="number">1</span> - alphas / alphas_prev))</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> sigmas, alphas, alphas_prev</span><br></pre></td></tr></table></figure><h2 id="加速采样">加速采样</h2><p>已经用DDPM训好了噪声预测模型，并且也已经计算好了中间的参数，其实剩下的部分直接套公式就可以了：</p><p><span class="math display">$$\begin{split}x_{t-1}=\sqrt{\alpha_{t-1}}{\left(\frac{x_t-\sqrt{1-\alpha_t}\epsilon_\theta^{(t)}(x_t)}{\sqrt{\alpha_t}}\right)}+{\sqrt{1-\alpha_{t-1}-\sigma_t^2}\cdot\epsilon_\theta^{(t)}(\mathbf{x}_t)}+{\sigma_t\epsilon_t}\end{split}$$</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">p_sample_ddim</span>(<span class="params">self, x, c, t, ...</span>):</span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 得到模型预测噪声(Classifier-free Guidance)</span></span><br><span class="line">    <span class="keyword">if</span> unconditional_conditioning <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> unconditional_guidance_scale == <span class="number">1.</span>:</span><br><span class="line">        e_t = <span class="variable language_">self</span>.model.apply_model(x, t, c)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        x_in = torch.cat([x] * <span class="number">2</span>)</span><br><span class="line">        t_in = torch.cat([t] * <span class="number">2</span>)</span><br><span class="line">        c_in = torch.cat([unconditional_conditioning, c])</span><br><span class="line">        e_t_uncond, e_t = <span class="variable language_">self</span>.model.apply_model(x_in, t_in, c_in).chunk(<span class="number">2</span>)</span><br><span class="line">        e_t = e_t_uncond + unconditional_guidance_scale * (e_t - e_t_uncond)</span><br><span class="line"></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 预测的x_0(即上述公式的第一部分)</span></span><br><span class="line">    pred_x0 = (x - sqrt_one_minus_at * e_t) / a_t.sqrt()</span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    <span class="comment"># direction pointing to x_t(即上述公式的第二部分)</span></span><br><span class="line">    dir_xt = (<span class="number">1.</span> - a_prev - sigma_t**<span class="number">2</span>).sqrt() * e_t</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 不确定性部分（即上述公式的第三部分），由于实验设置eta=0，这部分相当于确定性采样</span></span><br><span class="line">    noise = sigma_t * noise_like(x.shape, device, repeat_noise) * temperature</span><br><span class="line">    <span class="keyword">if</span> noise_dropout &gt; <span class="number">0.</span>:</span><br><span class="line">        noise = torch.nn.functional.dropout(noise, p=noise_dropout)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 求解x_&#123;t-1&#125;</span></span><br><span class="line">    x_prev = a_prev.sqrt() * pred_x0 + dir_xt + noise</span><br><span class="line">    <span class="keyword">return</span> x_prev, pred_x0</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AIGC </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Davinci学习之初级学习笔记</title>
      <link href="/2025/04/16/photograph/Davinci/%E5%88%9D%E7%BA%A7%E6%95%99%E7%A8%8B/"/>
      <url>/2025/04/16/photograph/Davinci/%E5%88%9D%E7%BA%A7%E6%95%99%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<h1 id="davinci入门学习">Davinci入门学习</h1><p>主要参考影视飓风的达芬奇教程，这里是leo的学习笔记，我的主要需求其实是调色，其他的部分应该只是大致浏览一下。</p><h2 id="入门简介">入门简介</h2><p>创作视频的一般流程：</p><ul><li>导入素材进行分类：<code>媒体</code> 面板中进行</li><li>进行视频的粗剪、然后进行精细剪辑：<code>快编</code> 及 <code>剪辑</code> 面板中进行</li><li>效果制作、调色、音频调整：分别对应 <code>Fusion</code>、<code>调色</code>、<code>Fairlight</code> 面板</li><li>导出视频：<code>交付</code> 面板</li></ul><h3 id="媒体面板">媒体面板</h3><p>通常流程是我们通过<strong>媒体浏览器</strong>找到素材所在文件夹，在<strong>监视器</strong>中回看，选择需要的素材拖拽到<strong>素材池</strong>中，等待剪辑。</p><p>达芬奇素材池进行项目管理有一个很大的弊端：一旦导入素材，就无法更改项目的帧率，容易造成后期视频的跳帧或卡顿（Davinci 17后可以更改了）。</p><blockquote><p>解决方法：需要在前期导入素材之前，进入 文件-项目设置-设置合适的帧率 配置项目</p></blockquote><h3 id="快编和剪辑面板">快编和剪辑面板</h3><p>在 <code>剪辑</code> 面板中进行剪辑的一般流程：</p><ul><li>媒体池进行筛选</li><li>在素材监视器中使用 <code>I</code> 键和 <code>O</code> 键打下出入点，选出有用的素材拖入时间线进行组装</li><li>通过特效库面板选择对应的音频或视频效果添加转场</li></ul><h3 id="fusion调色fairlight面板">Fusion、调色、Fairlight面板</h3><p>不推荐使用 <code>Fusion</code> 制作复杂的效果及动画，节点式工作逻辑复杂。<code>调色</code> 面板很强大（这其实也是我的主要需求），<code>Fairlight</code> 面板则相当于音频的调色面板。</p><h3 id="交付面板">交付面板</h3><p>通常传播互联网选择 <code>mp4</code> 封装，<code>h264</code> 编码，分辨率和帧率则根据需求调整。之后添加到渲染序列进行渲染即可。</p><h2 id="剪辑">剪辑</h2><p>一些Tips：</p><ul><li>建议在 <code>媒体</code> 面板中一次性将所有素材放入素材池，然后再进行分类，这样剪辑时不用因为缺少素材又重新添加。</li><li><code>I</code> 键和 <code>O</code> 键来打下视频的出入点，可以单独点击画面或音频拖入画面或音频轨道。</li><li>使用 <code>alt</code>+鼠标滚轮 来进行时间线的缩放</li></ul><h3 id="更多工具栏">更多工具栏</h3><table><thead><tr class="header"><th style="text-align: center;">工具</th><th style="text-align: center;">快捷键</th><th style="text-align: center;">描述</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">鼠标模式</td><td style="text-align: center;">-</td><td style="text-align: center;">快速编辑</td></tr><tr class="even"><td style="text-align: center;">修建编辑模式</td><td style="text-align: center;"><code>T</code></td><td style="text-align: center;">波纹删除（适合精剪）</td></tr><tr class="odd"><td style="text-align: center;">动态修剪模式</td><td style="text-align: center;">-</td><td style="text-align: center;">不经常用</td></tr><tr class="even"><td style="text-align: center;">剃刀工具</td><td style="text-align: center;"><code>B</code></td><td style="text-align: center;">素材切刀</td></tr><tr class="odd"><td style="text-align: center;">素材导入方式</td><td style="text-align: center;">-</td><td style="text-align: center;">分为插入、覆盖以及替换</td></tr></tbody></table><h2 id="添加效果与关键帧">添加效果与关键帧</h2><ul><li>添加转场：打开特效库，拖拽一个转场效果到素材拼接处，可以在检查器中精细调节转场效果</li><li>片段调整：点击一个素材片段，打开检查器，进行缩放、位置、变速等，按住 <code>alt</code> 再进行调整能更精细地控制</li><li>添加关键帧：打开检查器，起始帧选中需要变换的参数右侧红色菱形，接着选中结束帧改变参数即可自动加入关键帧。可以打开非线性变换选项，这样比线性的参数变化更有质感（缓动效果）</li><li>批处理：只需要 <code>ctrl+c</code> 复制某个已处理好的片段，然后选中需要批处理的其他片段进行粘贴即可</li></ul><blockquote><p>插件推荐：</p><p><a href="https://www.maxon.net/en/red-giant">红巨星宇宙</a>：很多效果和滤镜预设，一年订阅在1400元</p><p><a href="https://www.filmconvert.com/">FilmConvert</a>：调色插件，有很多相机预设</p></blockquote><h2 id="快编界面">快编界面</h2><h3 id="预备知识之代理文件的生成">预备知识之代理文件的生成</h3><p>媒体优化：减少电脑负荷</p><p><strong>方案一</strong>（适用于比较简单的剪辑）：播放–代理模式–选择合适的分辨率</p><p><strong>方案二</strong>（适用于复杂的剪辑）：媒体池选中素材，右键生成优化媒体文件</p><p>设置在 文件–项目设置–主设置–优化的媒体和渲染缓存</p><blockquote><p>常用设置：4K视频 选择 分辨率–二分之一 编码–DNXHR SQ</p></blockquote><h3 id="预备知识之智能媒体夹及双时间线剪辑">预备知识之智能媒体夹及双时间线剪辑</h3><p>Aroll 主要内容 Broll 辅助时间线</p>]]></content>
      
      
      <categories>
          
          <category> 摄影 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Davinci </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Lightroom入门笔记</title>
      <link href="/2025/04/16/photograph/Lightroom/%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/"/>
      <url>/2025/04/16/photograph/Lightroom/%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<p>教程来自摄影师泰罗</p><h1 id="lr系统教程">LR系统教程</h1><h2 id="第一期照片管理筛选与批量处理">第一期、照片管理、筛选与批量处理</h2><p>PS是重新分配空间来处理照片，而LR是建立软连接到导入照片路径。</p><ul><li>注意在导入时有几个选项：包括<strong>复制、移动、添加</strong></li><li>添加是不会改变数据存储路径的，只是建立了一个映射；</li><li>而移动则是将照片从硬盘中移动到本地。</li></ul><h2 id="第二期直方图与曝光补偿">第二期、直方图与曝光补偿</h2><h3 id="直方图的原理">直方图的原理</h3><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/image2.png" width="50%" /></p><p>直方图是关于亮度的统计报告图，横坐标是0-255的亮度级别，越右越亮；纵坐标表示该亮度级别下像素数量的多少。</p><h2 id="第三期曲线">第三期、曲线</h2><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/image3.png" width="30%" /></p><p>通过映射重塑直方图</p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/image4.png" width="50%" /></p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/image5.png" width="50%" /></p><p>胶片灰</p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/image6.png" width="30%" /></p><p><strong>曲线调色原理：</strong> 分别在RGB三种模式下调色，对应模式下调高曝光会加重对应色调、反之加重对应补色色调</p><p>曲线调色案例：</p><ul><li>日系小清新：RGB稍过曝（变亮）、G阴影稍高（阴影偏绿）、B阴影稍低（阴影偏暖）、R整体调低（整张图偏青）</li><li>暖色调电影感：RGB稍过曝、R整体偏低、B整体调低</li></ul><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/image7.png" width="50%" /></p><h2 id="第四期色温色调以及分离色调">第四期、色温色调以及分离色调</h2><p>调曲线过于复杂，不如关注眼前的工具</p><pre><code>城市夜景人文调色思路之一：降低色温、降低色调、提亮阴影、二次构图（2.35：1）</code></pre><p>色温色调的缺陷：过于整体</p><p>分离色调：分别为高光和阴影分别调色相</p><pre><code>一个有意思的调色思路：阴影偏黄绿、高光偏冷青</code></pre><p><strong>曲线工具 VS 分离色调</strong></p><p>曲线工具精确选择区域不精确赋予色彩；分离色调不精确选择区域精确选择色彩</p><h2 id="第五期hsl调色">第五期、HSL调色</h2><p>更精细的局部调色</p><p>一个调色思路是各种色相都向对应高光或阴影色调调整，以达成整体和谐的效果</p><p>用于学习大佬调色思路的网站：<a href="https://anvaka.github.io/pixchart/?d=4&amp;ignore=&amp;link=&amp;groupBy=hsl.h">https://anvaka.github.io/pixchart/?d=4&amp;ignore=&amp;link=&amp;groupBy=hsl.h</a></p><h2 id="第六期锐化清晰度等局部调整工具">第六期、锐化、清晰度等局部调整工具</h2><p>锐化的本质：描边 + 数量：描边越明显 + 半径：描边的粗细程度（一般保持默认） + 细节：描边的黑白反差（一般保持默认） + 蒙版：使用alt键可以查看，保护纯净的背景不被锐化</p><p>清晰度：没有描边、加大色块交界处的差异（黑色更黑、白色更白）</p><p>其实照片想要表现出清晰与高质，重点不在于锐化和清晰度，也不在于像素的多少，而在于<strong>信息的表达</strong></p><p>less is more（删繁就简）、秩序（画面元素达到统一）、色彩对比虚实对比明亮对比</p><p>在<code>蒙版</code>中将主体部分提亮、将背景部分变暗</p><h2 id="第七期提升摄影后期水准">第七期、提升摄影后期水准</h2><p>降噪：高ISO会造成明度噪点和彩色噪点这两种，分别使用明亮度和颜色按钮调节，一般明亮度不能给太高、而颜色可以给高</p><p>镜头校正：消除镜头带来的偏移以及曲变</p><p>导出：质量可以选择60（默认）、如果要发朋友圈调整图像大小以适合短边、像素选择1080（可以避免微信的二次压缩）</p><p><strong>平面构成、立体构成、色彩构成</strong></p><p>简单矫正<span class="math inline">→</span>局部修饰<span class="math inline">→</span>艺术加工</p><h2 id="第八期让图片更干净">第八期、让图片更干净</h2><ul><li>压低曲线的高光、提高曝光值（关键步骤）</li><li>压缩曲线的阴影</li><li>提高橙色，追加一些饱和度，红色追加饱和度（针对图片中有人像的情况，让皮肤和嘴唇更好看）</li><li>绿色的色相往右滑动，明度和饱和度提高（让树木更精神）</li><li>蓝色色相向右滑动</li><li>曲线红色通道稍微下压，使整张图偏青</li></ul><h2 id="第九期油画感教程">第九期、油画感教程</h2><blockquote><p>油画风特征分析： + 饱和度高、对比度中 + 色系统一 + 细节模糊、整体清晰</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/image8.png" width="50%" /></p>]]></content>
      
      
      <categories>
          
          <category> 摄影 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Lightroom </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>调色不重要？调色很重要</title>
      <link href="/2025/04/16/photograph/Lightroom/%E4%B8%80%E4%BA%9B%E4%B8%AA%E4%BA%BA%E8%B0%83%E8%89%B2%E6%80%9D%E8%B7%AF/"/>
      <url>/2025/04/16/photograph/Lightroom/%E4%B8%80%E4%BA%9B%E4%B8%AA%E4%BA%BA%E8%B0%83%E8%89%B2%E6%80%9D%E8%B7%AF/</url>
      
        <content type="html"><![CDATA[<h1 id="总结贴">总结贴</h1><p>示例图片有点糊…为了加载速度只能压缩画质了</p><h2 id="日系蓝色胶片">日系蓝色胶片</h2><h3 id="基本">基本</h3><p>色温 - 色调 +</p><p>曝光适度 对比 - 白色色阶 - 黑色色阶 +</p><h3 id="曲线">曲线</h3><p>稍微下拉，亮部降暗部升</p><p>红色通道暗部下拉</p><h3 id="混色器">混色器</h3><p>绿色相 - 蓝色相 +</p><p>暖色饱和度 + 冷色饱和度（尤其蓝色） -</p><p>暖色明度 + 冷色明度（尤其蓝色） -</p><h3 id="颜色分级">颜色分级</h3><p>阴影偏青，高光偏黄绿</p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/lr1.png" width="50%" /></p><h2 id="亮调春花">亮调春花</h2><h3 id="基本-1">基本</h3><p>色温 + 色调 -</p><p>曝光适度 对比 - 高光 - 阴影 + 白色色阶 - 黑色色阶 +</p><p>纹理 - 清晰度 + 去朦胧 -</p><p>鲜艳度 + 饱和度 -</p><h3 id="曲线-1">曲线</h3><p>稍微下拉，暗部升</p><h3 id="混色器-1">混色器</h3><p>橙绿色相 - 红蓝色相 +</p><p>暖色及蓝色饱和度 + 绿色饱和度 -</p><p>冷色明度 -</p><h3 id="颜色分级-1">颜色分级</h3><p>中间色调及阴影偏青，高光偏黄绿</p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/lr2.png" width="50%" /></p><h2 id="暗调春花">暗调春花</h2><h3 id="基本-2">基本</h3><p>色温 - 色调 -</p><p>曝光适度 对比 - 高光 - 阴影 + 白色色阶 - 黑色色阶 +</p><p>纹理 + 清晰度 - 去朦胧 +</p><p>鲜艳度 + 饱和度 -</p><h3 id="曲线-2">曲线</h3><p>稍微下拉，暗部升</p><h3 id="混色器-2">混色器</h3><p>蓝橙色相 - 红绿色相 +</p><p>暖色饱和度 + 绿色饱和度 -</p><p>冷色明度 -</p><h3 id="颜色分级-2">颜色分级</h3><p>阴影偏青，中间色调偏黄绿，高光偏橙黄</p><h3 id="蒙版">蒙版</h3><p>径向蒙版模拟打光（曝光+色温+），反向蒙版加深对比</p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/lr3.png" width="50%" /></p>]]></content>
      
      
      <categories>
          
          <category> 摄影 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Lightroom </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PS入门笔记</title>
      <link href="/2025/04/16/photograph/Photoshop/%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/"/>
      <url>/2025/04/16/photograph/Photoshop/%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> 摄影 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Photoshop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DDPM总结</title>
      <link href="/2025/04/15/AIGC/DDPM/"/>
      <url>/2025/04/15/AIGC/DDPM/</url>
      
        <content type="html"><![CDATA[<p><strong>系列文章：</strong></p><ol class="series-items"><li><a href="/2025/04/15/AIGC/DDPM/" title="DDPM总结">DDPM总结</a></li><li><a href="/2025/04/17/AIGC/DDIM/" title="DDIM总结">DDIM总结</a></li><li><a href="/2025/05/08/AIGC/DIT/" title="DiT的细节">DiT的细节</a></li><li><a href="/2025/05/08/AIGC/iDDPM/" title="iDDPM总结">iDDPM总结</a></li></ol><p>这里分析的源码并非来自DDPM原论文，而是LDM（<a href="https://github.com/CompVis/latent-diffusion">https://github.com/CompVis/latent-diffusion</a>）中的DDPM部分，因此和原论文可能有些出入。</p><h1 id="回顾">回顾</h1><p>首先在分析源码之前，可以先回顾一下DDPM的关键步骤。</p><p><strong>前向过程（Forward Process）</strong></p><p>对输入图像 <span class="math inline"><em>x</em><sub>0</sub></span> 按照预定义的加噪操作，逐步加入高斯噪声，直到最终接近纯高斯噪声 <span class="math inline"><em>x</em><sub><em>T</em></sub></span>。</p><p><span class="math display">$$\begin{split}    q(x_t \mid x_{t-1})= \mathcal{N}\bigl(x_t;\;\sqrt{\alpha_t}\,x_{t-1},\;\beta_t I\bigr).\end{split}$$</span></p><p>对于单步加噪，代入连续求解可以得到上式的闭式形式，其中<span class="math inline">$\alpha_t = 1-\beta_t,\quad \bar\alpha_t = \prod_{i=1}^t \alpha_i$</span>。</p><p><span class="math display">$$\begin{split}    q(x_t \mid x_0) = \mathcal{N}\bigl(x_t;\;\sqrt{\bar\alpha_t}\,x_0,\;(1-\bar\alpha_t)\,I\bigr).\end{split}$$</span></p><p><strong>去噪过程（Reverse Process）</strong></p><p><span class="math display">$$\begin{split}    q(x_{t-1} \mid x_t, x_0)=\mathcal{N}(x_t;\frac{1}{\sqrt{\alpha_t}}(x_t-\frac{1-\alpha_t}{\sqrt{1-\bar\alpha_t}}\epsilon_t), \beta_t\cdot\frac{1-\bar\alpha_{t-1}}{1-\bar\alpha_t}I).\end{split}$$</span></p><p><strong>损失函数</strong></p><p>主要是对噪声预测的MSE损失：</p><p><span class="math display">$$\begin{split}    \mathcal L=\mathbb{E}_{t,x_0,\epsilon}\left[\|\epsilon-\varepsilon_\theta(x_t,t)\|^2\right].\end{split}$$</span></p><h1 id="源码分析">源码分析</h1><p>主要代码逻辑定义在<code>ldm\models\diffusion\ddpm.py</code>中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, *args, **kwargs</span>):</span><br><span class="line">    <span class="comment"># b, c, h, w, device, img_size, = *x.shape, x.device, self.image_size</span></span><br><span class="line">    <span class="comment"># assert h == img_size and w == img_size, f&#x27;height and width of image must be &#123;img_size&#125;&#x27;</span></span><br><span class="line">    t = torch.randint(</span><br><span class="line">        <span class="number">0</span>, <span class="variable language_">self</span>.num_timesteps, (x.shape[<span class="number">0</span>],), device=<span class="variable language_">self</span>.device</span><br><span class="line">    ).long()</span><br><span class="line">    <span class="keyword">return</span> <span class="variable language_">self</span>.p_losses(x, t, *args, **kwargs)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">p_losses</span>(<span class="params">self, x_start, t, noise=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="comment"># --------------------</span></span><br><span class="line">    noise = default(noise, <span class="keyword">lambda</span>: torch.randn_like(x_start))</span><br><span class="line">    x_noisy = <span class="variable language_">self</span>.q_sample(x_start=x_start, t=t, noise=noise)</span><br><span class="line">    model_out = <span class="variable language_">self</span>.model(x_noisy, t)</span><br><span class="line">    <span class="comment"># --------------------</span></span><br><span class="line"></span><br><span class="line">    loss_dict = &#123;&#125;</span><br><span class="line">    <span class="keyword">if</span> <span class="variable language_">self</span>.parameterization == <span class="string">&quot;eps&quot;</span>:</span><br><span class="line">        target = noise</span><br><span class="line">    <span class="keyword">elif</span> <span class="variable language_">self</span>.parameterization == <span class="string">&quot;x0&quot;</span>:</span><br><span class="line">        target = x_start</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError(</span><br><span class="line">            <span class="string">f&quot;Paramterization <span class="subst">&#123;self.parameterization&#125;</span> not yet supported&quot;</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    loss = <span class="variable language_">self</span>.get_loss(model_out, target, mean=<span class="literal">False</span>).mean(dim=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">    log_prefix = <span class="string">&#x27;train&#x27;</span> <span class="keyword">if</span> <span class="variable language_">self</span>.training <span class="keyword">else</span> <span class="string">&#x27;val&#x27;</span></span><br><span class="line"></span><br><span class="line">    loss_dict.update(&#123;<span class="string">f&#x27;<span class="subst">&#123;log_prefix&#125;</span>/loss_simple&#x27;</span>: loss.mean()&#125;)</span><br><span class="line">    loss_simple = loss.mean() * <span class="variable language_">self</span>.l_simple_weight</span><br><span class="line"></span><br><span class="line">    loss_vlb = (<span class="variable language_">self</span>.lvlb_weights[t] * loss).mean()</span><br><span class="line">    loss_dict.update(&#123;<span class="string">f&#x27;<span class="subst">&#123;log_prefix&#125;</span>/loss_vlb&#x27;</span>: loss_vlb&#125;)</span><br><span class="line"></span><br><span class="line">    loss = loss_simple + <span class="variable language_">self</span>.original_elbo_weight * loss_vlb</span><br><span class="line"></span><br><span class="line">    loss_dict.update(&#123;<span class="string">f&#x27;<span class="subst">&#123;log_prefix&#125;</span>/loss&#x27;</span>: loss&#125;)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss, loss_dict</span><br></pre></td></tr></table></figure><p>根据这部分代码可以理解，每次训练都是<strong>对每个batch随机从 <span class="math inline">[0, <em>T</em>)</span> 中选择一个时刻进行训练</strong>，在该时间步内进行单独训练。而重点其实就在<code>p_losses</code>函数中的前三步，后续步骤主要是在计算损失。首先初始化高斯噪音<code>noise</code>，接着通过前向过程得到 <span class="math inline"><em>x</em><sub><em>t</em></sub></span> ，然后经过模型预测得到预测噪音 <span class="math inline"><em>ϵ</em><sub><em>θ</em></sub></span> 或是预测输出图像 <span class="math inline"><em>x</em><sub>0</sub></span> 。在这里 <code>parameterization</code> 被设置为 <code>eps</code> ，所以这里与原论文保持一致是<strong>预测噪声</strong>而不是DPM中的预测 <span class="math inline"><em>x</em><sub>0</sub></span> 。</p><p>还有这部分损失的计算还加上了<code>loss_vlb</code>，这其实是后续iDDPM的工作，可以先不用管。</p><h2 id="前向过程">前向过程</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">q_sample</span>(<span class="params">self, x_start, t, noise=<span class="literal">None</span></span>):</span><br><span class="line">    noise = default(noise, <span class="keyword">lambda</span>: torch.randn_like(x_start))</span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">        extract_into_tensor(<span class="variable language_">self</span>.sqrt_alphas_cumprod, t, x_start.shape) * x_start</span><br><span class="line">        + extract_into_tensor(<span class="variable language_">self</span>.sqrt_one_minus_alphas_cumprod, t, x_start.shape)</span><br><span class="line">        * noise</span><br><span class="line">    )</span><br></pre></td></tr></table></figure><p>很明显，这里的加噪部分和原论文的一步加噪的公式保持一致，其中的中间参数在源码中<code>register_schedule</code> 函数中都有提前定义好：</p><p><span class="math display">$$\begin{split}    q(x_t \mid x_0) = \mathcal{N}\bigl(x_t;\;\sqrt{\bar\alpha_t}\,x_0,\;(1-\bar\alpha_t)\,I\bigr).\end{split}$$</span></p><h2 id="预测噪声">预测噪声</h2><p>原论文预测噪声是直接用UNet进行预测的，源码里也是保持一致，这里的<code>model</code>是<code>UNetModel</code>类的一个实例。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">model_out = <span class="variable language_">self</span>.model(x_noisy, t)</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>对于<code>UNetModel</code>类的前向过程，其实主要包含几个部分：对于时间步<code>t</code>的embedding、其他条件的处理（原论文里不包含，该部分源码来自于LDM）、得到最终预测的噪声或 <span class="math inline"><em>x</em><sub>0</sub></span>。时间步的信息则是通过embedding层后通过attention嵌入到特征图中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">UNetModel</span>(nn.Module):</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, timesteps=<span class="literal">None</span>, context=<span class="literal">None</span>, y=<span class="literal">None</span>,**kwargs</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Apply the model to an input batch.</span></span><br><span class="line"><span class="string">        :param x: an [N x C x ...] Tensor of inputs.</span></span><br><span class="line"><span class="string">        :param timesteps: a 1-D batch of timesteps.</span></span><br><span class="line"><span class="string">        :param context: conditioning plugged in via crossattn</span></span><br><span class="line"><span class="string">        :param y: an [N] Tensor of labels, if class-conditional.</span></span><br><span class="line"><span class="string">        :return: an [N x C x ...] Tensor of outputs.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">assert</span> (y <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>) == (</span><br><span class="line">            <span class="variable language_">self</span>.num_classes <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line">        ), <span class="string">&quot;must specify y if and only if the model is class-conditional&quot;</span></span><br><span class="line">        hs = []</span><br><span class="line">        t_emb = timestep_embedding(timesteps, <span class="variable language_">self</span>.model_channels, repeat_only=<span class="literal">False</span>)</span><br><span class="line">        emb = <span class="variable language_">self</span>.time_embed(t_emb)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.num_classes <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">assert</span> y.shape == (x.shape[<span class="number">0</span>],)</span><br><span class="line">            emb = emb + <span class="variable language_">self</span>.label_emb(y)</span><br><span class="line"></span><br><span class="line">        h = x.<span class="built_in">type</span>(<span class="variable language_">self</span>.dtype)</span><br><span class="line">        <span class="keyword">for</span> module <span class="keyword">in</span> <span class="variable language_">self</span>.input_blocks:</span><br><span class="line">            h = module(h, emb, context)</span><br><span class="line">            hs.append(h)</span><br><span class="line">        h = <span class="variable language_">self</span>.middle_block(h, emb, context)</span><br><span class="line">        <span class="keyword">for</span> module <span class="keyword">in</span> <span class="variable language_">self</span>.output_blocks:</span><br><span class="line">            h = th.cat([h, hs.pop()], dim=<span class="number">1</span>)</span><br><span class="line">            h = module(h, emb, context)</span><br><span class="line">        h = h.<span class="built_in">type</span>(x.dtype)</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.predict_codebook_ids:</span><br><span class="line">            <span class="keyword">return</span> <span class="variable language_">self</span>.id_predictor(h)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="variable language_">self</span>.out(h)</span><br></pre></td></tr></table></figure><h2 id="采样过程">采样过程</h2><p>采样过程在DDPM中还是逐步逆向去噪的，一共迭代<code>t</code>次，每次单步执行<code>p_sample</code>函数进行单步去噪。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">p_sample_loop</span>(<span class="params">self, shape, return_intermediates=<span class="literal">False</span></span>):</span><br><span class="line">    device = <span class="variable language_">self</span>.betas.device</span><br><span class="line">    b = shape[<span class="number">0</span>]</span><br><span class="line">    img = torch.randn(shape, device=device)</span><br><span class="line">    intermediates = [img]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> tqdm(</span><br><span class="line">        <span class="built_in">reversed</span>(<span class="built_in">range</span>(<span class="number">0</span>, <span class="variable language_">self</span>.num_timesteps)),</span><br><span class="line">        desc=<span class="string">&#x27;Sampling t&#x27;</span>,</span><br><span class="line">        total=<span class="variable language_">self</span>.num_timesteps,</span><br><span class="line">    ):</span><br><span class="line">        img = <span class="variable language_">self</span>.p_sample(</span><br><span class="line">            img,</span><br><span class="line">            torch.full((b,), i, device=device, dtype=torch.long),</span><br><span class="line">            clip_denoised=<span class="variable language_">self</span>.clip_denoised,</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">if</span> i % <span class="variable language_">self</span>.log_every_t == <span class="number">0</span> <span class="keyword">or</span> i == <span class="variable language_">self</span>.num_timesteps - <span class="number">1</span>:</span><br><span class="line">            intermediates.append(img)</span><br><span class="line">    <span class="keyword">if</span> return_intermediates:</span><br><span class="line">        <span class="keyword">return</span> img, intermediates</span><br><span class="line">    <span class="keyword">return</span> img</span><br><span class="line"></span><br><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sample</span>(<span class="params">self, batch_size=<span class="number">16</span>, return_intermediates=<span class="literal">False</span></span>):</span><br><span class="line">    image_size = <span class="variable language_">self</span>.image_size</span><br><span class="line">    channels = <span class="variable language_">self</span>.channels</span><br><span class="line">    <span class="keyword">return</span> <span class="variable language_">self</span>.p_sample_loop(</span><br><span class="line">        (batch_size, channels, image_size, image_size),</span><br><span class="line">        return_intermediates=return_intermediates,</span><br><span class="line">    )</span><br></pre></td></tr></table></figure><p>下面我们分析单步去噪过程，回顾一下之前说的单步去噪：</p><p><span class="math display">$$\begin{split}    q(x_{t-1} \mid x_t, x_0)=\mathcal{N}(x_t;\frac{1}{\sqrt{\alpha_t}}(x_t-\frac{1-\alpha_t}{\sqrt{1-\bar\alpha_t}}\epsilon_t), \beta_t\cdot\frac{1-\bar\alpha_{t-1}}{1-\bar\alpha_t}I).\end{split}$$</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">p_sample</span>(<span class="params">self, x, t, clip_denoised=<span class="literal">True</span>, repeat_noise=<span class="literal">False</span></span>):</span><br><span class="line">    b, *_, device = *x.shape, x.device</span><br><span class="line">    model_mean, _, model_log_variance = <span class="variable language_">self</span>.p_mean_variance(</span><br><span class="line">        x=x, t=t, clip_denoised=clip_denoised</span><br><span class="line">    )</span><br><span class="line">    noise = noise_like(x.shape, device, repeat_noise)</span><br><span class="line">    <span class="comment"># no noise when t == 0</span></span><br><span class="line">    nonzero_mask = (<span class="number">1</span> - (t == <span class="number">0</span>).<span class="built_in">float</span>()).reshape(b, *((<span class="number">1</span>,) * (<span class="built_in">len</span>(x.shape) - <span class="number">1</span>)))</span><br><span class="line">    <span class="keyword">return</span> model_mean + nonzero_mask * (<span class="number">0.5</span> * model_log_variance).exp() * noise</span><br></pre></td></tr></table></figure><p>这里相当于是通过预测噪声 <span class="math inline"><em>ϵ</em><sub><em>θ</em></sub></span> 和 <span class="math inline"><em>x</em><sub><em>t</em></sub></span> 来预测 <span class="math inline"><em>x</em><sub><em>t</em> − 1</sub></span>，但是在代码实现上考虑到DPM每步直接预测 <span class="math inline"><em>x</em><sub>0</sub></span> 的做法，为了方便统一处理，这里在实现时即使是预测噪声 <span class="math inline"><em>ϵ</em><sub><em>θ</em></sub></span> ，也会进一步得到预测 <span class="math inline">$\hat{x_0}$</span> ，然后通过后验公式计算均值。即下面代码所展示的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">p_mean_variance</span>(<span class="params">self, x, t, clip_denoised: <span class="built_in">bool</span></span>):</span><br><span class="line">    model_out = <span class="variable language_">self</span>.model(x, t)</span><br><span class="line">    <span class="keyword">if</span> <span class="variable language_">self</span>.parameterization == <span class="string">&quot;eps&quot;</span>:</span><br><span class="line">        x_recon = <span class="variable language_">self</span>.predict_start_from_noise(x, t=t, noise=model_out)</span><br><span class="line">    <span class="keyword">elif</span> <span class="variable language_">self</span>.parameterization == <span class="string">&quot;x0&quot;</span>:</span><br><span class="line">        x_recon = model_out</span><br><span class="line">    <span class="keyword">if</span> clip_denoised:</span><br><span class="line">        x_recon.clamp_(-<span class="number">1.0</span>, <span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line">    model_mean, posterior_variance, posterior_log_variance = <span class="variable language_">self</span>.q_posterior(</span><br><span class="line">        x_start=x_recon, x_t=x, t=t</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> model_mean, posterior_variance, posterior_log_variance</span><br></pre></td></tr></table></figure><p>给定 <span class="math inline"><em>x</em><sub><em>t</em></sub></span> 和 <span class="math inline"><em>ϵ</em><sub><em>θ</em></sub></span> ，可以推导 <span class="math inline">$\hat{x_0}$</span> ：</p><p><span class="math display">$$\begin{split}    \bar{x_0} = \frac{1}{\sqrt{\bar\alpha_t}}x_t-\frac{\sqrt{1-\bar\alpha_t}}{\sqrt{\bar\alpha_t}}\epsilon_\theta(x_t,t)\end{split}$$</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">predict_start_from_noise</span>(<span class="params">self, x_t, t, noise</span>):</span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">        extract_into_tensor(<span class="variable language_">self</span>.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t</span><br><span class="line">        - extract_into_tensor(<span class="variable language_">self</span>.sqrt_recipm1_alphas_cumprod, t, x_t.shape)</span><br><span class="line">        * noise</span><br><span class="line">    )</span><br></pre></td></tr></table></figure><p>接着给定 <span class="math inline"><em>x</em><sub><em>t</em></sub></span> 和 <span class="math inline">$\hat{x_0}$</span> ，可以根据后验公式继续推导出 <span class="math inline"><em>x</em><sub><em>t</em> − 1</sub></span> ：</p><p><span class="math display">$$\begin{gather}\begin{split}    q(x_{t-1}\mid x_t,x_0)&amp;=\mathcal{N}(\mu_t,\sigma_t^2I) \\    \mu_t&amp;=\frac{\sqrt{\bar{\alpha}_{t-1}}\cdot\beta_t}{1-\bar{\alpha}_t}x_0+\frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t}x_t \\    \sigma_t^2&amp;=\beta_t\cdot\frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t}\end{split}\end{gather}$$</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">q_posterior</span>(<span class="params">self, x_start, x_t, t</span>):</span><br><span class="line">    posterior_mean = (</span><br><span class="line">        extract_into_tensor(<span class="variable language_">self</span>.posterior_mean_coef1, t, x_t.shape) * x_start</span><br><span class="line">        + extract_into_tensor(<span class="variable language_">self</span>.posterior_mean_coef2, t, x_t.shape) * x_t</span><br><span class="line">    )</span><br><span class="line">    posterior_variance = extract_into_tensor(<span class="variable language_">self</span>.posterior_variance, t, x_t.shape)</span><br><span class="line">    posterior_log_variance_clipped = extract_into_tensor(</span><br><span class="line">        <span class="variable language_">self</span>.posterior_log_variance_clipped, t, x_t.shape</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> posterior_mean, posterior_variance, posterior_log_variance_clipped</span><br></pre></td></tr></table></figure><p>其中用到的中间变量在<code>register_schedule</code>中提前定义好了，包括计算均值需要用到的<code>posterior_mean_coef1</code>和<code>posterior_mean_coef2</code>以及方差<code>posterior_variance</code>和log形态方差<code>posterior_log_variance_clipped</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># calculations for posterior q(x_&#123;t-1&#125; | x_t, x_0)</span></span><br><span class="line">posterior_variance = (<span class="number">1</span> - <span class="variable language_">self</span>.v_posterior) * betas * ( <span class="number">1.0</span> - alphas_cumprod_prev ) / (<span class="number">1.0</span> - alphas_cumprod) + <span class="variable language_">self</span>.v_posterior * betas</span><br><span class="line"><span class="comment"># above: equal to 1. / (1. / (1. - alpha_cumprod_tm1) + alpha_t / beta_t)</span></span><br><span class="line"><span class="variable language_">self</span>.register_buffer(<span class="string">&#x27;posterior_variance&#x27;</span>, to_torch(posterior_variance))</span><br><span class="line"><span class="comment"># below: log calculation clipped because the posterior variance is 0 at the beginning of the diffusion chain</span></span><br><span class="line"><span class="variable language_">self</span>.register_buffer(</span><br><span class="line">    <span class="string">&#x27;posterior_log_variance_clipped&#x27;</span>,</span><br><span class="line">    to_torch(np.log(np.maximum(posterior_variance, <span class="number">1e-20</span>))),</span><br><span class="line">)</span><br><span class="line"><span class="variable language_">self</span>.register_buffer(</span><br><span class="line">    <span class="string">&#x27;posterior_mean_coef1&#x27;</span>,</span><br><span class="line">    to_torch(betas * np.sqrt(alphas_cumprod_prev) / (<span class="number">1.0</span> - alphas_cumprod)),</span><br><span class="line">)</span><br><span class="line"><span class="variable language_">self</span>.register_buffer(</span><br><span class="line">    <span class="string">&#x27;posterior_mean_coef2&#x27;</span>,</span><br><span class="line">    to_torch(</span><br><span class="line">        (<span class="number">1.0</span> - alphas_cumprod_prev) * np.sqrt(alphas) / (<span class="number">1.0</span> - alphas_cumprod)</span><br><span class="line">    ),</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>这里细心的小伙伴就会注意到了，为什么要计算一个log形态的方差呢？事实上回看<code>p_sample</code>函数也会发现，计算 <span class="math inline"><em>x</em><sub><em>t</em> − 1</sub></span> 时用的也是<code>posterior_log_variance_clipped</code>而不是<code>posterior_variance</code>，然后最后一步还要还原成<code>posterior_variance</code>形态用于求解，这不是多此一举吗？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">p_sample</span>(<span class="params">self, x, t, clip_denoised=<span class="literal">True</span>, repeat_noise=<span class="literal">False</span></span>):</span><br><span class="line">    ...</span><br><span class="line">    model_mean, _, model_log_variance = <span class="variable language_">self</span>.p_mean_variance(</span><br><span class="line">        x=x, t=t, clip_denoised=clip_denoised</span><br><span class="line">    )</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> model_mean + nonzero_mask * (<span class="number">0.5</span> * model_log_variance).exp() * noise</span><br></pre></td></tr></table></figure><p>其实这里也是因为为了适配后续改进的原因，我们知道DDPM其实是固定方差的形式，即方差是像这里通过一系列参数直接计算得到的，但是后续iDDPM对这里进行了改进，模型直接预测得到log方差 <span class="math inline"><em>l</em><em>o</em><em>g</em><em>σ</em><sup>2</sup></span> ，将方差作为可学习表征，因此这里统一使用log形态的方差便于代码复用性。</p>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AIGC </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>扩散模型的发展（简略版）</title>
      <link href="/2025/03/28/AIGC/overview_DM/"/>
      <url>/2025/03/28/AIGC/overview_DM/</url>
      
        <content type="html"><![CDATA[<p><strong>系列文章：</strong></p><ol class="series-items"><li><a href="/2025/03/28/AIGC/overview_DM/" title="扩散模型的发展（简略版）">扩散模型的发展（简略版）</a></li><li><a href="/2025/04/27/AIGC/LDM/" title="LDM的细节">LDM的细节</a></li></ol><p>其实契机是因为要完成高级机器学习的综述作业（@^@），这里也顺便写到博客里。</p><h1 id="引言">引言</h1><p>自监督学习是一种特殊的无监督学习，它不需要人工标注数据，而是通过数据本身构造学习任务。而作为自监督学习中的一个重要分支方向，图像生成模型已有几十年的持续发展，其核心目标是学习数据的概率分布 <span class="math inline"><em>P</em>(<em>x</em>)</span> ，并从中采样出新的数据分布以生成新的图像。</p><figure><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%8F%91%E5%B1%95%E5%8E%86%E7%A8%8B_0.png" alt="" /><figcaption>图像生成模型的发展大致历程</figcaption></figure><h2 id="图像生成模型">图像生成模型</h2><p>目前主流的图像生成架构可大致分为三类：</p><ul><li><strong>基于自回归（Autoregressive）的模型</strong>，使用逐像素或逐子块的方式生成图像，每一步依赖于前面生成的部分，生成速度较慢并难以适应高分辨率生成任务；</li><li><strong>基于生成对抗网络（GANs）的方法</strong>，使用生成器和判别器组成的对抗网络生成图像，生成速度较快但训练不稳定，并且难以控制生成细节；</li><li><strong>基于扩散（Diffusion）模型的方法</strong>，在前向过程中逐步向图像添加噪声，并在反向过程中训练模型学习如何去噪，使得模型能从噪声中恢复出图像，从而生成新图像。基于扩散模型的方法生成质量高，训练稳定并能引入精确的条件控制，虽然早期存在采样速度缓慢的问题，但后期的各种改进版本使其能逐渐适应各种生成任务，特别是文生图（Text-to-Image）任务，基本统治了各大模型，包括OpenAI的DALL·E系列<sub><span style="color:blue;"><span class="citation" data-cites="ramesh2022hierarchical">[@ramesh2022hierarchical]</span></span></sub>和Google的IMAGEN<sub><span style="color:blue;"><span class="citation" data-cites="saharia2022photorealistic">[@saharia2022photorealistic]</span></span></sub>系列背后的核心技术都是扩散模型。</li></ul><h2 id="扩散模型的发展">扩散模型的发展</h2><p>其实扩散模型的提出最初是受到了非平衡统计物理（nonequilibrium thermodynamics）中的扩散过程的启发，后续经过一系列的改进和发展，逐渐成为最主流的生成模型之一。</p><p>近期的扩散理论基础可以追溯到去噪自编码器<sub><span style="color:blue;"><span class="citation" data-cites="vincent2011connection">[@vincent2011connection]</span></span></sub>的提出，该工作证明了去噪过程与分数匹配的相关性，为之后宋飏等人提出基于分数匹配的生成模型（Score-Based Generative Models，SGM）<sub><span style="color:blue;"><span class="citation" data-cites="song2019generative">[@song2019generative]</span></span></sub>奠定了基础。之后，DDPM（Denoising Diffusion Probabilistic Models）<sub><span style="color:blue;"><span class="citation" data-cites="ho2020denoising">[@ho2020denoising]</span></span></sub>作为现代扩散模型的开山之作，在DPM（Denoising Probabilistic Models）<sub><span style="color:blue;"><span class="citation" data-cites="sohl2015deep">[@sohl2015deep]</span></span></sub>的基础上建立了从高斯噪声逐步去噪生成图像的框架。2021年宋飏等人提出的随机微分方程（Stochastic Diffusion Equation，SDE）<sub><span style="color:blue;"><span class="citation" data-cites="song2020score">[@song2020score]</span></span></sub>则通过数学证明将基于分数匹配和基于去噪的生成模型的两种范式统一了起来，由此进一步巩固了扩散模型的理论基础。</p><p>但是此时的扩散模型有一个严重的问题，采样速度极慢，无法适应各大场景的需求，DDIM<sub><span style="color:blue;"><span class="citation" data-cites="song2020denoising">[@song2020denoising]</span></span></sub>提出了非马尔科夫链推理的想法，将采样步骤大幅减少，显著提升推理速度，却仍能保持与DDPM不相上下的生成质量。后续分类器引导（Classifier-Guidance）<sub><span style="color:blue;"><span class="citation" data-cites="dhariwal2021diffusion">[@dhariwal2021diffusion]</span></span></sub>和无分类器引导（Classifier-Free Guidance）<sub><span style="color:blue;"><span class="citation" data-cites="ho2022classifier">[@ho2022classifier]</span></span></sub>的提出推动了条件扩散模型的发展，并通过大量实验证明扩散模型在图像生成任务上首次超越GAN。之后GLIDE<sub><span style="color:blue;"><span class="citation" data-cites="nichol2021glide">[@nichol2021glide]</span></span></sub>和DALL·E 2<sub><span style="color:blue;"><span class="citation" data-cites="ramesh2022hierarchical">[@ramesh2022hierarchical]</span></span></sub>尝试使用Clip来进行文本引导的扩散模型，实现了高质量的文本到图像的生成。</p><p>潜在扩散模型（Latent Diffusion Model，LDM）<sub><span style="color:blue;"><span class="citation" data-cites="rombach2022high">[@rombach2022high]</span></span></sub>提出以潜在隐空间代替原像素空间以大幅减少计算量，成为之后Stable Diffusion的技术核心，支持高效的文本到图像生成。DiT（Diffusion Transformer）<sub><span style="color:blue;"><span class="citation" data-cites="peebles2023scalable">[@peebles2023scalable]</span></span></sub>则将原UNet结构替换为了纯Transformer架构，尽管计算量有所增加，但能适应更高分辨率的图像生成任务。除此之外，Video Diffusion<sub><span style="color:blue;"><span class="citation" data-cites="ho2022video">[@ho2022video]</span></span></sub>等工作则探索了3D Diffusion的可能，Consistency Models<sub><span style="color:blue;"><span class="citation" data-cites="song2023consistency">[@song2023consistency]</span></span></sub>则尝试进一步加快采样速度的极限。</p><h1 id="相关工作">相关工作</h1><p>本节将根据扩散模型的发展介绍部分重要的相关工作。</p><h2 id="ncsn">NCSN</h2><p>首先本文将介绍宋飏老师的基于分数匹配的生成模型（Score-based Generative Model，SGM）的工作，原论文这个工作又叫作Noise Conditional Score Networks（NCSN）<sub><span style="color:blue;"><span class="citation" data-cites="song2019generative">[@song2019generative]</span></span></sub>。NCSN通过估计数据分布的梯度，实现从噪声到数据的生成。</p><p>具体来说，假设有一个数据分布 <span class="math inline"><em>p</em><sub><em>d</em><em>a</em><em>t</em><em>a</em></sub>(<em>x</em>)</span> ，我们需要估计该数据分布的对数梯度，又定义为<strong>分数函数</strong>（Score Fuction）。但是直接估计数据分布的梯度很难，NCSN引入了多个噪声尺度来进行分数估计，类似于DDPM中加噪过程中的时间步，即：</p><p><span class="math display">$$  \begin{split}     s_\theta(x)\approx\nabla_x\log p_\mathrm{data}(x|\sigma)   \end{split}$$</span></p><p>由此可以得到分数匹配的目标函数为：</p><p><span class="math display">$$\begin{gather}\begin{split}    J(\theta) &amp;=\frac{1}{2}\int p_\mathrm{data}{(x)}\|s_{data}(x)-s_\theta(x)\|_2^2dx \\     &amp;=\frac{1}{2}\mathbb{E}_{p_{\mathrm{data}}(x)}\left[\left\|s_{data}(x)-s_\theta(x)\right\|_2^2\right]\end{split}\end{gather}$$</span></p><p>但是在实际求解时，<span class="math inline"><em>s</em><sub><em>d</em><em>a</em><em>t</em><em>a</em></sub>(<em>x</em>)</span> 是无法计算的。经过一系列变换，可以得到原目标函数的一个等价表示：</p><p><span class="math display">$$\begin{gather}\begin{split}    J(\theta) &amp;=\int p_{\text {data }}(x)\left[\operatorname{tr}\left(\nabla_x s_\theta(x)\right)+\frac{1}{2}\left\|s_\theta(x)\right\|_2^2\right] dx \\    &amp;=\mathbb{E}_{p_{\text {data }}(x)}\left[\operatorname{tr}\left(\nabla_x s_\theta(x)\right)+\frac{1}{2}\left\|s_\theta(x)\right\|_2^2\right]\end{split}\end{gather}$$</span></p><p>其中涉及到了二阶偏导的计算，这在网络层次很深的时候开销是很大的，因此NCSN提出了分层分数匹配和降噪分数匹配两种方法来解决这个问题。分数估计训练完成后，便可以使用<strong>郎之万动力学（Langevin Dynamics，LD）采样</strong>来生成数据分布：</p><p><span class="math display">$$     x_{t+1}=x_t+\frac{\sigma}{2}s_\theta(x_t)+\sqrt{\sigma}z_t $$</span></p><p>其中 <span class="math inline"><em>z</em><sub><em>t</em></sub></span> 表示高斯噪声，这个过程模拟的是粒子在数据分布的梯度场中进行随机行走，最终收敛到真实数据分布。NCSN可以看作是扩散模型的前身，后续宋飏老师提出的SDE也从数学形式上统一了DDPM和NCSN，虽然它的采样较慢，训练也不稳定，但是它为后续扩散模型的发展提供了坚实的理论基础。</p><h2 id="ddpm">DDPM</h2><p>去噪扩散概率模型（Denoising Diffusion Probabilistic Models）<sub><span style="color:blue;"><span class="citation" data-cites="ho2020denoising">[@ho2020denoising]</span></span></sub>是现代扩散模型的开山之作，它正式确立了扩散模型的数学框架，在图像生成任务上表现很出色。它的核心思想是前向扩散（Forward Diffusion）和逆向去噪（Reverse Denoising）两部分，其推导基于马尔科夫链进行逐步加噪和去噪。</p><figure><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/DDPM_0.png" alt="" /><figcaption>DDPM的扩散及去噪过程</figcaption></figure><h3 id="前向扩散">前向扩散</h3><p>首先对于前向扩散过程，给定真实数据 <span class="math inline"><em>x</em><sub>0</sub> ∼ <em>q</em>(<em>x</em>)</span> ，经过 <span class="math inline"><em>T</em></span> 步的加噪过程，数据最终符合标准高斯分布。定义单步扩散过程为：</p><p><span class="math display">$$    q(x_t|x_{t-1})=\mathcal{N}(x_t;\sqrt{1-\beta_t}x_{t-1},\beta_tI)$$</span></p><p>令 <span class="math inline"><em>α</em><sub><em>t</em></sub> = 1 − <em>β</em><sub><em>t</em></sub></span> ，由此可以发现 <span class="math inline"><em>q</em>(<em>x</em><sub><em>t</em></sub>|<em>x</em><sub><em>t</em> − 1</sub>)</span> 就是一个以 <span class="math inline">$\sqrt{\alpha_t}x_{t-1}$</span> 为均值，以 <span class="math inline">(1 − <em>α</em><sub><em>t</em></sub>)<em>I</em></span> 为方差的高斯分布，加噪过程相当于是 <span class="math inline">$\sqrt{\alpha_t}x_{t-1}$</span> 基础上加上一个 <span class="math inline">𝒩(0, (1 − <em>α</em><sub><em>t</em></sub>)<em>I</em>)</span> 的随机高斯噪声。进一步推导可以得到直接从 <span class="math inline"><em>x</em><sub>0</sub></span> 生成 <span class="math inline"><em>x</em><sub><em>t</em></sub></span> 的公式为：</p><p><span class="math display">$$    q(x_t|x_0)=\mathcal{N}(x_t;\sqrt{\bar{\alpha}_t}x_0,(1-\bar{\alpha}_t)I)$$</span></p><p>其中 <span class="math inline">$\bar{\alpha}_t=\prod_{i=1}^t\alpha_i$</span> ，表示累积噪声。最终当 <span class="math inline"><em>t</em> → <em>T</em></span> 时，<span class="math inline"><em>x</em><sub><em>T</em></sub></span> 近似服从于标准高斯分布 <span class="math inline">𝒩(0, <em>I</em>)</span>。</p><h3 id="逆向去噪">逆向去噪</h3><p>逆向去噪的目标是从 <span class="math inline"><em>x</em><sub><em>T</em></sub></span> 开始，逐步去噪恢复 <span class="math inline"><em>x</em><sub>0</sub></span> ，即 <span class="math inline">$p(x_{0:T})=p(x_T)\prod_{t=T-1}^0p(x_t|x_{t+1})$</span> ，那么问题的关键在于如何求解 <span class="math inline"><em>p</em>(<em>x</em><sub><em>t</em></sub>|<em>x</em><sub><em>t</em> + 1</sub>)</span> 。假设我们可以训练一个模型求解这个分布：</p><p><span class="math display">$$\begin{split}    p_{\theta}(x_{t-1} \mid x_t)=\mathcal{N}(x_{t-1} ; \mu_{\theta}(x_t, t),  \Sigma_{\theta}(x_t, t))\end{split}$$</span></p><p>不同于DPM<sub><span style="color:blue;"><span class="citation" data-cites="sohl2015deep">[@sohl2015deep]</span></span></sub>的直接预测 <span class="math inline"><em>x</em><sub>0</sub></span> ，DDPM使用UNet预测每一个时间步添加的噪声 <span class="math inline">$\hat{\epsilon_\theta}(x_t,t)\approx\epsilon$</span> ，然后通过一系列数学推导得到：</p><p><span class="math display">$$\begin{split}    \mu_\theta\left(x_t, t\right)=\frac{1}{\sqrt{\alpha_t}}\left(x_t-\frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}_t}} \hat{\epsilon}_\theta\left(x_t, t\right)\right)\end{split}$$</span></p><p><span class="math inline"><em>p</em><sub><em>θ</em></sub>(<em>x</em><sub><em>t</em> − 1</sub>∣<em>x</em><sub><em>t</em></sub>)</span> 的预测均值已经得到，而预测方差 <span class="math inline"><em>Σ</em><sub><em>θ</em></sub>(<em>x</em><sub><em>t</em></sub>,<em>t</em>)</span> 在DDPM中被设为随机高斯噪声 <span class="math inline"><em>σ</em><sub><em>t</em></sub><em>z</em></span> ，由此可以得到：</p><p><span class="math display">$$\begin{split}    x_{t-1}=\mu_\theta\left(x_t, t\right)+\sigma_t z\end{split}$$</span></p><p>可以看到，模型预测每一步的噪声 <span class="math inline"><em>ϵ</em><sub><em>t</em></sub></span> 要比之前直接预测 <span class="math inline"><em>x</em><sub>0</sub></span> 容易得多，DDPM的最终目标函数就是最小化去噪误差，即：</p><p><span class="math display">$$\begin{split}    \mathcal L(\theta)=\mathbb{E}_{x_0,\epsilon\sim\mathcal{N}(0,I),t}\left[\left\|\epsilon-\hat{\epsilon}_\theta(x_t,t)\right\|^2\right]\end{split}$$</span></p><p>这个损失函数就相当于训练模型预测噪声 <span class="math inline"><em>ϵ</em></span> 的均方误差，学习难度大幅降低。不仅如此，在ImageNet 256$$256任务上，DDPM取得了优于GAN的FID分数，这是扩散模型首次取得能和GAN竞争的分数，后续OpenAI发布了基于DDPM的DALL·E，也进一步提升了扩散模型的影响力。</p><h2 id="sde">SDE</h2><p>随机微分方程（Stochastic Differential Equations，SDE）<sub><span style="color:blue;"><span class="citation" data-cites="song2020score">[@song2020score]</span></span></sub>是宋飏老师提出的一个连续时间扩散框架，它从数学上统一了NCSN和DDPM。如前文所介绍的，NCSN采用分数匹配的方法来预测概率分布的对数梯度，之后使用郎之万动力学采样以生成图像，它可以看作是连续噪声扰动；而DDPM则在有限的 <span class="math inline"><em>T</em></span> 步内加噪随后逆向去噪，采用预测噪声的方法来生成数据分布，基础理论依据是离散马尔可夫过程。</p><p><strong>随机微分方程</strong>是一类在经典微分方程基础上引入随机过程的数学方程，用于描述具有随机性或不确定性系统的演化，数据的随机扩散过程就可以用随机微分方程进行描述：</p><p><span class="math display">$$\begin{split}    dx=f(x,t)dt+g(t)dW\end{split}$$</span></p><p>其中 <span class="math inline"><em>f</em>(<em>x</em>, <em>t</em>)</span> 是漂移项（drift term），用于描述数据的确定性过程，<span class="math inline"><em>g</em>(<em>t</em>)</span>是扩散项（diffusion term），描述系统的随机性过程，<span class="math inline"><em>d</em><em>W</em></span>是标准布朗运动（Wiener Process）。经过一系列数学推导，可以分别得到NCSN和DDPM所对应的SDE，分别命名为 <strong>VE-SDE</strong>（Variance Exploding SDE）和 <strong>VP-SDE</strong>（Variance Preserving SDE）：</p><p><span class="math display">$$\begin{gather}\begin{split}        dx&amp;=\sqrt{d[\sigma^2(t)]}dW\quad\text{(VESDE)} \\        dx&amp;=-\frac{1}{2}\beta(t)xdt+\sqrt{\beta(t)}dW\quad\text{(VPSDE)} \end{split}\end{gather}$$</span></p><p>可以发现VESDE没有漂移项，噪声会随着 <span class="math inline"><em>t</em></span> 的增加而爆炸性增长，因此叫作“Variance Exploding”，而VPSDE的噪声变化则较为平缓，因此叫作“Variance Preserving”。其实除了这两种方程外，原文还针对VPSDE提出了改进版本sub-VPSDE，为扩散项加上了一个额外的衰减因子让噪声水平增长得更慢，不会在早期就过度加噪，造成采样时需要更多的去噪步数。</p><p>使用SDE采样的关键是要确定逆向SDE，即从 <span class="math inline"><em>x</em><sub><em>T</em></sub></span> 逆推 <span class="math inline"><em>x</em><sub>0</sub></span>，由于扩散过程的反向过程也是一个扩散过程，因此我们可以得到逆向SDE的方程，其中的漂移系数和扩散系数和SDE保持一致：</p><p><span class="math display">$$\begin{split}    dx=[f(x,t)-g^2(t)\nabla_x\log p_t(x)]dt+g(t)d\bar{W}\end{split}$$</span></p><p>其中 <span class="math inline">∇<sub><em>x</em></sub>log <em>p</em><sub><em>t</em></sub>(<em>x</em>)</span> 可由神经网络 <span class="math inline"><em>s</em><sub><em>θ</em></sub>(<em>x</em>, <em>t</em>)</span> 预测得到，由此可以分别得到两个方程的逆向SDE离散表达：</p><p><span class="math display">$$\begin{gather}\begin{split}    x_i&amp;=x_{i+1}+(\sigma_{i+1}^2-\sigma_i^2)s_{\theta^*}(x_{i+1},i+1) +\sqrt{\sigma_{i+1}^2-\sigma_i^2}z_{i+1}\quad\text{(VESDE)} \\    x_i&amp;=(2-\sqrt{1-\beta_{i+1}})x_{i+1}+\beta_{i+1}s_{\theta^*}(x_{i+1},i+1)+\sqrt{\beta_{i+1}}z_{i+1}\quad\text{(VPSDE)} \end{split}\end{gather}$$</span></p><p>根据两个逆向SDE可以发现，它们的形式分别等效于NCSN所使用的郎之万采样和DDPM所使用的马尔科夫链，由此原文使用连续的SDE统一了两种生成框架。</p><p>除此之外，原文还提出了<strong>PC采样方法</strong>，这也是SDE数值求解常用的一种方法，因为SDE是对时间连续的方程，所以不同的离散化方案总是存在一定误差，可以使用score-based MCMC采样方法进行进一步校正，即在每一步采样中额外添加一个修正步骤，让数据更快收敛。这里PC采样采用的修正器还是郎之万动力学方程，总结来说PC采样分两步：（1）Predictor，使用逆向SDE进行一步采样；（2）Corrector，使用郎之万动力学采样进一步调整。</p><p>去掉逆向SDE中的随机项（布朗运动 <span class="math inline"><em>d</em><em>w</em></span> 项）后，可以得到概率流常微分方程（Probabilistic Flow ODE），此时整个采样过程是一条确定性的轨迹，求解更快，因此适用于高效采样需求的任务。</p><h2 id="ddim">DDIM</h2><p>DDPM中，前向扩散过程定义为马尔科夫过程，为数据 <span class="math inline"><em>x</em><sub>0</sub></span> 逐步添加噪声，逆向过程同样需要逐步去噪。但是为了保证最终前向过程的 <span class="math inline"><em>x</em><sub><em>T</em></sub></span> 满足高斯噪声，这里的步数 <span class="math inline"><em>T</em></span> 要设置得足够大（1000+），导致逆向过程的采样步数也非常大，推理的时间和计算开销巨大。DDIM<sub><span style="color:blue;"><span class="citation" data-cites="song2020denoising">[@song2020denoising]</span></span></sub>提出了确定性采样方法，是扩散过程变为确定性过程（ODE），从而加速采样。</p><p>首先回顾一下定义在马尔科夫链的DDPM中单步加噪和单步去噪过程：</p><p><span class="math display">$$\begin{gather}\begin{split}        x_t &amp;= \sqrt{\alpha_t}x_{t-1}+\sqrt{1-\alpha_t}\epsilon_t \\        x_{t-1}&amp;=\frac{1}{\alpha_t}(x_t-\frac{\beta_t}{\sqrt{1-\bar{\alpha_t}}}\epsilon_\theta(x_t,t))+\sigma_t z\end{split}\end{gather}$$</span></p><p>基于此重新推导DDPM的优化目标，可以得到：</p><p><span class="math display">$$ \begin{split}\mathcal L=\mathbb{E}_{x_0,\epsilon\sim\mathcal{N}(\mathbf{0},\mathbf{I})}\left[\|\epsilon-\epsilon_\theta\left(\sqrt{\bar{\alpha}_t}x_0+\sqrt{1-\bar{\alpha}_t}\epsilon,t\right)\|^2\right] \end{split}$$</span></p><p>可以发现，DDPM的优化目标其实仅仅依赖于边缘分布 <span class="math inline"><em>q</em>(<em>x</em><sub><em>t</em></sub>|<em>x</em><sub>0</sub>)</span> ，而不依赖于联合分布 <span class="math inline"><em>q</em>(<em>x</em><sub>1 : <em>T</em></sub>|<em>x</em><sub>0</sub>)</span> ，因此可以看出DDPM其实并不要求推理过程一定要是马尔科夫过程，只要推理分布满足边缘分布条件即可。经过一系列重新推导，可以得到非马尔科夫链下的单步逆向过程为：</p><p><span class="math display">$$\begin{split}\mathbf{x}_{t-1}=\sqrt{\alpha_{t-1}}\left(\underbrace{\frac{\mathbf{x}_t-\sqrt{1-\alpha_t}\epsilon_\theta(\mathbf{x}_t,t)}{\sqrt{\alpha_t}}}_{\mathrm{predicted~}\mathbf{x}_0}\right) +\underbrace{\sqrt{1-\alpha_{t-1}-\sigma_t^2}\cdot\epsilon_\theta(\mathbf{x}_t,t)}_{\text{direction pointing to }\mathbf{x}_t}+\underbrace{\sigma_t\epsilon_t}_{\text{random noise}}\end{split}$$</span></p><p>其中 <span class="math inline">$\sigma_t^2=\eta\cdot\sqrt{(1-\alpha_{t-1})/(1-\alpha_t)}\sqrt{(1-\alpha_t/\alpha_{t-1})}$</span> 。</p><ul><li>当 <span class="math inline"><em>η</em> = 1</span> 时，此时的生成过程和DDPM一致；</li><li>当 <span class="math inline"><em>η</em> = 0</span> 时，此时的生成过程就没有随机噪声项了，是一个确定性的过程，这就是DDIM（Denoising Diffusion Implic Model），此时的样本生成就变成了确定的过程，有点类似于SDE中的概率流ODE。</li></ul><figure><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/DDIM_0.png" alt="" /><figcaption>DDIM的采样过程</figcaption></figure><p>由于DDIM并没有明确前向过程，这意味这可以定义一个更短的步数的前向过程以减少采样步数。原文从原始序列 <span class="math inline">[1, …, <em>T</em>]</span> 采样一个长度为 <span class="math inline"><em>S</em></span> 的子序列 <span class="math inline">[<em>τ</em><sub>1</sub>, …, <em>τ</em><sub><em>S</em></sub>]</span> ，此时前向过程 <span class="math inline">[<em>x</em><sub><em>τ</em><sub>1</sub></sub>, …, <em>x</em><sub><em>τ</em><sub><em>S</em></sub></sub>]</span> 同样满足 <span class="math inline">$q({x}_{\tau_i}|{x}_0)=\mathcal{N}({x}_t;\sqrt{\alpha_{\tau_i}}{x}_0,(1-\alpha_{\tau_i}){I})$</span> 。由此该生成过程也可以用这个子序列进行采样，最终加速生成过程，能从DDPM的1000步采样减少为50步采样，大幅降低推理开销。</p><h2 id="cdm">CDM</h2><p>条件扩散模型（Conditional Diffusion Model，CDM）指使用条件控制生成的扩散模型，其中最常用的方法是引导（guidance）方法，可以用于增强生成质量或是让模型朝向某个目标分布进行采样。最主要的两种方法是：分类器引导（Classifier Guidance）<sub><span style="color:blue;"><span class="citation" data-cites="dhariwal2021diffusion">[@dhariwal2021diffusion]</span></span></sub>和无分类器引导（Classifier-Free Guidance）<sub><span style="color:blue;"><span class="citation" data-cites="ho2022classifier">[@ho2022classifier]</span></span></sub>，因此这里将分别简要介绍这两篇工作。</p><h3 id="分类器引导">分类器引导</h3><p>假设我们需要在扩散过程中引入条件信息 <span class="math inline"><em>y</em></span> ，直觉上来说，条件信息不会影响前向过程，因为最终加噪都会变为高斯噪音，即 <span class="math inline"><em>q</em>(<em>x</em><sub>1 : <em>T</em></sub>|<em>x</em><sub>0</sub>, <em>y</em>) = <em>q</em>(<em>x</em><sub>1 : <em>T</em></sub>|<em>x</em><sub>0</sub>)</span> 。因此我们需要重点关注逆向过程的条件控制，不妨说我们需要重点关注分数函数 <span class="math inline"><em>ŝ</em><sub><em>θ</em></sub>(<em>x</em><sub><em>t</em></sub>, <em>t</em>) ≈ ∇<sub><em>x</em><sub><em>t</em></sub></sub>log <em>p</em>(<em>x</em><sub><em>t</em></sub>)</span> 的变化。引入条件信息后，我们可以使用贝叶斯公式进行几步变换：</p><p><span class="math display">$$\begin{gather}\begin{split}        \nabla_{x_t}\log p(x_t|y)&amp;=\nabla_{x_t}\log(\frac{p(x_t)p(y|x_t)}{p(y)}) \\        &amp;= \nabla_{x_t}\log p(x_t)+\nabla_{x_t}\log p(y|x_t)-\nabla_{x_t}\log p(y) \\         &amp;= \nabla_{x_t}\log p(x_t)+\nabla_{x_t}\log p(y|x_t)\end{split}\end{gather}$$</span></p><p>可以看到，展开后有两项，其中第一项相当于无条件生成时的分数函数，可以称为无条件分数（Unconditional Score）；而第二项相当于一个分类器的对数梯度，称为对抗梯度（Adversarial Gradient）。为了能更好的控制生成内容的方向，论文额外引入了一个超参数 <span class="math inline"><em>λ</em></span> 作为引导强度：</p><p><span class="math display">$$\begin{gather}\nabla_{x_t}\log p(x_t|y)=\nabla_{x_t}\log p(x_t)+\lambda\nabla_{x_t}\log p(y|x_t)\end{gather}$$</span></p><h3 id="无分类器引导">无分类器引导</h3><p>自OpenAI发布分类器引导的条件生成范式后，GoogleBrain团队提出了无分类器引导方法，直接在扩散模型内部学习引导信息，无需额外的分类器。还是从分数估计的角度来进行推导，从条件引导的工作我们得知 <span class="math inline">∇<sub><em>x</em><sub><em>t</em></sub></sub>log <em>p</em>(<em>x</em><sub><em>t</em></sub>|<em>y</em>) = ∇<sub><em>x</em><sub><em>t</em></sub></sub>log <em>p</em>(<em>x</em><sub><em>t</em></sub>) + ∇<sub><em>x</em><sub><em>t</em></sub></sub>log <em>p</em>(<em>y</em>|<em>x</em><sub><em>t</em></sub>)</span> ，变换一下可以得到 <span class="math inline">∇<sub><em>x</em><sub><em>t</em></sub></sub>log <em>p</em>(<em>y</em>|<em>x</em><sub><em>t</em></sub>) = ∇<sub><em>x</em><sub><em>t</em></sub></sub>log <em>p</em>(<em>x</em><sub><em>t</em></sub>|<em>y</em>) − ∇<sub><em>x</em><sub><em>t</em></sub></sub>log <em>p</em>(<em>x</em><sub><em>t</em></sub>)</span> ，由此可以得到：</p><p><span class="math display">$$\begin{gather}\begin{split}    \nabla_{x_t}\log p(x_t|y)&amp;=\nabla_{x_t}\log p(x_t)+\lambda\nabla_{x_t}\log p(y|x_t) \\    &amp;=\nabla_{x_t}\log p(x_t)+\lambda(\nabla_{x_t}\log p(x_t|y)-\nabla_{x_t}\log p(x_t)) \\    &amp;=\lambda\nabla_{x_t}\log p(x_t|y)+(1-\lambda)\log p(x_t)\end{split}\end{gather}$$</span></p><p>可以发现，此时分数仍然分为两部分：第一项可以看作是条件分数（Conditional Score），第二项则是无条件分数（Unconditional Score）。并且 <span class="math inline"><em>λ</em></span> 的取值会影响到条件控制的强弱：</p><ul><li>当 <span class="math inline"><em>λ</em> = 0</span> 时，此时相当于无条件生成；</li><li>当 <span class="math inline"><em>λ</em> &gt; 1</span> 时，模型会优先考虑条件控制而远离无条件分数网络方向。</li></ul><p>这样看似乎还是要训练两个网络，但实际上无条件可看作是条件控制的特殊情况，即 <span class="math inline"><em>y</em> = ⌀</span> ，这样在训练时可以交替训练有条件和无条件的情况。</p><h2 id="ldm">LDM</h2><p>DDPM生成的图像质量已经非常好了，但是训练开销很大，一个问题在于中间的加噪状态 <span class="math inline"><em>x</em><sub><em>t</em></sub></span> 的尺寸是和输入保持一致的，这使得其训练开销随图像分辨率的增大而加重，无法适应高质量图像生成任务。因此潜在扩散模型（Latent Diffusion Model，LDM）<sub><span style="color:blue;"><span class="citation" data-cites="rombach2022high">[@rombach2022high]</span></span></sub>针对这个问题做了一些改进，<strong>将图像从像素空间表示（Pixel Space）转变为潜在空间表示（Latent Space）实现高分辨率图像生成任务</strong>。</p><figure><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/LDM_0.png" alt="" /><figcaption>潜在扩散模型的模型架构</figcaption></figure><p>LDM的主要架构由三部分组成：VAE编码器（Encoder）、扩散模型以及VAE解码器（Decoder）。其中VAE编码器将高维图像 <span class="math inline"><em>x</em><sub>0</sub></span> 编码表示为潜在表示 <span class="math inline"><em>z</em><sub>0</sub></span> ，然后送入扩散模型中进行扩散和去噪，最终VAE解码器将去噪得到的潜在空间表示 <span class="math inline">$\hat{z_0}$</span> 还原为像素空间表示 <span class="math inline">$\hat{x_0}$</span> ，得到高质量的图像。这个VAE可以是预训练好的模型，在训练扩散模型时，其参数是被冻结的。</p><p>而对于条件生成处理上，LDM引入条件融合模块 <span class="math inline"><em>τ</em><sub><em>θ</em></sub></span> 来处理多种模态的条件信息 <span class="math inline"><em>y</em></span> 。比如对于文生图任务，这里的 <span class="math inline"><em>τ</em><sub><em>θ</em></sub></span> 就是一个文本编码器，可以使用预训练好的CLIP模型<sub><span style="color:blue;"><span class="citation" data-cites="radford2021learning">[@radford2021learning]</span></span></sub>中的文本编码器。同时引入条件融合开关：</p><ul><li>对于文本输入，这里在Unet网络中添加了Attention层将Embedding向量 <span class="math inline"><em>τ</em><sub><em>θ</em></sub>(<em>y</em>)</span> 融合到每层特征中；</li><li>而对于其他空间的条件（语义图、修复图等），则直接通过拼接完成条件融合。</li></ul><p>由此我们可以得到LDM的目标函数为：</p><p><span class="math display">$$\begin{split}\mathcal L_{L D M}:=\mathbb{E}_{\mathcal{E}(x), y, \epsilon \sim \mathcal{N}(0,1), t}\left[\left\|\epsilon-\epsilon_\theta\left(z_t, t, \tau_\theta(y)\right)\right\|_2^2\right]\end{split}$$</span></p><p>后续爆火的Stable Diffusion就是LDM的一个开源预训练模型，一度占据图像生成开源领域的主导地位。</p><h2 id="dit">DiT</h2><p>DiT（Diffusion Transformer）<sub><span style="color:blue;"><span class="citation" data-cites="peebles2023scalable">[@peebles2023scalable]</span></span></sub>是Meta AI提出的基于Transformer的扩散模型，它首次在扩散模型完全用Transformer替代了UNet，提升了扩散模型的可扩展性和生成质量。</p><figure><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/DiT_0.png" alt="" /><figcaption>DiT的模型架构</figcaption></figure><p>DiT也是一个作用在潜在空间上的模型，同样使用一个VQVAE将图像编码到潜在空间上，之后送入DiT模块中加工。不同的是，由于舍弃掉了CNN，这里使用了ViT进一步将潜在空间特征转换为一维序列特征（Patch Token），并将时间步 <span class="math inline"><em>t</em></span> 和条件信息 <span class="math inline"><em>y</em></span> 融合后嵌入到图像的Patch Token中。</p><p>为了选择融合条件特征效果最好的DiT模块，原文一共探索了四种不同的DiT模块：</p><ul><li>基于上下文条件（In-context conditioning）的DiT模块，直接将条件特征嵌入到输入序列中；</li><li>基于交叉注意力（Cross Attention）的DiT模块，将时间步 <span class="math inline"><em>t</em></span> 和条件信息 <span class="math inline"><em>y</em></span> 拼成长度为2的序列，然后输入到多头交叉注意力模块中和图像特征进行融合；</li><li>基于自适应层归一化（Adaptive Layer Normalization，AdaLN）的DiT模块，通过使用条件信息学习 <span class="math inline"><em>β</em></span> 和 <span class="math inline"><em>γ</em></span> 两个归一化参数来调整中间特征；</li><li>基于Zero初始化的AdaLN的DiT模块，是AdaLN方案的改进版本，将AdaLN的线性层参数初始化为zero，并额外在每个残差模块结束之前引入回归缩放参数 <span class="math inline"><em>α</em></span> 。</li></ul><p>通过对四种模块进行对比实验，发现AdaLN-Zero的效果是最好的，DiT模块默认采用这种方式来嵌入条件。</p><p>同时，需要注意的是DiT所使用的扩散模型沿用了OpenAI的改进版DDPM<sub><span style="color:blue;"><span class="citation" data-cites="song2020improved">[@song2020improved]</span></span></sub>，不再采用固定的方差，而是采用另一个网络来预测方差 <span class="math inline"><em>Σ</em><sub><em>θ</em></sub>(<em>x</em><sub><em>t</em></sub>,<em>t</em>) = exp (<em>v</em>log<em>β</em><sub><em>t</em></sub>+(1−<em>v</em>)<em>β̃</em><sub><em>t</em></sub>)</span> ，训练时采用无分类器引导的范式进行学习。该种方法对于视频生成等需要强可扩展性的任务来说很适用，OpenAI推出的Sora就是用了DiT作为模型架构。</p><h2 id="pixart">PixArt</h2><p>在DiT推出之后，华为诺亚方舟实验室又提出了PixArt-<span class="math inline"><em>α</em></span><sub><span style="color:blue;"><span class="citation" data-cites="chen2023pixart">[@chen2023pixart]</span></span></sub>，这也是一种基于transformer的文本到图像的扩散模型，它在显著降低训练成本的同时，也实现了很不错的图像生成质量。</p><p>PixArt-<span class="math inline"><em>α</em></span>模型还是使用DiT作为基础架构，但是进行了一些改进。原DiT架构中的每个DiT模块中的<span class="math inline"><em>S</em><sup><em>i</em></sup></span>都是通过独立的MLP计算得到的，即 <span class="math inline"><em>S</em><sup>(<em>i</em>)</sup> = <em>f</em><sup>(<em>i</em>)</sup>(<em>c</em> + <em>t</em>)</span> ，其中 <span class="math inline"><em>c</em>, <em>t</em></span> 分别表示类别条件和时间步信息，这会占据很高的开销。基于此，PixArt提出了AdaLN-single，定义一个全局 <span class="math inline"><em>S̄</em> = <em>f</em>(<em>t</em>)</span> ，只使用时间步信息生成 <span class="math inline"><em>S̄</em></span> ，在第 <span class="math inline"><em>i</em></span> 个模块中，通过计算 <span class="math inline"><em>S</em><sup>(<em>i</em>)</sup> = <em>g</em>(<em>S̄</em>, <em>E</em><sup>(<em>i</em>)</sup>)</span> 得到每个模块的缩放和偏移参数，其中 <span class="math inline"><em>E</em><sup>(<em>i</em>)</sup></span> 是可训练的嵌入表示；而文本条件 <span class="math inline"><em>c</em></span> 则通过一个额外的多头交叉注意力嵌入到模块中。大量实验表明，通过引入全局MLP和逐层嵌入处理时间步 <span class="math inline"><em>t</em></span> 信息、使用交叉注意力层处理文本信息 <span class="math inline"><em>c</em></span> 的改进，能在有效减小模型大小的同时保持原生成能力。</p><figure><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/PixArt_0.png" alt="" /><figcaption>PixArt-<span class="math inline"><em>α</em></span>的模型架构</figcaption></figure><p>除此之外，PixArt-<span class="math inline"><em>α</em></span>将复杂的文本到图像的生成任务分解为三个子任务以逐步训练：</p><ul><li>像素级依赖的学习（Pixel Dependency Learning），为了实现后续高质量的生成，PixArt先在ImageNet上预训练类引导生成模型，这一过程成本低廉并能帮助模型有效学习到图像的像素级依赖性；</li><li>文本到图像的精确对齐学习（Text-image Alignment Learning），原论文构建了一个包含高概念密度的精确文本-图像对数据集，相较于以往的数据集，歧义显著减少，并能处理更多的名词；</li><li>高质量图像微调（High-resolution and Aesthetic Image Generation），为了生成高审美质量的图片，原论文最后使用高质量的图片对模型进行进一步微调。</li></ul><h1 id="总结与展望">总结与展望</h1><p>在扩散模型成为主流之前，基于能量的生成模型和分数匹配已经被研究了许多年，这些工作为扩散模型的出现奠定了理论基础，直到后来DDPM的提出正式标志着扩散模型的爆发。之后一系列的工作探讨了对原始模型的改进，体现在加速采样、降低训练开销、提升图像生成质量等，一度使扩散模型超越基于自回归和GAN的生成模型成为大规模生成任务的首选模型。后来进入大模型时代，包括DALL·E、IMAGEN、Stable Diffusion的文生图大模型更是进一步引爆了Diffusion的影响力。总的来说，作为当下Aigc领域中热门研究领域之一，扩散模型的发展正值草长莺飞的时期，它开创了一种全新的生成模型范式，并被广泛应用于各类生成式任务以及当下视觉生成大模型中。</p><p>未来对于扩散模型的研究还在继续，比如如何进一步提升采样速度、创造更通用的多模态扩散模型、更精细的条件控制、量化优化以实现Edge AI等等，还有很多课题值得探索...</p>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AIGC </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>建站日志以及记录</title>
      <link href="/2025/03/27/hello-world/"/>
      <url>/2025/03/27/hello-world/</url>
      
        <content type="html"><![CDATA[<h1 id="leo小破站的诞生日">Leo小破站的诞生日</h1><p><strong>2025.3.31</strong></p><h2 id="关于建站">关于建站</h2><p>使用Hexo框架 + Github Pages + Butterfly主题，这里要感谢<a href="https://blog.csdn.net/m0_51269961/article/details/122575897">杰森的教程</a>，向大佬低头orz…</p><h2 id="关于契机">关于契机</h2><p>其实很早之前就想建站了，但是拖延症一直拖到现在…上本研共修课看到了大二学弟做的博客，觉得自愧不如，趁现在还有时间，随便建一个站吧</p><h1 id="hexo">Hexo</h1><p>关于hexo的一些常用命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 事实上已经修改json，调试时只需要hexo s即可</span></span><br><span class="line">$ hexo clean</span><br><span class="line">$ hexo generate</span><br><span class="line">$ hexo server</span><br><span class="line"></span><br><span class="line">$ hexo d</span><br></pre></td></tr></table></figure><p>More info about writing: <a href="https://hexo.io/docs/writing.html">Writing</a></p><p>More info about server: <a href="https://hexo.io/docs/server.html">Server</a></p><p>More info about deployment: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p><h1 id="butterfly">Butterfly</h1><p>根据<a href="https://butterfly.js.org/">原主题配置文档</a>修改 <code>_config.butterfly.yml</code> 文件自定义博客配置。</p><h2 id="发布文章">发布文章</h2><p>需要修改 <code>Post Fron-matter</code> ，于文章最上方以 <code>---</code> 分隔的区域，常用配置参数如下。</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title: （required）文章标题</span><br><span class="line">date: （optional）创建时间</span><br><span class="line">updated: （optional）更新时间</span><br><span class="line">tags: （optional）文章标签</span><br><span class="line">categories: （optional）文章分类</span><br><span class="line">description: （optional）文章简介</span><br><span class="line">top<span class="emphasis">_img: （optional）文章头图</span></span><br><span class="line"><span class="emphasis">% 转载其他作品时放置</span></span><br><span class="line"><span class="emphasis">copyright: （optional）显示文章版权模块</span></span><br><span class="line"><span class="emphasis">copyright_</span>author: （optional）文章作者</span><br><span class="line">copyright<span class="emphasis">_author_</span>href: （optional）文章作者url</span><br><span class="line">copyright<span class="emphasis">_url: （optional）文章url</span></span><br><span class="line"><span class="emphasis">copyright_</span>info: （optional）版权声明</span><br><span class="line">---</span><br></pre></td></tr></table></figure><h2 id="发布页面">发布页面</h2><p>需要修改 <code>Page Front-matter</code>，于文章最上方以 <code>---</code> 分隔的区域，常用配置参数如下。</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title: （required）文章标题</span><br><span class="line">date: （optional）创建时间</span><br><span class="line">updated: （optional）更新时间</span><br><span class="line">type: （required）标签、分类等页面必须标明</span><br><span class="line">description: 页面描述</span><br><span class="line"><span class="section">top<span class="emphasis">_img: 页面头图</span></span></span><br><span class="line"><span class="emphasis"><span class="section">---</span></span></span><br></pre></td></tr></table></figure><h2 id="关于公式">关于公式</h2><p>原公式渲染器无法正常渲染复杂latex公式，卸载了原公式渲染器：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm un hexo-renderer-marked</span><br></pre></td></tr></table></figure><p>并安装了 <code>pandoc</code> 对应的渲染器：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm i hexo-renderer-pandoc</span><br></pre></td></tr></table></figure><p>启用 <code>mathjax</code> 设置，但是这样做行内公式其实不太好看、行间公式也还是无法正常渲染，调试发现需要额外插入 <code>begin-end</code> 块才能正常渲染。以下为演示：</p><blockquote><p>行内公式： 引入缩放因子 <span class="math inline"><em>λ</em><sub><em>θ</em></sub></span></p></blockquote><blockquote><p>行间公式： <span class="math display">$$\begin{split}   s_\theta(x)\approx\nabla_x\log p_\mathrm{data}(x|\sigma) \end{split}$$</span></p><p><span class="math display">$$\begin{gather}\begin{split}   J(\theta) &amp;=\frac{1}{2}\int p_\mathrm{data}{(x)}\|s_{data}(x)-s_\theta(x)\|_2^2dx \\    &amp;=\frac{1}{2}\mathbb{E}_{p_{\mathrm{data}}(x)}\left[\left\|s_{data}(x)-s_\theta(x)\right\|_2^2\right]\end{split}\end{gather}$$</span></p></blockquote><p>以上行间公式对应的md写法为： <figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">$$</span><br><span class="line">  \begin&#123;split&#125; </span><br><span class="line"><span class="code">    s_\theta(x)\approx\nabla_x\log p_\mathrm&#123;data&#125;(x|\sigma) </span></span><br><span class="line"><span class="code">  \end&#123;split&#125;</span></span><br><span class="line"><span class="code">$$</span></span><br><span class="line"><span class="code"></span></span><br><span class="line">$$</span><br><span class="line">\begin&#123;gather&#125;</span><br><span class="line">\begin&#123;split&#125;</span><br><span class="line"><span class="code">    J(\theta) &amp;=\frac&#123;1&#125;&#123;2&#125;\int p_\mathrm&#123;data&#125;&#123;(x)&#125;\|s_&#123;data&#125;(x)-s_\theta(x)\|_2^2dx \\</span></span><br><span class="line"><span class="code">     &amp;=\frac&#123;1&#125;&#123;2&#125;\mathbb&#123;E&#125;_&#123;p_&#123;\mathrm&#123;data&#125;&#125;(x)&#125;\left[\left\|s_&#123;data&#125;(x)-s_\theta(x)\right\|_2^2\right]</span></span><br><span class="line"><span class="code">\end&#123;split&#125;</span></span><br><span class="line"><span class="code">\end&#123;gather&#125;</span></span><br><span class="line"><span class="code">$$</span></span><br></pre></td></tr></table></figure></p><div class="note warning simple"><p>暂时没有找到什么其他解决办法，留待解决…</p></div><h1 id="github-pages">Github Pages</h1><p>Leo的博客：<a href="https://litchi-lee.github.io/">https://litchi-lee.github.io/</a></p><h2 id="图床">图床</h2><p>额外建库搭配 <code>PicGo</code> 生成图床，配合Github插件<a href="https://github.com/marketplace/imgbot">Github imgbot</a>压缩库的图片。</p><p>由于Github Raw链接被大量匿名访问会被封禁，因此这里使用免费的jsDelivr加速GitHub资源：</p><pre><code>https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main</code></pre>]]></content>
      
      
      <categories>
          
          <category> 杂七杂八 </category>
          
      </categories>
      
      
    </entry>
    
    
  
  
</search>
