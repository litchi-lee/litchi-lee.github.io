<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>PixelDiT &amp; JiT总结</title>
      <link href="/2026/02/27/AIGC/JiT/"/>
      <url>/2026/02/27/AIGC/JiT/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AIGC </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flow Matching简要介绍</title>
      <link href="/2026/02/27/AIGC/Flow_Matching/"/>
      <url>/2026/02/27/AIGC/Flow_Matching/</url>
      
        <content type="html"><![CDATA[<h1 id="关于流匹配的一些直观记录">关于流匹配的一些直观记录</h1><h2 id="flow-matching-for-generative-modelinglipman-2023">Flow Matchingfor Generative Modeling（Lipman 2023）</h2><h3 id="引言">1. 引言</h3><p>之前的扩散模型如果用SDE（随机微分方程）来解释，可以定义为：</p><p><span class="math display">$$\begin{split}    dx = f(x,t)dt + g(t)dW_t\end{split}$$</span></p><p>我们训练的网络预测的是分数 <spanclass="math inline">∇<sub><em>x</em></sub>log <em>p</em><sub><em>t</em></sub>(<em>x</em>)</span>。</p><p>如果用DDPM的路线来解释，可以定义为：</p><p><span class="math display">$$\begin{split}    q(x_t \mid x_0) =\mathcal{N}\bigl(x_t;\;\sqrt{\bar\alpha_t}\,x_0,\;(1-\bar\alpha_t)\,I\bigr)\end{split}$$</span></p><p>我们训练的去噪网络预测的是噪声<spanclass="math inline"><em>ϵ</em><sub><em>t</em></sub></span>或者<spanclass="math inline"><em>x</em><sub><em>t</em></sub></span>/<spanclass="math inline"><em>x</em><sub>0</sub></span>。</p><p>流匹配的思路：<strong>我们不再学习分数或者噪声，而是直接学习概率路径上的速度场（velocityfield）。</strong>即：<span class="math inline">$\frac{dx}{dt} =v_\theta(x,t)$</span></p><p>关于这里的直观理解可以参考<ahref="https://zhuanlan.zhihu.com/p/11228697012">王峰的知乎博客</a>。</p><h3 id="flow-matching">2. Flow Matching</h3><p>这篇论文的核心贡献在于提出了一种名为FlowMatching的新框架，用于高效且无模拟（simulation-free）地训练<strong>连续标准化流（ContinuousNormalizing Flows，CNFs）</strong>。假设我们有一个未知的数据分布<spanclass="math inline"><em>q</em>(<em>x</em><sub>1</sub>)</span>，我们的目标是学习一个时间相关的向量场（VectorField，也可以叫做速度场）<spanclass="math inline"><em>v</em><sub><em>t</em></sub>(<em>x</em>; <em>θ</em>)</span>，使得从简单的噪声<spanclass="math inline"><em>p</em><sub>0</sub>(<em>x</em>)</span>（例如标准高斯分布<spanclass="math inline">𝒩(0, <em>I</em>)</span>出发，经过该向量场驱动的常微分方程流动后，能够在<spanclass="math inline"><em>t</em> = 1</span>时拟合数据分布，即<spanclass="math inline"><em>p</em><sub>1</sub>(<em>x</em>) ≈ <em>q</em>(<em>x</em>)</span>。</p><h4 id="理想情况">2.1. 理想情况</h4><p>假设存在一个理想的、我们期望目标模型去拟合的边缘概率路径<spanclass="math inline"><em>p</em><sub><em>t</em></sub>(<em>x</em>)</span>，以及生成该路径的理想目标向量场<spanclass="math inline"><em>u</em><sub><em>t</em></sub>(<em>x</em>)</span>，那么最直观的流匹配损失函数可以定义为神经网络<spanclass="math inline"><em>v</em><sub><em>t</em></sub></span>和目标向量场<spanclass="math inline"><em>u</em><sub><em>t</em></sub></span>之间的均方误差：</p><p><span class="math display">$$\begin{split}    \mathcal{L}_{FM}(\theta) = \mathbb{E}_{t, p_t(x)} ||v_t(x) -u_t(x)||^2\end{split}$$</span></p><p>但是显然，这个目标函数是<strong>完全不可解（Intractable）</strong>的，因为我们根本不知道宏观的概率路径<spanclass="math inline"><em>p</em><sub><em>t</em></sub>(<em>x</em>)</span>是什么样子的，更不用说对应的理想目标向量场<spanclass="math inline"><em>u</em><sub><em>t</em></sub>(<em>x</em>)</span>了。</p><h4id="条件概率路径与边缘化constructing-marginals-from-conditionals">2.2.条件概率路径与边缘化（Constructing Marginals from Conditionals）</h4><p>为了使目标变得可计算，我们可以先不要直接去想整个数据是如何流动的，我们先关注<strong>单个数据样本</strong><spanclass="math inline"><em>x</em><sub>1</sub></span>是怎么由噪声产生的，给定一个数据点<spanclass="math inline"><em>x</em><sub>1</sub></span>，我们可以定义一个条件概率路径<spanclass="math inline"><em>p</em><sub><em>t</em></sub>(<em>x</em> ∣ <em>x</em><sub>1</sub>)</span>，它描述了从噪声到数据点的演化过程，可以自然的得到两个边界情况：</p><ul><li>在<span class="math inline"><em>t</em> = 0</span>时，它是一个与<spanclass="math inline"><em>x</em><sub>1</sub></span>无关的分布，即<spanclass="math inline"><em>p</em><sub>0</sub>(<em>x</em>|<em>x</em><sub>1</sub>) = 𝒩(<em>x</em>|0, <em>I</em>)</span>；</li><li>在<spanclass="math inline"><em>t</em> = 1</span>时，它是一个极度集中在<spanclass="math inline"><em>x</em><sub>1</sub></span>周围的分布，即<spanclass="math inline"><em>p</em><sub>1</sub>(<em>x</em>|<em>x</em><sub>1</sub>) = 𝒩(<em>x</em>|<em>x</em><sub>1</sub>, <em>σ</em><sup>2</sup><em>I</em>)</span>。</li></ul><p>通过对所有数据点<spanclass="math inline"><em>x</em><sub>1</sub> ∼ <em>q</em>(<em>x</em><sub>1</sub>)</span>进行边缘化（求积分），我们可以得到一个宏观的概率路径：</p><p><span class="math display">$$\begin{split}    p_t(x) = \int p_t(x|x_1)q(x_1)dx_1\end{split}$$</span></p><p>特别的，当<spanclass="math inline"><em>t</em> = 1</span>时，边缘分布<spanclass="math inline"><em>p</em><sub>1</sub>(<em>x</em>)</span>会非常接近数据分布<spanclass="math inline"><em>q</em>(<em>x</em>)</span>，即：</p><p><span class="math display">$$\begin{split}    p_1(x) = \int p_1(x|x_1)q(x_1)dx_1 \approx q(x)\end{split}$$</span></p><p>由此我们可以推导第一个核心结论：<strong>在任意时刻<spanclass="math inline"><em>t</em></span>，宏观速度场（目标向量场）<spanclass="math inline"><em>u</em><sub><em>t</em></sub>(<em>x</em>)</span>就是经过<span class="math inline"><em>x</em></span>处的所有流线的速度<spanclass="math inline"><em>u</em><sub><em>t</em></sub>(<em>x</em>|<em>x</em><sub>1</sub>)</span>的加权平均</strong>，即：</p><p><span class="math display">$$\begin{split}    \boxed{u_t(x) = \frac{\int p_t(x|x_1)q(x_1)u_t(x|x_1)dx_1}{\intp_t(x|x_1)q(x_1)dx_1} = \mathbb{E}_{p_t(x_1|x)}[u_t(x|x_1)]}\end{split}$$</span></p><p>这部分如果需要直观理解可以参考<ahref="https://zhuanlan.zhihu.com/p/1992320636459181071?share_code=j0c5sdcWAWwQ&amp;utm_psn=1995693737892917764">张晓宇的知乎博客</a>，从虚粒子流的物理过程来理解。</p><p>虽然我们已经得到了 <spanclass="math inline"><em>u</em><sub><em>t</em></sub>(<em>x</em>)</span>的表达式，但它仍然是不可计算的（包含复杂积分），直接计算<spanclass="math inline">ℒ<sub><em>F</em><em>M</em></sub>(<em>θ</em>) = 𝔼<sub><em>t</em>, <em>p</em><sub><em>t</em></sub>(<em>x</em>)</sub>||<em>v</em><sub><em>t</em></sub>(<em>x</em>) − <em>u</em><sub><em>t</em></sub>(<em>x</em>)||<sup>2</sup></span>依然时不可行的。</p><h4 id="可计算的目标引入条件流匹配conditional-flow-matchingcfm">2.3.可计算的目标：引入条件流匹配（Conditional Flow Matching，CFM）</h4><p>受到去噪得分匹配的启发，作者引入了条件流匹配，即<strong>我们不再让模型<spanclass="math inline"><em>v</em><sub><em>t</em></sub></span>去拟合宏观的速度场<spanclass="math inline"><em>u</em><sub><em>t</em></sub>(<em>x</em>)</span>，而是让它去拟合在给定样本<spanclass="math inline"><em>x</em><sub>1</sub></span>对应的条件速度场 <spanclass="math inline"><em>u</em><sub><em>t</em></sub>(<em>x</em>|<em>x</em><sub>1</sub>)</span></strong>，这样我们就得到了一个可计算的目标函数：</p><p><span class="math display">$$\begin{split}    \mathcal{L}_{CFM}(\theta) = \mathbb{E}_{t, q(x_1), p_t(x|x_1)}||v_t(x) - u_t(x|x_1)||^2\end{split}$$</span></p><p>根据我们的第一个核心结论，我们可以得到第二个核心结论：<strong>在期望意义下，CFM目标函数与原始的 FM 目标函数的梯度是完全等价的（相差一个与 <spanclass="math inline"><em>θ</em></span> 无关的常数）</strong>，即：</p><p><span class="math display">$$\begin{split}    \boxed{\nabla_\theta \mathcal{L}_{CFM}(\theta) = \nabla_\theta\mathcal{L}_{FM}(\theta)}\end{split}$$</span></p><p>这意味着在训练时，我们只需要随机抽取时间<spanclass="math inline"><em>t</em></span>、随机抽取数据样本<spanclass="math inline"><em>x</em><sub>1</sub></span>、随机抽取噪声，并计算简单的条件向量场差异，就能间接地优化宏观速度场。</p><h4 id="条件向量场的设计">2.4. 条件向量场的设计</h4><p>最后一个核心问题是：<strong>我们该如何设计条件向量场 <spanclass="math inline"><em>u</em><sub><em>t</em></sub>(<em>x</em>|<em>x</em><sub>1</sub>)</span>和条件概率路径 <spanclass="math inline"><em>p</em><sub><em>t</em></sub>(<em>x</em>|<em>x</em><sub>1</sub>)</span>呢？</strong></p><p>这里我们假设条件概率路径是均值和方差随时间变化的高斯分布：</p><p><span class="math display">$$\begin{split}    p_t(x|x_1) = \mathcal{N}(x|\mu_t(x_1), \sigma_t(x_1)^2 I)\end{split}$$</span></p><p>使用最简单的仿射变换（流映射<spanclass="math inline"><em>ψ</em><sub><em>t</em></sub></span>）将标准噪声<spanclass="math inline"><em>x</em><sub>0</sub></span>映射到<spanclass="math inline"><em>x</em></span>：</p><p><span class="math display">$$\begin{split}    x = \psi_t(x_0) = \sigma_t(x_1)x_0 + \mu_t(x_1)\end{split}$$</span></p><p>由此我们可以推导出第三个重要结论：<strong>即该高斯路径对应的唯一条件向量场的闭式解为：</strong></p><p><span class="math display">$$\begin{split}    \boxed{u_t(x|x_1) = \frac{\sigma_t'(x_1)}{\sigma_t(x_1)}(x -\mu_t(x_1)) + \mu_t'(x_1)}\end{split}$$</span></p><p>即我们只要定义了均值<spanclass="math inline"><em>μ</em><sub><em>t</em></sub>(<em>x</em><sub>1</sub>)</span>和方差<spanclass="math inline"><em>σ</em><sub><em>t</em></sub>(<em>x</em><sub>1</sub>)</span>的时间演化，就可以直接计算出对应的条件向量场，进而构造出我们的训练目标。</p><h4 id="最优传输路径optimal-transport-conditional-vfs">2.5.最优传输路径（Optimal Transport Conditional VFs）</h4><p>在上面的设计中，我们并没有对条件概率路径<spanclass="math inline"><em>p</em><sub><em>t</em></sub>(<em>x</em>|<em>x</em><sub>1</sub>)</span>进行任何限制，导致它可能会非常复杂，训练起来也可能会非常困难。为了简化问题，我们可以引入一个额外的约束：<strong>我们希望条件概率路径<spanclass="math inline"><em>p</em><sub><em>t</em></sub>(<em>x</em>|<em>x</em><sub>1</sub>)</span>是从噪声分布到数据点的最优传输路径（OptimalTransportPath）</strong>，即在所有满足边界条件的路径中，选择一个使得平均运输成本最小的路径，即基于最优传输（OptimalTransport，OT）的条件向量场。</p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20260227153115385.png" width="30%" /></p><p>扩散模型中，我们是通过逐步添加噪声的方式来进行传输的，这里的路径根据不同的噪声调度函数（noiseschedule）会有不同的形状。在这里我们不妨考虑一种最简单的传输路径，即均值和标准差随时间线性变化：</p><p><span class="math display">$$\begin{split}    \mu_t(x_1) = t x_1, \quad \sigma_t(x) = 1 - (1 - \sigma_{min})t\end{split}$$</span></p><p>带入之前的条件向量场闭式解，我们可以得到对应的OT条件向量场：</p><p><span class="math display">$$\begin{aligned}    u_t(x|x_1) &amp;= \frac{\sigma_{min} - 1}{1 - (1 - \sigma_{min})t}(x- t x_1) + x_1 \\    &amp;= \frac{\sigma_{min} - 1}{1 - (1 - \sigma_{min})t}x + \left(1 -\frac{\sigma_{min} - 1}{1 - (1 - \sigma_{min})t}\right)x_1 \\    &amp;= x_1 - (1-\sigma_{min})x_0\end{aligned}$$</span></p><p>由此可以得到最终的OT条件流匹配（OT-CFM）目标函数：</p><p><span class="math display">$$\begin{split}    \mathcal{L}_{CFM}(\theta) = \mathbb{E}_{t, q(x_1), p(x_0)}||v_t(\psi_t(x_0)) - (x_1 - (1 - \sigma_{min})x_0)||^2\end{split}$$</span></p><p>从OT的条件向量场可以看出，相比于扩散模型复杂且随机的采样轨迹，OT映射对应的是两个高斯分布之间的最优传输位移映射，<strong>在OT路径下，粒子始终以恒定的速度沿直线轨迹进行移动</strong>，这极大降低了神经网络的回归难度，从而实现了更快的训练和更高效的生成采样。</p><h2 id="rectified-flowliu-2023">Rectified Flow（Liu 2023）</h2><h3 id="引言-1">1.引言</h3><p>这篇工作和FM在基本目标函数上非常相似（几乎是同期工作），但它引入了一个新的概念：<strong>校正（Rectification）</strong>，这是本文的核心贡献，基本的FM通常只进行一次训练（1-RectifiedFlow），而本论文提出了迭代应用Rectify操作的Reflow算法：用训练好的1-RectifiedFlow模型来生成样本，然后用这些样本来训练一个新的2-RectifiedFlow模型，如此迭代下去，最终得到一个性能更好的模型。</p><p>生成模型的目标是学习从已知分布到未知分布的映射关系，在实践中可以使用ODE模型来建模数据的流动，但是一般情况下的ODE的概率路径都是弯曲的，这增加了模型的训练与采样难度，而RectifiedFlow的核心思想是通过重构样本，使<strong>点对之间概率路径尽可能走直线</strong>，提升整体训练与推理的效率。</p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/rectified_flow.png" width="60%" /></p><h3 id="rectified-flow">2. Rectified Flow</h3><h4 id="学习因果流1-rectified-flow">2.1. 学习因果流（1-RectifiedFlow）</h4><p>假设我们有两个分布的样本<spanclass="math inline"><em>X</em><sub>0</sub> ∼ <em>π</em><sub>0</sub></span>和<spanclass="math inline"><em>X</em><sub>1</sub> ∼ <em>π</em><sub>1</sub></span>，我们使用最简单的连接方式：</p><p><span class="math display">$$\begin{split}    X_t = tX_1 + (1-t)X_0, \quad t \in [0, 1]\end{split}$$</span></p><p>这个过程对应的速度向量场为：</p><p><span class="math display">$$\begin{split}    u_t(x) = \frac{dX_t}{dt} = X_1 - X_0\end{split}$$</span></p><p>我们学习一个仅依赖于当前位置<spanclass="math inline"><em>X</em><sub><em>t</em></sub></span>和时间<spanclass="math inline"><em>t</em></span>的速度场<spanclass="math inline"><em>d</em><em>Z</em><sub><em>t</em></sub> = <em>v</em>(<em>Z</em><sub><em>t</em></sub>, <em>t</em>)<em>d</em><em>t</em></span>，在这里我们想让漂移项尽可能的以速度<spanclass="math inline"><em>X</em><sub>1</sub> − <em>X</em><sub>0</sub></span>使数据点从<spanclass="math inline"><em>π</em><sub>0</sub></span>流向<spanclass="math inline"><em>π</em><sub>1</sub></span>，因此我们可以定义一个简单的目标函数：</p><p><span class="math display">$$\begin{split}    \min_v \int_0^1 E[||(X_1 - X_0) - v(X_t, t)||^2] dt\end{split}$$</span></p><p>该过程的理论最优解是条件期望：</p><p><span class="math display">$$\begin{split}    v^X(z, t) = E[X_1 - X_0 | X_t = z]\end{split}$$</span></p><p>这个过程被称作Rectification（矫正，或者Rewiring），因为不同的线性插值路径可能会有交叉，导致同一个位置<spanclass="math inline"><em>z</em></span>出现多个速度方向，条件期望的作用是将这些冲突的速度方向进行平均，从而重排路径解决路径交叉问题。<strong>通过这种方式，给定一个<spanclass="math inline"><em>t</em></span>和一个位置<spanclass="math inline"><em>z</em></span>，我们可以得到一个唯一的速度方向，这样可以保证从原始分布到目的分布的曲线必不可能会相交。</strong></p><p>正如下图，在没有Rectification的情况下，数据点的流动路径会出现交叉，导致同一个位置<spanclass="math inline"><em>z</em></span>有多个速度方向；而在Rectification之后，所有路径都被重排成了不相交的形式，每个位置<spanclass="math inline"><em>z</em></span>都有一个唯一的速度方向，尽管这些路径不再是直线。</p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/rectified.png" width="80%" /></p><p>这部分观察数据的流动可以参考<ahref="https://rectifiedflow.github.io/blog/2024/intro/">RectifiedFlow原博客</a>。</p><h4 id="迭代校正reflow">2.2. 迭代校正（Reflow）</h4><p>通过一次训练，我们可以初步得到训练场<spanclass="math inline"><em>v</em><sub>1</sub></span>，以及生成的配对样本<spanclass="math inline">{<em>Z</em><sub>0</sub>, <em>Z</em><sub>1</sub>}</span>，我们可以使用这些样本来训练一个新的模型<spanclass="math inline"><em>v</em><sub>2</sub></span>，以此类推，我们可以得到一个迭代的Reflow算法：</p><ol type="1"><li>Step 1：训练1-Rectified Flow得到速度场<spanclass="math inline"><em>v</em><sub>1</sub></span>;</li><li>Step 2：使用<spanclass="math inline"><em>v</em><sub>1</sub></span>生成新的配对样本数据集<spanclass="math inline">{<em>Z</em><sub>0</sub>, <em>Z</em><sub>1</sub>}</span>;</li><li>Step 3：使用新的配对样本数据集<spanclass="math inline">{<em>Z</em><sub>0</sub>, <em>Z</em><sub>1</sub>}</span>重新进行最小二乘训练，得到2-RectifiedFlow：</li></ol><p><span class="math display">$$\begin{split}    \min_{v_2} \mathbb{E} [\|(Z_1 - Z_0) - v_2(tZ_1 + (1-t)Z_0, t)\|^2]\end{split}$$</span></p><p>由于新数据集中的<spanclass="math inline"><em>Z</em><sub>0</sub></span>和<spanclass="math inline"><em>Z</em><sub>1</sub></span>已经是由同一个ODE产生的确定性对应关系，线性插值路径几乎不再交叉，因此<spanclass="math inline"><em>v</em><sub>2</sub></span>学习到的轨迹会变得极其平直。</p>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AIGC </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>小红书实习landing记录</title>
      <link href="/2026/02/24/SSR/xhs_landing/"/>
      <url>/2026/02/24/SSR/xhs_landing/</url>
      
        <content type="html"><![CDATA[<h1 id="小红书推荐系统实习">小红书推荐系统实习</h1><p>组里的大方向：<strong>如何在精排阶段高效且准确地利用长序列信息？</strong></p><ul><li>模型重构：直接改精排主模型结构，相对黑盒；</li><li>长序列中的Top-K检索：选择Top-K条最重要的历史行为，就是ESU/GSU类方案的核心；</li><li>长序列表征学习：将用户的长序列行为压缩为多个兴趣向量；</li><li>生成式推荐：预测用户下一个兴趣状态（生成用户下一个兴趣状态），生成打分向量；</li></ul><h2 id="模型重构">模型重构</h2><ul><li>训练稳定性&amp;工程优化：分层学习率，梯度裁剪，PS优化等</li><li>目标函数重构：ESMM loss，loss权重调整</li><li>模型容量扩展：参数规模、TA容量</li><li>模型结构创新：Mixer改造、添加Globel Gate</li></ul><h2 id="section"></h2>]]></content>
      
      
      <categories>
          
          <category> SSR </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>B站Zomi老师科普课程</title>
      <link href="/2026/01/07/LLM/zomi_lesson/"/>
      <url>/2026/01/07/LLM/zomi_lesson/</url>
      
        <content type="html"><![CDATA[<h1 id="b站zomi老师科普课程">B站Zomi老师科普课程</h1><h2 id="生成式推荐">1. 生成式推荐</h2><p><strong>传统推荐</strong></p><p>推荐系统最重要2个阶段： +召回：根据用户部分特征，从海量物品库中快速召回一小部分用户潜在感兴趣的物品（速度要快）；+排序：可以融入较多特征，使用复杂模型，来精准地做个性化推荐（结果精准）。</p><p><strong>红利期</strong></p><p>DLRM（Deep Learning RecommenderModel）时代，推荐系统进入红利期。大规模深度学习模型在推荐系统中取得了显著效果，推动了推荐技术的发展。</p><p>google的wide&amp;deep模型讨论利用深度学习模型进行推荐系统的CTR预测。</p><p><strong>瓶颈期</strong></p><p>Scale-Out DLRM：TB 级别规模 DLRM在线推理成本、时延增加，提出分布式推理架构；</p><p>Scaling Laws DLRM：DLRM 架构下 TB级别模型精度基本饱和，继续提升依赖于新架构的出现。</p><p><strong>生成式推荐</strong></p><p>采用 Transformer 架构作为推荐，有两个天然的特点相吻合： +长序列支持： Transformer擅长处理长序列数据，能够捕捉用户行为中的长期依赖关系，这对于推荐系统来说非常重要，因为用户的兴趣和行为往往是多样且复杂的。+ 有序生成： Transformer可以生成有序的输出序列，这使得它能够根据用户的历史行为生成个性化的推荐列表。</p><figure><img src="https://arxiv.org/abs/2402.17152"alt="Actions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations" /><figcaption aria-hidden="true">Actions Speak Louder than Words:Trillion-Parameter Sequential Transducers for GenerativeRecommendations</figcaption></figure><p>生成式推荐新范式，将推荐系统的主要任务（召回+排序）转化为生成模型框架内的序列任务。HSTU Encoder 架构：使用PointWise Attention 代替 Softmax Attention机制，提升模型在推荐任务中的表现。</p><h2 id="gpt原理">2. GPT原理</h2><h3 id="gpt家族">GPT家族</h3>]]></content>
      
      
      <categories>
          
          <category> LLM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>特殊技巧算法题</title>
      <link href="/2025/12/25/Algo/others/"/>
      <url>/2025/12/25/Algo/others/</url>
      
        <content type="html"><![CDATA[<h1 id="例题">例题</h1><h2 id="只出现一次的数字">1. 只出现一次的数字</h2><p><a href="https://leetcode.cn/problems/single-number/">LeetCodeHot100: 136. 只出现一次的数字</a></p><p><strong>简要描述</strong>：给定一个非空整数数组，除了某个元素只出现一次以外，其余每个元素均出现两次。找出那个只出现了一次的元素。</p><p><strong>解题关键</strong>：这道题的解法比较巧妙，我们可以利用异或运算的性质来解决这个问题。异或运算有以下几个重要性质：</p><ol type="1"><li>任何数与 0 做异或运算，结果仍然是原来的数：a ^ 0 = a；</li><li>任何数与其自身做异或运算，结果是 0：a ^ a = 0；</li><li>异或运算满足交换律和结合律。</li></ol><p>因此，我们可以将数组中的所有元素进行异或运算，成对出现的元素会相互抵消，最终剩下的就是那个只出现一次的元素。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">singleNumber</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> result = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> num : nums) &#123;</span><br><span class="line">        result ^= num;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="多数元素">2. 多数元素</h2><p><a href="https://leetcode.cn/problems/majority-element/">LeetCodeHot100: 169. 多数元素</a></p><p><strong>简要描述</strong>：给定一个大小为 n的数组，找到其中的多数元素。多数元素是指在数组中出现次数大于 ⌊ n/2 ⌋的元素。</p><p><strong>解题关键</strong>：如果不考虑时间复杂度和空间复杂度的要求，这题其实有很多种解法，比如使用哈希表统计每个元素的出现次数，或者先对数组进行排序，然后取中间位置的元素。但是这里要求时间复杂度为O(n)，空间复杂度为 O(1)，因此我们需要使用更巧妙的方法。</p><p>我们可以使用摩尔投票算法（Boyer-Moore算法）来解决这个问题。该算法的核心思想是通过抵消的方式找到可能的多数元素。具体步骤如下：</p><ol type="1"><li>初始化一个候选元素 candidate 和一个计数器 count，初始时 count 为0；</li><li>逐个遍历数组中的元素，对于每个元素 num：<ul><li>如果 count 为 0，则将 candidate 更新为当前元素 num；</li><li>如果当前元素 num 等于 candidate，则将 count 加 1；</li><li>否则，将 count 减 1；</li></ul></li><li>遍历结束后，candidate即为可能的多数元素。由于题目保证多数元素一定存在，因此直接返回 candidate即可。</li></ol><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">majorityElement</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> candidate = <span class="number">0</span>;</span><br><span class="line">    <span class="type">int</span> count = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> num : nums) &#123;</span><br><span class="line">        <span class="keyword">if</span> (count == <span class="number">0</span>) &#123;</span><br><span class="line">            candidate = num;</span><br><span class="line">        &#125;</span><br><span class="line">        count += (num == candidate) ? <span class="number">1</span> : <span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> candidate;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="颜色分类">3. 颜色分类</h2><p><a href="https://leetcode.cn/problems/sort-colors/">LeetCode Hot100:75. 颜色分类</a></p><p><strong>简要描述</strong>：给定一个包含红色、白色和蓝色、共 n个元素的数组，原地对它们进行排序，使得相同颜色的元素相邻，并按照红色、白色、蓝色顺序排列。我们使用整数0、1 和 2 分别表示红色、白色和蓝色。</p><p><strong>解题关键</strong>：这道题可以使用三指针的方法来解决。我们维护三个指针：low、mid和 high。low 指向下一个 0 应该放置的位置，mid 用于遍历数组，high指向下一个 2 应该放置的位置。具体步骤如下：</p><ol type="1"><li>初始化 low 和 mid 为 0，high 为数组的最后一个索引；</li><li>当 mid 小于等于 high 时，进行以下操作：<ul><li>如果 nums[mid] == 0，则将 nums[low] 和 nums[mid] 交换，low 和 mid都加 1；</li><li>如果 nums[mid] == 1，则 mid 加 1；</li><li>如果 nums[mid] == 2，则将 nums[mid] 和 nums[high] 交换，high 减1；</li></ul></li><li>重复上述步骤直到 mid 超过 high。</li></ol><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">sortColors</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> low = <span class="number">0</span>, mid = <span class="number">0</span>;</span><br><span class="line">    <span class="type">int</span> high = nums.<span class="built_in">size</span>() - <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (mid &lt;= high) &#123;</span><br><span class="line">        <span class="keyword">if</span> (nums[mid] == <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="built_in">swap</span>(nums[low], nums[mid]);</span><br><span class="line">            low++;</span><br><span class="line">            mid++;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (nums[mid] == <span class="number">1</span>) &#123;</span><br><span class="line">            mid++;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123; <span class="comment">// nums[mid] == 2</span></span><br><span class="line">            <span class="built_in">swap</span>(nums[mid], nums[high]);</span><br><span class="line">            high--;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="下一个排列">4. 下一个排列</h2><p><a href="https://leetcode.cn/problems/next-permutation/">LeetCodeHot100: 31. 下一个排列</a></p><p><strong>简要描述</strong>：实现获取下一个排列的函数，算法需要将给定数字序列重新排列成字典序中下一个更大的排列。如果不存在下一个更大的排列，则将数字重新排列成最小的排列（即升序排列）。</p><p><strong>解题关键</strong>：这道题需要这样思考，我们需要找到下一个排列，该排列需要比当前排列大，但又是所有比当前排列大的排列中最小的那个，即变大的幅度需要尽可能小。具体地：</p><ol type="1"><li><p>我们需要将一个左边的「较小数」与一个右边的「较大数」交换，以能够让当前排列变大，从而得到下一个排列。</p></li><li><p>同时我们要让这个「较小数」尽量靠右，而「较大数」尽可能小。当交换完成后，「较大数」右边的数需要按照升序重新排列。这样可以在保证新排列大于原来排列的情况下，使变大的幅度尽可能小。</p></li></ol><p>因此可以构造出算法为：</p><ol type="1"><li><p>首先从后向前查找第一个顺序对 <code>(i,i+1)</code>，满足<code>a[i]&lt;a[i+1]</code>。这样「较小数」即为 <code>a[i]</code>。此时<code>[i+1,n)</code> 必然是下降序列。</p></li><li><p>如果找到了顺序对，那么在区间 <code>[i+1,n)</code>中从后向前查找第一个元素 <code>j</code> 满足<code>a[i]&lt;a[j]</code>。这样「较大数」即为<code>a[j]</code>。</p></li><li><p>交换 <code>a[i]</code> 与 <code>a[j]</code>，此时可以证明区间<code>[i+1,n)</code> 必为降序。我们可以直接使用双指针反转区间<code>[i+1,n)</code> 使其变为升序，而无需对该区间进行排序。</p></li></ol><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">nextPermutation</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> i = nums.<span class="built_in">size</span>() - <span class="number">2</span>;</span><br><span class="line">        <span class="keyword">while</span>(i&gt;=<span class="number">0</span> &amp;&amp; nums[i]&gt;=nums[i<span class="number">+1</span>])&#123;</span><br><span class="line">            i--;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span>(i&gt;=<span class="number">0</span>)&#123;</span><br><span class="line">            <span class="type">int</span> j = nums.<span class="built_in">size</span>() - <span class="number">1</span>;</span><br><span class="line">            <span class="keyword">while</span>(j&gt;i &amp;&amp; nums[j]&lt;=nums[i])&#123;</span><br><span class="line">                j--;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="built_in">swap</span>(nums[i], nums[j]);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">reverse</span>(nums.<span class="built_in">begin</span>()+i<span class="number">+1</span>, nums.<span class="built_in">end</span>());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h2 id="寻找重复数">5. 寻找重复数</h2><p><ahref="https://leetcode.cn/problems/find-the-duplicate-number/">LeetCodeHot100: 287. 寻找重复数</a></p><p><strong>简要描述</strong>：给定一个包含 n + 1 个整数的数组nums，其数字都在 1 到 n 之间（包括 1 和n），可知至少存在一个重复的整数。假设只有一个重复的整数，找出这个重复的数。</p><p><strong>解题关键</strong>：这道题最取巧的做法可以使用快慢指针（Floyd判圈算法）来解决。我们可以将数组视为一个链表，其中每个元素的值表示下一个节点的索引（由于这里数组的长度最多只有n+1，因此不会越位）。由于数组中存在重复的数字，因此链表中必然存在环。我们可以参考<ahref="https://leetcode.cn/problems/linked-list-cycle-ii/description/">142.环形链表II</a>的做法通过以下步骤找到重复的数字：</p><ol type="1"><li>使用快慢指针找到相遇点：初始化两个指针 slow 和 fast，slow每次移动一步，fast 每次移动两步。当它们相遇时，说明链表中存在环。</li><li>找到环的入口节点：将其中一个指针重新指向链表的头节点，然后两个指针每次都移动一步。当它们再次相遇时，相遇点即为环的入口节点，也就是重复的数字。</li></ol><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">findDuplicate</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> slow = nums[<span class="number">0</span>];</span><br><span class="line">    <span class="type">int</span> fast = nums[<span class="number">0</span>];</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 找到相遇点</span></span><br><span class="line">    <span class="keyword">do</span> &#123;</span><br><span class="line">        slow = nums[slow];</span><br><span class="line">        fast = nums[nums[fast]];</span><br><span class="line">    &#125; <span class="keyword">while</span> (slow != fast);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 找到环的入口节点</span></span><br><span class="line">    slow = nums[<span class="number">0</span>];</span><br><span class="line">    <span class="keyword">while</span> (slow != fast) &#123;</span><br><span class="line">        slow = nums[slow];</span><br><span class="line">        fast = nums[fast];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> slow;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 刷题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Algorithm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>贪心算法题</title>
      <link href="/2025/12/16/Algo/greedy/"/>
      <url>/2025/12/16/Algo/greedy/</url>
      
        <content type="html"><![CDATA[<h1 id="贪心算法">贪心算法</h1><p>贪心算法是一种在每一步选择中都采取在当前状态下最好或最优（即最有利）的选择，从而希望导致结果是全局最好或最优的算法。贪心算法在某些问题上能够得到最优解，但并不适用于所有问题。一般使用场景包括：</p><ul><li>最小生成树问题（如Kruskal和Prim算法）</li><li>单源最短路径问题（如Dijkstra算法）</li><li>活动选择问题</li><li>硬币找零问题</li><li>任务调度问题</li></ul><h2 id="例题">例题</h2><h3 id="跳跃游戏">1. 跳跃游戏</h3><p><a href="https://leetcode.cn/problems/jump-game/">LeetCode Hot100:55. 跳跃游戏</a></p><p><strong>简要描述</strong>： 给定一个非负整数数组<code>nums</code>，你最初位于数组的第一个位置。数组中的每个元素代表你在该位置可以跳跃的最大长度。判断你是否能够到达最后一个位置。</p><p><strong>解题关键</strong>： 使用贪心算法，维护一个变量<code>maxReach</code>，表示当前能够到达的最远位置。遍历数组时，更新<code>maxReach</code>，如果在某个位置 <code>i</code> 时，<code>i</code>超过了 <code>maxReach</code>，则无法到达最后一个位置；而如果某个位置<code>i</code> 能够更新 <code>maxReach</code>到最后一个位置或更远，则可以到达最后一个位置。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">bool</span> <span class="title">canJump</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> n = nums.<span class="built_in">size</span>(), maxReach = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;n;i++)&#123;</span><br><span class="line">            <span class="keyword">if</span>(i&gt;maxReach) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">            maxReach = <span class="built_in">max</span>(maxReach, i+nums[i]);</span><br><span class="line">            <span class="keyword">if</span>(maxReach&gt;=n<span class="number">-1</span>) <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="跳跃游戏-ii">2. 跳跃游戏 II</h3><p><a href="https://leetcode.cn/problems/jump-game-ii/">LeetCode Hot100:45. 跳跃游戏 II</a></p><p><strong>简要描述</strong>： 给定一个非负整数数组<code>nums</code>，你最初位于数组的第一个位置。数组中的每个元素代表你在该位置可以跳跃的最大长度。你的目标是使用最少的跳跃次数到达最后一个位置。</p><p><strong>解题关键</strong>：这个变体还是使用贪心算法，需要维护两个变量 <code>currentEnd</code> 和<code>farthest</code>。<code>currentEnd</code>表示使用当前跳跃步数能到达的跳跃边界，<code>farthest</code>表示在当前跳跃范围内能够到达的最远位置。当遍历到 <code>currentEnd</code>时，说明需要多进行一次跳跃，更新 <code>currentEnd</code> 为<code>farthest</code>，并增加跳跃次数。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">jump</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> n = nums.<span class="built_in">size</span>();</span><br><span class="line">        <span class="keyword">if</span>(n&lt;=<span class="number">1</span>) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        <span class="type">int</span> jumps = <span class="number">0</span>, currentEnd = <span class="number">0</span>, farthest = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;n<span class="number">-1</span>;i++)&#123;</span><br><span class="line">            farthest = <span class="built_in">max</span>(farthest, i+nums[i]);</span><br><span class="line">            <span class="keyword">if</span>(i==currentEnd)&#123;</span><br><span class="line">                jumps++;</span><br><span class="line">                currentEnd = farthest;</span><br><span class="line">                <span class="keyword">if</span>(currentEnd&gt;=n<span class="number">-1</span>) <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> jumps;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="划分字母区间">3. 划分字母区间</h3><p><a href="https://leetcode.cn/problems/partition-labels/">LeetCodeHot100: 763. 划分字母区间</a></p><p><strong>简要描述</strong>： 给定一个字符串<code>s</code>，将字符串划分为尽可能多的片段，使得每个字母最多出现在一个片段中。返回这些片段的长度列表。</p><p><strong>解题关键</strong>：首先需要遍历一遍字符串，记录每个字符最后出现的位置。然后遍历字符串，根据当前字符最后出现的位置维护当前片段的结束位置<code>end</code>，当遍历到当前位置等于 <code>end</code>时，说明可以划分一个片段，记录其长度并更新起始位置。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">partitionLabels</span><span class="params">(string s)</span> </span>&#123;</span><br><span class="line">        <span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">lastIndex</span><span class="params">(<span class="number">26</span>, <span class="number">-1</span>)</span></span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;s.<span class="built_in">size</span>();i++)&#123;</span><br><span class="line">            lastIndex[s[i]-<span class="string">&#x27;a&#x27;</span>] = i;</span><br><span class="line">        &#125;</span><br><span class="line">        vector&lt;<span class="type">int</span>&gt; result;</span><br><span class="line">        <span class="type">int</span> start = <span class="number">0</span>, end = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;s.<span class="built_in">size</span>();i++)&#123;</span><br><span class="line">            end = <span class="built_in">max</span>(end, lastIndex[s[i]-<span class="string">&#x27;a&#x27;</span>]);</span><br><span class="line">            <span class="keyword">if</span>(i==end)&#123;</span><br><span class="line">                result.<span class="built_in">push_back</span>(end - start + <span class="number">1</span>);</span><br><span class="line">                start = end + <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="加油站">4. 加油站</h3><p><a href="https://leetcode.cn/problems/gas-station/">LeetCode 经典150:134. 加油站</a></p><p><strong>简要描述</strong>： 在一条环路上有 <code>n</code>个加油站，第 <code>i</code> 个加油站有汽油 <code>gas[i]</code>升。你有一辆油箱容量无限的汽车，从第 <code>i</code> 个加油站开到第<code>i+1</code> 个加油站需要消耗汽油 <code>cost[i]</code>升。你从其中的一个加油站出发，开始时油箱为空，能否绕环路行驶一周？如果可以，返回出发的加油站编号，否则返回-1。保证每个输入只对应一个答案。</p><p><strong>解题关键</strong>： 使用贪心算法，维护两个变量<code>SumofGas</code> 和<code>SumofCost</code>，分别表示总的油量和总的油耗。遍历加油站时，更新这两个变量，如果在某个加油站<code>i</code>时，<code>SumofGas&lt;SumofCost</code>，说明从之前的起点到达不了加油站<code>i+1</code>，需要将起点更新为 <code>i+1</code>来判断能否行驶一周。这里隐含一个结论，如果从加油站 <code>x</code>出发，每经过一个加油站就加一次油（包括起始加油站），最后一个可以到达的加油站是<code>y</code>（不妨设 <code>x&lt;y</code>），那么从 <code>x,y</code>之间的任何一个加油站出发，都无法到达加油站 <code>y</code>的下一个加油站。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">canCompleteCircuit</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; gas, vector&lt;<span class="type">int</span>&gt;&amp; cost)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> n = gas.<span class="built_in">size</span>();</span><br><span class="line">        <span class="type">int</span> GasSum, CostSum;</span><br><span class="line">        <span class="type">int</span> i=<span class="number">0</span>, cnt;</span><br><span class="line">        <span class="keyword">while</span>(i&lt;n)&#123;</span><br><span class="line">            GasSum = CostSum = <span class="number">0</span>;</span><br><span class="line">            cnt = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">while</span>(cnt&lt;n)&#123;</span><br><span class="line">                <span class="type">int</span> j = (i+cnt)%n;</span><br><span class="line">                GasSum+=gas[j];</span><br><span class="line">                CostSum+=cost[j];</span><br><span class="line">                <span class="keyword">if</span>(GasSum &lt; CostSum)&#123;</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                cnt++;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span>(cnt==n) <span class="keyword">return</span> i;</span><br><span class="line">            i += cnt<span class="number">+1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 刷题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Algorithm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>B站王树森老师推荐系统公开课</title>
      <link href="/2025/12/15/SSR/wss_lesson/"/>
      <url>/2025/12/15/SSR/wss_lesson/</url>
      
        <content type="html"><![CDATA[<h1 id="b站王树森老师推荐系统公开课">B站王树森老师推荐系统公开课</h1><p>链接: <ahref="https://b23.tv/6IS2uni">推荐系统公开课——8小时完整版，讲解工业界真实的推荐系统-哔哩哔哩</a></p><h2 id="推荐系统概况">1.推荐系统概况</h2><h3 id="消费指标短期指标">1.1.消费指标（短期指标）</h3><ul><li>点击率（CTR）:用户点击某个推荐内容的概率。<strong>点击率=点击次数/曝光次数</strong></li><li>点赞率（Like Rate）:用户对推荐内容点赞的比例。<strong>点赞率=点赞次数/点击次数</strong></li><li>收藏率（Favorite Rate）:用户收藏推荐内容的比例。<strong>收藏率=收藏次数/点击次数</strong></li><li>转化率（Conversion Rate）:用户完成特定行为（如购买、注册等）的比例。<strong>转化率=转发次数/点击次数</strong></li><li>阅读完成率（Completion Rate）:用户阅读完推荐内容的比例。**阅读完成率=完成阅读次数/点击次数*f（笔记长度）**</li></ul><h3 id="北极星指标">1.2.北极星指标</h3><ul><li>用户规模： 日活跃用户数（DAU）、月活跃用户数（MAU）</li><li>消费：人均使用推荐时长、人均阅读笔记数量</li><li>发布：发布渗透率、人均发布量</li></ul><h3 id="实验流程">1.3.实验流程</h3><ol type="1"><li>离线实验：使用历史数据进行模型训练和评估，验证模型的有效性。</li><li>在线实验（小流量A/B测试）：将用户随机分配到不同的实验组和对照组，实验组用新策略、对照组用旧策略，实时监测各组的关键指标，评估模型在实际环境中的表现。</li><li>全量上线：在小流量实验成功后，将新模型推广到所有用户，持续监测关键指标，确保模型稳定运行。</li></ol><h2 id="推荐系统链路">2.推荐系统链路</h2><p>推荐系统通常包括以下几个关键环节：</p><p><strong>召回</strong>→<strong>粗排</strong>→<strong>精排</strong>→<strong>重排</strong></p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/88aa9c2de93fe6e2b8e391aa59bfff1a.jpg" width="80%"/></p><p>粗排和精排：</p><ul><li>粗排：从海量候选内容中快速筛选出一部分可能相关的内容，通常使用简单的特征和模型，注重效率。</li><li>精排：对粗排得到的候选内容进行更细致的排序，使用复杂的特征和模型，注重准确性。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20251215153739238.png" width="80%"/></p><p>重排：做多样性抽样（比如用MMR、DPP等方法），增加推荐结果的多样性，提升用户体验。</p><h2 id="ab测试">3. A/B测试</h2><h3 id="实验设计">3.1.实验设计</h3><p>假设召回团队实现了一种 GNN召回通道，离线实验结果是正向的，那么下一步就是做线上的小流量A/B测试，考察新的召回通道对线上指标的影响。同时如果模型中有一些参数需要调优，也可以在小流量实验中进行参数搜索。</p><p>这需要对用户进行随机分桶，用哈希函数把用户 ID映射成某个区间的整数，然后把这些整数均匀随机分成b个桶。某些桶作为实验组，使用新的推荐策略，其他桶作为对照组，使用旧的推荐策略。计算每个桶的业务指标，比如DAU、人均使用时长、CTR等，如果某个实验组的指标显著优于对照组，则说明对应的策略有效，值得推全。</p><h3 id="分层实验">3.2.分层实验</h3><p>如果用户流量不够用该怎么办？信息流产品公司有很多部门和团队，如果大家都需要做A/B测试，把用户随机分成10组，一组作对照，9组做实验，那么只能同时做9组实验。</p><p><strong>分层实验：</strong>把实验分成很多层，召回、粗排、精排、重排、用户界面、广告等，采取<strong>同层互斥</strong>（同一层只能做一个实验）、<strong>跨层不互斥</strong>（不同层可以同时做实验）的原则。这样就可以大大提高实验的并发度。</p><h3 id="holdout机制">3.3.Holdout机制</h3><p>每个实验（召回、粗排、精排、重排）独立汇报对业务指标的提升，但实际业务指标的提升是各个实验的综合效果。为了避免实验之间的相互影响，可以引入Holdout机制。取10%的用户作为holdout桶，推荐系统使用剩余的用户做实验，两者的diff为整个部门的业务指标收益。</p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20251215161914999.png" width="80%"/></p><p>每个考核周期结束之后，清除holdout桶，让推全实验从90%用户扩大到100%用户。同时重新随机划分用户，得到holdout桶和实验桶，开始新一轮考核周期。</p><h3 id="实验推全和反转实验">3.4.实验推全和反转实验</h3><p>实验推全：新建一个推全层，与其他层正交。反转实验：在新的推全层上，保留一个小的反转桶，使用旧策略，长期观测新旧策略的diff。</p><h2 id="基于物品的协同过滤itemcf">4.基于物品的协同过滤（ItemCF）</h2><h3 id="基本原理">4.1.基本原理</h3><p>基于物品的协同过滤（Item-based Collaborative Filtering,ItemCF）是一种推荐系统技术，主要通过分析用户对物品的历史交互数据，来发现物品之间的相似性，从而为用户推荐与其历史偏好相似的物品。简单来说，就是通过用户的行为来找到相似物品，比如看过A物品的用户也看过B物品，那么A和B就是相似的物品。</p><h3 id="实现">4.2.实现</h3><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20251215163732904.png" width="80%"/></p><p><strong>物品相似度的计算</strong></p><p>两个物品的受众重合度越高，它们的相似度就越高。假设喜欢物品<spanclass="math inline"><em>i</em><sub>1</sub></span>的用户记作集合<spanclass="math inline"><em>W</em><sub>1</sub></span>，喜欢物品<spanclass="math inline"><em>i</em><sub>2</sub></span>的用户记作集合<spanclass="math inline"><em>W</em><sub>2</sub></span>，定义用户交集<spanclass="math inline"><em>V</em> = <em>W</em><sub>1</sub> ∩ <em>W</em><sub>2</sub></span>，则物品<spanclass="math inline"><em>i</em><sub>1</sub></span>和<spanclass="math inline"><em>i</em><sub>2</sub></span>的相似度记作：</p><p><span class="math display">$$\begin{split}sim(i_1, i_2) &amp; = \frac{|V|}{\sqrt{|W_1| \times |W_2|}} \\\end{split}$$</span></p><p>考虑用户喜欢物品的程度不同，设为 <spanclass="math inline"><em>l</em><em>i</em><em>k</em><em>e</em>(<em>v</em>, <em>i</em><sub>1</sub>)</span>。则物品<spanclass="math inline"><em>i</em><sub>1</sub></span>和<spanclass="math inline"><em>i</em><sub>2</sub></span>的相似度记作（余弦相似度）：</p><p><span class="math display">$$\begin{split}    sim(i_1, i_2) &amp; = \frac{\sum_{v \in V} like(v, i_1) \timeslike(v, i_2)}{\sqrt{\sum_{v \in W_1} like^2(v, i_1)} \times\sqrt{\sum_{v \in W_2} like^2(v, i_2)}} \\\end{split}$$</span></p><h3 id="itemcf的召回通道">4.3.ItemCF的召回通道</h3><ul><li>维护两个索引：<ul><li>用户→物品列表：记录用户最近交互过的n个物品</li><li>物品→物品列表：记录相似度最高的k个物品</li></ul></li><li>线上做召回：<ul><li>利用两个索引，每次取回nk个物品</li><li>预估用户对每个物品的兴趣分数： <span class="math display">$$\begin{split}    score(u, i) &amp; = \sum_{j \in L_u} sim(j, i) \times like(u, j) \\\end{split}$$</span></li><li>返回分数最高的100个物品，作为召回结果</li></ul></li></ul><h2 id="swing召回通道">5.Swing召回通道</h2><p>假如重合的用户是一个小圈子？</p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20251215185947471.png" width="80%"/></p><h3 id="基本原理-1">5.1.基本原理</h3><p>定义用户<spanclass="math inline"><em>u</em><sub>1</sub></span>喜欢的物品为集合<spanclass="math inline"><em>J</em><sub>1</sub></span>，用户<spanclass="math inline"><em>u</em><sub>2</sub></span>喜欢的物品为集合<spanclass="math inline"><em>J</em><sub>2</sub></span>，则用户<spanclass="math inline"><em>u</em><sub>1</sub></span>和<spanclass="math inline"><em>u</em><sub>2</sub></span>的重合度为：</p><p><span class="math display">$$\begin{split}    overlap(u_1, u_2) = |J_1 \cap J_2| \\\end{split}$$</span></p><p>用户<span class="math inline"><em>u</em><sub>1</sub></span>和<spanclass="math inline"><em>u</em><sub>2</sub></span>的重合度越高，则他们可能来自一个小圈子，需要降低他们的权重。</p><p>定义喜欢物品<spanclass="math inline"><em>i</em><sub>1</sub></span>的用户为集合<spanclass="math inline"><em>W</em><sub>1</sub></span>，喜欢物品<spanclass="math inline"><em>i</em><sub>2</sub></span>的用户集合为<spanclass="math inline"><em>W</em><sub>2</sub></span>，定义交集<spanclass="math inline"><em>V</em> = <em>W</em><sub>1</sub> ∩ <em>W</em><sub>2</sub></span>，则物品<spanclass="math inline"><em>i</em><sub>1</sub></span>和<spanclass="math inline"><em>i</em><sub>2</sub></span>的相似度为：</p><p><span class="math display">$$\begin{split}    sim(i_1, i_2) &amp; = \sum_{u_1 \in V} \sum_{u_2 \in V}\frac{1}{\alpha+overlap(u_1, u_2)} \\\end{split}$$</span></p><h3 id="总结">5.2.总结</h3><p>Swing和ItemCF的唯一区别在于物品相似度。</p><ul><li>ItemCF：两个物品重合的用户比例高，则判定两个物品相似；</li><li>Swing：额外考虑重合的用户是否来自一个小圈子，如果是，则降低他们的权重。</li></ul><h2 id="基于用户的协同过滤usercf">6.基于用户的协同过滤（UserCF）</h2><h3 id="基本原理-2">6.1.基本原理</h3><p>不同于之前基于物品相似度进行推荐的方法，UserCF通过分析用户之间的相似性来进行推荐。即如果用户A和用户B在过去有类似的行为（例如喜欢相似的物品），那么用户A可能会对用户B喜欢的其他物品感兴趣。</p><h3 id="实现-1">6.2.实现</h3><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20251216153700380.png" width="80%"/></p><p><strong>用户相似度的计算</strong></p><p>假设用户<spanclass="math inline"><em>u</em><sub>1</sub></span>喜欢的物品集合为<spanclass="math inline"><em>J</em><sub>1</sub></span>，用户<spanclass="math inline"><em>u</em><sub>2</sub></span>喜欢的物品集合为<spanclass="math inline"><em>J</em><sub>2</sub></span>，定义交集<spanclass="math inline"><em>V</em> = <em>J</em><sub>1</sub> ∩ <em>J</em><sub>2</sub></span>，则用户<spanclass="math inline"><em>u</em><sub>1</sub></span>和<spanclass="math inline"><em>u</em><sub>2</sub></span>的相似度为：</p><p><span class="math display">$$\begin{split}    sim(u_1, u_2) &amp; = \frac{|V|}{\sqrt{|J_1| \times |J_2|}} \\\end{split}$$</span></p><p>但是这样会导致一些热门物品对相似度的影响过大。比如所有人都喜欢《三体》，那么喜欢《三体》的用户之间的相似度都会被拉高，而并不是所有人都会喜欢《深度学习》，所以喜欢《深度学习》的用户之间的相似度更有意义。因此我们需要降低热门物品的权重，定义<spanclass="math inline"><em>n</em><sub><em>i</em></sub></span>为喜欢物品<spanclass="math inline"><em>i</em></span>的用户数，反映物品的热门程度：</p><p><span class="math display">$$\begin{split}    sim(u_1, u_2) &amp; = \frac{\sum_{i \in V}\frac{1}{\log(1+n_i)}}{\sqrt{|J_1| \times |J_2|}} \\\end{split}$$</span></p><h3 id="usercf的召回通道">6.3.UserCF的召回通道</h3><ul><li>维护两个索引：<ul><li>用户→用户列表：记录与该用户相似度最高的k个用户</li><li>用户→物品列表：记录用户最近交互过的n个物品</li></ul></li><li>线上做召回：<ul><li>给定用户ID，通过用户→用户列表索引，取回top-k用户</li><li>对于每个相似用户，通过用户→物品列表索引，找到用户近期感兴趣的物品列表（last-n物品）</li><li>预估用户对每个物品的兴趣分数： <span class="math display">$$\begin{split}    score(u, i) &amp; = \sum_{v \in S_u} sim(u, v) \times like(v, i) \\\end{split}$$</span></li></ul></li></ul><h2 id="矩阵补充">7.矩阵补充</h2><p>矩阵补充是向量召回中的一种最简单的方法，现在不常用，但是是后续双塔模型的基础。</p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20251216160214272.png" width="80%"/></p><h3 id="数据集">7.1.数据集</h3><p>数据集的形式：<strong>（用户ID，物品ID，兴趣分数）</strong>的集合，记作<spanclass="math inline"><em>Ω</em> = {(<em>u</em>, <em>i</em>, <em>y</em><sub><em>u</em>, <em>i</em></sub>)}</span>，其中<spanclass="math inline"><em>y</em><sub><em>u</em>, <em>i</em></sub></span>表示用户<spanclass="math inline"><em>u</em></span>对物品<spanclass="math inline"><em>i</em></span>的兴趣分数。</p><h3 id="训练">7.2.训练</h3><p>把用户ID、物品ID分别映射成低维向量： - 第u号用户：向量<spanclass="math inline"><em>a</em><sub><em>u</em></sub></span> -第i号物品：向量<spanclass="math inline"><em>b</em><sub><em>i</em></sub></span></p><p>求解优化问题，得到参数<spanclass="math inline"><em>A</em></span>和<spanclass="math inline"><em>B</em></span>：</p><p><span class="math display">$$\begin{split}    \min_{A, B} &amp; \sum_{(u, i, y_{u,i}) \in \Omega} (y_{u,i} - a_u^Tb_i)^2 \\\end{split}$$</span></p><h3 id="缺点">7.3.缺点</h3><ul><li>仅用ID Embedding作为特征，信息量有限，没有利用物品、用户属性</li><li>负样本的选取方式不对</li><li>训练模型的方法不好</li></ul><h3 id="最近邻查找">最近邻查找</h3><p>衡量最近邻的标准： - 欧氏距离最小（L2距离） -向量内积最大（内积相似度） -向量夹角余弦最大（余弦相似度）工业界最常用</p><h2 id="双塔模型">8.双塔模型</h2><p>双塔模型是目前工业界主流的向量召回模型，广泛应用于推荐系统中。它通过分别对用户和物品进行编码，将它们映射到一个共同的向量空间中，从而实现高效的相似度计算和召回。可以看作是矩阵补充的升级版。</p><h3 id="模型结构">8.1.模型结构</h3><p>计算用户和物品的向量表示：</p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20251216163343175.png" width="80%"/></p><p>双塔模型不再使用内积相似度，而是使用<strong>向量余弦相似度</strong>来衡量用户和物品之间的相似性：</p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20251216163515907.png" width="80%"/></p><h3 id="训练方法">8.2.训练方法</h3><ul><li>Pointwise方法：独立看待每个正样本和负样本，做简单的二元分类</li><li>Pairwise方法：成对看待正样本和负样本，具体可查阅<ahref="https://dl.acm.org/doi/abs/10.1145/3394486.3403305">Embedding-basedRetrieval in Facebook Search</a></li><li>Listwise方法：每次取一个正样本、多个负样本，具体可查阅<ahref="https://dl.acm.org/doi/abs/10.1145/3298689.3346996">Sampling-Bias-CorrectedNeural Modeling for Large Corpus Item Recommendation</a></li></ul><p><strong>PointWise训练</strong></p><p>把召回看作是二元分类任务，对于正样本，激励余弦相似度输出+1；对于负样本，激励余弦相似度输出-1。使用二元交叉熵损失函数。控制正负样本数量为1：2到1：3.</p><p><strong>PairWise训练</strong></p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20251216164935681.png" width="80%"/></p><p>基本想法：鼓励<spanclass="math inline">cos (<em>a</em>, <em>b</em><sup>+</sup>)</span>大于<spanclass="math inline">cos (<em>a</em>, <em>b</em><sup>−</sup>)</span>，即用户向量和正样本物品向量的余弦相似度大于用户向量和负样本物品向量的余弦相似度。使用<strong>Triplet Hinge Loss</strong> ：</p><p><span class="math display">$$\begin{split}    L(a, b^+, b^-) &amp; =  \max(0, cos(a, b^-) + \gamma -  cos(a, b^+))\\\end{split}$$</span></p><p>也可以用 <strong>Triplet Logistic Loss</strong> ：</p><p><span class="math display">$$\begin{split}    L(a, b^+, b^-) &amp; =  \log(1+\exp[\sigma(cos(a, b^-) - cos(a,b^+))]) \\\end{split}$$</span></p><p><strong>ListWise训练</strong></p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20251216170052175.png" width="80%"/></p><p>一条数据包含一个用户向量<spanclass="math inline"><em>a</em></span>，一个正样本物品向量<spanclass="math inline"><em>b</em><sup>+</sup></span>，多个负样本物品向量<spanclass="math inline">{<em>b</em><sub>1</sub><sup>−</sup>, <em>b</em><sub>2</sub><sup>−</sup>, ..., <em>b</em><sub><em>N</em></sub><sup>−</sup>}</span>。我们需要鼓励<spanclass="math inline">cos (<em>a</em>, <em>b</em><sup>+</sup>)</span>尽量大，鼓励<spanclass="math inline">cos (<em>a</em>, <em>b</em><sub><em>i</em></sub><sup>−</sup>)</span>尽量小。</p><h3 id="正负样本">8.3.正负样本</h3><p>召回的目标是快速找到用户可能感兴趣的物品，所以召回的负样本应该是用户不感兴趣的物品。</p><h4 id="正样本">8.3.1.正样本</h4><p>曝光且有点击的用户–物品二元组。但是在实际中一般只有少部分物品占据大部分点击，这会导致正样本大多是热门物品，对冷门物品的推荐效果不好。解决方法是过采样冷门物品，或降采样热门物品。</p><h4 id="负样本">8.3.2.负样本</h4><p><strong>简单负样本</strong>：未被召回的物品。</p><ul><li><em>抽样方法一：从全体样本中选择。</em>未被召回的物品大概率是用户不感兴趣的，其数量基本约等于全体样本。因此可以直接从全体样本中进行抽样，但是不能直接使用均匀抽样（因为正样本大多是热门物品，如果均匀抽样产生负样本，负样本大多是冷门物品），<strong>负样本抽样概率与热门程度（点击次数）正相关</strong>，<spanclass="math inline"><em>抽</em><em>样</em><em>概</em><em>率</em> ∝ (<em>点</em><em>击</em><em>次</em><em>数</em>)<sup>0.75</sup></span><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20251219165543764.png" width="60%"/></li><li><em>抽样方法二：Batch内负样本。</em>假设一个batch中有n个正样本，那么一个用户和n-1个物品可以组成负样本，这样这个batch内一共有n(n-1)个负样本。但是这样存在一个问题，一个物品出现在bacth内的概率<spanclass="math inline">∝</span>(点击次数)，这样热门物品更容易出现在一个batch内，也更容易被选为负样本，而不同于对全体物品的<spanclass="math inline"><em>抽</em><em>样</em><em>概</em><em>率</em> ∝ (<em>点</em><em>击</em><em>次</em><em>数</em>)<sup>0.75</sup></span>，这里热门物品成为负样本的概率过大，需要修正一下。训练的时候，在计算余弦相似度的时候，对热门物品的相似度进行衰减，<spanclass="math inline">cos (<em>a</em>, <em>b</em><sub><em>i</em></sub>) − log <em>p</em><sub><em>i</em></sub></span>，具体可参考<ahref="https://dl.acm.org/doi/abs/10.1145/3298689.3346996">Sampling-Bias-CorrectedNeural Modeling for Large Corpus Item Recommendation</a>。</li></ul><p><strong>困难负样本</strong>：被粗排淘汰的物品（训练比较困难）以及精排分数靠后的物品（训练非常困难）。</p><p>工业界常用的做法是混合几种负样本，比如一半负样本是简单负样本，一半是困难负样本。</p><blockquote><p><em>选取负样本的错误观念：</em></p><p>选取曝光但是没有点击的物品作为负样本（×）：曝光但是没有点击的物品不一定是用户不感兴趣的物品，可能是用户没有看到这个物品，或者用户看了但是没有时间点击这个物品。一般曝光但是没有点击的物品可以作为排序算法的负样本。</p></blockquote><h3 id="线上召回">8.4.线上召回</h3>]]></content>
      
      
      <categories>
          
          <category> SSR </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>排序</title>
      <link href="/2025/11/24/Algo/sort/"/>
      <url>/2025/11/24/Algo/sort/</url>
      
        <content type="html"><![CDATA[<h1 id="排序算法">排序算法</h1><p>排序算法是计算机科学中的基本算法之一，用于将一组数据按照特定的顺序进行排列。常见的排序算法包括<strong>快速排序、归并排序、堆排序、冒泡排序、选择排序、插入排序</strong>等。以下是一些常见排序算法的比较：</p><table><colgroup><col style="width: 17%" /><col style="width: 16%" /><col style="width: 12%" /><col style="width: 8%" /><col style="width: 10%" /><col style="width: 5%" /><col style="width: 29%" /></colgroup><thead><tr><th>排序算法</th><th>最好时间复杂度</th><th>最坏时间复杂度</th><th>平均时间复杂度</th><th>空间复杂度</th><th>稳定性</th><th>常用场景与特点</th></tr></thead><tbody><tr><td><strong>快速排序 Quick Sort</strong></td><td>O(n log n)</td><td><strong>O(n²)</strong></td><td>O(n log n)</td><td>O(log n)（递归栈）</td><td>不稳定</td><td>实际表现最快；最常用；适合大数据；TopK；原地排序；随机化可避开最坏情况</td></tr><tr><td><strong>归并排序 Merge Sort</strong></td><td>O(n log n)</td><td>O(n log n)</td><td>O(n log n)</td><td><strong>O(n)</strong></td><td>稳定</td><td>链表排序首选；对大规模数据表现稳定；适合外排序（磁盘/文件排序）</td></tr><tr><td><strong>堆排序 Heap Sort</strong></td><td>O(n log n)</td><td>O(n log n)</td><td>O(n log n)</td><td>O(1)</td><td>不稳定</td><td>内建优先队列基础；TopK、流数据、海量数据；不要求稳定性的情况</td></tr><tr><td><strong>冒泡排序 Bubble Sort</strong></td><td>O(n)</td><td>O(n²)</td><td>O(n²)</td><td>O(1)</td><td>稳定</td><td>代码最简单；适合几乎有序数据</td></tr><tr><td><strong>选择排序 Selection Sort</strong></td><td>O(n²)</td><td>O(n²)</td><td>O(n²)</td><td>O(1)</td><td>不稳定</td><td>简单但慢；用于对空间要求严格但速度无所谓的场景</td></tr><tr><td><strong>插入排序 Insertion Sort</strong></td><td><strong>O(n)</strong></td><td>O(n²)</td><td>O(n²)</td><td>O(1)</td><td>稳定</td><td>最适合<strong>几乎有序</strong>的数据；数组小规模时最快；用于优化快排（小区间改插排）</td></tr><tr><td><strong>希尔排序 Shell Sort</strong></td><td>O(n log n) ~ O(n^1.3)</td><td>O(n²)</td><td>O(n^1.3)</td><td>O(1)</td><td>不稳定</td><td>插入排序的升级版；数据量中等时表现不错；依赖 gap 序列</td></tr><tr><td><strong>计数排序 Counting Sort</strong></td><td>O(n + k)</td><td>O(n + k)</td><td>O(n + k)</td><td>O(k)</td><td>稳定</td><td>非比较排序；适合大量整数、取值范围小（如 0~100 万）</td></tr><tr><td><strong>桶排序 Bucket Sort</strong></td><td>O(n + k)</td><td>O(n²)（极端情况下桶不均匀）</td><td>O(n + k)</td><td>O(n + k)</td><td>取决于桶内排序</td><td>适合数据分布均匀（如浮点数）；用于外排序；直方图类问题</td></tr><tr><td><strong>基数排序 Radix Sort</strong></td><td>O(d(n + k))</td><td>O(d(n + k))</td><td>O(d(n + k))</td><td>O(n + k)</td><td>稳定</td><td>非比较排序；用于手机号、身份证号、字符串、固定长度数字排序</td></tr></tbody></table><h2 id="快速排序">1. 快速排序</h2><p>快速排序的核心想法在于<strong>分治</strong>，即通过一个基准元素将数组划分为两部分，使得左侧部分的元素都小于基准元素，右侧部分的元素都大于基准元素，然后递归地对这两部分进行排序。其步骤可以概括为：</p><ol type="1"><li>选择一个枢轴（pivot）。常用的选择枢轴（pivot）的方法有三种：选择第一个元素（最后一个元素）、选择中间元素，或者随机选择一个元素作为枢轴，以减少最坏情况的发生。</li><li>分区（Partitioning）：重新排列数组，使得所有小于枢轴的元素都在枢轴的左侧，所有大于枢轴的元素都在枢轴的右侧。完成后，枢轴就处于其最终位置。</li><li>递归地对枢轴左侧和右侧的子数组进行排序。</li></ol><h3 id="单路快排">1.1. 单路快排</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">partition</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums, <span class="type">int</span> l, <span class="type">int</span> r)</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> pivot = nums[r]; <span class="comment">// 选择最后一个作为 pivot</span></span><br><span class="line">    <span class="comment">// 最终[l..i] &lt; pivot, [i+1..j) &gt;= pivot</span></span><br><span class="line">    <span class="type">int</span> i = l - <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> j = l; j &lt; r; j++) &#123;</span><br><span class="line">        <span class="keyword">if</span> (nums[j] &lt; pivot) &#123;</span><br><span class="line">            i++;</span><br><span class="line">            <span class="comment">// 将小于 pivot 的元素交换到前面</span></span><br><span class="line">            <span class="built_in">swap</span>(nums[i], nums[j]);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">swap</span>(nums[i + <span class="number">1</span>], nums[r]);</span><br><span class="line">    <span class="keyword">return</span> i + <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">quickSort</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums, <span class="type">int</span> l, <span class="type">int</span> r)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (l &gt;= r) <span class="keyword">return</span>;</span><br><span class="line">    <span class="type">int</span> p = <span class="built_in">partition</span>(nums, l, r);</span><br><span class="line">    <span class="built_in">quickSort</span>(nums, l, p - <span class="number">1</span>);</span><br><span class="line">    <span class="built_in">quickSort</span>(nums, p + <span class="number">1</span>, r);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">sortArray</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">    <span class="built_in">quickSort</span>(nums, <span class="number">0</span>, nums.<span class="built_in">size</span>() - <span class="number">1</span>);</span><br><span class="line">    <span class="keyword">return</span> nums;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="随机化快排">1.2. 随机化快排</h3><p>上一个版本很简单，但是在pivot选择不当时，可能会退化成O(n²)的时间复杂度。为了避免这种情况，我们可以随机选择pivot：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">randomizedPartition</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums, <span class="type">int</span> l, <span class="type">int</span> r)</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> idx = <span class="built_in">rand</span>() % (r - l + <span class="number">1</span>) + l; <span class="comment">// 随机</span></span><br><span class="line">    <span class="built_in">swap</span>(nums[idx], nums[r]); <span class="comment">// 把随机 pivot 放到末尾</span></span><br><span class="line">    <span class="type">int</span> pivot = nums[r];</span><br><span class="line">    <span class="type">int</span> i = l - <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> j = l; j &lt; r; j++) &#123;</span><br><span class="line">        <span class="keyword">if</span> (nums[j] &lt; pivot) &#123;</span><br><span class="line">            i++;</span><br><span class="line">            <span class="built_in">swap</span>(nums[i], nums[j]);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">swap</span>(nums[i + <span class="number">1</span>], nums[r]);</span><br><span class="line">    <span class="keyword">return</span> i + <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">quickSort</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums, <span class="type">int</span> l, <span class="type">int</span> r)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (l &gt;= r) <span class="keyword">return</span>;</span><br><span class="line">    <span class="type">int</span> p = <span class="built_in">randomizedPartition</span>(nums, l, r);</span><br><span class="line">    <span class="built_in">quickSort</span>(nums, l, p - <span class="number">1</span>);</span><br><span class="line">    <span class="built_in">quickSort</span>(nums, p + <span class="number">1</span>, r);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">sortArray</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">    <span class="built_in">quickSort</span>(nums, <span class="number">0</span>, nums.<span class="built_in">size</span>() - <span class="number">1</span>);</span><br><span class="line">    <span class="keyword">return</span> nums;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="双路快排">1.3. 双路快排</h3><p>双路快排适用于处理包含大量重复元素的数组，比如<code>[2, 3, 1, 2, 2, 2, 4, 2, 3]</code>，单路快排会导致大量元素都挤到左边（因为边界条件是严格的<code>&lt; pivot</code>。它通过两个指针从数组的两端向中间扫描，分别寻找小于和大于枢轴的元素进行交换，能更均匀地分配等于pivot的元素，从而减少了重复元素对排序效率的影响。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">partitionTwoWay</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums, <span class="type">int</span> l, <span class="type">int</span> r)</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> pivotIndex = <span class="built_in">rand</span>() % (r - l + <span class="number">1</span>) + l;</span><br><span class="line">    <span class="built_in">swap</span>(nums[l], nums[pivotIndex]);</span><br><span class="line">    <span class="type">int</span> pivot = nums[l];</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> i = l + <span class="number">1</span>, j = r;</span><br><span class="line">    <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">        <span class="keyword">while</span> (i &lt;= r &amp;&amp; nums[i] &lt; pivot) i++; <span class="comment">// 从左向右找第一个大于等于 pivot 的元素</span></span><br><span class="line">        <span class="keyword">while</span> (j &gt;= l + <span class="number">1</span> &amp;&amp; nums[j] &gt; pivot) j--; <span class="comment">// 从右向左找第一个小于等于 pivot 的元素</span></span><br><span class="line">        <span class="keyword">if</span> (i &gt; j) <span class="keyword">break</span>;</span><br><span class="line">        <span class="built_in">swap</span>(nums[i], nums[j]);</span><br><span class="line">        i++;</span><br><span class="line">        j--;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">swap</span>(nums[l], nums[j]);</span><br><span class="line">    <span class="keyword">return</span> j;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">quickSort</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums, <span class="type">int</span> l, <span class="type">int</span> r)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (l &gt;= r) <span class="keyword">return</span>;</span><br><span class="line">    <span class="type">int</span> p = <span class="built_in">partitionTwoWay</span>(nums, l, r);</span><br><span class="line">    <span class="built_in">quickSort</span>(nums, l, p - <span class="number">1</span>);</span><br><span class="line">    <span class="built_in">quickSort</span>(nums, p + <span class="number">1</span>, r);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">sortArray</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">    <span class="built_in">quickSort</span>(nums, <span class="number">0</span>, nums.<span class="built_in">size</span>() - <span class="number">1</span>);</span><br><span class="line">    <span class="keyword">return</span> nums;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p><em>为什么<code>while (i &lt;= r &amp;&amp; nums[i] &lt; pivot)</code> 而不是<code>while (i &lt;= r &amp;&amp; nums[i] &lt;= pivot)</code>？</em></p><p>因为我们希望将等于 pivot的元素留在右侧，以避免在处理大量重复元素时出现不均匀分布的情况。如果这里加上了等于的情况，可能会导致所有等于pivot 的元素都被留到左侧，从而影响排序效率。</p></blockquote><h2 id="归并排序">2. 归并排序</h2><p>归并排序是一种典型的分治算法，常用于链表等操作中。它将数组递归地分成两半，分别对这两半进行排序，然后将排序好的两半合并在一起。归并排序的时间复杂度始终是O(n log n)，无论是最好、最坏还是平均情况。它的空间复杂度为O(n)，因为需要额外的数组来存储合并后的结果。</p><h3 id="归并排序实现">2.1. 归并排序实现</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">merge</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums, <span class="type">int</span> l, <span class="type">int</span> mid, <span class="type">int</span> r)</span> </span>&#123;</span><br><span class="line">    <span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">temp</span><span class="params">(r - l + <span class="number">1</span>)</span></span>;</span><br><span class="line">    <span class="type">int</span> i = l, j = mid + <span class="number">1</span>, k = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (i &lt;= mid &amp;&amp; j &lt;= r) &#123;</span><br><span class="line">        <span class="keyword">if</span> (nums[i] &lt;= nums[j]) temp[k++] = nums[i++];</span><br><span class="line">        <span class="keyword">else</span> temp[k++] = nums[j++];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">while</span> (i &lt;= mid) temp[k++] = nums[i++];</span><br><span class="line">    <span class="keyword">while</span> (j &lt;= r) temp[k++] = nums[j++];</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> t = <span class="number">0</span>; t &lt; temp.<span class="built_in">size</span>(); t++) </span><br><span class="line">        nums[l + t] = temp[t];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">mergeSort</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums, <span class="type">int</span> l, <span class="type">int</span> r)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (l &gt;= r) <span class="keyword">return</span>;</span><br><span class="line">    <span class="type">int</span> mid = (l + r) / <span class="number">2</span>;</span><br><span class="line">    <span class="built_in">mergeSort</span>(nums, l, mid);</span><br><span class="line">    <span class="built_in">mergeSort</span>(nums, mid + <span class="number">1</span>, r);</span><br><span class="line">    <span class="built_in">merge</span>(nums, l, mid, r);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="自底向上归并排序">2.2. 自底向上归并排序</h3><p>有一种无需递归用迭代方式实现的归并排序，称为自底向上归并排序。它通过不断合并相邻的子数组来实现排序。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">mergeSortIterative</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> n = nums.<span class="built_in">size</span>();</span><br><span class="line">    <span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">temp</span><span class="params">(n)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> len = <span class="number">1</span>; len &lt; n; len *= <span class="number">2</span>) &#123; </span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> l = <span class="number">0</span>; l + len &lt; n; l += <span class="number">2</span> * len) &#123;</span><br><span class="line">            <span class="type">int</span> mid = l + len - <span class="number">1</span>;</span><br><span class="line">            <span class="type">int</span> r = <span class="built_in">min</span>(l + <span class="number">2</span> * len - <span class="number">1</span>, n - <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">            <span class="type">int</span> i = l, j = mid + <span class="number">1</span>, k = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">while</span> (i &lt;= mid &amp;&amp; j &lt;= r) &#123;</span><br><span class="line">                <span class="keyword">if</span> (nums[i] &lt;= nums[j]) temp[k++] = nums[i++];</span><br><span class="line">                <span class="keyword">else</span> temp[k++] = nums[j++];</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">while</span> (i &lt;= mid) temp[k++] = nums[i++];</span><br><span class="line">            <span class="keyword">while</span> (j &lt;= r) temp[k++] = nums[j++];</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> t = <span class="number">0</span>; t &lt; k; t++) nums[l + t] = temp[t];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="堆排序">3. 堆排序</h2><p>堆排序是一种基于堆数据结构的排序算法。它首先将数组构建成一个最大堆（或最小堆），然后不断地将堆顶元素（最大值或最小值）与数组的最后一个元素交换，并缩小堆的大小，重复这个过程直到堆为空。堆排序的时间复杂度为O(n log n)，空间复杂度为 O(1)，因为它是原地排序。</p><h3 id="最大堆排序">3.1. 最大堆排序</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">heapify</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums, <span class="type">int</span> n, <span class="type">int</span> i)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 模拟二叉树左右节点</span></span><br><span class="line">    <span class="type">int</span> largest = i; </span><br><span class="line">    <span class="type">int</span> left = <span class="number">2</span> * i + <span class="number">1</span>; </span><br><span class="line">    <span class="type">int</span> right = <span class="number">2</span> * i + <span class="number">2</span>; </span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (left &lt; n &amp;&amp; nums[left] &gt; nums[largest])</span><br><span class="line">        largest = left;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (right &lt; n &amp;&amp; nums[right] &gt; nums[largest])</span><br><span class="line">        largest = right;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (largest != i) &#123;</span><br><span class="line">        <span class="built_in">swap</span>(nums[i], nums[largest]);</span><br><span class="line">        <span class="built_in">heapify</span>(nums, n, largest);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">heapSort</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> n = nums.<span class="built_in">size</span>();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 构建最大堆：从最后一个非叶子节点开始</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = n / <span class="number">2</span> - <span class="number">1</span>; i &gt;= <span class="number">0</span>; i--)</span><br><span class="line">        <span class="built_in">heapify</span>(nums, n, i);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 一个个从堆中取出元素：将堆顶元素与当前堆的最后一个元素交换，然后重新调整堆</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = n - <span class="number">1</span>; i &gt; <span class="number">0</span>; i--) &#123;</span><br><span class="line">        <span class="built_in">swap</span>(nums[<span class="number">0</span>], nums[i]);</span><br><span class="line">        <span class="built_in">heapify</span>(nums, i, <span class="number">0</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="最小堆排序">3.2.最小堆排序</h3><p>其实和最大堆类似，只需要把比较符号改一下就行了。</p><p>在C++中，有现成的优先队列可以直接使用，下面是一个使用最大堆/最小堆的例子：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;queue&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">heapSortUsingPQ</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums, <span class="type">bool</span> ascending = <span class="literal">true</span>)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (ascending) &#123;</span><br><span class="line">        <span class="comment">// 最小堆</span></span><br><span class="line">        priority_queue&lt;<span class="type">int</span>, vector&lt;<span class="type">int</span>&gt;, greater&lt;<span class="type">int</span>&gt;&gt; minHeap;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> num : nums) minHeap.<span class="built_in">push</span>(num);</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; nums.<span class="built_in">size</span>(); i++) &#123;</span><br><span class="line">            nums[i] = minHeap.<span class="built_in">top</span>();</span><br><span class="line">            minHeap.<span class="built_in">pop</span>();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// 最大堆</span></span><br><span class="line">        priority_queue&lt;<span class="type">int</span>&gt; maxHeap;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> num : nums) maxHeap.<span class="built_in">push</span>(num);</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; nums.<span class="built_in">size</span>(); i++) &#123;</span><br><span class="line">            nums[i] = maxHeap.<span class="built_in">top</span>();</span><br><span class="line">            maxHeap.<span class="built_in">pop</span>();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> nums;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="有关例题">有关例题</h1><h2 id="数组中的第k个最大元素">1. 数组中的第K个最大元素</h2><p><ahref="https://leetcode.cn/problems/kth-largest-element-in-an-array/description/">LeetCodeHot100: 215.数组中的第K个最大元素</a></p><p><strong>简要描述</strong>：给定整数数组 nums 和整数 k，请返回数组中第k 个最大的元素。</p><p><strong>解题关键</strong>：使用<strong>快速选择算法（Quickselect）</strong>，这是快速排序的一个变种。我们通过选择一个枢轴并进行分区，将数组划分为两部分，然后根据枢轴的位置决定下一步搜索的方向。这样可以在平均O(n) 的时间复杂度内找到第 k个最大元素。同时，由于题目说会有很多重复的元素，所以这里需要用双路快排来优化。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">partition</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums, <span class="type">int</span> l, <span class="type">int</span> r)</span></span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(l==r) <span class="keyword">return</span> l;</span><br><span class="line">        <span class="type">int</span> pivot = nums[l];</span><br><span class="line">        <span class="type">int</span> i = l<span class="number">+1</span>, j = r; </span><br><span class="line">        <span class="keyword">while</span>(i&lt;=j)&#123;</span><br><span class="line">            <span class="keyword">while</span>(i&lt;=r &amp;&amp; nums[i]&lt;pivot) i++;</span><br><span class="line">            <span class="keyword">while</span>(j&gt;=l<span class="number">+1</span> &amp;&amp; nums[j]&gt;pivot) j--;</span><br><span class="line">            <span class="keyword">if</span>(i&lt;=j)&#123;</span><br><span class="line">                <span class="built_in">swap</span>(nums[i], nums[j]);</span><br><span class="line">                i++;</span><br><span class="line">                j--;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">swap</span>(nums[l], nums[j]);</span><br><span class="line">        <span class="keyword">return</span> j;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">quickselect</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums, <span class="type">int</span> l, <span class="type">int</span> r, <span class="type">int</span> k)</span></span>&#123;</span><br><span class="line">        <span class="type">int</span> pivot = <span class="built_in">partition</span>(nums, l, r);</span><br><span class="line">        <span class="keyword">if</span> (pivot == k) <span class="keyword">return</span> nums[pivot];</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (pivot &lt; k) <span class="keyword">return</span> <span class="built_in">quickselect</span>(nums, pivot<span class="number">+1</span>, r, k);</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">return</span> <span class="built_in">quickselect</span>(nums, l, pivot<span class="number">-1</span>, k);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">findKthLargest</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums, <span class="type">int</span> k)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> n = nums.<span class="built_in">size</span>();</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">quickselect</span>(nums, <span class="number">0</span>, n<span class="number">-1</span>, n-k);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 刷题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Algorithm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>关于栈和堆的算法题</title>
      <link href="/2025/11/21/Algo/stack/"/>
      <url>/2025/11/21/Algo/stack/</url>
      
        <content type="html"><![CDATA[<h1 id="栈和堆">栈和堆</h1><p>栈是一种常见的数据结构，遵循后进先出（LIFO）的原则。栈在算法题中经常出现，常见的操作包括入栈、出栈和获取栈顶元素等。在C++中，栈通常通过标准库中的<code>std::stack</code> 来实现。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stack&gt;</span></span></span><br><span class="line">std::stack&lt;<span class="type">int</span>&gt; myStack;</span><br><span class="line">myStack.<span class="built_in">push</span>(<span class="number">1</span>);          <span class="comment">// 入栈</span></span><br><span class="line">myStack.<span class="built_in">pop</span>();           <span class="comment">// 出栈</span></span><br><span class="line"><span class="type">int</span> topElement = myStack.<span class="built_in">top</span>(); <span class="comment">// 获取栈顶元素</span></span><br></pre></td></tr></table></figure><p>堆是一种特殊的树形数据结构，通常用于实现优先队列。堆分为最大堆和最小堆，最大堆中每个节点的值都大于或等于其子节点的值，最小堆则相反。在C++中，堆通常通过标准库中的<code>std::priority_queue</code> 来实现。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;queue&gt;</span></span></span><br><span class="line">std::priority_queue&lt;<span class="type">int</span>&gt; maxHeap; <span class="comment">// 最大堆</span></span><br><span class="line">std::priority_queue&lt;<span class="type">int</span>, vector&lt;<span class="type">int</span>&gt;, greater&lt;<span class="type">int</span>&gt;&gt; minHeap; <span class="comment">// 最小堆</span></span><br><span class="line">std::priority_queue&lt;pair&lt;<span class="type">int</span>, <span class="type">int</span>&gt;&gt; pq; <span class="comment">// 存储pair，按第一个元素排序</span></span><br><span class="line">std::priority_queue&lt;pair&lt;<span class="type">int</span>, <span class="type">int</span>&gt;, vector&lt;pair&lt;<span class="type">int</span>, <span class="type">int</span>&gt;&gt;, greater&lt;pair&lt;<span class="type">int</span>, <span class="type">int</span>&gt;&gt;&gt; minPq; <span class="comment">// 最小堆，按第一个元素排序</span></span><br><span class="line"></span><br><span class="line">maxHeap.<span class="built_in">push</span>(<span class="number">5</span>);        <span class="comment">// 入堆</span></span><br><span class="line">maxHeap.<span class="built_in">pop</span>();         <span class="comment">// 出堆</span></span><br><span class="line"><span class="type">int</span> topElement = maxHeap.<span class="built_in">top</span>(); <span class="comment">// 获取堆顶元素</span></span><br></pre></td></tr></table></figure><h2 id="例题">例题</h2><h3 id="最小栈">1. 最小栈</h3><p><ahref="https://leetcode.cn/problems/min-stack/description/">LeetCodeHot100: 155.最小栈</a></p><p><strong>简要描述</strong>：设计一个支持 push，pop，top操作，并能在常数时间内检索到最小元素的栈。</p><p><strong>解题关键</strong>：使用两个栈来实现最小栈，一个栈用于存储所有元素，另一个栈用于存储当前的最小元素。在每次入栈时，如果新元素小于或等于当前最小元素，则将其也入栈到最小栈中；在出栈时，如果出栈的元素等于当前最小元素，则也从最小栈中出栈。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MinStack</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    stack&lt;<span class="type">int</span>&gt; s;</span><br><span class="line">    stack&lt;<span class="type">int</span>&gt; min_s;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">MinStack</span>() &#123;</span><br><span class="line">        min_s.<span class="built_in">push</span>(INT_MAX);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">push</span><span class="params">(<span class="type">int</span> val)</span> </span>&#123;</span><br><span class="line">        s.<span class="built_in">push</span>(val);</span><br><span class="line">        min_s.<span class="built_in">push</span>(<span class="built_in">min</span>(min_s.<span class="built_in">top</span>(), val));</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">pop</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        s.<span class="built_in">pop</span>();</span><br><span class="line">        min_s.<span class="built_in">pop</span>();</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">top</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> s.<span class="built_in">top</span>();</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">getMin</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> min_s.<span class="built_in">top</span>();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>这题还有一个改进的思路，就是只使用一个栈来存储差值，从而节省空间。具体来说，使用一个变量<code>minVal</code>来记录当前的最小值，在入栈时，并不直接存储当前值，而是<strong>存储当前值与<code>minVal</code> 的差值</strong>，如果当前值小于<code>minVal</code>，则更新<code>minVal</code>。在出栈时，可以用当前差值加上最小值来恢复原始值，并在差值为负数的时候（在入栈的时候更新过最小值）相应地更新<code>minVal</code>。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MinStack</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    stack&lt;<span class="type">long</span> <span class="type">long</span>&gt; s;</span><br><span class="line">    <span class="type">long</span> <span class="type">long</span> minVal;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">MinStack</span>() &#123;</span><br><span class="line">        minVal = <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">push</span><span class="params">(<span class="type">int</span> val)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(s.<span class="built_in">empty</span>())&#123;</span><br><span class="line">            s.<span class="built_in">push</span>(<span class="number">0</span>); <span class="comment">// 差值为0</span></span><br><span class="line">            minVal = val;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="type">long</span> <span class="type">long</span> diff = (<span class="type">long</span> <span class="type">long</span>)val - minVal;</span><br><span class="line">            s.<span class="built_in">push</span>(diff);</span><br><span class="line">            <span class="keyword">if</span>(diff &lt; <span class="number">0</span>) &#123;</span><br><span class="line">                minVal = val; <span class="comment">// 更新最小值</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">pop</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="type">long</span> <span class="type">long</span> diff = s.<span class="built_in">top</span>();</span><br><span class="line">        s.<span class="built_in">pop</span>();</span><br><span class="line">        <span class="keyword">if</span>(diff &lt; <span class="number">0</span>) &#123;</span><br><span class="line">            minVal = minVal - diff; <span class="comment">// 恢复之前的最小值</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">top</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="type">long</span> <span class="type">long</span> diff = s.<span class="built_in">top</span>();</span><br><span class="line">        <span class="keyword">if</span>(diff &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> (<span class="type">int</span>)(minVal + diff);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="built_in">return</span> (<span class="type">int</span>)(minVal);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">getMin</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> (<span class="type">int</span>)minVal;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><blockquote><p><em>为什么这里要用long long类型？</em></p><p>因为在计算差值时，<code>val - minVal</code>可能会导致整数溢出，特别是在 <code>val</code> 非常大而<code>minVal</code> 非常小时。使用 <code>long long</code>可以确保差值的计算不会溢出，从而保证算法的正确性。</p></blockquote><h3 id="字符串解码">2. 字符串解码</h3><p><ahref="https://leetcode.cn/problems/decode-string/description/">LeetCodeHot100: 394.字符串解码</a></p><p><strong>简要描述</strong>：给定一个经过编码的字符串，返回它解码后的字符串。编码规则为<code>k[encoded_string]</code>，表示 <code>encoded_string</code>正好重复 <code>k</code> 次。</p><p><strong>解题关键</strong>：本题中可能出现括号嵌套的情况，比如2[a2[bc]]，这种情况下我们可以先转化成 2[abcbc]，再转化成abcbcabcbc。我们可以把字母、数字和括号看成是独立的TOKEN，并用栈来维护这些 TOKEN。具体的做法是，遍历这个栈：</p><ul><li>如果当前的字符为数位，解析出一个数字（连续的多个数位）并进栈；</li><li>如果当前的字符为字母或者左括号，直接进栈；</li><li>如果当前的字符为右括号，开始出栈，一直到左括号出栈，出栈序列反转后拼接成一个字符串，此时取出栈顶的数字，就是这个字符串应该出现的次数，我们根据这个次数和字符串构造出新的字符串并进栈。</li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">string <span class="title">decodeString</span><span class="params">(string s)</span> </span>&#123;</span><br><span class="line">        stack&lt;string&gt; stk;</span><br><span class="line">        <span class="type">int</span> i = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span>(i&lt;s.<span class="built_in">size</span>())&#123;</span><br><span class="line">            <span class="type">char</span> cur = s[i];</span><br><span class="line">            <span class="comment">// 这里有一个小坑，这里的数字不限于一位数，因此需要将连续的数字取出来</span></span><br><span class="line">            <span class="keyword">if</span>(<span class="built_in">isdigit</span>(cur))&#123;</span><br><span class="line">                string digits = <span class="string">&quot;&quot;</span>;</span><br><span class="line">                <span class="keyword">while</span>(<span class="built_in">isdigit</span>(cur))&#123;</span><br><span class="line">                    digits.<span class="built_in">push_back</span>(cur);</span><br><span class="line">                    i++;</span><br><span class="line">                    cur=s[i];</span><br><span class="line">                &#125;</span><br><span class="line">                stk.<span class="built_in">push</span>(digits);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// 如果当前的字符为字母或者左括号，直接进栈</span></span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span>(<span class="built_in">isalpha</span>(cur) || cur==<span class="string">&#x27;[&#x27;</span>)&#123;</span><br><span class="line">                i++;</span><br><span class="line">                stk.<span class="built_in">push</span>(<span class="built_in">string</span>(<span class="number">1</span>, cur));</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// 如果当前的字符为右括号，开始出栈</span></span><br><span class="line">            <span class="keyword">else</span>&#123;</span><br><span class="line">                i++;</span><br><span class="line">                vector&lt;string&gt; sub;</span><br><span class="line">                <span class="keyword">while</span>(stk.<span class="built_in">top</span>()!=<span class="string">&quot;[&quot;</span>)&#123;</span><br><span class="line">                    sub.<span class="built_in">push_back</span>(stk.<span class="built_in">top</span>());</span><br><span class="line">                    stk.<span class="built_in">pop</span>();</span><br><span class="line">                &#125;</span><br><span class="line">                stk.<span class="built_in">pop</span>();</span><br><span class="line">                <span class="comment">// 反转字符串</span></span><br><span class="line">                <span class="built_in">reverse</span>(sub.<span class="built_in">begin</span>(), sub.<span class="built_in">end</span>());</span><br><span class="line">                string str = <span class="string">&quot;&quot;</span>;</span><br><span class="line">                <span class="keyword">for</span>(<span class="keyword">auto</span> &amp;p:sub)&#123;</span><br><span class="line">                    str += p;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="comment">// 取出栈顶的数字，就是这个字符串应该出现的次数</span></span><br><span class="line">                <span class="type">int</span> n = <span class="built_in">stoi</span>(stk.<span class="built_in">top</span>());</span><br><span class="line">                stk.<span class="built_in">pop</span>();</span><br><span class="line">                string res = <span class="string">&quot;&quot;</span>;</span><br><span class="line">                <span class="comment">// 重复n次</span></span><br><span class="line">                <span class="keyword">while</span>(n--) res += str;</span><br><span class="line">                stk.<span class="built_in">push</span>(res);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        vector&lt;string&gt; sub;</span><br><span class="line">        <span class="keyword">while</span>(!stk.<span class="built_in">empty</span>())&#123;</span><br><span class="line">            sub.<span class="built_in">push_back</span>(stk.<span class="built_in">top</span>());</span><br><span class="line">            stk.<span class="built_in">pop</span>();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">reverse</span>(sub.<span class="built_in">begin</span>(), sub.<span class="built_in">end</span>());</span><br><span class="line">        string ans = <span class="string">&quot;&quot;</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">auto</span> &amp;p:sub) ans += p;</span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="每日温度">3. 每日温度</h3><p><ahref="https://leetcode.cn/problems/daily-temperatures/description/">LeetCodeHot100: 739.每日温度</a></p><p><strong>简要描述</strong>：给定一个整数数组<code>temperatures</code>，表示每天的温度，返回一个数组<code>answer</code>，其中 <code>answer[i]</code> 是指对于第<code>i</code>天，下一个更高温度出现在多少天后。如果不存在更高温度，则在该位置用<code>0</code> 来代替。</p><p><strong>解题关键</strong>：可以维护一个存储下标的<strong>单调栈</strong>，从栈底到栈顶的下标对应的温度列表中的温度依次递减。如果一个下标在单调栈里，则表示尚未找到下一次温度更高的下标。正向遍历温度列表，对于温度列表中的每个元素temperatures[i]，如果栈为空，则直接将 i进栈，如果栈不为空，则比较栈顶元素 prevIndex 对应的温度temperatures[prevIndex] 和当前温度 temperatures[i]，如果 temperatures[i]&gt; temperatures[prevIndex]，则将 prevIndex 移除，并将 prevIndex对应的等待天数赋为 i -prevIndex，重复上述操作直到栈为空或者栈顶元素对应的温度大于等于当前温度，然后将i 进栈。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">dailyTemperatures</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; temperatures)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> n = temperatures.<span class="built_in">size</span>();</span><br><span class="line">        <span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">ans</span><span class="params">(n, <span class="number">0</span>)</span></span>;</span><br><span class="line">        stack&lt;<span class="type">int</span>&gt; stk; <span class="comment">// 存储下标的单调栈</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>; i&lt;n; i++)&#123;</span><br><span class="line">            <span class="keyword">while</span>(!stk.<span class="built_in">empty</span>() &amp;&amp; temperatures[i] &gt; temperatures[stk.<span class="built_in">top</span>()])&#123;</span><br><span class="line">                <span class="type">int</span> prevIndex = stk.<span class="built_in">top</span>();</span><br><span class="line">                stk.<span class="built_in">pop</span>();</span><br><span class="line">                ans[prevIndex] = i - prevIndex;</span><br><span class="line">            &#125;</span><br><span class="line">            stk.<span class="built_in">push</span>(i);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="柱状图中最大的矩形">4. 柱状图中最大的矩形</h3><p><ahref="https://leetcode.cn/problems/largest-rectangle-in-histogram/description/">LeetCodeHot100: 84.柱状图中最大的矩形</a></p><p><strong>简要描述</strong>：给定一个整数数组<code>heights</code>，表示柱状图中每个柱子的高度，返回柱状图中最大的矩形面积。</p><p><strong>解题关键</strong>：这题最关键的核心在于，我们需要找到每个柱子向左和向右扩展的边界（找到左右两侧最近的比当前柱子更矮的柱子），从而可以计算以该柱子为高度的最大矩形面积。</p><p>明白这一点后，我们使用<strong>单调栈</strong>来解决此问题。以寻找每根柱子左侧的最近边界为例：我们维护一个单调递增栈，栈中存储柱子的下标，从栈底到栈顶对应的柱子高度依次递增。当我们遍历到某个柱子时，如果当前柱子的高度大于栈顶柱子的高度，那么当前的栈顶柱子就是当前柱子的左边界，同时当前柱子入栈；而如果当前柱子的高度小于等于栈顶柱子的高度，那么我们就不断地将栈顶柱子出栈，直到栈为空或者栈顶柱子的高度小于当前柱子的高度。</p><p>左边界寻找完毕后，同样的我们从右向左遍历一次，寻找每根柱子右侧的最近边界。最后我们就可以计算每根柱子为高度的最大矩形面积，并取其中的最大值。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">largestRectangleArea</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; heights)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> n = heights.<span class="built_in">size</span>();</span><br><span class="line">        <span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">left</span><span class="params">(n)</span>, <span class="title">right</span><span class="params">(n)</span></span>;</span><br><span class="line"></span><br><span class="line">        stack&lt;<span class="type">int</span>&gt; stk;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;n;i++)&#123;</span><br><span class="line">            <span class="keyword">while</span>(!stk.<span class="built_in">empty</span>() &amp;&amp; heights[stk.<span class="built_in">top</span>()]&gt;=heights[i])&#123;</span><br><span class="line">                stk.<span class="built_in">pop</span>();</span><br><span class="line">            &#125;</span><br><span class="line">            left[i] = stk.<span class="built_in">empty</span>() ? <span class="number">-1</span> : stk.<span class="built_in">top</span>();</span><br><span class="line">            stk.<span class="built_in">push</span>(i);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        stk = <span class="built_in">stack</span>&lt;<span class="type">int</span>&gt;();</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span>  i=n<span class="number">-1</span>;i&gt;=<span class="number">0</span>;i--)&#123;</span><br><span class="line">            <span class="keyword">while</span>(!stk.<span class="built_in">empty</span>() &amp;&amp; heights[stk.<span class="built_in">top</span>()]&gt;=heights[i])&#123;</span><br><span class="line">                stk.<span class="built_in">pop</span>();</span><br><span class="line">            &#125;</span><br><span class="line">            right[i] = stk.<span class="built_in">empty</span>() ? n : stk.<span class="built_in">top</span>();</span><br><span class="line">            stk.<span class="built_in">push</span>(i);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="type">int</span> ans = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;n;i++)&#123;</span><br><span class="line">            ans = <span class="built_in">max</span>(ans, (right[i] - left[i] - <span class="number">1</span>)*heights[i]);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>这题还有一个优化的解法，只需要遍历一次就可以找到每根柱子的左右边界，即只用找左边界，是因为在遍历过程中，如果当前柱子的高度小于等于栈顶柱子的高度这一步判断相当于就是在找栈顶柱子的右边界，此时可以直接计算出以栈顶柱子为高度的最大矩形面积，最后再将当前柱子入栈。遍历结束后，栈中可能还有一些柱子，我们需要将它们全部出栈并计算面积。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">largestRectangleArea</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; heights)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> n = heights.<span class="built_in">size</span>();</span><br><span class="line">        stack&lt;<span class="type">int</span>&gt; stk;</span><br><span class="line">        <span class="type">int</span> ans = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 这里我们遍历到 n 时，视为高度为 0 的柱子，方便将栈中剩余的柱子全部出栈</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;=n;i++)&#123;</span><br><span class="line">            <span class="type">int</span> curHeight = (i==n ? <span class="number">0</span> : heights[i]);</span><br><span class="line">            <span class="keyword">while</span>(!stk.<span class="built_in">empty</span>() &amp;&amp; heights[stk.<span class="built_in">top</span>()]&gt;=curHeight)&#123;</span><br><span class="line">                <span class="type">int</span> height = heights[stk.<span class="built_in">top</span>()];</span><br><span class="line">                stk.<span class="built_in">pop</span>();</span><br><span class="line">                <span class="type">int</span> width = stk.<span class="built_in">empty</span>() ? i : (i - stk.<span class="built_in">top</span>() - <span class="number">1</span>);</span><br><span class="line">                ans = <span class="built_in">max</span>(ans, height * width);</span><br><span class="line">            &#125;</span><br><span class="line">            stk.<span class="built_in">push</span>(i);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="数据流的中位数">5. 数据流的中位数</h3><p><ahref="https://leetcode.cn/problems/find-median-from-data-stream/description/">LeetCodeHot100: 295.数据流的中位数</a></p><p><strong>简要描述</strong>：中位数是有序数据集中位于中间位置的数。如果数据集的大小是偶数，则中位数是中间两个数的平均值。请设计一个支持以下两种操作的数据结构：-<code>void addNum(int num)</code>：从数据流中添加一个整数到数据结构中。- <code>double findMedian()</code>：返回目前所有元素的中位数。</p><p><strong>解题关键</strong>：使用两个堆来维护数据流中的元素，一个最大堆用于存储较小（小于等于中位数）的一半元素，另一个最小堆用于存储较大（大于中位数）的一半元素，在计算中位数的时候，如果两个堆大小相同，则中位数是两个堆顶元素的平均值；如果大小不相同（最大堆大小=最小堆大小+1），则中位数是最大堆的堆顶元素。而在添加元素时，需要讨论两种情况：</p><ul><li>如果添加的数小于等于最大堆的堆顶元素：将其添加到最大堆中，并且如果最大堆的大小超过了最小堆的大小+1，则将最大堆的堆顶元素弹出并添加到最小堆中；</li><li>如果添加的数大于最大堆的堆顶元素：将其添加到最小堆中，并且如果最小堆的大小超过了最大堆的大小，则将最小堆的堆顶元素弹出并添加到最大堆中。</li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MedianFinder</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    priority_queue&lt;<span class="type">int</span>&gt; maxHeap; <span class="comment">// 存储较小的一半元素</span></span><br><span class="line">    priority_queue&lt;<span class="type">int</span>, vector&lt;<span class="type">int</span>&gt;, greater&lt;<span class="type">int</span>&gt;&gt; minHeap; <span class="comment">// 存储较大的一半元素</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">MedianFinder</span>() &#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">addNum</span><span class="params">(<span class="type">int</span> num)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(maxHeap.<span class="built_in">empty</span>() || num &lt;= maxHeap.<span class="built_in">top</span>())&#123;</span><br><span class="line">            maxHeap.<span class="built_in">push</span>(num);</span><br><span class="line">            <span class="keyword">if</span>(maxHeap.<span class="built_in">size</span>() &gt; minHeap.<span class="built_in">size</span>() + <span class="number">1</span>)&#123;</span><br><span class="line">                <span class="type">int</span> top = maxHeap.<span class="built_in">top</span>();</span><br><span class="line">                maxHeap.<span class="built_in">pop</span>();</span><br><span class="line">                minHeap.<span class="built_in">push</span>(top);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            minHeap.<span class="built_in">push</span>(num);</span><br><span class="line">            <span class="keyword">if</span>(minHeap.<span class="built_in">size</span>() &gt; maxHeap.<span class="built_in">size</span>())&#123;</span><br><span class="line">                <span class="type">int</span> top = minHeap.<span class="built_in">top</span>();</span><br><span class="line">                minHeap.<span class="built_in">pop</span>();</span><br><span class="line">                maxHeap.<span class="built_in">push</span>(top);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="type">double</span> <span class="title">findMedian</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(maxHeap.<span class="built_in">size</span>() == minHeap.<span class="built_in">size</span>())&#123;</span><br><span class="line">            <span class="keyword">return</span> (maxHeap.<span class="built_in">top</span>() + minHeap.<span class="built_in">top</span>()) / <span class="number">2.0</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> maxHeap.<span class="built_in">top</span>();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 刷题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Algorithm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>二分查找算法题</title>
      <link href="/2025/11/19/Algo/binary_search/"/>
      <url>/2025/11/19/Algo/binary_search/</url>
      
        <content type="html"><![CDATA[<h1 id="二分查找">二分查找</h1><p>二分查找是一种高效的查找算法，适用于在有序数组中查找特定元素。它通过不断将搜索范围缩小一半来快速定位目标元素。在C++中，二分查找通常通过循环或递归实现。该算法的时间复杂度为O(log n)，想法很简单但是有很多细节需要注意。</p><p>在做使用二分的算法时，需要时刻记住<strong>二分不是在找mid，而是在缩小一个一定包含答案的区间</strong>，我们需要定好几件事：</p><ol type="1"><li><strong>确定搜索区间（确定二分模型）</strong>：即答案是一定在 <spanclass="math inline">[<em>l</em>, <em>r</em>]</span>（左闭右闭模型）还是<spanclass="math inline">[<em>l</em>, <em>r</em>)</span>（左闭右开模型）之间；</li><li><strong>确定终止条件</strong>：即循环什么时候结束，通常是 <spanclass="math inline"><em>l</em> &lt; <em>r</em></span> 或 <spanclass="math inline"><em>l</em> &lt;  = <em>r</em></span>，一般如果使用左闭右闭模型，终止条件为<spanclass="math inline"><em>l</em> &lt;  = <em>r</em></span>，而左闭右开模型则为<span class="math inline"><em>l</em> &lt; <em>r</em></span>；</li><li><strong>确定如何更新边界</strong>：根据中间元素与目标值的比较结果，决定更新边界的时候到底是<code>mid±1</code> 还是<code>mid</code>，这取决于当前的搜索区间是左闭右闭还是左闭右开，一般来说，左闭右闭模型在更新边界时通常使用<code>mid±1</code>，而左闭右开模型则使用 <code>mid</code>。</li></ol><p>那么该如何选择这两种模型呢？一般来说，<strong>如果是找某个确定的值，推荐使用左闭右闭模型；而如果是找“边界、第一个、最后一个、最小可行值”等问题，推荐使用左闭右开模型。</strong></p><p>这里首先给出左闭右闭模型的模板：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">binarySearch</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums, <span class="type">int</span> target)</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> l = <span class="number">0</span>, r = nums.<span class="built_in">size</span>() - <span class="number">1</span>; <span class="comment">// 左闭右闭模型 [l, r]</span></span><br><span class="line">    <span class="keyword">while</span> (l &lt;= r) &#123; <span class="comment">// 终止条件有等号，因为是左闭右闭，[l, r]表示还没被排除的答案区间，当l&gt;r时表示区间为空</span></span><br><span class="line">        <span class="type">int</span> mid = l + (r - l) / <span class="number">2</span>;</span><br><span class="line">        <span class="keyword">if</span> (nums[mid] == target) &#123;</span><br><span class="line">            <span class="keyword">return</span> mid; <span class="comment">// 找到目标值</span></span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (nums[mid] &lt; target) &#123;</span><br><span class="line">            l = mid + <span class="number">1</span>; <span class="comment">// 更新左边界，这里要加1，因为mid不可能满足答案要求</span></span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            r = mid - <span class="number">1</span>; <span class="comment">// 更新右边界，这里要减1，因为mid不可能满足答案要求</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1</span>; <span class="comment">// 未找到目标值</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>接下来是左闭右开模型的模板：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">lowerBound</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums, <span class="type">int</span> target)</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> l = <span class="number">0</span>, r = nums.<span class="built_in">size</span>(); <span class="comment">// 左闭右开模型 [l, r)</span></span><br><span class="line">    <span class="keyword">while</span> (l &lt; r) &#123; <span class="comment">// 终止条件没有等号，因为是左闭右开，[l, r)表示还没被排除的答案区间，当l==r时表示区间为空</span></span><br><span class="line">        <span class="type">int</span> mid = l + (r - l) / <span class="number">2</span>;</span><br><span class="line">        <span class="keyword">if</span> (nums[mid] &gt;= target) &#123;</span><br><span class="line">            r = mid; <span class="comment">// 更新右边界，这里不减1，因为mid有可能满足答案要求</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span>&#123;</span><br><span class="line">            l = mid + <span class="number">1</span>; <span class="comment">// 更新左边界，这里要加1，因为mid不可能满足答案要求</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> l; <span class="comment">// 最后终止的时候，l == r，返回任意一个都行</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="例题">例题</h2><h3 id="在排序数组中查找元素的第一个和最后一个位置">1.在排序数组中查找元素的第一个和最后一个位置</h3><p><ahref="https://leetcode.cn/problems/find-first-and-last-position-of-element-in-sorted-array/description/">LeetCodeHot100: 34.在排序数组中查找元素的第一个和最后一个位置</a></p><p><strong>简要描述</strong>：给你一个按照非递减顺序排列的整数数组<code>nums</code>，和一个目标值<code>target</code>。请你找出给定目标值在数组中的开始位置和结束位置。如果数组中不存在目标值<code>target</code>，返回 <code>[-1, -1]</code>。</p><p><strong>解题关键</strong>：使用二分查找分别找到目标值的第一个位置和最后一个位置，这可以通过修改二分查找的边界条件（是否包含目标值），可以实现对左边界和右边界的查找。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">findLeft</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums, <span class="type">int</span> target)</span></span>&#123;</span><br><span class="line">        <span class="type">int</span> l = <span class="number">0</span>, r = nums.<span class="built_in">size</span>() - <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">while</span>(l &lt;= r)&#123;</span><br><span class="line">            <span class="type">int</span> mid = l + (r - l) / <span class="number">2</span>;</span><br><span class="line">            <span class="comment">// 不考虑等号，缩小右边界</span></span><br><span class="line">            <span class="keyword">if</span>(nums[mid] &lt; target)&#123;</span><br><span class="line">                l = mid + <span class="number">1</span>;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                r = mid - <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> l;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">findRight</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums, <span class="type">int</span> target)</span></span>&#123;</span><br><span class="line">        <span class="type">int</span> l = <span class="number">0</span>, r = nums.<span class="built_in">size</span>() - <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">while</span>(l &lt;= r)&#123;</span><br><span class="line">            <span class="type">int</span> mid = l + (r - l) / <span class="number">2</span>;</span><br><span class="line">            <span class="comment">// 考虑等号，缩小左边界</span></span><br><span class="line">            <span class="keyword">if</span>(nums[mid] &lt;= target)&#123;</span><br><span class="line">                l = mid + <span class="number">1</span>;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                r = mid - <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> r;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">searchRange</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums, <span class="type">int</span> target)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> left = <span class="built_in">findLeft</span>(nums, target);</span><br><span class="line">        <span class="type">int</span> right = <span class="built_in">findRight</span>(nums, target);</span><br><span class="line">        <span class="keyword">if</span>(left &lt;= right)&#123;</span><br><span class="line">            <span class="keyword">return</span> &#123;left, right&#125;;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="number">-1</span>, <span class="number">-1</span>&#125;;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="搜索旋转排序数组">2. 搜索旋转排序数组</h3><p><ahref="https://leetcode.cn/problems/search-in-rotated-sorted-array/description/">LeetCodeHot100: 33.搜索旋转排序数组</a></p><p><strong>简要描述</strong>：整数数组 <code>nums</code>按升序排列，数组中的值 互不相同 。在传递给函数之前，<code>nums</code>在预先未知的某个下标 <code>k</code>（0 &lt;= k &lt;nums.length）上进行了旋转，使数组变为<code>[nums[k], nums[k+1], ..., nums[n-1], nums[0], nums[1], ..., nums[k-1]]</code>（下标从0 开始）。例如，数组 <code>[0,1,2,4,5,6,7]</code> 在下标 3处旋转后可能变为 <code>[4,5,6,7,0,1,2]</code>。给你旋转后的数组<code>nums</code> 和一个整数 <code>target</code> ，如果<code>target</code> 存在于 <code>nums</code>中，则返回它的下标，否则返回 -1 。</p><p><strong>解题关键</strong>：将数组一分为二，其中一定有一个是有序的，另一个可能是有序，也可能是部分有序。先判断target是否在有序部分中（通过比较边界），如果在就用二分查找；不在的话就跳到另一个部分（可能是无序部分），无序部分再一分为二，其中一个一定有序，另一个可能有序，可能无序。就这样循环.</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">search</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums, <span class="type">int</span> target)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> n = nums.<span class="built_in">size</span>();</span><br><span class="line">        <span class="keyword">if</span>(n==<span class="number">0</span>) <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">        <span class="keyword">if</span>(n==<span class="number">1</span>) <span class="keyword">return</span> nums[<span class="number">0</span>]==target ? <span class="number">0</span> : <span class="number">-1</span>;</span><br><span class="line">        <span class="type">int</span> l=<span class="number">0</span>, r=n<span class="number">-1</span>;</span><br><span class="line">        <span class="keyword">while</span>(l&lt;=r)&#123;</span><br><span class="line">            <span class="type">int</span> mid = l+(r-l)/<span class="number">2</span>;</span><br><span class="line">            <span class="keyword">if</span>(nums[mid]==target) <span class="keyword">return</span> mid;</span><br><span class="line">            <span class="keyword">if</span>(nums[l]&lt;=nums[mid])&#123;</span><br><span class="line">                <span class="keyword">if</span>(nums[l]&lt;=target &amp;&amp; target&lt;nums[mid])&#123;</span><br><span class="line">                    r = mid - <span class="number">1</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">else</span>&#123;</span><br><span class="line">                    l = mid + <span class="number">1</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span>&#123;</span><br><span class="line">                <span class="keyword">if</span>(nums[mid]&lt;target &amp;&amp; target&lt;=nums[r])&#123;</span><br><span class="line">                    l = mid + <span class="number">1</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">else</span>&#123;</span><br><span class="line">                    r = mid - <span class="number">1</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="寻找旋转排序数组中的最小值">3. 寻找旋转排序数组中的最小值</h3><p><ahref="https://leetcode.cn/problems/find-minimum-in-rotated-sorted-array/description/">LeetCodeHot100: 153.寻找旋转排序数组中的最小值</a></p><p><strong>简要描述</strong>：整数数组 <code>nums</code>按升序排列，数组中的值 互不相同 。在传递给函数之前，<code>nums</code>在预先未知的某个下标 <code>k</code>（0 &lt;= k &lt;nums.length）上进行了旋转，使数组变为<code>[nums[k], nums[k+1], ..., nums[n-1], nums[0], nums[1], ..., nums[k-1]]</code>（下标从0 开始）。例如，数组 <code>[0,1,2,4,5,6,7]</code> 在下标 3处旋转后可能变为 <code>[4,5,6,7,0,1,2]</code>。请你找出并返回数组中的最小元素 。</p><p><strong>解题关键</strong>：利用二分查找的思想，通过比较中间元素和右边界元素来判断最小值所在的区间，从而缩小搜索范围。<strong>二分的思想在于，每次淘汰一半的元素。</strong>我们考虑数组中的最后一个元素x：在最小值右侧的元素（不包括最后一个元素本身），它们的值一定都严格小于x；而在最小值左侧的元素，它们的值一定都严格大于x。因此，我们可以根据这一条性质，通过二分查找的方法找出最小值。在二分查找的每一步中，左边界为low，右边界为 high，区间的中点为pivot，最小值就在该区间内。我们将中轴元素 nums[pivot] 与右边界元素nums[high] 进行比较，可能会有以下的三种情况：</p><ul><li>第一种情况是 nums[pivot] &lt; nums[high]。这说明 nums[pivot]是最小值右侧的元素，因此我们可以忽略二分查找区间的右半部分。</li><li>第二种情况是 nums[pivot] &gt; nums[high]。这说明 nums[pivot]是最小值左侧的元素，因此我们可以忽略二分查找区间的左半部分。</li><li>而如果当前的区间长度为1，这说明我们已经可以结束二分查找了，因此不会存在 nums[pivot]=nums[high]的情况。</li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">findMin</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> l = <span class="number">0</span>, r = (<span class="type">int</span>)nums.<span class="built_in">size</span>() - <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">while</span> (l &lt; r) &#123;</span><br><span class="line">            <span class="type">int</span> mid = l + (r - l) / <span class="number">2</span>;</span><br><span class="line">            <span class="keyword">if</span> (nums[mid] &gt; nums[r]) &#123;</span><br><span class="line">                <span class="comment">// 最小值在右半区（不包括 mid）</span></span><br><span class="line">                l = mid + <span class="number">1</span>;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="comment">// nums[mid] &lt;= nums[r]，最小值在左半区（包括 mid）</span></span><br><span class="line">                r = mid;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> nums[l];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="寻找两个正序数组的中位数">4. 寻找两个正序数组的中位数</h3><p><ahref="https://leetcode.cn/problems/median-of-two-sorted-arrays/description/">LeetCodeHard: 4.寻找两个正序数组的中位数</a></p><p><strong>简要描述</strong>：给定两个大小分别为 <code>m</code> 和<code>n</code> 的正序（从小到大）数组 <code>nums1</code> 和<code>nums2</code>。请你找出并返回这两个正序数组的 中位数。算法的时间复杂度应该为 O(log (m+n)) 。</p><p><strong>解题关键</strong>：根据中位数的定义，当 m+n是奇数时，中位数是两个有序数组中的第 (m+n+1)/2 个元素，当 m+n是偶数时，中位数是两个有序数组中的第 (m+n)/2 个元素和第 (m+n)/2+1个元素的平均值。因此，这道题可以<strong>转化成寻找两个有序数组中的第 k小的数</strong>，其中 k 为 (m+n)/2 或 (m+n)/2+1。</p><p>假设两个有序数组分别是 A 和 B。要找到第 k 个元素，我们可以比较A[k/2−1] 和 B[k/2−1]，其中 / 表示整数除法。由于 A[k/2−1] 和 B[k/2−1]的前面分别有 A[0..k/2−2] 和 B[0..k/2−2]，即 k/2−1 个元素，对于 A[k/2−1]和 B[k/2−1] 中的较小值，最多只会有 (k/2−1)+(k/2−1)≤k−2个元素比它小，那么它就不能是第 k 小的数了。</p><p>因此我们可以归纳出三种情况：</p><ul><li><p>如果 A[k/2−1] &lt; B[k/2−1]，则比 A[k/2−1] 小的数最多只有 A 的前k/2−1 个数和 B 的前 k/2−1 个数，即比 A[k/2−1] 小的数最多只有 k−2个，因此 A[k/2−1] 不可能是第 k 个数，A[0] 到 A[k/2−1] 也都不可能是第 k个数，可以全部排除。</p></li><li><p>如果 A[k/2−1] &gt; B[k/2−1]，则可以排除 B[0] 到B[k/2−1]。</p></li><li><p>如果 A[k/2−1] = B[k/2−1]，则可以归入第一种情况处理。</p></li></ul><p>除此之外，还有一些特殊情况需要额外考虑：</p><ul><li>如果 A[k/2−1] 或者 B[k/2−1]越界，那么我们可以选取对应数组中的最后一个元素。在这种情况下，我们必须根据排除数的个数减少k 的值，而不能直接将 k 减去 k/2。</li><li>如果 k=1，那么我们只需要返回 A[0] 和 B[0] 中的较小值即可。</li><li>如果一个数组为空，那么第 k 个元素就是另一个数组中的第 k个元素。</li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">getKthNumber</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; num1, vector&lt;<span class="type">int</span>&gt;&amp; num2, <span class="type">int</span> k)</span></span>&#123;</span><br><span class="line">        <span class="type">int</span> m = num<span class="number">1.</span><span class="built_in">size</span>(), n = num<span class="number">2.</span><span class="built_in">size</span>();</span><br><span class="line">        <span class="type">int</span> ptr1 = <span class="number">0</span>, ptr2 = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">while</span>(<span class="literal">true</span>)&#123;</span><br><span class="line">            <span class="keyword">if</span>(ptr1==m) <span class="keyword">return</span> num2[ptr2+k<span class="number">-1</span>];</span><br><span class="line">            <span class="keyword">if</span>(ptr2==n) <span class="keyword">return</span> num1[ptr1+k<span class="number">-1</span>];</span><br><span class="line">            <span class="keyword">if</span>(k==<span class="number">1</span>) <span class="keyword">return</span> <span class="built_in">min</span>(num1[ptr1], num2[ptr2]);</span><br><span class="line">            <span class="type">int</span> newptr1 = <span class="built_in">min</span>(ptr1+k/<span class="number">2</span><span class="number">-1</span>, m<span class="number">-1</span>);</span><br><span class="line">            <span class="type">int</span> newptr2 = <span class="built_in">min</span>(ptr2+k/<span class="number">2</span><span class="number">-1</span>, n<span class="number">-1</span>);</span><br><span class="line">            <span class="keyword">if</span>(num1[newptr1]&lt;=num2[newptr2])&#123;</span><br><span class="line">                k -= newptr1 - ptr1 + <span class="number">1</span>;</span><br><span class="line">                ptr1 = newptr1<span class="number">+1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span>&#123;</span><br><span class="line">                k -= newptr2 - ptr2 + <span class="number">1</span>;</span><br><span class="line">                ptr2 = newptr2<span class="number">+1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="type">double</span> <span class="title">findMedianSortedArrays</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums1, vector&lt;<span class="type">int</span>&gt;&amp; nums2)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> totalLength = nums<span class="number">1.</span><span class="built_in">size</span>() + nums<span class="number">2.</span><span class="built_in">size</span>();</span><br><span class="line">        <span class="keyword">if</span>(totalLength%<span class="number">2</span>==<span class="number">1</span>)&#123;</span><br><span class="line">            <span class="comment">// 奇数情况，直接找第 (totalLength+1)/2 个数</span></span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">getKthNumber</span>(nums1, nums2, (totalLength<span class="number">+1</span>)/<span class="number">2</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span>&#123;</span><br><span class="line">            <span class="comment">// 偶数情况，找第 totalLength/2 和 totalLength/2+1 个数的平均值</span></span><br><span class="line">            <span class="built_in">return</span> (<span class="built_in">getKthNumber</span>(nums1, nums2, totalLength/<span class="number">2</span>)+<span class="built_in">getKthNumber</span>(nums1, nums2, totalLength/<span class="number">2</span><span class="number">+1</span>))/<span class="number">2.0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 刷题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Algorithm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>回溯算法题</title>
      <link href="/2025/11/11/Algo/dfs/"/>
      <url>/2025/11/11/Algo/dfs/</url>
      
        <content type="html"><![CDATA[<h1 id="回溯算法">回溯算法</h1><p>回溯算法是一种通过探索所有可能的解决方案来解决问题的算法设计技术。它通常用于组合问题、排列问题、子集问题等。在C++中，回溯算法通常通过递归实现。</p><h2 id="例题">例题</h2><h3 id="全排列">1. 全排列</h3><p><ahref="https://leetcode.cn/problems/permutations/description/">LeetCodeHot100: 46.全排列</a></p><p><strong>简要描述</strong>：给定一个不含重复数字的数组 nums，返回其所有可能的全排列。你可以按任意顺序返回答案。</p><p><strong>解题关键</strong>：使用回溯算法生成全排列。我们可以通过递归地构建排列，并在每一步选择一个未使用的数字添加到当前排列中。当当前排列的长度等于输入数组的长度时，我们将其添加到结果集中。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">backtrack</span><span class="params">(vector&lt;vector&lt;<span class="type">int</span>&gt;&gt; &amp;res, <span class="type">int</span> first, vector&lt;<span class="type">int</span>&gt;&amp; nums)</span></span>&#123;</span><br><span class="line">        <span class="type">int</span> len = nums.<span class="built_in">size</span>();</span><br><span class="line">        <span class="keyword">if</span>(first == len)&#123;</span><br><span class="line">            res.<span class="built_in">push_back</span>(nums);</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i = first;i&lt;len;i++)&#123;</span><br><span class="line">            <span class="built_in">swap</span>(nums[i], nums[first]);</span><br><span class="line">            <span class="built_in">backtrack</span>(res, first<span class="number">+1</span>, nums);</span><br><span class="line">            <span class="built_in">swap</span>(nums[i], nums[first]);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    vector&lt;vector&lt;<span class="type">int</span>&gt;&gt; <span class="built_in">permute</span>(vector&lt;<span class="type">int</span>&gt;&amp; nums) &#123;</span><br><span class="line">        vector&lt;vector&lt;<span class="type">int</span>&gt;&gt; res;</span><br><span class="line">        <span class="built_in">backtrack</span>(res, <span class="number">0</span>, nums);</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="电话号码的字母组合">2. 电话号码的字母组合</h3><p><ahref="https://leetcode.cn/problems/letter-combinations-of-a-phone-number/description/">LeetCodeHot100: 17.电话号码的字母组合</a></p><p><strong>简要描述</strong>：给定一个仅包含数字 2-9的字符串，返回所有它能表示的字母组合。答案可以按 任意顺序 返回。</p><p><strong>解题关键</strong>：使用回溯算法生成所有可能的字母组合。我们可以通过递归地构建组合，并在每一步选择一个数字对应的字母添加到当前组合中。当当前组合的长度等于输入数字字符串的长度时，我们将其添加到结果集中。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    unordered_map&lt;<span class="type">char</span>, string&gt; mp&#123;</span><br><span class="line">        &#123;<span class="string">&#x27;2&#x27;</span>, <span class="string">&quot;abc&quot;</span>&#125;,</span><br><span class="line">        &#123;<span class="string">&#x27;3&#x27;</span>, <span class="string">&quot;def&quot;</span>&#125;,</span><br><span class="line">        &#123;<span class="string">&#x27;4&#x27;</span>, <span class="string">&quot;ghi&quot;</span>&#125;,</span><br><span class="line">        &#123;<span class="string">&#x27;5&#x27;</span>, <span class="string">&quot;jkl&quot;</span>&#125;,</span><br><span class="line">        &#123;<span class="string">&#x27;6&#x27;</span>, <span class="string">&quot;mno&quot;</span>&#125;,</span><br><span class="line">        &#123;<span class="string">&#x27;7&#x27;</span>, <span class="string">&quot;pqrs&quot;</span>&#125;,</span><br><span class="line">        &#123;<span class="string">&#x27;8&#x27;</span>, <span class="string">&quot;tuv&quot;</span>&#125;,</span><br><span class="line">        &#123;<span class="string">&#x27;9&#x27;</span>, <span class="string">&quot;wxyz&quot;</span>&#125;</span><br><span class="line">    &#125;;</span><br><span class="line">    vector&lt;string&gt; ans;</span><br><span class="line"></span><br><span class="line">    <span class="function">vector&lt;string&gt; <span class="title">letterCombinations</span><span class="params">(string digits)</span> </span>&#123;</span><br><span class="line">        string combination;</span><br><span class="line">        <span class="built_in">combine</span>(digits, <span class="number">0</span>, combination);</span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">combine</span><span class="params">(string&amp; digits, <span class="type">int</span> index, string&amp; combination)</span></span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(index==digits.<span class="built_in">size</span>())&#123;</span><br><span class="line">            ans.<span class="built_in">push_back</span>(combination);</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="type">char</span> digit = digits[index];</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">char</span> c:mp[digit])&#123;</span><br><span class="line">            combination.<span class="built_in">push_back</span>(c);</span><br><span class="line">            <span class="built_in">combine</span>(digits, index<span class="number">+1</span>, combination);</span><br><span class="line">            combination.<span class="built_in">pop_back</span>();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="括号生成">3. 括号生成</h3><p><ahref="https://leetcode.cn/problems/generate-parentheses/description/">LeetCodeHot100: 22.括号生成</a></p><p><strong>简要描述</strong>：给你一个整数 n ，请你生成所有由 n对括号组成的有效括号组合。</p><p><strong>解题关键</strong>：使用回溯算法生成所有可能的括号组合。我们可以通过递归地构建组合，并在每一步选择添加一个左括号或右括号。当当前组合的长度等于2*n时（或者说剩下可以用的左括号和右括号都为0时），我们将其添加到结果集中。需要确保在任何时候，右括号的数量不能超过左括号的数量。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    vector&lt;string&gt; ans;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">generate</span><span class="params">(string&amp; combination, <span class="type">int</span> l, <span class="type">int</span> r)</span></span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(l&gt;r || l&lt;<span class="number">0</span>) <span class="keyword">return</span>;</span><br><span class="line">        <span class="keyword">if</span>(l==<span class="number">0</span> &amp;&amp; r==<span class="number">0</span>)&#123;</span><br><span class="line">            ans.<span class="built_in">push_back</span>(combination);</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        combination.<span class="built_in">push_back</span>(<span class="string">&#x27;(&#x27;</span>);</span><br><span class="line">        <span class="built_in">generate</span>(combination, l<span class="number">-1</span>, r);</span><br><span class="line">        combination.<span class="built_in">pop_back</span>();</span><br><span class="line">        combination.<span class="built_in">push_back</span>(<span class="string">&#x27;)&#x27;</span>);</span><br><span class="line">        <span class="built_in">generate</span>(combination, l, r<span class="number">-1</span>);</span><br><span class="line">        combination.<span class="built_in">pop_back</span>();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function">vector&lt;string&gt; <span class="title">generateParenthesis</span><span class="params">(<span class="type">int</span> n)</span> </span>&#123;</span><br><span class="line">        string combination;</span><br><span class="line">        <span class="built_in">generate</span>(combination, n, n);</span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="分割回文串">4. 分割回文串</h3><p><ahref="https://leetcode.cn/problems/palindrome-partitioning/description/">LeetCodeHot100: 131.分割回文串</a></p><p><strong>简要描述</strong>：给你一个字符串 s，请你将 s分割成一些子串，使每个子串都是回文串。返回 s 所有可能的分割方案。</p><p><strong>解题关键</strong>：使用回溯算法生成所有可能的回文子串分割。我们可以通过递归地构建分割方案，并在每一步选择一个回文子串添加到当前分割方案中。当我们遍历完整个字符串时，我们将当前分割方案添加到结果集中。这里还有一步优化需要考虑，就是在确定一个子串是否是回文串时，我们可以使用动态规划预处理，这样在递归过程中可以免去可能出现的重复计算。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    vector&lt;vector&lt;string&gt;&gt; ans;</span><br><span class="line">    vector&lt;vector&lt;<span class="type">bool</span>&gt;&gt; dp;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">backtrack</span><span class="params">(string&amp; s, <span class="type">int</span> start, vector&lt;string&gt;&amp; path)</span></span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(start == s.<span class="built_in">size</span>())&#123;</span><br><span class="line">            ans.<span class="built_in">push_back</span>(path);</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> end = start;end&lt;s.<span class="built_in">size</span>();end++)&#123;</span><br><span class="line">            <span class="keyword">if</span>(dp[start][end])&#123;</span><br><span class="line">                path.<span class="built_in">push_back</span>(s.<span class="built_in">substr</span>(start, end-start<span class="number">+1</span>));</span><br><span class="line">                <span class="built_in">backtrack</span>(s, end<span class="number">+1</span>, path);</span><br><span class="line">                path.<span class="built_in">pop_back</span>();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    vector&lt;vector&lt;string&gt;&gt; <span class="built_in">partition</span>(string s) &#123;</span><br><span class="line">        <span class="type">int</span> n = s.<span class="built_in">size</span>();</span><br><span class="line">        dp.<span class="built_in">resize</span>(n, <span class="built_in">vector</span>&lt;<span class="type">bool</span>&gt;(n, <span class="literal">false</span>));</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=n<span class="number">-1</span>;i&gt;=<span class="number">0</span>;i--)&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="type">int</span> j=i;j&lt;n;j++)&#123;</span><br><span class="line">                <span class="keyword">if</span>(s[i]==s[j] &amp;&amp; (j-i&lt;=<span class="number">2</span> || dp[i<span class="number">+1</span>][j<span class="number">-1</span>]))&#123;</span><br><span class="line">                    dp[i][j] = <span class="literal">true</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        vector&lt;string&gt; path;</span><br><span class="line">        <span class="built_in">backtrack</span>(s, <span class="number">0</span>, path);</span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="n-皇后">5. N 皇后</h3><p><a href="https://leetcode.cn/problems/n-queens/description/">LeetCodeHot100: 51.N皇后</a></p><p><strong>简要描述</strong>：n 皇后问题研究的是如何将 n 个皇后放置在n×n 的棋盘上，并且使皇后彼此之间不能相互攻击。给你一个整数 n，返回所有不同的 n 皇后问题的解决方案。</p><p><strong>解题关键</strong>：使用回溯算法生成所有可能的皇后放置方案。我们可以通过递归地尝试在每一行放置一个皇后，并在每一步检查当前放置是否合法（即不与之前放置的皇后冲突）。当我们成功放置了n个皇后时，我们将当前方案添加到结果集中。这里最关键的在于如何高效地判断当前位置是否合法，可以使用三个数组<code>column</code>、<code>diag1</code>、<code>diag2</code>分别记录列、主对角线、副对角线的占用情况。列的索引很好维护，如果当前行占据了某列，那么<code>column[col]</code>就标记为已占用。但是对于对角线，我们需要一些数学技巧来计算它们的索引。主对角线（从左上到右下）上的元素满足<code>row - col</code>是常数，而副对角线（从右上到左下）上的元素满足<code>row + col</code>是常数。因此我们可以用这个作为索引来标记每个主对角线和副对角线的占用情况。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    vector&lt;vector&lt;string&gt;&gt; <span class="built_in">solveNQueens</span>(<span class="type">int</span> n) &#123;</span><br><span class="line">        <span class="keyword">auto</span> ans = vector&lt;vector&lt;string&gt;&gt;();</span><br><span class="line">        <span class="keyword">auto</span> queens = <span class="built_in">vector</span>&lt;<span class="type">int</span>&gt;(n, <span class="number">-1</span>);</span><br><span class="line">        <span class="keyword">auto</span> columns = <span class="built_in">unordered_set</span>&lt;<span class="type">int</span>&gt;();</span><br><span class="line">        <span class="keyword">auto</span> diag1 = <span class="built_in">unordered_set</span>&lt;<span class="type">int</span>&gt;();</span><br><span class="line">        <span class="keyword">auto</span> diag2 = <span class="built_in">unordered_set</span>&lt;<span class="type">int</span>&gt;();</span><br><span class="line">        <span class="built_in">backtrack</span>(ans, queens, n, <span class="number">0</span>, columns, diag1, diag2);</span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">backtrack</span><span class="params">(vector&lt;vector&lt;string&gt;&gt; &amp;ans, vector&lt;<span class="type">int</span>&gt; &amp;queens, <span class="type">int</span> n, <span class="type">int</span> row, unordered_set&lt;<span class="type">int</span>&gt; &amp;columns, unordered_set&lt;<span class="type">int</span>&gt; &amp;diag1, unordered_set&lt;<span class="type">int</span>&gt; &amp;diag2)</span></span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(row == n)&#123;</span><br><span class="line">            vector&lt;string&gt; board;</span><br><span class="line">            <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;n;i++)&#123;</span><br><span class="line">                string r = <span class="built_in">string</span>(n, <span class="string">&#x27;.&#x27;</span>);</span><br><span class="line">                r[queens[i]] = <span class="string">&#x27;Q&#x27;</span>;</span><br><span class="line">                board.<span class="built_in">push_back</span>(r);</span><br><span class="line">            &#125;</span><br><span class="line">            ans.<span class="built_in">push_back</span>(board);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span>&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;n;i++)&#123;</span><br><span class="line">                <span class="keyword">if</span>(columns.<span class="built_in">find</span>(i)!=columns.<span class="built_in">end</span>())&#123;</span><br><span class="line">                    <span class="keyword">continue</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="type">int</span> d1 = row - i;</span><br><span class="line">                <span class="keyword">if</span>(diag<span class="number">1.f</span>ind(d1)!=diag<span class="number">1.</span><span class="built_in">end</span>())&#123;</span><br><span class="line">                    <span class="keyword">continue</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="type">int</span> d2 = row + i;</span><br><span class="line">                <span class="keyword">if</span>(diag<span class="number">2.f</span>ind(d2)!=diag<span class="number">2.</span><span class="built_in">end</span>())&#123;</span><br><span class="line">                    <span class="keyword">continue</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                queens[row] = i;</span><br><span class="line">                columns.<span class="built_in">insert</span>(i);</span><br><span class="line">                diag<span class="number">1.</span><span class="built_in">insert</span>(d1);</span><br><span class="line">                diag<span class="number">2.</span><span class="built_in">insert</span>(d2);</span><br><span class="line">                <span class="built_in">backtrack</span>(ans, queens, n, row<span class="number">+1</span>, columns, diag1, diag2);</span><br><span class="line">                queens[row] = <span class="number">-1</span>;</span><br><span class="line">                columns.<span class="built_in">erase</span>(i);</span><br><span class="line">                diag<span class="number">1.</span><span class="built_in">erase</span>(d1);</span><br><span class="line">                diag<span class="number">2.</span><span class="built_in">erase</span>(d2);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 刷题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Algorithm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>图论算法题</title>
      <link href="/2025/11/06/Algo/graph/"/>
      <url>/2025/11/06/Algo/graph/</url>
      
        <content type="html"><![CDATA[<h1 id="图论">图论</h1><p>图论是研究图的性质和应用的数学分支，图由顶点和边组成。在算法题中，图论常常用于解决网络流、最短路径、最小生成树等问题。在C++中，图可以通过邻接矩阵或邻接表来表示。</p><p>除此之外，图论算法题中常用的算法包括深度优先搜索（DFS）、广度优先搜索（BFS）、Dijkstra算法、Floyd-Warshall算法、Kruskal算法和Prim算法等，以及常用的数据结构如并查集（Union-Find）。</p><p>并查集是一种用于处理不相交集合的数据结构，支持合并集合和查找元素所属集合的操作。它在图论中常用于判断连通性、检测环等问题。其C++实现通常包括路径压缩和按大小合并两种优化技术，以提高操作效率：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">UnionFind</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">UnionFind</span>(<span class="type">int</span> n) : <span class="built_in">parent</span>(n), <span class="built_in">size</span>(n, <span class="number">1</span>), <span class="built_in">count</span>(n) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; ++i) &#123;</span><br><span class="line">            parent[i] = i;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">find</span><span class="params">(<span class="type">int</span> x)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (parent[x] != x) &#123;</span><br><span class="line">            parent[x] = <span class="built_in">find</span>(parent[x]); <span class="comment">// 路径压缩</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> parent[x];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">unite</span><span class="params">(<span class="type">int</span> x, <span class="type">int</span> y)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> rootX = <span class="built_in">find</span>(x);</span><br><span class="line">        <span class="type">int</span> rootY = <span class="built_in">find</span>(y);</span><br><span class="line">        <span class="keyword">if</span> (rootX != rootY) &#123;</span><br><span class="line">            <span class="comment">// 按大小合并</span></span><br><span class="line">            <span class="keyword">if</span> (size[rootX] &lt; size[rootY]) &#123;</span><br><span class="line">                parent[rootX] = rootY;</span><br><span class="line">                size[rootY] += size[rootX];</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                parent[rootY] = rootX;</span><br><span class="line">                size[rootX] += size[rootY];</span><br><span class="line">            &#125;</span><br><span class="line">            --count;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="type">bool</span> <span class="title">connected</span><span class="params">(<span class="type">int</span> x, <span class="type">int</span> y)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">find</span>(x) == <span class="built_in">find</span>(y);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">getCount</span><span class="params">()</span> <span class="type">const</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> count;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    std::vector&lt;<span class="type">int</span>&gt; parent;</span><br><span class="line">    std::vector&lt;<span class="type">int</span>&gt; size;</span><br><span class="line">    <span class="type">int</span> count;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h2 id="例题">例题</h2><h3 id="岛屿数量">1. 岛屿数量</h3><p><ahref="https://leetcode.cn/problems/number-of-islands/description/">LeetCodeHot100: 200.岛屿数量</a></p><p><strong>简要描述</strong>：给你一个由 <code>'1'</code>（陆地）和<code>'0'</code>（水）组成的的二维网格，请你计算网格中岛屿的数量。岛屿总是被水包围，并且每座岛屿只能由水平方向和竖直方向上相邻的陆地连接形成。</p><p><strong>解题关键</strong>：可以使用深度优先搜索（DFS）或广度优先搜索（BFS）来遍历网格中的每个陆地节点，并将其相邻的陆地节点标记为已访问。每次遇到一个未访问的陆地节点时，岛屿数量加一。除此之外，也可以使用并查集来实现，这里给出用并查集实现的思路：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">UnionFind</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">UnionFind</span>(<span class="type">int</span> n) : <span class="built_in">parent</span>(n), <span class="built_in">size</span>(n, <span class="number">1</span>), <span class="built_in">count</span>(n) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; ++i) &#123;</span><br><span class="line">            parent[i] = i;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">find</span><span class="params">(<span class="type">int</span> x)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (parent[x] != x) &#123;</span><br><span class="line">            parent[x] = <span class="built_in">find</span>(parent[x]); <span class="comment">// 路径压缩</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> parent[x];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">unite</span><span class="params">(<span class="type">int</span> x, <span class="type">int</span> y)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> rootX = <span class="built_in">find</span>(x);</span><br><span class="line">        <span class="type">int</span> rootY = <span class="built_in">find</span>(y);</span><br><span class="line">        <span class="keyword">if</span> (rootX != rootY) &#123;</span><br><span class="line">            <span class="comment">// 按大小合并</span></span><br><span class="line">            <span class="keyword">if</span> (size[rootX] &lt; size[rootY]) &#123;</span><br><span class="line">                parent[rootX] = rootY;</span><br><span class="line">                size[rootY] += size[rootX];</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                parent[rootY] = rootX;</span><br><span class="line">                size[rootX] += size[rootY];</span><br><span class="line">            &#125;</span><br><span class="line">            --count;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="type">bool</span> <span class="title">connected</span><span class="params">(<span class="type">int</span> x, <span class="type">int</span> y)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">find</span>(x) == <span class="built_in">find</span>(y);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">getCount</span><span class="params">()</span> <span class="type">const</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> count;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">numIslands</span><span class="params">(vector&lt;vector&lt;<span class="type">char</span>&gt;&gt;&amp; grid)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> m = grid.<span class="built_in">size</span>();</span><br><span class="line">        <span class="keyword">if</span>(m==<span class="number">0</span>) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        <span class="type">int</span> n = grid[<span class="number">0</span>].<span class="built_in">size</span>();</span><br><span class="line">        <span class="function">UnionFind <span class="title">uf</span><span class="params">(m*n)</span></span>;</span><br><span class="line">        <span class="type">int</span> waterCount = <span class="number">0</span>;</span><br><span class="line">        vector&lt;pair&lt;<span class="type">int</span>,<span class="type">int</span>&gt;&gt; directions = &#123;&#123;<span class="number">1</span>,<span class="number">0</span>&#125;,&#123;<span class="number">-1</span>,<span class="number">0</span>&#125;,&#123;<span class="number">0</span>,<span class="number">1</span>&#125;,&#123;<span class="number">0</span>,<span class="number">-1</span>&#125;&#125;;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;m;i++)&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="type">int</span> j=<span class="number">0</span>;j&lt;n;j++)&#123;</span><br><span class="line">                <span class="keyword">if</span>(grid[i][j]==<span class="string">&#x27;1&#x27;</span>)&#123;</span><br><span class="line">                    grid[i][j] = <span class="string">&#x27;0&#x27;</span>; <span class="comment">// 标记为已访问</span></span><br><span class="line">                    <span class="keyword">for</span>(<span class="keyword">auto</span>&amp; dir: directions)&#123;</span><br><span class="line">                        <span class="type">int</span> ni = i + dir.first;</span><br><span class="line">                        <span class="type">int</span> nj = j + dir.second;</span><br><span class="line">                        <span class="keyword">if</span>(ni&gt;=<span class="number">0</span> &amp;&amp; ni&lt;m &amp;&amp; nj&gt;=<span class="number">0</span> &amp;&amp; nj&lt;n &amp;&amp; grid[ni][nj]==<span class="string">&#x27;1&#x27;</span>)&#123;</span><br><span class="line">                            uf.<span class="built_in">unite</span>(i*n+j, ni*n+nj);</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    waterCount++;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> uf.<span class="built_in">getCount</span>() - waterCount;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="腐烂的橘子">2. 腐烂的橘子</h3><p><ahref="https://leetcode.cn/problems/rotting-oranges/description/">LeetCodeHot100: 994.腐烂的橘子</a></p><p><strong>简要描述</strong>：在给定的网格中，每个单元格可以有以下三个值之一：</p><ul><li>值 <code>0</code> 代表空单元格；</li><li>值 <code>1</code> 代表新鲜橘子；</li><li>值 <code>2</code> 代表腐烂的橘子。</li><li>每分钟，任何与腐烂橘子（在 4个正方向上相邻）相邻的新鲜橘子都会腐烂。返回直到单元格中没有新鲜橘子为止所必须经过的最小分钟数。如果不可能，返回<code>-1</code>。</li></ul><p><strong>解题关键</strong>：可以使用广度优先搜索（BFS）来模拟腐烂过程。首先将所有腐烂的橘子加入队列，然后每次从队列中取出一个腐烂的橘子，检查其四个方向上的新鲜橘子，并将它们标记为腐烂并加入队列。重复此过程直到队列为空，最后检查是否还有新鲜橘子未被腐烂。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">orangesRotting</span><span class="params">(vector&lt;vector&lt;<span class="type">int</span>&gt;&gt;&amp; grid)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> m = grid.<span class="built_in">size</span>();</span><br><span class="line">        <span class="keyword">if</span>(m==<span class="number">0</span>) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        <span class="type">int</span> n = grid[<span class="number">0</span>].<span class="built_in">size</span>();</span><br><span class="line">        queue&lt;pair&lt;<span class="type">int</span>,<span class="type">int</span>&gt;&gt; q;</span><br><span class="line">        <span class="type">int</span> freshCount = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;m;i++)&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="type">int</span> j=<span class="number">0</span>;j&lt;n;j++)&#123;</span><br><span class="line">                <span class="keyword">if</span>(grid[i][j]==<span class="number">2</span>)&#123;</span><br><span class="line">                    q.<span class="built_in">push</span>(&#123;i,j&#125;);</span><br><span class="line">                &#125; <span class="keyword">else</span> <span class="keyword">if</span>(grid[i][j]==<span class="number">1</span>)&#123;</span><br><span class="line">                    freshCount++;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span>(freshCount==<span class="number">0</span>) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        vector&lt;pair&lt;<span class="type">int</span>,<span class="type">int</span>&gt;&gt; directions = &#123;&#123;<span class="number">1</span>,<span class="number">0</span>&#125;,&#123;<span class="number">-1</span>,<span class="number">0</span>&#125;,&#123;<span class="number">0</span>,<span class="number">1</span>&#125;,&#123;<span class="number">0</span>,<span class="number">-1</span>&#125;&#125;;</span><br><span class="line">        <span class="type">int</span> minutes = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">while</span>(!q.<span class="built_in">empty</span>())&#123;</span><br><span class="line">            <span class="type">int</span> size = q.<span class="built_in">size</span>();</span><br><span class="line">            <span class="type">bool</span> rottenThisMinute = <span class="literal">false</span>;</span><br><span class="line">            <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;size;i++)&#123;</span><br><span class="line">                <span class="keyword">auto</span> [x,y] = q.<span class="built_in">front</span>();</span><br><span class="line">                q.<span class="built_in">pop</span>();</span><br><span class="line">                <span class="keyword">for</span>(<span class="keyword">auto</span>&amp; dir: directions)&#123;</span><br><span class="line">                    <span class="type">int</span> nx = x + dir.first;</span><br><span class="line">                    <span class="type">int</span> ny = y + dir.second;</span><br><span class="line">                    <span class="keyword">if</span>(nx&gt;=<span class="number">0</span> &amp;&amp; nx&lt;m &amp;&amp; ny&gt;=<span class="number">0</span> &amp;&amp; ny&lt;n &amp;&amp; grid[nx][ny]==<span class="number">1</span>)&#123;</span><br><span class="line">                        grid[nx][ny] = <span class="number">2</span>;</span><br><span class="line">                        q.<span class="built_in">push</span>(&#123;nx,ny&#125;);</span><br><span class="line">                        freshCount--;</span><br><span class="line">                        rottenThisMinute = <span class="literal">true</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span>(rottenThisMinute) minutes++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> freshCount==<span class="number">0</span> ? minutes : <span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="课程表">3. 课程表</h3><p><ahref="https://leetcode.cn/problems/course-schedule/description/">LeetCodeHot100: 207.课程表</a></p><p><strong>简要描述</strong>：你这个学期必须选修 <code>numCourses</code>门课程，记为 <code>0</code> 到<code>numCourses-1</code>。在选修某些课程之前需要一些先修课程。先修课程按数组<code>prerequisites</code> 给出，其中<code>prerequisites[i] = [ai, bi]</code> 表示如果要选修课程<code>ai</code> 则必须先选修课程<code>bi</code>。请你判断是否可能完成所有课程的学习？</p><p><strong>解题关键</strong>：可以将课程和先修关系表示为有向图，然后使用拓扑排序来判断图中是否存在环。如果存在环，则无法完成所有课程；否则可以完成所有课程。而这个过程可以使用Kahn算法实现，即计算每个节点的入度，然后从入度为0的节点（没有前置课程，可学习）开始进行BFS遍历，维护一个队列，逐步减少相邻节点的入度（前置课程已有一门被学习），每次将入度为0的节点加入队列，最终判断是否所有节点都被访问过。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">bool</span> <span class="title">canFinish</span><span class="params">(<span class="type">int</span> numCourses, vector&lt;vector&lt;<span class="type">int</span>&gt;&gt;&amp; prerequisites)</span> </span>&#123;</span><br><span class="line">        vector&lt;vector&lt;<span class="type">int</span>&gt;&gt; <span class="built_in">graph</span>(numCourses);</span><br><span class="line">        <span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">indegree</span><span class="params">(numCourses, <span class="number">0</span>)</span></span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">auto</span>&amp; pre: prerequisites)&#123;</span><br><span class="line">            graph[pre[<span class="number">1</span>]].<span class="built_in">push_back</span>(pre[<span class="number">0</span>]);</span><br><span class="line">            indegree[pre[<span class="number">0</span>]]++;</span><br><span class="line">        &#125;</span><br><span class="line">        queue&lt;<span class="type">int</span>&gt; q;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;numCourses;i++)&#123;</span><br><span class="line">            <span class="keyword">if</span>(indegree[i]==<span class="number">0</span>) q.<span class="built_in">push</span>(i);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="type">int</span> count = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">while</span>(!q.<span class="built_in">empty</span>())&#123;</span><br><span class="line">            <span class="type">int</span> curr = q.<span class="built_in">front</span>();</span><br><span class="line">            q.<span class="built_in">pop</span>();</span><br><span class="line">            count++;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">auto</span>&amp; neighbor: graph[curr])&#123;</span><br><span class="line">                indegree[neighbor]--;</span><br><span class="line">                <span class="keyword">if</span>(indegree[neighbor]==<span class="number">0</span>) q.<span class="built_in">push</span>(neighbor);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> count==numCourses;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="实现trie-前缀树">4. 实现Trie (前缀树)</h3><p><ahref="https://leetcode.cn/problems/implement-trie-prefix-tree/description/">LeetCodeHot100: 208.实现Trie (前缀树)</a></p><p><strong>简要描述</strong>：Trie（前缀树，又叫字典树）是一种用于高效存储和检索字符串的数据结构，具体来说，它可以用于实现单词的插入、搜索和前缀匹配等操作，Trie的每一层代表字符串的一个字符，每一个节点存储具体的字符。实现一个Trie类，包含插入、搜索和前缀搜索等功能。</p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/Tries.png" width="50%"/></p><p><strong>解题关键</strong>：Trie的实现通常使用嵌套的节点结构，每个节点包含一个字符数组或哈希表来存储子节点，以及一个布尔值来标记该节点是否为一个完整单词的结尾。以下是Trie的C++实现：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Trie</span> &#123;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    vector&lt;Trie*&gt; children;</span><br><span class="line">    <span class="type">bool</span> isEnd;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">Trie</span>() : <span class="built_in">children</span>(<span class="number">26</span>), <span class="built_in">isEnd</span>(<span class="literal">false</span>) &#123;&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">insert</span><span class="params">(string word)</span> </span>&#123;</span><br><span class="line">        Trie* node = <span class="keyword">this</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">auto</span> s:word)&#123;</span><br><span class="line">            <span class="type">int</span> i = s - <span class="string">&#x27;a&#x27;</span>;</span><br><span class="line">            <span class="keyword">if</span>(!node-&gt;children[i]) node-&gt;children[i] = <span class="keyword">new</span> <span class="built_in">Trie</span>();</span><br><span class="line">            node = node-&gt;children[i];</span><br><span class="line">        &#125;</span><br><span class="line">        node-&gt;isEnd = <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function">Trie* <span class="title">findPrefix</span><span class="params">(string word)</span></span>&#123;</span><br><span class="line">        Trie* node = <span class="keyword">this</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">auto</span> s:word)&#123;</span><br><span class="line">            <span class="type">int</span> i = s - <span class="string">&#x27;a&#x27;</span>;</span><br><span class="line">            <span class="keyword">if</span>(!node-&gt;children[i]) <span class="keyword">return</span> <span class="literal">nullptr</span>;</span><br><span class="line">            node = node-&gt;children[i];</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> node;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="type">bool</span> <span class="title">search</span><span class="params">(string word)</span> </span>&#123;</span><br><span class="line">        Trie* node = <span class="built_in">findPrefix</span>(word);</span><br><span class="line">        <span class="keyword">return</span> node!=<span class="literal">nullptr</span> &amp;&amp; node-&gt;isEnd ;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="type">bool</span> <span class="title">startsWith</span><span class="params">(string prefix)</span> </span>&#123;</span><br><span class="line">        Trie* node = <span class="built_in">findPrefix</span>(prefix);</span><br><span class="line">        <span class="keyword">return</span> node!=<span class="literal">nullptr</span> ;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="被围绕的区域">5. 被围绕的区域</h3><p><ahref="https://leetcode.cn/problems/surrounded-regions/description/">LeetCode经典150: 130.被围绕的区域</a></p><p><strong>简要描述</strong>：给你一个 <code>m x n</code> 的矩阵 board，由若干字符 <code>'X'</code> 和 <code>'O'</code> ，找到所有被<code>'X'</code> 围绕的区域，并将这些区域里所有的 <code>'O'</code> 用<code>'X'</code> 填充。</p><p><strong>解题关键</strong>：可以使用深度优先搜索（DFS）或广度优先搜索（BFS）来标记与边界相连的<code>'O'</code>，然后将未被标记的 <code>'O'</code> 替换为<code>'X'</code>。具体步骤如下：</p><ul><li>从边界上的 <code>'O'</code> 开始，使用DFS或BFS将所有与之相连的<code>'O'</code> 标记为特殊字符（如 <code>'#'</code>）。</li><li>遍历整个矩阵，将未被标记的 <code>'O'</code> 替换为<code>'X'</code>，将标记为 <code>'#'</code> 的字符还原为<code>'O'</code>。</li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="type">int</span> m, n;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">dfs</span><span class="params">(<span class="type">int</span> i, <span class="type">int</span> j, vector&lt;vector&lt;<span class="type">char</span>&gt;&gt;&amp; board)</span></span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(i&lt;<span class="number">0</span> || i&gt;=m || j&lt;<span class="number">0</span> || j&gt;=n || board[i][j]!=<span class="string">&#x27;O&#x27;</span>)&#123;</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        board[i][j] = <span class="string">&#x27;P&#x27;</span>;</span><br><span class="line">        <span class="built_in">dfs</span>(i<span class="number">-1</span>, j, board);</span><br><span class="line">        <span class="built_in">dfs</span>(i<span class="number">+1</span>, j, board);</span><br><span class="line">        <span class="built_in">dfs</span>(i, j<span class="number">-1</span>, board);</span><br><span class="line">        <span class="built_in">dfs</span>(i, j<span class="number">+1</span>, board);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">solve</span><span class="params">(vector&lt;vector&lt;<span class="type">char</span>&gt;&gt;&amp; board)</span> </span>&#123;</span><br><span class="line">        m = board.<span class="built_in">size</span>();</span><br><span class="line">        n = board[<span class="number">0</span>].<span class="built_in">size</span>();</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;m;i++)&#123;</span><br><span class="line">            <span class="built_in">dfs</span>(i, <span class="number">0</span>, board);</span><br><span class="line">            <span class="built_in">dfs</span>(i, n<span class="number">-1</span>, board);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;n;i++)&#123;</span><br><span class="line">            <span class="built_in">dfs</span>(<span class="number">0</span>, i, board);</span><br><span class="line">            <span class="built_in">dfs</span>(m<span class="number">-1</span>, i, board);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;m;i++)&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="type">int</span> j=<span class="number">0</span>;j&lt;n;j++)&#123;</span><br><span class="line">                <span class="keyword">if</span>(board[i][j]==<span class="string">&#x27;X&#x27;</span>) <span class="keyword">continue</span>;</span><br><span class="line">                <span class="keyword">else</span> <span class="keyword">if</span>(board[i][j]==<span class="string">&#x27;O&#x27;</span>) board[i][j]=<span class="string">&#x27;X&#x27;</span>;</span><br><span class="line">                <span class="keyword">else</span> board[i][j]=<span class="string">&#x27;O&#x27;</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="克隆图">6. 克隆图</h3><p><ahref="https://leetcode.cn/problems/clone-graph/description/">LeetCode经典150: 133.克隆图</a></p><p><strong>简要描述</strong>：给你无向连通图中一个节点的引用，请你返回该图的深拷贝（克隆）。图中的每个节点都包含它的值<code>val</code> （int） 和其邻居列表（list[Node]）。</p><p><strong>解题关键</strong>：我们可以用广度优先来遍历原图，关键在于邻居列表该如何创建，考虑到在创建一个节点时，其邻居节点可能还没有被创建，因此我们需要使用一个哈希表来存储已经创建的节点，储存从原节点到新节点的映射关系。在遍历过程中，如果遇到一个未创建的邻居节点，就创建它并加入映射表，然后将其加入队列继续遍历。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">Node* <span class="title">cloneGraph</span><span class="params">(Node* node)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(node==<span class="literal">nullptr</span>) <span class="keyword">return</span> <span class="literal">nullptr</span>;</span><br><span class="line">        unordered_map&lt;Node*, Node*&gt; visit;</span><br><span class="line">        queue&lt;Node*&gt; q;</span><br><span class="line">        Node* root = <span class="keyword">new</span> <span class="built_in">Node</span>(node-&gt;val);</span><br><span class="line">        visit[node] = root;</span><br><span class="line">        q.<span class="built_in">push</span>(node);</span><br><span class="line">        <span class="keyword">while</span>(!q.<span class="built_in">empty</span>())&#123;</span><br><span class="line">            Node* cur = q.<span class="built_in">front</span>();</span><br><span class="line">            q.<span class="built_in">pop</span>();</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">auto</span> n:cur-&gt;neighbors)&#123;</span><br><span class="line">                <span class="keyword">if</span>(!visit.<span class="built_in">count</span>(n))&#123;</span><br><span class="line">                    Node* tmp = <span class="keyword">new</span> <span class="built_in">Node</span>(n-&gt;val);</span><br><span class="line">                    visit[n] = tmp;</span><br><span class="line">                    q.<span class="built_in">push</span>(n);</span><br><span class="line">                &#125;</span><br><span class="line">                visit[cur]-&gt;neighbors.<span class="built_in">push_back</span>(visit[n]);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> root;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="课程表ii">7. 课程表II</h3><p><ahref="https://leetcode.cn/problems/course-schedule-ii/description/">LeetCode经典150: 210.课程表 II</a></p><p><strong>简要描述</strong>：你这个学期必须选修 <code>numCourses</code>门课程，记为 <code>0</code> 到<code>numCourses-1</code>。在选修某些课程之前需要一些先修课程。先修课程按数组<code>prerequisites</code> 给出，其中<code>prerequisites[i] = [ai, bi]</code> 表示如果要选修课程<code>ai</code> 则必须先选修课程<code>bi</code>。请你返回一个可能的修课顺序。如果不存在合法的修课顺序，返回一个空数组。</p><p><strong>解题关键</strong>：与课程表I类似，我们可以使用拓扑排序来解决这个问题。这里我们不仅需要判断是否存在环，还需要记录课程的顺序。我们可以在BFS遍历过程中，将每次访问的节点加入结果数组，最后如果结果数组的长度等于课程数，则返回该数组，否则返回空数组。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">findOrder</span><span class="params">(<span class="type">int</span> numCourses, vector&lt;vector&lt;<span class="type">int</span>&gt;&gt;&amp; prerequisites)</span> </span>&#123;</span><br><span class="line">        vector&lt;vector&lt;<span class="type">int</span>&gt;&gt; <span class="built_in">graph</span>(numCourses);</span><br><span class="line">        <span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">indegree</span><span class="params">(numCourses, <span class="number">0</span>)</span></span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">auto</span>&amp; pre: prerequisites)&#123;</span><br><span class="line">            graph[pre[<span class="number">1</span>]].<span class="built_in">push_back</span>(pre[<span class="number">0</span>]);</span><br><span class="line">            indegree[pre[<span class="number">0</span>]]++;</span><br><span class="line">        &#125;</span><br><span class="line">        queue&lt;<span class="type">int</span>&gt; q;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;numCourses;i++)&#123;</span><br><span class="line">            <span class="keyword">if</span>(indegree[i]==<span class="number">0</span>) q.<span class="built_in">push</span>(i);</span><br><span class="line">        &#125;</span><br><span class="line">        vector&lt;<span class="type">int</span>&gt; order;</span><br><span class="line">        <span class="keyword">while</span>(!q.<span class="built_in">empty</span>())&#123;</span><br><span class="line">            <span class="type">int</span> curr = q.<span class="built_in">front</span>();</span><br><span class="line">            q.<span class="built_in">pop</span>();</span><br><span class="line">            order.<span class="built_in">push_back</span>(curr);</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">auto</span>&amp; neighbor: graph[curr])&#123;</span><br><span class="line">                indegree[neighbor]--;</span><br><span class="line">                <span class="keyword">if</span>(indegree[neighbor]==<span class="number">0</span>) q.<span class="built_in">push</span>(neighbor);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> order.<span class="built_in">size</span>()==numCourses ? order : <span class="built_in">vector</span>&lt;<span class="type">int</span>&gt;();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 刷题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Algorithm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>二叉树算法题</title>
      <link href="/2025/10/29/Algo/binary_tree/"/>
      <url>/2025/10/29/Algo/binary_tree/</url>
      
        <content type="html"><![CDATA[<h1 id="二叉树">二叉树</h1><p>二叉树是一种常见的数据结构，每个节点最多有两个子节点，分别称为左子节点和右子节点。二叉树在算法题中经常出现，常见的操作包括遍历、插入、删除等。在C++中，二叉树通常通过定义一个结构体或类来表示节点，然后使用指针来连接各个节点。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">TreeNode</span> &#123;</span><br><span class="line">    <span class="type">int</span> val;</span><br><span class="line">    TreeNode *left;</span><br><span class="line">    TreeNode *right;</span><br><span class="line">    <span class="built_in">TreeNode</span>(): <span class="built_in">val</span>(<span class="number">0</span>), <span class="built_in">left</span>(<span class="literal">nullptr</span>), <span class="built_in">right</span>(<span class="literal">nullptr</span>) &#123;&#125;</span><br><span class="line">    <span class="built_in">TreeNode</span>(<span class="type">int</span> x) : <span class="built_in">val</span>(x), <span class="built_in">left</span>(<span class="literal">NULL</span>), <span class="built_in">right</span>(<span class="literal">NULL</span>) &#123;&#125;</span><br><span class="line">    <span class="built_in">TreeNode</span>(<span class="type">int</span> x, TreeNode *left, TreeNode *right) : <span class="built_in">val</span>(x), <span class="built_in">left</span>(left), <span class="built_in">right</span>(right) &#123;&#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h2 id="例题">例题</h2><h3 id="将有序数组转换为二叉搜索树">1. 将有序数组转换为二叉搜索树</h3><p><ahref="https://leetcode.cn/problems/convert-sorted-array-to-binary-search-tree/description/">LeetCodeHot100: 108.将有序数组转换为二叉搜索树</a></p><p><strong>简要描述</strong>：给你一个整数数组 <code>nums</code>，其中元素已经按 <strong>升序</strong>排列，请你将其转换为一棵高度平衡二叉搜索树。</p><p><strong>解题关键</strong>：要将有序数组转换为高度平衡的二叉搜索树，可以采用递归的方法。选择数组的中间元素作为根节点，然后递归地对左半部分和右半部分分别构建左子树和右子树。</p><blockquote><p><strong>二叉搜索树（BST）</strong>是一种特殊的二叉树，满足以下性质：对于每个节点，其左子树中的所有节点值都小于该节点值，其右子树中的所有节点值都大于该节点值。</p><p><strong>高度平衡二叉树</strong>是一种特殊的二叉树，满足以下性质：对于每个节点，其左子树和右子树的高度差不超过1。</p></blockquote><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">TreeNode* <span class="title">sortBST</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums, <span class="type">int</span> l, <span class="type">int</span> r)</span></span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(r&lt;l) <span class="keyword">return</span> <span class="literal">nullptr</span>;</span><br><span class="line">        <span class="type">int</span> mid = (l+r)&gt;&gt;<span class="number">1</span>;</span><br><span class="line">        TreeNode* root = <span class="keyword">new</span> <span class="built_in">TreeNode</span>(nums[mid]);</span><br><span class="line">        root-&gt;left = <span class="built_in">sortBST</span>(nums, l, mid - <span class="number">1</span>);</span><br><span class="line">        root-&gt;right = <span class="built_in">sortBST</span>(nums, mid + <span class="number">1</span>, r);</span><br><span class="line">        <span class="keyword">return</span> root;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function">TreeNode* <span class="title">sortedArrayToBST</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">sortBST</span>(nums, <span class="number">0</span>, nums.<span class="built_in">size</span>()<span class="number">-1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="二叉搜索树中第k小的元素">2. 二叉搜索树中第K小的元素</h3><p><ahref="https://leetcode.cn/problems/kth-smallest-element-in-a-bst/description/">LeetCodeHot100: 230.二叉搜索树中第K小的元素</a></p><p><strong>简要描述</strong>：给定一个二叉搜索树的根节点<code>root</code> ，和一个整数 <code>k</code>，请你设计一个算法查找其中第 <code>k</code> 个最小的元素。</p><p><strong>解题关键</strong>：由于二叉搜索树的中序遍历结果是一个有序序列，因此我们可以通过中序遍历来找到第<code>k</code>个最小的元素。在中序遍历过程中，维护一个计数器，当计数器达到<code>k</code> 时，当前节点即为所求节点。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="type">int</span> count = <span class="number">0</span>;</span><br><span class="line">    <span class="type">int</span> res = <span class="number">-1</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">inorder</span><span class="params">(TreeNode* root, <span class="type">int</span> k)</span></span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(!root) <span class="keyword">return</span>;</span><br><span class="line">        <span class="built_in">inorder</span>(root-&gt;left, k);</span><br><span class="line">        count++;</span><br><span class="line">        <span class="keyword">if</span>(count == k)&#123;</span><br><span class="line">            res = root-&gt;val;</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">inorder</span>(root-&gt;right, k);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">kthSmallest</span><span class="params">(TreeNode* root, <span class="type">int</span> k)</span> </span>&#123;</span><br><span class="line">        <span class="built_in">inorder</span>(root, k);</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>这里另外讨论两种情况：</p><ol type="1"><li>如果需要一直频繁的查找第 k 小的元素，而树结构不变。</li><li>如果树结构会频繁变化（插入或删除节点），并且我们需要频繁地查找第k小的值。</li></ol><p>对于第一种情况，我们可以记录下以每个结点为根结点的子树的结点数，并在查找第k 小的值时，使用如下方法搜索：</p><ul><li><p>令 <code>node</code> 等于根结点，开始搜索。</p></li><li><p>对当前结点 <code>node</code> 进行如下操作：</p><ul><li>如果 <code>node</code> 的左子树的结点数 left 小于 k−1，则第 k小的元素一定在 <code>node</code> 的右子树中，令 <code>node</code>等于其的右子结点，k 等于 k−left−1，并继续搜索；</li><li>如果 <code>node</code> 的左子树的结点数 left 等于 k−1，则第 k小的元素即为 <code>node</code> ，结束搜索并返回 <code>node</code>即可；</li><li>如果 <code>node</code> 的左子树的结点数 left 大于 k−1，则第 k小的元素一定在 <code>node</code> 的左子树中，令 <code>node</code>等于其左子结点，并继续搜索。</li></ul></li></ul><p>在实现中，我们既可以将以每个结点为根结点的子树的结点数存储在结点中，也可以将其记录在哈希表中。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MyBst</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">MyBst</span>(TreeNode *root) &#123;</span><br><span class="line">        <span class="keyword">this</span>-&gt;root = root;</span><br><span class="line">        <span class="built_in">countNodeNum</span>(root);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 返回二叉搜索树中第k小的元素</span></span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">kthSmallest</span><span class="params">(<span class="type">int</span> k)</span> </span>&#123;</span><br><span class="line">        TreeNode *node = root;</span><br><span class="line">        <span class="keyword">while</span> (node != <span class="literal">nullptr</span>) &#123;</span><br><span class="line">            <span class="type">int</span> left = <span class="built_in">getNodeNum</span>(node-&gt;left);</span><br><span class="line">            <span class="keyword">if</span> (left &lt; k - <span class="number">1</span>) &#123;</span><br><span class="line">                node = node-&gt;right;</span><br><span class="line">                k -= left + <span class="number">1</span>;</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (left == k - <span class="number">1</span>) &#123;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                node = node-&gt;left;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> node-&gt;val;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    TreeNode *root;</span><br><span class="line">    unordered_map&lt;TreeNode *, <span class="type">int</span>&gt; nodeNum;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 统计以node为根结点的子树的结点数</span></span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">countNodeNum</span><span class="params">(TreeNode * node)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (node == <span class="literal">nullptr</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        nodeNum[node] = <span class="number">1</span> + <span class="built_in">countNodeNum</span>(node-&gt;left) + <span class="built_in">countNodeNum</span>(node-&gt;right);</span><br><span class="line">        <span class="keyword">return</span> nodeNum[node];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取以node为根结点的子树的结点数</span></span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">getNodeNum</span><span class="params">(TreeNode * node)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (node != <span class="literal">nullptr</span> &amp;&amp; nodeNum.<span class="built_in">count</span>(node)) &#123;</span><br><span class="line">            <span class="keyword">return</span> nodeNum[node];</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">kthSmallest</span><span class="params">(TreeNode* root, <span class="type">int</span> k)</span> </span>&#123;</span><br><span class="line">        <span class="function">MyBst <span class="title">bst</span><span class="params">(root)</span></span>;</span><br><span class="line">        <span class="keyword">return</span> bst.<span class="built_in">kthSmallest</span>(k);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>对于第二种情况，我们可以使用<strong>平衡二叉搜索树</strong>（如AVL树或红黑树）来维护树的平衡性，从而保证插入、删除和查找操作的时间复杂度均为<spanclass="math inline">𝒪(log <em>n</em>)</span>。在每个节点中，我们同样需要记录以该节点为根节点的子树的节点数，以便在查找第k 小的元素时进行快速定位。</p><h3 id="二叉树的右视图">3. 二叉树的右视图</h3><p><ahref="https://leetcode.cn/problems/binary-tree-right-side-view/description/">LeetCodeHot100: 199.二叉树的右视图</a></p><p><strong>简要描述</strong>：给定一棵二叉树的根节点 <code>root</code>，想象自己站在它的右侧，返回从顶部到底部所能看到的节点值。</p><p><strong>解题关键</strong>：可以使用广度优先搜索（BFS）来遍历二叉树的每一层，使用栈结构记录每一层的最后一个节点值，这些节点值即为右视图的节点值。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">rightSideView</span><span class="params">(TreeNode* root)</span> </span>&#123;</span><br><span class="line">        vector&lt;<span class="type">int</span>&gt; res;</span><br><span class="line">        <span class="keyword">if</span>(!root) <span class="keyword">return</span> res;</span><br><span class="line">        queue&lt;TreeNode*&gt; q;</span><br><span class="line">        q.<span class="built_in">push</span>(root);</span><br><span class="line">        <span class="keyword">while</span>(!q.<span class="built_in">empty</span>())&#123;</span><br><span class="line">            <span class="type">int</span> size = q.<span class="built_in">size</span>();</span><br><span class="line">            <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>; i&lt;size; i++)&#123;</span><br><span class="line">                TreeNode* node = q.<span class="built_in">front</span>();</span><br><span class="line">                q.<span class="built_in">pop</span>();</span><br><span class="line">                <span class="keyword">if</span>(i == size - <span class="number">1</span>)&#123;</span><br><span class="line">                    res.<span class="built_in">push_back</span>(node-&gt;val);</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">if</span>(node-&gt;left) q.<span class="built_in">push</span>(node-&gt;left);</span><br><span class="line">                <span class="keyword">if</span>(node-&gt;right) q.<span class="built_in">push</span>(node-&gt;right);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="从前序与中序遍历序列构造二叉树">4.从前序与中序遍历序列构造二叉树</h3><p><ahref="https://leetcode.cn/problems/construct-binary-tree-from-preorder-and-inorder-traversal/description/">LeetCodeHot100: 105.从前序与中序遍历序列构造二叉树</a></p><p><strong>简要描述</strong>：给定两个整数数组 <code>preorder</code> 和<code>inorder</code> ，其中 <code>preorder</code>是二叉树的前序遍历，<code>inorder</code>是同一棵树的中序遍历，请你构造并返回这棵二叉树。</p><p><strong>解题关键</strong>：前序遍历的第一个元素是根节点，在中序遍历中找到该根节点的位置，左侧为左子树，右侧为右子树。递归地对左右子树进行相同的操作。这题还有一个小条件是题目中说了数组中没有重复的元素，所以我们可以使用哈希表来存储中序遍历中每个值对应的索引位置，以便快速查找根节点的位置。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    unordered_map&lt;<span class="type">int</span>, <span class="type">int</span>&gt; inorderIndexMap;</span><br><span class="line"></span><br><span class="line">    <span class="function">TreeNode* <span class="title">buildTreeHelper</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; preorder, <span class="type">int</span> preStart, <span class="type">int</span> preEnd,</span></span></span><br><span class="line"><span class="params"><span class="function">                              vector&lt;<span class="type">int</span>&gt;&amp; inorder, <span class="type">int</span> inStart, <span class="type">int</span> inEnd)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (preStart &gt; preEnd || inStart &gt; inEnd) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">nullptr</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="type">int</span> rootVal = preorder[preStart];</span><br><span class="line">        TreeNode* root = <span class="keyword">new</span> <span class="built_in">TreeNode</span>(rootVal);</span><br><span class="line"></span><br><span class="line">        <span class="type">int</span> inRootIndex = inorderIndexMap[rootVal];</span><br><span class="line">        <span class="type">int</span> leftTreeSize = inRootIndex - inStart;</span><br><span class="line"></span><br><span class="line">        root-&gt;left = <span class="built_in">buildTreeHelper</span>(preorder, preStart + <span class="number">1</span>, preStart + leftTreeSize,</span><br><span class="line">                                     inorder, inStart, inRootIndex - <span class="number">1</span>);</span><br><span class="line">        root-&gt;right = <span class="built_in">buildTreeHelper</span>(preorder, preStart + leftTreeSize + <span class="number">1</span>, preEnd,</span><br><span class="line">                                      inorder, inRootIndex + <span class="number">1</span>, inEnd);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> root;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function">TreeNode* <span class="title">buildTree</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; preorder, vector&lt;<span class="type">int</span>&gt;&amp; inorder)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; inorder.<span class="built_in">size</span>(); i++) &#123;</span><br><span class="line">            inorderIndexMap[inorder[i]] = i;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">buildTreeHelper</span>(preorder, <span class="number">0</span>, preorder.<span class="built_in">size</span>() - <span class="number">1</span>,</span><br><span class="line">                               inorder, <span class="number">0</span>, inorder.<span class="built_in">size</span>() - <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="根据前序和后序遍历构造二叉树">5.根据前序和后序遍历构造二叉树</h3><p><ahref="https://leetcode.cn/problems/construct-binary-tree-from-preorder-and-postorder-traversal/description/">LeetCode:889.根据前序和后序遍历构造二叉树</a></p><p><strong>简要描述</strong>：给定两个整数数组 <code>preorder</code> 和<code>postorder</code> ，其中 <code>preorder</code>是一棵二叉树的前序遍历，<code>postorder</code>是同一棵树的后序遍历，请你返回构造出的二叉树。</p><p><strong>解题关键</strong>：首先需要明确从前序和后序遍历是无法唯一确定一棵二叉树的，除非这棵树是满二叉树或者有其他限制条件。由前序遍历的第一个元素可以确定根节点，接着我们假设左子树的根节点是前序遍历的第二个元素，然后在后序遍历中找到该节点的位置，从而确定左子树的范围。递归地对左右子树进行相同的操作。由此可以找到符合条件的一种二叉树。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    unordered_map&lt;<span class="type">int</span>, <span class="type">int</span>&gt; postorderIndexMap;</span><br><span class="line"></span><br><span class="line">    <span class="function">TreeNode* <span class="title">buildTreeHelper</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; preorder, <span class="type">int</span> preStart, <span class="type">int</span> preEnd,</span></span></span><br><span class="line"><span class="params"><span class="function">                              vector&lt;<span class="type">int</span>&gt;&amp; postorder, <span class="type">int</span> postStart, <span class="type">int</span> postEnd)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (preStart &gt; preEnd || postStart &gt; postEnd) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">nullptr</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        TreeNode* root = <span class="keyword">new</span> <span class="built_in">TreeNode</span>(preorder[preStart]);</span><br><span class="line">        <span class="keyword">if</span> (preStart == preEnd) &#123;</span><br><span class="line">            <span class="keyword">return</span> root;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="type">int</span> leftRootVal = preorder[preStart + <span class="number">1</span>];</span><br><span class="line">        <span class="type">int</span> leftRootIndex = postorderIndexMap[leftRootVal];</span><br><span class="line">        <span class="type">int</span> leftTreeSize = leftRootIndex - postStart + <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">        root-&gt;left = <span class="built_in">buildTreeHelper</span>(preorder, preStart + <span class="number">1</span>, preStart + leftTreeSize,</span><br><span class="line">                                     postorder, postStart, leftRootIndex);</span><br><span class="line">        root-&gt;right = <span class="built_in">buildTreeHelper</span>(preorder, preStart + leftTreeSize + <span class="number">1</span>, preEnd,</span><br><span class="line">                                      postorder, leftRootIndex + <span class="number">1</span>, postEnd - <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> root;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function">TreeNode* <span class="title">constructFromPrePost</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; preorder, vector&lt;<span class="type">int</span>&gt;&amp; postorder)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; postorder.<span class="built_in">size</span>(); i++) &#123;</span><br><span class="line">            postorderIndexMap[postorder[i]] = i;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">buildTreeHelper</span>(preorder, <span class="number">0</span>, preorder.<span class="built_in">size</span>() - <span class="number">1</span>,</span><br><span class="line">                               postorder, <span class="number">0</span>, postorder.<span class="built_in">size</span>() - <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="二叉树的最近公共祖先">6. 二叉树的最近公共祖先</h3><p><ahref="https://leetcode.cn/problems/lowest-common-ancestor-of-a-binary-tree/description/">LeetCodeHot100: 236.二叉树的最近公共祖先</a></p><p><strong>简要描述</strong>：给定一个二叉树,找到该树中两个指定节点的最近公共祖先。</p><p><strong>解题关键</strong>：可以使用递归的方法来查找两个节点的最近公共祖先。对于每个节点，检查其左子树和右子树是否包含目标节点。如果一个子树包含一个目标节点，而另一个子树包含另一个目标节点，那么当前节点就是最近公共祖先。除此之外，如果当前节点就是其中一个目标节点，那么当前节点也是最近公共祖先。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">TreeNode* <span class="title">lowestCommonAncestor</span><span class="params">(TreeNode* root, TreeNode* p, TreeNode* q)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 如果当前节点为空，或者等于p或q，则返回当前节点（发现了目标节点或者到达叶子节点）</span></span><br><span class="line">        <span class="keyword">if</span> (root == <span class="literal">nullptr</span> || root == p || root == q) &#123;</span><br><span class="line">            <span class="keyword">return</span> root;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 递归搜索左右子树</span></span><br><span class="line">        TreeNode* left = <span class="built_in">lowestCommonAncestor</span>(root-&gt;left, p, q);</span><br><span class="line">        TreeNode* right = <span class="built_in">lowestCommonAncestor</span>(root-&gt;right, p, q);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 如果左右子树都返回非空，说明p和q分别在左右子树中，当前节点是最近公共祖先</span></span><br><span class="line">        <span class="keyword">if</span> (left != <span class="literal">nullptr</span> &amp;&amp; right != <span class="literal">nullptr</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> root;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 否则返回非空的子树结果</span></span><br><span class="line">        <span class="keyword">return</span> left != <span class="literal">nullptr</span> ? left : right;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="验证二叉树的前序序列化">7. 验证二叉树的前序序列化</h3><p><ahref="https://leetcode.cn/problems/verify-preorder-serialization-of-a-binary-tree/description/">LeetCode:331.验证二叉树的前序序列化</a></p><p><strong>简要描述</strong>：序列化是将一个数据结构或对象转换为连续位的过程，以便将其存储在文件或内存缓冲区中，或者通过网络连接传输，以便稍后重建原始数据结构。二叉树的前序序列化使用逗号分隔的字符串表示，其中空节点用<code>#</code> 表示。给定一个以逗号分隔的字符串 <code>preorder</code>，请你验证它是否是一个正确的二叉树前序序列化。</p><p><strong>解题关键</strong>：可以使用栈来模拟二叉树的构建过程，并且我们引入槽位的概念。每当我们遇到一个非空节点时，我们需要消耗一个槽位，并且为该节点创建两个新的槽位（因为非空节点有两个子节点）。当我们遇到一个空节点时，我们只需要消耗一个槽位。最终，如果所有的槽位都被正确地消耗完，那么这个前序序列化就是有效的。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">bool</span> <span class="title">isValidSerialization</span><span class="params">(string preorder)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> n = preorder.<span class="built_in">size</span>();</span><br><span class="line">        <span class="type">int</span> i = <span class="number">0</span>;</span><br><span class="line">        stack&lt;<span class="type">int</span>&gt; stk;</span><br><span class="line">        stk.<span class="built_in">push</span>(<span class="number">1</span>); <span class="comment">// 初始时有一个槽位</span></span><br><span class="line">        <span class="keyword">while</span> (i &lt; n) &#123;</span><br><span class="line">            <span class="keyword">if</span> (stk.<span class="built_in">empty</span>()) &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">false</span>; <span class="comment">// 没有槽位可用，说明序列化无效</span></span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (preorder[i] == <span class="string">&#x27;,&#x27;</span>) &#123;</span><br><span class="line">                i++;</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (preorder[i] == <span class="string">&#x27;#&#x27;</span>) &#123;</span><br><span class="line">                stk.<span class="built_in">top</span>()--; <span class="comment">// 消耗一个槽位</span></span><br><span class="line">                <span class="keyword">if</span> (stk.<span class="built_in">top</span>() == <span class="number">0</span>) &#123;</span><br><span class="line">                    stk.<span class="built_in">pop</span>(); <span class="comment">// 如果槽位用完，弹出栈顶</span></span><br><span class="line">                &#125;</span><br><span class="line">                i++;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="comment">// 读取数字节点</span></span><br><span class="line">                <span class="keyword">while</span> (i &lt; n &amp;&amp; preorder[i] != <span class="string">&#x27;,&#x27;</span>) &#123;</span><br><span class="line">                    i++;</span><br><span class="line">                &#125;</span><br><span class="line">                stk.<span class="built_in">top</span>()--; <span class="comment">// 消耗一个槽位</span></span><br><span class="line">                <span class="keyword">if</span> (stk.<span class="built_in">top</span>() == <span class="number">0</span>) &#123;</span><br><span class="line">                    stk.<span class="built_in">pop</span>(); <span class="comment">// 如果槽位用完，弹出栈顶</span></span><br><span class="line">                &#125;</span><br><span class="line">                stk.<span class="built_in">push</span>(<span class="number">2</span>); <span class="comment">// 为非空节点创建两个新的槽位</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> stk.<span class="built_in">empty</span>();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>其实实现一遍上述思路之后，我们可以发现，其实并不需要使用栈来维护槽位的数量，我们只需要一个整数变量来记录当前的槽位数量即可：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">bool</span> <span class="title">isValidSerialization</span><span class="params">(string preorder)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> n = preorder.<span class="built_in">size</span>();</span><br><span class="line">        <span class="type">int</span> i = <span class="number">0</span>;</span><br><span class="line">        <span class="type">int</span> slots = <span class="number">1</span>; <span class="comment">// 初始时有一个槽位</span></span><br><span class="line">        <span class="keyword">while</span> (i &lt; n) &#123;</span><br><span class="line">            <span class="keyword">if</span> (slots == <span class="number">0</span>) &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">false</span>; <span class="comment">// 没有槽位可用，说明序列化无效</span></span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (preorder[i] == <span class="string">&#x27;,&#x27;</span>) &#123;</span><br><span class="line">                i++;</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (preorder[i] == <span class="string">&#x27;#&#x27;</span>) &#123;</span><br><span class="line">                slots--; <span class="comment">// 消耗一个槽位</span></span><br><span class="line">                i++;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="comment">// 读取数字节点</span></span><br><span class="line">                <span class="keyword">while</span> (i &lt; n &amp;&amp; preorder[i] != <span class="string">&#x27;,&#x27;</span>) &#123;</span><br><span class="line">                    i++;</span><br><span class="line">                &#125;</span><br><span class="line">                slots--; <span class="comment">// 消耗一个槽位</span></span><br><span class="line">                slots += <span class="number">2</span>; <span class="comment">// 为非空节点创建两个新的槽位</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> slots == <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 刷题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Algorithm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>链表算法题</title>
      <link href="/2025/10/10/Algo/linked_list/"/>
      <url>/2025/10/10/Algo/linked_list/</url>
      
        <content type="html"><![CDATA[<h1 id="链表">链表</h1><p>链表是一种常见的数据结构，适用于需要频繁插入和删除操作的场景。在算法题中，链表通常用于解决一些特定的问题，比如反转链表、合并两个有序链表等。在C++中，链表可以通过自定义结构体或类来实现，也可以使用标准库中的<code>std::list</code>。</p><p>C++链表定义标准模板（具体详见<ahref="https://litchi-lee.github.io/2025/09/19/Algo/cpp_use/">这里</a>）：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">ListNode</span> &#123;</span><br><span class="line">    <span class="type">int</span> val;</span><br><span class="line">    ListNode *next;</span><br><span class="line">    <span class="built_in">ListNode</span>() : <span class="built_in">val</span>(<span class="number">0</span>), <span class="built_in">next</span>(<span class="literal">nullptr</span>) &#123;&#125;</span><br><span class="line">    <span class="built_in">ListNode</span>(<span class="type">int</span> x) : <span class="built_in">val</span>(x), <span class="built_in">next</span>(<span class="literal">nullptr</span>) &#123;&#125;</span><br><span class="line">    <span class="built_in">ListNode</span>(<span class="type">int</span> x, ListNode *next) : <span class="built_in">val</span>(x), <span class="built_in">next</span>(next) &#123;&#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h2 id="例题">例题</h2><h3 id="回文链表">1. 回文链表</h3><p><ahref="https://leetcode.cn/problems/palindrome-linked-list/description/">LeetCodeHot100: 234.回文链表</a></p><p><strong>简要描述</strong>：给你一个单链表的头节点 head，请你判断该链表是否为回文链表。如果是，返回 true ；否则，返回 false。</p><p><strong>解题关键</strong>：要判断一个链表是否为回文链表，最容易想到的做法是将链表的值存储到一个数组中，然后使用双指针从数组的两端向中间移动，比较对应位置的值是否相同。如果所有对应位置的值都相同，则链表是回文链表。但是这样做的空间复杂度是O(n)，不符合题目的进阶要求。因此我们的思路是，先使用<strong>快慢指针</strong>找到链表的中点，然后将后半部分链表反转，最后再从头和中点开始同时遍历前后两部分链表，比较对应节点的值是否相同。如果所有对应节点的值都相同，则链表是回文链表。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">bool</span> <span class="title">isPalindrome</span><span class="params">(ListNode* head)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(head == <span class="literal">nullptr</span> || head-&gt;next == <span class="literal">nullptr</span>) <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        ListNode *end = <span class="built_in">halfOfList</span>(head);</span><br><span class="line">        ListNode *half = <span class="built_in">reverseListNode</span>(end-&gt;next);</span><br><span class="line">        ListNode *ptr1 = head;</span><br><span class="line">        ListNode *ptr2 = half;</span><br><span class="line">        <span class="keyword">while</span>(ptr2!=<span class="literal">nullptr</span>)&#123;</span><br><span class="line">            <span class="keyword">if</span>(ptr1-&gt;val != ptr2-&gt;val)&#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            ptr1 = ptr1-&gt;next;</span><br><span class="line">            ptr2 = ptr2-&gt;next;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function">ListNode* <span class="title">reverseListNode</span><span class="params">(ListNode* head)</span></span>&#123;</span><br><span class="line">        ListNode *prev, *cur, *next;</span><br><span class="line">        prev = <span class="literal">nullptr</span>;</span><br><span class="line">        cur = head;</span><br><span class="line">        <span class="keyword">while</span>(cur)&#123;</span><br><span class="line">            next = cur-&gt;next;</span><br><span class="line">            cur-&gt;next = prev;</span><br><span class="line">            prev = cur;</span><br><span class="line">            cur = next;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> prev;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function">ListNode* <span class="title">halfOfList</span><span class="params">(ListNode* head)</span></span>&#123;</span><br><span class="line">        ListNode *ptr1 = head;</span><br><span class="line">        ListNode *ptr2 = head;</span><br><span class="line">        <span class="keyword">while</span>(ptr2-&gt;next!=<span class="literal">nullptr</span> &amp;&amp; ptr2-&gt;next-&gt;next!=<span class="literal">nullptr</span>)&#123;</span><br><span class="line">            ptr1 = ptr1-&gt;next;</span><br><span class="line">            ptr2 = ptr2-&gt;next-&gt;next;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> ptr1;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="环形链表">2. 环形链表</h3><p><ahref="https://leetcode.cn/problems/linked-list-cycle/description/">LeetCodeHot100: 141.环形链表</a></p><p><strong>简要描述</strong>：给你一个链表的头节点 head，判断链表中是否有环。如果链表中有某个节点，可以通过连续跟踪 next指针再次到达，则链表中存在环。为了表示给定链表中的环，评测系统内部使用整数pos 来表示链表尾连接到链表中的位置（索引从 0 开始）。注意：pos不作为参数进行传递，仅仅是为了标识链表的实际情况。</p><p><strong>解题关键</strong>：这题最容易想到的做法是用哈希表存储已经访问过的节点，然后在遍历链表时检查当前节点是否已经在哈希表中出现过。如果出现过，则说明链表中有环；否则继续遍历直到链表结束。但是这种方法的空间复杂度是O(n)，不符合题目的进阶要求。而一种更取巧的做法需要我们对<strong>Floyd判圈法（又称龟兔赛跑算法）</strong>有所了解。该算法使用两个快慢指针，一个快指针每次移动两步，一个慢指针每次移动一步。如果链表中有环，快指针最终会追上慢指针；如果链表中没有环，快指针会先到达链表的末尾。要如何理解最终两个指针一定会相遇呢，可以理解为快指针相对于慢指针的相对步长为1，相当于总存在一个时刻他们经过的距离的差值正好是环的长度，此时两个指针相遇。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">bool</span> <span class="title">hasCycle</span><span class="params">(ListNode *head)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(head == <span class="literal">nullptr</span> || head-&gt;next == <span class="literal">nullptr</span>)&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        ListNode *slow = head;</span><br><span class="line">        ListNode *fast = head-&gt;next;</span><br><span class="line">        <span class="keyword">while</span>(slow!=fast)&#123;</span><br><span class="line">            <span class="keyword">if</span>(fast-&gt;next == <span class="literal">nullptr</span> || fast-&gt;next-&gt;next == <span class="literal">nullptr</span>)&#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            slow = slow-&gt;next;</span><br><span class="line">            fast = fast-&gt;next-&gt;next;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>其实这一题还有一种做法，考虑到题目没有要求这个链表不发生改变，我们可以在遍历链表的过程中将访问过的节点的值改为一个不可能出现的值（比如INT_MIN），如果在遍历过程中遇到一个节点的值已经是INT_MIN，则说明链表中有环。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">bool</span> <span class="title">hasCycle</span><span class="params">(ListNode *head)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(head == <span class="literal">nullptr</span> || head-&gt;next == <span class="literal">nullptr</span>)&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        ListNode *cur = head;</span><br><span class="line">        <span class="keyword">while</span>(cur != <span class="literal">nullptr</span>)&#123;</span><br><span class="line">            <span class="keyword">if</span>(cur-&gt;val == INT_MIN)&#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            cur-&gt;val = INT_MIN;</span><br><span class="line">            cur = cur-&gt;next;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="环形链表ii">3. 环形链表II</h3><p><ahref="https://leetcode.cn/problems/linked-list-cycle-ii/description/">LeetCodeHot100: 142.环形链表 II</a></p><p><strong>简要描述</strong>：给定一个链表的头节点 head，返回链表开始入环的第一个节点。如果链表无环，则返回 null。</p><p><strong>解题关键</strong>：这题是上一题的进阶版，要求我们不仅判断链表中是否有环，还要找到环的入口节点。我们仍然可以使用<strong>Floyd判圈法</strong>来解决这个问题。首先，我们使用快慢指针来判断链表中是否有环。如果有环，快慢指针最终会相遇在环中的某个节点。</p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/142_fig1.png" width="60%"/></p><p>如上图假设链表的头节点到环的入口节点的距离为a，环的入口节点到快慢指针相遇节点的距离为b，相遇节点到环的入口节点的距离为c。那么根据快慢指针的移动规律，我们可以得到以下关系：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2 * (a + b) = a + b + n * (b + c)</span><br></pre></td></tr></table></figure><p>其中 n 是快指针在环中绕圈的次数。化简后得到：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a = n * (b + c) - b = (n - 1) * (b + c) + c</span><br></pre></td></tr></table></figure><p>其中 (b + c) 是环的长度，记为L。那么我们可以看出，从链表的头节点出发，走 a步会到达环的入口节点；而从相遇节点出发，走 (n - 1) * L + c步也会到达环的入口节点。由于 (n - 1) * L是环的整数倍，因此我们只需要从相遇节点出发，走 c步就能到达环的入口节点。基于这个分析，我们可以设计如下算法：</p><ol type="1"><li>使用快慢指针判断链表中是否有环。如果没有环，返回 null。</li><li>如果有环，记录快慢指针相遇的节点。</li><li>从链表的头节点和相遇节点分别出发，每次移动一步，直到两个指针相遇。相遇的节点就是环的入口节点。</li></ol><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">ListNode *<span class="title">detectCycle</span><span class="params">(ListNode *head)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(head == <span class="literal">nullptr</span> || head-&gt;next ==<span class="literal">nullptr</span>)&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">nullptr</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        ListNode *slow = head;</span><br><span class="line">        ListNode *fast = head;</span><br><span class="line">        <span class="keyword">do</span>&#123;</span><br><span class="line">            <span class="keyword">if</span>(!fast || fast-&gt;next == <span class="literal">nullptr</span>)&#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">nullptr</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            slow = slow-&gt;next;</span><br><span class="line">            fast = fast-&gt;next-&gt;next;</span><br><span class="line">        &#125;<span class="keyword">while</span>(slow!=fast);</span><br><span class="line">        ListNode *ptr = head;</span><br><span class="line">        <span class="keyword">while</span>(ptr!=slow)&#123;</span><br><span class="line">            ptr = ptr-&gt;next;</span><br><span class="line">            slow = slow-&gt;next;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> ptr;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="删除链表的倒数第n个节点">4. 删除链表的倒数第N个节点</h3><p><ahref="https://leetcode.cn/problems/remove-nth-node-from-end-of-list/description/">LeetCodeHot100: 19.删除链表的倒数第 N 个结点</a></p><p><strong>简要描述</strong>：给你一个链表，删除链表的倒数第 n个结点，并且返回链表的头结点。</p><p><strong>解题关键</strong>：这题可以使用双指针来解决。我们可以设置两个指针，<code>first</code>和 <code>second</code>，初始时都指向链表的头节点。然后我们让<code>first</code> 指针先移动 n 步，这样 <code>first</code> 和<code>second</code> 之间就相隔了 n 个节点。接着我们同时移动<code>first</code> 和 <code>second</code> 指针，直到 <code>first</code>指针到达链表的末尾。此时，<code>second</code> 指针正好指向倒数第 n个节点的前一个节点。我们只需要将 <code>second</code>的下一个节点删除即可。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">ListNode* <span class="title">removeNthFromEnd</span><span class="params">(ListNode* head, <span class="type">int</span> n)</span> </span>&#123;</span><br><span class="line">        ListNode dummy = <span class="built_in">ListNode</span>(<span class="number">0</span>, head);</span><br><span class="line">        ListNode* first = &amp;dummy;</span><br><span class="line">        ListNode* second = &amp;dummy;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i++)&#123;</span><br><span class="line">            first = first-&gt;next;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">while</span>(first-&gt;next != <span class="literal">nullptr</span>)&#123;</span><br><span class="line">            first = first-&gt;next;</span><br><span class="line">            second = second-&gt;next;</span><br><span class="line">        &#125;</span><br><span class="line">        ListNode* toDelete = second-&gt;next;</span><br><span class="line">        second-&gt;next = second-&gt;next-&gt;next;</span><br><span class="line">        <span class="keyword">delete</span> toDelete;</span><br><span class="line">        <span class="keyword">return</span> dummy-&gt;next;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="k个一组翻转链表">5. K个一组翻转链表</h3><p><ahref="https://leetcode.cn/problems/reverse-nodes-in-k-group/description/">LeetCodeHot100: 25.K 个一组翻转链表</a></p><p><strong>简要描述</strong>：给你一个链表，每 k个节点一组进行翻转，请你返回翻转后的链表。k是一个正整数，它的值小于或等于链表的长度。如果节点总数不是 k的整数倍，那么请将最后剩余的节点保持原有顺序。</p><p><strong>解题关键</strong>：这题直接用迭代的方法来做即可，题目的要求是每k个节点一组进行翻转，因此我们可以使用一个循环来处理每一组节点。在每一组中，我们需要先检查是否有足够的节点进行翻转，如果不足k 个节点，则直接返回当前链表。否则，我们就对这 k个节点进行翻转（反转链表的方式和之前一致），并将翻转后的链表连接到前一组的末尾。为了方便处理链表的头节点，我们可以使用一个虚拟头节点（哑结点）。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">ListNode* <span class="title">reverseKGroup</span><span class="params">(ListNode* head, <span class="type">int</span> k)</span> </span>&#123;</span><br><span class="line">        ListNode dummy = <span class="built_in">ListNode</span>(<span class="number">0</span>, head);</span><br><span class="line">        ListNode* head_prev = &amp;dummy;</span><br><span class="line">        <span class="keyword">while</span>(head_prev)&#123;</span><br><span class="line">            ListNode* tail = head_prev;</span><br><span class="line">            <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">1</span>;i&lt;=k;i++)&#123;</span><br><span class="line">                tail = tail-&gt;next;</span><br><span class="line">                <span class="keyword">if</span>(!tail)&#123;</span><br><span class="line">                    <span class="keyword">return</span> dummy.next;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            head_prev = <span class="built_in">reverseList</span>(head_prev, k);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> dummy.next;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function">ListNode* <span class="title">reverseList</span><span class="params">(ListNode* head_prev, <span class="type">int</span> k)</span></span>&#123;</span><br><span class="line">        ListNode* head = head_prev-&gt;next;</span><br><span class="line">        ListNode *prev, *cur, *next;</span><br><span class="line">        prev = <span class="literal">nullptr</span>;</span><br><span class="line">        cur = head;</span><br><span class="line">        <span class="keyword">while</span>(k&gt;<span class="number">0</span>)&#123;</span><br><span class="line">            next = cur-&gt;next;</span><br><span class="line">            cur-&gt;next = prev;</span><br><span class="line">            prev = cur;</span><br><span class="line">            cur = next;</span><br><span class="line">            k--;</span><br><span class="line">        &#125;</span><br><span class="line">        head_prev-&gt;next = prev;</span><br><span class="line">        head-&gt;next = cur;</span><br><span class="line">        <span class="keyword">return</span> head;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="随机链表的复制">6. 随机链表的复制</h3><p><ahref="https://leetcode.cn/problems/copy-list-with-random-pointer/description/">LeetCodeHot100: 138.复制带随机指针的链表</a></p><p><strong>简要描述</strong>：给你一个长度为 n的链表，每个节点包含一个额外增加的随机指针 random，该指针可以指向链表中的任何节点或空节点。构造这个链表的 深拷贝。深拷贝应该正好由 n 个 全新节点组成，其中每个新节点的值都设为其对应的原节点的值。新节点的 next指针和 random指针也都应指向复制链表中的新节点，以确保原链表和复制链表中对应节点之间的相对位置关系保持不变。返回复制链表的头节点。</p><p><strong>解题关键</strong>：这题可以使用<strong>哈希表</strong>来解决。我们可以遍历原链表，对于每个节点，创建一个新的节点，并将原节点和新节点的映射关系存储在哈希表中。然后我们再遍历一次原链表，使用哈希表来设置新节点的next 和 random 指针。最后返回新链表的头节点即可。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">Node* <span class="title">copyRandomList</span><span class="params">(Node* head)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(!head)&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">nullptr</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        unordered_map&lt;Node*, Node*&gt; mp;</span><br><span class="line">        Node* cur = head;</span><br><span class="line">        <span class="keyword">while</span>(cur)&#123;</span><br><span class="line">            mp[cur] = <span class="keyword">new</span> <span class="built_in">Node</span>(cur-&gt;val);</span><br><span class="line">            cur = cur-&gt;next;</span><br><span class="line">        &#125;</span><br><span class="line">        Node *old, *_new;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">auto</span> &amp;p:mp)&#123;</span><br><span class="line">            old = p.first;</span><br><span class="line">            _new = p.second;</span><br><span class="line">            _new-&gt;next = old-&gt;next==<span class="literal">nullptr</span> ? <span class="literal">nullptr</span>: mp[old-&gt;next];</span><br><span class="line">            _new-&gt;random = old-&gt;random==<span class="literal">nullptr</span> ? <span class="literal">nullptr</span>: mp[old-&gt;random];</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> mp[head];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>这样做的空间复杂度是 <spanclass="math inline">𝒪(<em>n</em>)</span>，如果想要降低空间复杂度，可以使用一种更巧妙的方法，在原链表中插入新节点，然后再分离出新链表，这样做的空间复杂度是<span class="math inline">𝒪(1)</span>。具体做法如下： 1.遍历原链表，对于每个节点，创建一个新的节点，并将新节点插入到原节点的后面。2. 再次遍历链表，设置新节点的 random 指针（random指针指向原链表中对应节点的 random 指针的后一项）。 3.最后通过设置每个原节点和新节点的 next 指针分离出新链表。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">Node* <span class="title">copyRandomList</span><span class="params">(Node* head)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(!head)&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">nullptr</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        Node* cur = head;</span><br><span class="line">        <span class="keyword">while</span>(cur)&#123;</span><br><span class="line">            Node* newNode = <span class="keyword">new</span> <span class="built_in">Node</span>(cur-&gt;val);</span><br><span class="line">            newNode-&gt;next = cur-&gt;next;</span><br><span class="line">            cur-&gt;next = newNode;</span><br><span class="line">            cur = newNode-&gt;next;</span><br><span class="line">        &#125;</span><br><span class="line">        cur = head;</span><br><span class="line">        <span class="keyword">while</span>(cur)&#123;</span><br><span class="line">            <span class="keyword">if</span>(cur-&gt;random)&#123;</span><br><span class="line">                cur-&gt;next-&gt;random = cur-&gt;random-&gt;next;</span><br><span class="line">            &#125;</span><br><span class="line">            cur = cur-&gt;next-&gt;next;</span><br><span class="line">        &#125;</span><br><span class="line">        Node* newHead = head-&gt;next;</span><br><span class="line">        cur = head;</span><br><span class="line">        <span class="keyword">while</span>(cur)&#123;</span><br><span class="line">            Node* temp = cur-&gt;next;</span><br><span class="line">            cur-&gt;next = temp-&gt;next;</span><br><span class="line">            <span class="keyword">if</span>(temp-&gt;next)&#123;</span><br><span class="line">                temp-&gt;next = temp-&gt;next-&gt;next;</span><br><span class="line">            &#125;</span><br><span class="line">            cur = cur-&gt;next;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> newHead;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="排序链表">7. 排序链表</h3><p><ahref="https://leetcode.cn/problems/sort-list/description/">LeetCodeHot100: 148.排序链表</a></p><p><strong>简要描述</strong>：给你链表的头结点 head ，请将其按 升序排列并返回 排序后的链表 ，要求时间复杂度为 O(n log n) 。</p><p><strong>解题关键</strong>：这题可以使用<strong>归并排序</strong>来解决。归并排序是一种分治算法，适用于链表排序，因为它不需要额外的空间来存储数组。具体做法如下：1. 使用快慢指针找到链表的中点，将链表分成两半。 2.递归地对两半链表进行归并排序。 3. 合并两个排序后的链表。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">ListNode* <span class="title">sortList</span><span class="params">(ListNode* head)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(!head || !head-&gt;next)&#123;</span><br><span class="line">            <span class="keyword">return</span> head;</span><br><span class="line">        &#125;</span><br><span class="line">        ListNode* mid = <span class="built_in">findMiddle</span>(head);</span><br><span class="line">        ListNode* left = <span class="built_in">sortList</span>(head);</span><br><span class="line">        ListNode* right = <span class="built_in">sortList</span>(mid);</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">merge</span>(left, right);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function">ListNode* <span class="title">findMiddle</span><span class="params">(ListNode* head)</span> </span>&#123;</span><br><span class="line">        ListNode* slow=head;</span><br><span class="line">        ListNode* fast=head;</span><br><span class="line">        <span class="keyword">while</span>(fast!=tail)&#123;</span><br><span class="line">            slow = slow-&gt;next;</span><br><span class="line">            fast = fast-&gt;next;</span><br><span class="line">            <span class="keyword">if</span>(fast!=tail)&#123;</span><br><span class="line">                fast = fast-&gt;next;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> slow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function">ListNode* <span class="title">merge</span><span class="params">(ListNode* l1, ListNode* l2)</span> </span>&#123;</span><br><span class="line">        ListNode dummy = <span class="built_in">ListNode</span>(<span class="number">0</span>);</span><br><span class="line">        ListNode* cur = &amp;dummy;</span><br><span class="line">        <span class="keyword">while</span>(l1 &amp;&amp; l2) &#123;</span><br><span class="line">            <span class="keyword">if</span>(l1-&gt;val &lt; l2-&gt;val) &#123;</span><br><span class="line">                cur-&gt;next = l1;</span><br><span class="line">                l1 = l1-&gt;next;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                cur-&gt;next = l2;</span><br><span class="line">                l2 = l2-&gt;next;</span><br><span class="line">            &#125;</span><br><span class="line">            cur = cur-&gt;next;</span><br><span class="line">        &#125;</span><br><span class="line">        cur-&gt;next = l1 ? l1 : l2;</span><br><span class="line">        <span class="keyword">return</span> dummy.next;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="合并k个排序链表">8. 合并K个排序链表</h3><p><ahref="https://leetcode.cn/problems/merge-k-sorted-lists/description/">LeetCodeHot100: 23.合并K个排序链表</a></p><p><strong>简要描述</strong>：给你一个链表数组，每个链表都已经按升序排列，请将它们合并成一个新的升序链表。</p><p><strong>解题关键</strong>：这题可以使用<strong>优先队列</strong>（小根堆）来解决。我们可以将每个链表的头节点放入优先队列中，然后不断取出最小的节点，并将其加入到结果链表中，同时将该节点的下一个节点放入优先队列中。这样可以保证每次取出的节点都是当前所有链表中的最小值。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">ListNode* <span class="title">mergeKLists</span><span class="params">(vector&lt;ListNode*&gt;&amp; lists)</span> </span>&#123;</span><br><span class="line">        priority_queue&lt;pair&lt;<span class="type">int</span>, ListNode*&gt;, vector&lt;pair&lt;<span class="type">int</span>, ListNode*&gt;&gt;, greater&lt;pair&lt;<span class="type">int</span>, ListNode*&gt;&gt;&gt; pq;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">auto</span>&amp; head : lists) &#123;</span><br><span class="line">            <span class="keyword">if</span>(head) &#123;</span><br><span class="line">                pq.<span class="built_in">push</span>(&#123;head-&gt;val, head&#125;);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        ListNode dummy = <span class="built_in">ListNode</span>(<span class="number">0</span>);</span><br><span class="line">        ListNode* cur = &amp;dummy;</span><br><span class="line">        <span class="keyword">while</span>(!pq.<span class="built_in">empty</span>()) &#123;</span><br><span class="line">            <span class="keyword">auto</span> [val, node] = pq.<span class="built_in">top</span>();</span><br><span class="line">            pq.<span class="built_in">pop</span>();</span><br><span class="line">            cur-&gt;next = node;</span><br><span class="line">            cur = cur-&gt;next;</span><br><span class="line">            <span class="keyword">if</span>(node-&gt;next) &#123;</span><br><span class="line">                pq.<span class="built_in">push</span>(&#123;node-&gt;next-&gt;val, node-&gt;next&#125;);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> dummy.next;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>也可以使用分治法来合并 K个排序链表。具体做法是将链表两两合并，直到只剩下一个链表，这种做法可以直接利用之前实现的合并两个排序链表的函数：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">ListNode* <span class="title">merge2Lists</span><span class="params">(ListNode* l1, ListNode* l2)</span></span>&#123;</span><br><span class="line">        ListNode* dummy = <span class="keyword">new</span> <span class="built_in">ListNode</span>();</span><br><span class="line">        ListNode* cur = dummy;</span><br><span class="line">        <span class="keyword">while</span>(l1 &amp;&amp; l2)&#123;</span><br><span class="line">            <span class="keyword">if</span>(l1-&gt;val&lt;=l2-&gt;val)&#123;</span><br><span class="line">                cur-&gt;next = l1;</span><br><span class="line">                l1 = l1-&gt;next;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span>&#123;</span><br><span class="line">                cur-&gt;next = l2;</span><br><span class="line">                l2 = l2-&gt;next;</span><br><span class="line">            &#125;</span><br><span class="line">            cur = cur-&gt;next;</span><br><span class="line">        &#125;</span><br><span class="line">        cur-&gt;next = l1 ? l1 : l2;</span><br><span class="line">        <span class="keyword">return</span> dummy-&gt;next;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function">ListNode* <span class="title">merge</span><span class="params">(vector&lt;ListNode*&gt;&amp; lists, <span class="type">int</span> l, <span class="type">int</span> r)</span></span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(l&gt;r) <span class="keyword">return</span> <span class="literal">nullptr</span>;</span><br><span class="line">        <span class="keyword">if</span>(l==r) <span class="keyword">return</span> lists[l];</span><br><span class="line">        <span class="type">int</span> mid = (l+r)/<span class="number">2</span>;</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">merge2Lists</span>(<span class="built_in">merge</span>(lists, l, mid), <span class="built_in">merge</span>(lists, mid<span class="number">+1</span>, r));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function">ListNode* <span class="title">mergeKLists</span><span class="params">(vector&lt;ListNode*&gt;&amp; lists)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">merge</span>(lists, <span class="number">0</span>, lists.<span class="built_in">size</span>()<span class="number">-1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="lru缓存机制">9. LRU缓存机制</h3><p><ahref="https://leetcode.cn/problems/lru-cache/description/">LeetCodeHot100: 146.LRU缓存机制</a></p><p><strong>简要描述</strong>：设计并实现一个满足 LRU (最近最少使用) 缓存约束的数据结构。实现 LRUCache 类： - <code>LRUCache(int capacity)</code>以 正整数 作为容量 capacity 初始化 LRU - <code>int get(int key)</code>如果关键字 key 存在于缓存中，则返回关键字的值，否则返回 -1 。 -<code>void put(int key, int value)</code> 如果关键字 key已经存在，则变更其数据值 value ；如果不存在，则向缓存中插入该组key-value。当缓存容量达到上限时，它应该在写入新数据之前删除最久未使用的数据值，从而为新的数据值留出空间。</p><p><strong>解题关键</strong>：这题可以使用<strong>哈希表</strong>和<strong>双向链表</strong>来实现。哈希表用于快速查找缓存中的数据，双向链表用于维护数据的使用顺序。每次访问数据时，我们将该数据节点移动到链表的头部，表示它是最近使用的数据。当缓存容量达到上限时，我们删除链表尾部的节点，表示删除最久未使用的数据。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">DListNode</span>&#123;</span><br><span class="line">    <span class="type">int</span> key, val;</span><br><span class="line">    DListNode *prev;</span><br><span class="line">    DListNode *next;</span><br><span class="line">    <span class="built_in">DListNode</span>(): <span class="built_in">key</span>(<span class="number">0</span>), <span class="built_in">val</span>(<span class="number">0</span>), <span class="built_in">prev</span>(<span class="literal">nullptr</span>), <span class="built_in">next</span>(<span class="literal">nullptr</span>) &#123;&#125;</span><br><span class="line">    <span class="built_in">DListNode</span>(<span class="type">int</span> _key, <span class="type">int</span> _val): <span class="built_in">key</span>(_key), <span class="built_in">val</span>(_val), <span class="built_in">prev</span>(<span class="literal">nullptr</span>), <span class="built_in">next</span>(<span class="literal">nullptr</span>) &#123;&#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LRUCache</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    unordered_map&lt;<span class="type">int</span>, DListNode*&gt; cache;</span><br><span class="line">    DListNode *head,  *tail;</span><br><span class="line">    <span class="type">int</span> size;</span><br><span class="line">    <span class="type">int</span> volume;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">LRUCache</span>(<span class="type">int</span> capacity) &#123;</span><br><span class="line">        head = <span class="keyword">new</span> <span class="built_in">DListNode</span>();</span><br><span class="line">        tail = <span class="keyword">new</span> <span class="built_in">DListNode</span>();</span><br><span class="line">        head-&gt;next = tail;</span><br><span class="line">        tail-&gt;prev = head;</span><br><span class="line">        volume = capacity;</span><br><span class="line">        size = <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">get</span><span class="params">(<span class="type">int</span> key)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(!cache.<span class="built_in">count</span>(key))&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        DListNode* node = cache[key];</span><br><span class="line">        <span class="built_in">moveToHead</span>(node);</span><br><span class="line">        <span class="keyword">return</span> node-&gt;val;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">put</span><span class="params">(<span class="type">int</span> key, <span class="type">int</span> value)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(!cache.<span class="built_in">count</span>(key))&#123;</span><br><span class="line">            DListNode *node = <span class="keyword">new</span> <span class="built_in">DListNode</span>(key, value);</span><br><span class="line">            cache[key] = node;</span><br><span class="line">            <span class="built_in">addToHead</span>(node);</span><br><span class="line">            size++;</span><br><span class="line">            <span class="keyword">if</span>(size &gt; volume)&#123;</span><br><span class="line">                DListNode* removed = <span class="built_in">removeTail</span>();</span><br><span class="line">                cache.<span class="built_in">erase</span>(removed-&gt;key);</span><br><span class="line">                <span class="keyword">delete</span> removed;</span><br><span class="line">                size--;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span>&#123;</span><br><span class="line">            DListNode* node = cache[key];</span><br><span class="line">            node-&gt;val = value;</span><br><span class="line">            <span class="built_in">moveToHead</span>(node);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">addToHead</span><span class="params">(DListNode* node)</span></span>&#123;</span><br><span class="line">        node-&gt;next = head-&gt;next;</span><br><span class="line">        node-&gt;prev = head;</span><br><span class="line">        head-&gt;next-&gt;prev = node;</span><br><span class="line">        head-&gt;next = node;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">removeNode</span><span class="params">(DListNode* node)</span></span>&#123;</span><br><span class="line">        node-&gt;prev-&gt;next = node-&gt;next;</span><br><span class="line">        node-&gt;next-&gt;prev = node-&gt;prev;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">moveToHead</span><span class="params">(DListNode* node)</span></span>&#123;</span><br><span class="line">        <span class="built_in">removeNode</span>(node);</span><br><span class="line">        <span class="built_in">addToHead</span>(node);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function">DListNode* <span class="title">removeTail</span><span class="params">()</span></span>&#123;</span><br><span class="line">        DListNode* node = tail-&gt;prev;</span><br><span class="line">        <span class="built_in">removeNode</span>(node);</span><br><span class="line">        <span class="keyword">return</span> node;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 刷题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Algorithm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>普通数组算法题</title>
      <link href="/2025/09/28/Algo/array/"/>
      <url>/2025/09/28/Algo/array/</url>
      
        <content type="html"><![CDATA[<h1 id="普通数组">普通数组</h1><p>一般在算法题中，使用普通数组来存储数据时，往往会结合一些技巧来提高效率，比如前缀和、差分数组等。在C++中，普通数组可以通过静态数组或动态数组（如<code>vector</code>）来实现。</p><h2 id="例题">例题</h2><h3 id="合并区间">1. 合并区间</h3><p><ahref="https://leetcode.cn/problems/merge-intervals/description/">LeetCodeHot100: 56.合并区间</a></p><p><strong>简要描述</strong>：以数组 intervals表示若干个区间的集合，其中单个区间为 intervals[i] = [starti,endi]。请你合并所有重叠的区间，并返回一个不重叠的区间数组，该数组需恰好覆盖输入中的所有区间。</p><p><strong>解题关键</strong>：这里可能有些难想到，我们首先对区间按照起始位置进行排序，然后遍历排序后的区间列表，使用一个新的数组来存储合并后的区间。如果当前区间的起始位置小于或等于上一个合并区间的结束位置，则说明两个区间重叠，可以更新上一个合并区间的结束位置为两个区间结束位置的最大值。否则，将当前区间添加到合并后的数组中。具体来说，我们用数组<code>merged</code> 存储最终的答案：</p><ul><li><p>首先，我们将列表中的区间按照左端点升序排序。然后我们将第一个区间加入<code>merged</code> 数组中，并按顺序依次考虑之后的每个区间：</p></li><li><p>如果当前区间的左端点在数组 <code>merged</code>中最后一个区间的右端点之后，那么它们不会重合，我们可以直接将这个区间加入数组<code>merged</code> 的末尾；</p></li><li><p>否则，它们重合，我们需要用当前区间的右端点更新数组<code>merged</code>中最后一个区间的右端点，将其置为二者的较大值。</p></li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    vector&lt;vector&lt;<span class="type">int</span>&gt;&gt; <span class="built_in">merge</span>(vector&lt;vector&lt;<span class="type">int</span>&gt;&gt;&amp; intervals) &#123;</span><br><span class="line">        <span class="built_in">sort</span>(intervals.<span class="built_in">begin</span>(), intervals.<span class="built_in">end</span>());</span><br><span class="line">        vector&lt;vector&lt;<span class="type">int</span>&gt;&gt; ans;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">auto</span> &amp;interval:intervals)&#123;</span><br><span class="line">            <span class="keyword">if</span>(ans.<span class="built_in">empty</span>())&#123;</span><br><span class="line">                ans.<span class="built_in">push_back</span>(interval);</span><br><span class="line">                <span class="keyword">continue</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="type">int</span> l = interval[<span class="number">0</span>], r = interval[<span class="number">1</span>];</span><br><span class="line">            <span class="keyword">if</span>(l&gt;ans.<span class="built_in">back</span>()[<span class="number">1</span>])&#123;</span><br><span class="line">                ans.<span class="built_in">push_back</span>(interval);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span>&#123;</span><br><span class="line">                ans.<span class="built_in">back</span>()[<span class="number">1</span>] = <span class="built_in">max</span>(ans.<span class="built_in">back</span>()[<span class="number">1</span>], r);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="缺失的第一个正数">2. 缺失的第一个正数</h3><p><ahref="https://leetcode.cn/problems/first-missing-positive/description/">LeetCodeHot100: 41.缺失的第一个正数</a></p><p><strong>简要描述</strong>：给你一个未排序的整数数组 nums，请你找出其中没有出现的最小的正整数。</p><p><strong>解题关键</strong>：如果本题没有额外的时空复杂度要求，那么就很容易实现：</p><ul><li><p>我们可以将数组所有的数放入哈希表，随后从 1开始依次枚举正整数，并判断其是否在哈希表中；</p></li><li><p>我们可以从 1开始依次枚举正整数，并遍历数组，判断其是否在数组中。</p></li></ul><p>如果数组的长度为 N，那么第一种做法的时间复杂度为 <spanclass="math inline">𝒪(<em>N</em>)</span>，空间复杂度为 <spanclass="math inline">𝒪(<em>N</em>)</span>；第二种做法的时间复杂度为 <spanclass="math inline">𝒪(<em>N</em><sup>2</sup>)</span>，空间复杂度为 <spanclass="math inline">𝒪(1)</span>。但它们都不满足时间复杂度为 <spanclass="math inline">𝒪(<em>N</em>)</span> 且空间复杂度为 <spanclass="math inline">𝒪(1)</span>。满足时间复杂度为 <spanclass="math inline">𝒪(<em>N</em>)</span> 且空间复杂度为 <spanclass="math inline">𝒪(1)</span>的算法是不存在的，但是我们可以退而求其次：<strong>利用给定数组中的空间来存储一些状态</strong>。也就是说，如果题目给定的数组是不可修改的，那么就不存在满足时空复杂度要求的算法；但如果我们可以修改给定的数组，那么是存在满足要求的算法的。</p><p>实际上，对于一个长度为 N 的数组，答案一定在区间 [1,N+1]内（如果[1,N]都出现了，答案为N+1；否则是该区间内没有出现的最小整数）。因此我们可以直接将这个数组改造成一个带标记的数组，但是这个标记要如何设计才能不占用额外存储空间也不改变原位置的数值呢？这里我们可以想到用<strong>取负</strong>的方式来进行标记，但是这样做首先我们需要将数组中的所有负数全部排除掉，以免和标记混淆，可以考虑将负数全部改为n+1，然后再遍历数组进行标记。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">firstMissingPositive</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> n = nums.<span class="built_in">size</span>();</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">auto</span> &amp;num:nums)&#123;</span><br><span class="line">            <span class="keyword">if</span>(num&lt;=<span class="number">0</span>)&#123;</span><br><span class="line">                num = n<span class="number">+1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;n;i++)&#123;</span><br><span class="line">            <span class="type">int</span> num = <span class="built_in">abs</span>(nums[i]);</span><br><span class="line">            <span class="keyword">if</span>(num&lt;=n)&#123;</span><br><span class="line">                nums[num<span class="number">-1</span>] = -<span class="built_in">abs</span>(nums[num<span class="number">-1</span>]);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;n;i++)&#123;</span><br><span class="line">            <span class="keyword">if</span>(nums[i]&gt;<span class="number">0</span>)&#123;</span><br><span class="line">                <span class="keyword">return</span> i<span class="number">+1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> n<span class="number">+1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="搜索二维矩阵">3. 搜索二维矩阵</h3><p><ahref="https://leetcode.cn/problems/search-a-2d-matrix-ii/description/">LeetCodeHot100: 240.搜索二维矩阵2</a></p><p><strong>简要描述</strong>：编写一个高效的算法来搜索 m x n 矩阵 matrix中的一个目标值 target 。该矩阵具有以下特性：</p><ul><li>每行的元素从左到右升序排列。</li><li>每列的元素从上到下升序排列。</li></ul><p><strong>解题关键</strong>：这一题比较取巧，我们可以从整个矩阵的左下角（或者右上角）开始搜索，这里以左下角为例，搜索过程呈现出一个“Z”字形：</p><ul><li>如果当前元素等于目标值，搜索结束；</li><li>如果当前元素小于目标值，说明目标值不可能在当前元素的这一行（因为这一行的其他元素都比当前元素大），因此将行索引减少1，移动到上一行；</li><li>如果当前元素大于目标值，说明目标值不可能在当前元素的这一列（因为这一列的其他元素都比当前元素小），因此将列索引增加1，移动到下一列。</li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">bool</span> <span class="title">searchMatrix</span><span class="params">(vector&lt;vector&lt;<span class="type">int</span>&gt;&gt;&amp; matrix, <span class="type">int</span> target)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> m = matrix.<span class="built_in">size</span>();</span><br><span class="line">        <span class="type">int</span> n = matrix[<span class="number">0</span>].<span class="built_in">size</span>();</span><br><span class="line">        <span class="type">int</span> r = m<span class="number">-1</span>, c = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">while</span>(r&gt;=<span class="number">0</span> &amp;&amp; c&lt;n)&#123;</span><br><span class="line">            <span class="keyword">if</span>(matrix[r][c]==target)&#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span>(matrix[r][c]&gt;target)&#123;</span><br><span class="line">                r--;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span>&#123;</span><br><span class="line">                c++;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="分发糖果">4. 分发糖果</h3><p><a href="https://leetcode.cn/problems/candy/description/">LeetCode经典150: 135.分发糖果</a></p><p><strong>简要描述</strong>：有 N个孩子站成一排。给每个孩子分发糖果，要求如下： - 每个孩子至少分配到 1个糖果。 - 相邻的孩子中，评分更高的孩子必须获得更多的糖果。这样的设置下求问需要至少多少颗糖果。</p><p><strong>解题关键</strong>：我们可以使用两次遍历来解决这个问题。第一次从左到右遍历数组，确保每个孩子比左边的孩子评分高时获得更多的糖果；第二次从右到左遍历数组，确保每个孩子比右边的孩子评分高时获得更多的糖果。最终，每个孩子获得的糖果数为两次遍历中较大的值。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">candy</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; ratings)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> n = ratings.<span class="built_in">size</span>();</span><br><span class="line">        <span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">left2right</span><span class="params">(n, <span class="number">1</span>)</span></span>;</span><br><span class="line">        <span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">right2left</span><span class="params">(n, <span class="number">1</span>)</span></span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">1</span>;i&lt;n;i++)&#123;</span><br><span class="line">            <span class="keyword">if</span>(ratings[i]&gt;ratings[i<span class="number">-1</span>])&#123;</span><br><span class="line">                left2right[i] = left2right[i<span class="number">-1</span>]<span class="number">+1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=n<span class="number">-2</span>;i&gt;=<span class="number">0</span>;i--)&#123;</span><br><span class="line">            <span class="keyword">if</span>(ratings[i]&gt;ratings[i<span class="number">+1</span>])&#123;</span><br><span class="line">                right2left[i] = right2left[i<span class="number">+1</span>]<span class="number">+1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="type">int</span> ans = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;n;i++)&#123;</span><br><span class="line">            ans += <span class="built_in">max</span>(left2right[i], right2left[i]);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 刷题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Algorithm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>滑动窗口的算法题</title>
      <link href="/2025/09/26/Algo/sliding_window/"/>
      <url>/2025/09/26/Algo/sliding_window/</url>
      
        <content type="html"><![CDATA[<h1 id="滑动窗口的使用">滑动窗口的使用</h1><h2 id="应用场景">应用场景</h2><p>滑动窗口是一种用于处理数组或字符串中连续子序列问题的高效算法技术。它通过维护一个动态调整的窗口来遍历数据结构，从而避免了重复计算，提高了算法的效率。滑动窗口常用于解决涉及子数组或子串的最大值、最小值、和等问题，特别是在需要频繁更新和查询的场景中。</p><h2 id="例题">例题</h2><h3 id="滑动窗口最大值">1. 滑动窗口最大值</h3><p><ahref="https://leetcode.cn/problems/sliding-window-maximum/description/">LeetCodeHot100: 239.滑动窗口最大值</a></p><p><strong>简要描述</strong>：给你一个整数数组 nums，有一个大小为 k的滑动窗口从数组的最左侧移动到数组的最右侧。你只可以看到在滑动窗口内的 k个数字。滑动窗口每次只向右移动一位。返回滑动窗口中的最大值。</p><p><strong>解题关键</strong>：最直观的做法是，对于每一个窗口位置，遍历窗口内的k 个元素，找到最大值，这样的时间复杂度为 <spanclass="math inline">𝒪(<em>N</em> ⋅ <em>K</em>)</span>，在数据量较大时效率较低。但是对于频繁访问最大值/最小值，我们初步的想法就是使用<strong>大根堆</strong><code>priority_queue</code>：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">maxSlidingWindow</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums, <span class="type">int</span> k)</span> </span>&#123;</span><br><span class="line">        priority_queue&lt;pair&lt;<span class="type">int</span>, <span class="type">int</span>&gt;&gt; pq;</span><br><span class="line">        vector&lt;<span class="type">int</span>&gt; ans;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;k;i++)&#123;</span><br><span class="line">            pq.<span class="built_in">emplace</span>(nums[i],i);</span><br><span class="line">        &#125;</span><br><span class="line">        ans.<span class="built_in">push_back</span>(pq.<span class="built_in">top</span>().first);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i+k&lt;nums.<span class="built_in">size</span>();i++)&#123;</span><br><span class="line">            pq.<span class="built_in">emplace</span>(nums[i+k],i+k);</span><br><span class="line">            <span class="keyword">while</span>(pq.<span class="built_in">top</span>().second&lt;=i)&#123;</span><br><span class="line">                pq.<span class="built_in">pop</span>();</span><br><span class="line">            &#125;</span><br><span class="line">            ans.<span class="built_in">push_back</span>(pq.<span class="built_in">top</span>().first);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>使用一个大根堆来维护当前窗口内的元素，每次滑动窗口时，将新元素加入堆中，并移除堆中不在窗口内的元素。这样每次获取最大值的时间复杂度为<span class="math inline">𝒪(log <em>K</em>)</span>，整体时间复杂度为<spanclass="math inline">𝒪(<em>N</em>log <em>K</em>)</span>。如果要进一步优化，可以使用单调队列来实现，时间复杂度可以降到<spanclass="math inline">𝒪(<em>N</em>)</span>，具体来说，使用一个<strong>单调递减队列</strong>存储下标（队列头存最大元素下标）：</p><p>保证队列里的元素对应的值从队头到队尾单调递减：</p><ul><li><p>新元素进来时，把队尾比它小的元素都弹出（因为它们永远不可能成为最大值）；</p></li><li><p>再把新元素加入队尾；</p></li><li><p>如果队头已经滑出窗口，就弹出队头；</p></li><li><p>队头就是当前窗口的最大值。</p></li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">maxSlidingWindow</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums, <span class="type">int</span> k)</span> </span>&#123;</span><br><span class="line">        deque&lt;<span class="type">int</span>&gt; dq;</span><br><span class="line">        vector&lt;<span class="type">int</span>&gt; ans;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;k;i++)&#123;</span><br><span class="line">            <span class="keyword">while</span>(!dq.<span class="built_in">empty</span>() &amp;&amp; nums[dq.<span class="built_in">back</span>()]&lt;=nums[i])&#123;</span><br><span class="line">                dq.<span class="built_in">pop_back</span>();</span><br><span class="line">            &#125;</span><br><span class="line">            dq.<span class="built_in">push_back</span>(i);</span><br><span class="line">        &#125;</span><br><span class="line">        ans.<span class="built_in">push_back</span>(nums[dq.<span class="built_in">front</span>()]);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=k;i&lt;nums.<span class="built_in">size</span>();i++)&#123;</span><br><span class="line">            <span class="keyword">while</span>(!dq.<span class="built_in">empty</span>() &amp;&amp; nums[dq.<span class="built_in">back</span>()]&lt;=nums[i])&#123;</span><br><span class="line">                dq.<span class="built_in">pop_back</span>();</span><br><span class="line">            &#125;</span><br><span class="line">            dq.<span class="built_in">push_back</span>(i);</span><br><span class="line">            <span class="keyword">while</span>(dq.<span class="built_in">front</span>()&lt;=i-k)&#123;</span><br><span class="line">                dq.<span class="built_in">pop_front</span>();</span><br><span class="line">            &#125;</span><br><span class="line">            ans.<span class="built_in">push_back</span>(nums[dq.<span class="built_in">front</span>()]);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 刷题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Algorithm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>字符串与子串的算法题</title>
      <link href="/2025/09/24/Algo/string/"/>
      <url>/2025/09/24/Algo/string/</url>
      
        <content type="html"><![CDATA[<h1 id="字符串与子串">字符串与子串</h1><h2 id="字符串基本操作">字符串基本操作</h2><p>在 c++ 中，字符串是通过 <code>string</code>类来表示的。字符串是字符的有序集合，可以进行各种操作，如连接、比较、查找等。</p><p>string 类提供了丰富的成员函数来操作字符串，以下是一些常用的操作：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;string&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    string str1 = <span class="string">&quot;Hello&quot;</span>;</span><br><span class="line">    string str2 = <span class="string">&quot;World&quot;</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 连接字符串</span></span><br><span class="line">    string str3 = str1 + <span class="string">&quot; &quot;</span> + str2; <span class="comment">// &quot;Hello World&quot;</span></span><br><span class="line">    cout &lt;&lt; str3 &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取字符串长度</span></span><br><span class="line">    cout &lt;&lt; <span class="string">&quot;Length of str3: &quot;</span> &lt;&lt; str<span class="number">3.l</span>ength() &lt;&lt; endl; <span class="comment">// 11</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 访问字符</span></span><br><span class="line">    cout &lt;&lt; <span class="string">&quot;First character: &quot;</span> &lt;&lt; str3[<span class="number">0</span>] &lt;&lt; endl; <span class="comment">// &#x27;H&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 查找子串</span></span><br><span class="line">    <span class="type">size_t</span> pos = str<span class="number">3.f</span>ind(<span class="string">&quot;World&quot;</span>);</span><br><span class="line">    <span class="keyword">if</span> (pos != string::npos) &#123;</span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;&#x27;World&#x27; found at position: &quot;</span> &lt;&lt; pos &lt;&lt; endl; <span class="comment">// 6</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 提取子串</span></span><br><span class="line">    string subStr = str<span class="number">3.</span><span class="built_in">substr</span>(<span class="number">6</span>, <span class="number">5</span>); <span class="comment">// &quot;World&quot;</span></span><br><span class="line">    cout &lt;&lt; <span class="string">&quot;Substring: &quot;</span> &lt;&lt; subStr &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 替换子串</span></span><br><span class="line">    str<span class="number">3.</span><span class="built_in">replace</span>(<span class="number">6</span>, <span class="number">5</span>, <span class="string">&quot;C++&quot;</span>); <span class="comment">// &quot;Hello C++&quot;</span></span><br><span class="line">    cout &lt;&lt; <span class="string">&quot;After replacement: &quot;</span> &lt;&lt; str3 &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 将指定子串替换成另一个子串</span></span><br><span class="line">    string str4 = <span class="string">&quot;I love programming. Programming is fun.&quot;</span>;</span><br><span class="line">    string toReplace = <span class="string">&quot;Programming&quot;</span>;</span><br><span class="line">    string replaceWith = <span class="string">&quot;coding&quot;</span>;</span><br><span class="line">    <span class="type">size_t</span> startPos = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span> ((startPos = str<span class="number">4.f</span>ind(toReplace, startPos)) != string::npos) &#123;</span><br><span class="line">        str<span class="number">4.</span><span class="built_in">replace</span>(startPos, toReplace.<span class="built_in">length</span>(), replaceWith);</span><br><span class="line">        startPos += replaceWith.<span class="built_in">length</span>(); <span class="comment">// Move past the replaced substring</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>同时对于<code>char</code>类型，可以使用ASCII码进行操作，例如：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">char</span> ch = <span class="string">&#x27;A&#x27;</span>;</span><br><span class="line">ch = ch + <span class="number">1</span>; <span class="comment">// ch 现在是 &#x27;B&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="type">int</span> asciiValue = (<span class="type">int</span>)ch; <span class="comment">// asciiValue 现在是 66</span></span><br><span class="line"><span class="type">int</span> distance = ch - <span class="string">&#x27;a&#x27;</span>; <span class="comment">// 计算 ch 和 &#x27;a&#x27; 之间的距离</span></span><br></pre></td></tr></table></figure><h2 id="例题">例题</h2><h3 id="找到字符串中所有字母异位词">1. 找到字符串中所有字母异位词</h3><p><ahref="https://leetcode.cn/problems/find-all-anagrams-in-a-string/description/">LeetCodeHot100: 438.找到字符串中所有字母异位词</a></p><p><strong>简要描述</strong>：给定两个字符串 s 和 p，找到 s 中所有 p的字母异位词的起始索引。字母异位词指字母相同，但排列不同的字符串。返回这些子串的起始索引，可以按任意顺序返回。</p><p><strong>解题关键</strong>：我们可以使用数组来存储字符串 p和滑动窗口中每种字母的数量，然后固定窗口大小进行滑动，比较两个数组是否相等。在此基础上，可以进一步优化，不再分别统计滑动窗口和字符串p 中每种字母的数量，而是统计滑动窗口和字符串 p中每种字母数量的差；并引入变量 differ 来记录当前窗口与字符串 p中数量不同的字母的个数，并在滑动窗口的过程中维护它。在判断滑动窗口中每种字母的数量与字符串p 中每种字母的数量是否相同时，只需要判断 differ 是否为零即可。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">findAnagrams</span><span class="params">(string s, string p)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> slen = s.<span class="built_in">size</span>();</span><br><span class="line">        <span class="type">int</span> plen = p.<span class="built_in">size</span>();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span>(slen&lt;plen)&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">vector</span>&lt;<span class="type">int</span>&gt;();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 统计26个字母的频次差</span></span><br><span class="line">        <span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">count</span><span class="params">(<span class="number">26</span>)</span></span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;plen;i++)&#123;</span><br><span class="line">            count[s[i]-<span class="string">&#x27;a&#x27;</span>]++;</span><br><span class="line">            count[p[i]-<span class="string">&#x27;a&#x27;</span>]--;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="type">int</span> diff = <span class="number">0</span>;</span><br><span class="line">        vector&lt;<span class="type">int</span>&gt; ans;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">auto</span> &amp;c:count)&#123;</span><br><span class="line">            <span class="keyword">if</span>(c!=<span class="number">0</span>)&#123;</span><br><span class="line">                diff++;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span>(diff==<span class="number">0</span>)&#123;</span><br><span class="line">            ans.<span class="built_in">push_back</span>(<span class="number">0</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> cur=<span class="number">0</span>; cur&lt;slen-plen;cur++)&#123;</span><br><span class="line">            count[s[cur]-<span class="string">&#x27;a&#x27;</span>]--;</span><br><span class="line">            <span class="keyword">if</span>(count[s[cur]-<span class="string">&#x27;a&#x27;</span>]==<span class="number">0</span>)&#123;</span><br><span class="line">                diff--; </span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span>(count[s[cur]-<span class="string">&#x27;a&#x27;</span>]==<span class="number">-1</span>)&#123;</span><br><span class="line">                diff++;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            count[s[cur+plen]-<span class="string">&#x27;a&#x27;</span>]++;</span><br><span class="line">            <span class="keyword">if</span>(count[s[cur+plen]-<span class="string">&#x27;a&#x27;</span>]==<span class="number">0</span>)&#123;</span><br><span class="line">                diff--;</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span>(count[s[cur+plen]-<span class="string">&#x27;a&#x27;</span>]==<span class="number">1</span>)&#123;</span><br><span class="line">                diff++;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span>(diff==<span class="number">0</span>)&#123;</span><br><span class="line">                ans.<span class="built_in">push_back</span>(cur<span class="number">+1</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="找出字符串中第一个匹配项的下标">2.找出字符串中第一个匹配项的下标</h3><p><ahref="https://leetcode.cn/problems/implement-strstr/description/">LeetCodeHot100: 28.找出字符串中第一个匹配项的下标</a></p><p><strong>简要描述</strong>：实现 strStr() 函数。给定一个 haystack字符串和一个 needle 字符串，在 haystack 字符串中找出 needle字符串的第一个匹配项的下标（下标从 0 开始）。如果 needle 不是 haystack的一部分，则返回 -1。</p><p><strong>解题关键</strong>：我们可以使用双指针的方法来解决这个问题。一个指针遍历haystack，另一个指针遍历needle。当两个指针指向的字符相同时，两个指针都向前移动；否则，haystack指针回到上次匹配的下一个位置，needle指针回到起始位置。继续这个过程直到找到匹配或者遍历完整个 haystack。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">strStr</span><span class="params">(string haystack, string needle)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> hlen = haystack.<span class="built_in">size</span>();</span><br><span class="line">        <span class="type">int</span> nlen = needle.<span class="built_in">size</span>();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span>(nlen==<span class="number">0</span>)&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;=hlen-nlen;i++)&#123;</span><br><span class="line">            <span class="type">int</span> j=<span class="number">0</span>;</span><br><span class="line">            <span class="keyword">for</span>(;j&lt;nlen;j++)&#123;</span><br><span class="line">                <span class="keyword">if</span>(haystack[i+j]!=needle[j])&#123;</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span>(j==nlen)&#123;</span><br><span class="line">                <span class="keyword">return</span> i;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>这题还有一个更复杂的 KMP 算法（Knuth-Morris-Pratt），可以在 <spanclass="math inline">𝒪(<em>N</em> + <em>M</em>)</span>的时间复杂度内完成匹配，具体算法可以参考<ahref="https://b23.tv/hXdhWi7">最浅显易懂的 KMP算法讲解-哔哩哔哩</a>进行学习，具体实现如下：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">// 构建 next 数组</span></span><br><span class="line">    <span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">getNext</span><span class="params">(string &amp;needle)</span></span>&#123;</span><br><span class="line">        <span class="type">int</span> nlen = needle.<span class="built_in">size</span>();</span><br><span class="line">        <span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">next</span><span class="params">(nlen, <span class="number">-1</span>)</span></span>;</span><br><span class="line">        <span class="type">int</span> k = <span class="number">-1</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> j=<span class="number">1</span>;j&lt;nlen;j++)&#123;</span><br><span class="line">            <span class="keyword">while</span>(k!=<span class="number">-1</span> &amp;&amp; needle[k<span class="number">+1</span>]!=needle[j])&#123;</span><br><span class="line">                k = next[k];</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span>(needle[k<span class="number">+1</span>]==needle[j])&#123;</span><br><span class="line">                k++;</span><br><span class="line">            &#125;</span><br><span class="line">            next[j] = k;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> next;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">strStr</span><span class="params">(string haystack, string needle)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> hlen = haystack.<span class="built_in">size</span>();</span><br><span class="line">        <span class="type">int</span> nlen = needle.<span class="built_in">size</span>();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span>(nlen==<span class="number">0</span>)&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        vector&lt;<span class="type">int</span>&gt; next = <span class="built_in">getNext</span>(needle);</span><br><span class="line">        <span class="type">int</span> k = <span class="number">-1</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;hlen;i++)&#123;</span><br><span class="line">            <span class="keyword">while</span>(k!=<span class="number">-1</span> &amp;&amp; needle[k<span class="number">+1</span>]!=haystack[i])&#123;</span><br><span class="line">                k = next[k];</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span>(needle[k<span class="number">+1</span>]==haystack[i])&#123;</span><br><span class="line">                k++;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span>(k==nlen<span class="number">-1</span>)&#123;</span><br><span class="line">                <span class="keyword">return</span> i - nlen + <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 刷题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Algorithm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DPM-Solver和DPM-Solver++</title>
      <link href="/2025/09/23/AIGC/DPM-Solver/"/>
      <url>/2025/09/23/AIGC/DPM-Solver/</url>
      
        <content type="html"><![CDATA[<h1 id="dpm-solver和dpm-solver">DPM-Solver和DPM-Solver++</h1><h2 id="dpm-solver">1.DPM-Solver</h2><p>原论文：<ahref="http://arxiv.org/abs/2206.00927">http://arxiv.org/abs/2206.00927</a></p><p>源码：<ahref="https://github.com/LuChengTHU/dpm-solver">https://github.com/LuChengTHU/dpm-solver</a></p><h3 id="背景介绍">1.1.背景介绍</h3><p>当时扩散概率模型（Diffusion Probabilistic Models,DPMs）在图像生成等任务上效果非常好，但它们的一个核心问题是<strong>采样速度慢</strong>。通常需要数百到上千步神经网络前向计算才能生成一张样本，这相比GAN、VAE 的单步生成要慢得多，限制了实际应用。</p><p>而采样可以被看作是求解扩散过程对应的 ODE（概率流 ODE）或SDE。现有两类加速方法：</p><ul><li><p>需要再训练的蒸馏类方法（如 ProgressiveDistillation），可以减少采样步数，但需要额外训练，泛化性差；</p></li><li><p>训练无关的方法（如 DDIM, Analytic-DDPM,PNDM），直接作用在预训练模型上，但一般仍需 50步左右才能得到高质量样本。</p></li></ul><h3 id="dpm-solver-方法">1.2.DPM-Solver 方法</h3><p>作者提出了DPM-Solver，一种专为扩散ODE设计的高阶求解器。<strong>DPM-Solver 能够在10 步以内生成高质量样本，且无需额外训练。</strong></p><h4 id="ode求解器">1.2.1.ODE求解器</h4><p>我们回顾一下在SDE中的逆向SDE过程：</p><p><span class="math display">$$\begin{split}    \mathrm{d} x_t=\left[f(t) x_t+\frac{g^2(t)}{\sigma_t}\epsilon_\theta\left(x_t, t\right)\right] \mathrm{d} t+g(t) \mathrm{d}\bar{w}_t, x_T \sim q_T\left(x_T\right)\end{split}$$</span></p><blockquote><p>上述式子是一个连续时间SDE，为了使用计算机求解需要将其离散化，<spanclass="math inline"><em>d</em><em>t</em></span>需要变成时间步长<spanclass="math inline"><em>Δ</em><em>t</em></span>。由于最后一项<spanclass="math inline"><em>g</em>(<em>t</em>)d<em>w̄</em><sub><em>t</em></sub></span>维纳过程的存在，SDE的求解存在随机性，一旦步长变大就会出现很大的误差，甚至无法收敛，因此原DDPM等方法需要非常小的步长（大约1000步）才能得到高质量样本。</p></blockquote><p>但是后续宋飏老师等人发现该逆向SDE存在一个在每个时间步的边际分布都相同的<strong>ODE</strong>，可以通过求解ODE得到和SDE同样的结果：</p><p><span class="math display">$$\begin{split}    \frac{\mathrm{d} x_t}{\mathrm{~d} t}=f(t) x_t+\frac{g^2(t)}{2\sigma_t} \epsilon_\theta\left(x_t, t\right), \quad x_T \simq_T\left(x_T\right)\end{split}$$</span></p><p>通过求解ODE，已经可以将采样步数从1000步减少到50步左右。比如DDIM就是一种一阶的ODE求解器，PNDM是二阶ODE求解器。</p><h4 id="dpm-solver的核心思想">1.2.2.DPM-Solver的核心思想</h4><p>DPM-Solver观察到了ODE方程的半线性性质，即前一项是关于<spanclass="math inline"><em>x</em><sub><em>t</em></sub></span>的线性函数，后一项是关于<spanclass="math inline"><em>t</em></span>的非线性函数。基于此，DPM-Solver将ODE方程转化为一个等价的积分形式（推导省略）：</p><p><span class="math display">$$\begin{split}    x_t=e^{\int_s^t f(\tau) d \tau} x_s+\int_s^t e^{\int_\tau^t f(r)\mathrm{d} r} \frac{g^2(\tau)}{2 \sigma_\tau}\epsilon_\theta\left(x_\tau, \tau\right) \mathrm{d} \tau\end{split}$$</span></p><p>定义<spanclass="math inline"><em>λ</em><sub><em>t</em></sub> := <em>l</em><em>o</em><em>g</em>(<em>α</em><sub><em>t</em></sub>/<em>σ</em><sub><em>t</em></sub>)</span>，并用<spanclass="math inline"><em>λ</em><sub><em>t</em></sub></span>代替<spanclass="math inline"><em>t</em></span>，则上式可以继续推导成一个非常简洁的形式：</p><p><span class="math display">$$\begin{split}    \boxed{x_t=\frac{\alpha_t}{\alpha_s} x_s-\alpha_t\int_{\lambda_s}^{\lambda_t} e^{-\lambda}\epsilon_\theta\left(x_\lambda, \lambda\right) \mathrm{d} \lambda}\end{split}$$</span></p><p>观察这个式子，可以发现形式非常简洁，其实从EDM开始大家就注意到<spanclass="math inline"><em>t</em></span>和noiselevel之间存在一个一一对应的关系，并尝试抛弃<spanclass="math inline"><em>t</em></span>，这里DPM-Solver使用log(SNR)作为新的时间变量，进一步简化了形式。式子中的第一项<spanclass="math inline">$\frac{\alpha_t}{\alpha_s}x_s$</span>是一个线性项，可以直接计算出来，不受步长影响；第二项是一个指数衰减积分，无法精确计算，作者提出使用泰勒展开来近似计算该积分。</p><h4 id="dpm-solver的具体形式">1.2.3.DPM-Solver的具体形式</h4><p>我们现在将无法进行精确计算的那一项式子单拎出来看：</p><p><span class="math display">$$\begin{split}    I=\int_{\lambda_s}^{\lambda_t} e^{-\lambda}\epsilon_\theta\left(x_\lambda, \lambda\right) \mathrm{d} \lambda\end{split}$$</span></p><p>作者在<spanclass="math inline"><em>λ</em><sub><em>i</em></sub></span>处对<spanclass="math inline"><em>ϵ</em><sub><em>θ</em></sub>(<em>x</em><sub><em>λ</em></sub>, <em>λ</em>)</span>进行泰勒展开，记区间长度为<spanclass="math inline"><em>h</em> = <em>λ</em><sub><em>t</em></sub> − <em>λ</em><sub><em>s</em></sub></span>，则有：</p><p><span class="math display">$$\begin{split}    {\varepsilon}_\theta({x}_\lambda,\lambda)\approx{\varepsilon}_\theta({x}_{\lambda_i},\lambda_i)+(\lambda-\lambda_i)\frac{d}{d\lambda}{\varepsilon}_\theta({x}_\lambda,\lambda)|_{\lambda_i}+\cdots\end{split}$$</span></p><p>代入积分，可以得到：</p><p><span class="math display">$$\begin{split}    I\approx\sum_{n=0}^{k-1}\frac{1}{n!}{\varepsilon}_\theta^{(n)}({x}_{\lambda_i},\lambda_i)\int_{\lambda_i}^{\lambda_{i+1}}e^{-\lambda}(\lambda-\lambda_i)^nd\lambda+O(h^k)\end{split}$$</span></p><p>这样积分的误差项就是<spanclass="math inline"><em>O</em>(<em>h</em><sup><em>k</em></sup>)</span>，得到一个k阶精度的数值解法。</p><p><strong>一阶求解器（DPM-Solver-1）：</strong></p><p>最简单的情况就是取<spanclass="math inline"><em>k</em> = 1</span>，即在每个区间内使用常数近似：</p><p><span class="math display">$$\begin{split}\begin{aligned}    x_{t}&amp;=\frac{\alpha_{t}}{\alpha_{s}}x_{s}-\sigma_{t}(e^{h}-1)\epsilon_{\theta}(x_{s},s)+\mathcal{O}(h)\\    &amp;=\frac{\alpha_{t}}{\alpha_{s}}x_{s}-\alpha_{t}(\frac{\sigma_{s}}{\alpha_{s}}-\frac{\sigma_{t}}{\alpha_{t}})\epsilon_{\theta}(x_{s},s)+\mathcal{O}(h)\end{aligned}\end{split}$$</span></p><p>可以发现上述形式就是DDIM的形式。</p><p><strong>二阶求解器（DPM-Solver-2）：</strong></p><p>在求解高阶求解器时，有一个问题，就是需要计算高阶导数项<spanclass="math inline"><em>ε</em><sub><em>θ</em></sub><sup>(<em>n</em>)</sup>(<em>x</em><sub><em>λ</em><sub><em>i</em></sub></sub>, <em>λ</em><sub><em>i</em></sub>)</span>，但是这些项是由神经网络计算出来的，无法直接计算。作者提出使用有限差分来近似计算这些高阶导数项，即用中间点来接近近似导数（多采样一次）。</p><blockquote><p><em>为什么可以近似?</em></p><p>数值积分中常用做法： <span class="math display">$$\begin{split}  \int_a^bf(x)dx\approx\left(b-a\right)f\left(\frac{a+b}{2}\right)\end{split}$$</span>这里的思想是如果函数在区间上变化不大，那么用区间中点的函数值乘以区间长度来近似积分值是合理的。</p></blockquote><p><strong>三阶求解器（DPM-Solver-3）：</strong></p><p>在求解三阶求解器时，作者同样使用有限差分来近似计算高阶导数项，由于要计算二阶导数，因此相当于需要多采样两次（分别取<spanclass="math inline">$\frac{2}{3}$</span>和<spanclass="math inline">$\frac{1}{3}$</span>位置的点）。</p><h2 id="dpm-solver-1">2.DPM-Solver++</h2><p>原论文：<ahref="https://arxiv.org/abs/2211.01095">https://arxiv.org/abs/2211.01095</a></p><p>源码：<ahref="https://github.com/LuChengTHU/dpm-solver">https://github.com/LuChengTHU/dpm-solver</a></p><h3 id="背景介绍-1">2.1.背景介绍</h3><p>之前提出的DPM-Solver使用高阶求解器来求解扩散ODE，在无引导采样时效果很好，但是引导采样下，尤其是在largeguidancescale下，会出现不稳定的情况。不仅如此，在大的引导尺度下，会出现”train-testmismatch”问题，即生成的样本会超过训练时的分布范围，导致生成样本质量下降，业界主要使用动态阈值等方法来缓解这个问题，但没有一个高效求解器将这些方法结合起来。</p><h3 id="dpm-solver-方法-1">2.2.DPM-Solver++ 方法</h3><p>总的来说，DPM-Solver++ 的改进可以从三个方面来看：</p><h4 id="从噪声预测到数据预测">2.2.1.从噪声预测到数据预测</h4><p>之前用噪声重参数化的逆向ODE为：</p><p><span class="math display">$$\begin{split}    \frac{\mathrm{d} x_t}{\mathrm{~d} t}=f(t) x_t+\frac{g^2(t)}{2\sigma_t} \epsilon_\theta\left(x_t, t\right), \quad x_T \simq_T\left(x_T\right)\end{split}$$</span></p><p>DPM-Solver是在这个ODE上进行求解的，而DPM-Solver++提出使用<strong>数据重参数化</strong>的逆向ODE：</p><p><span class="math display">$$\begin{split}    \frac{\mathrm{d} \boldsymbol{x}_t}{\mathrm{~d}t}=\left(f(t)+\frac{g^2(t)}{2 \sigma_t^2}\right)\boldsymbol{x}_t-\frac{\alpha_t g^2(t)}{2 \sigma_t^2}\boldsymbol{x}_\theta\left(\boldsymbol{x}_t, t\right), \quad\boldsymbol{x}_T \sim \mathcal{N}\left(\mathbf{0}, \tilde{\sigma}^2\boldsymbol{I}\right)\end{split}$$</span></p><p>这个转变的动机是数据预测模型可以直接预测去噪后的图像，其输出通常被限制在训练数据的边界内（例如常用的[-1,1]范围），从而缓解了train-testmismatch问题。经过一系列推导，我们可以得到：</p><p><span class="math display">$$\begin{split}    \boxed{\tilde{x}_{t_i}=\frac{\sigma_{t_i}}{\sigma_{t_{i-1}}}\tilde{x}_{t_{i-1}}+\sigma_{t_i}\underbrace{\sum_{n=0}^{k-1}x_\theta^{(n)}(\hat{x}_{\lambda_{t_{i-1}}},\lambda_{t_{i-1}})}_{\mathrm{estimated}}\underbrace{\int_{\lambda_{t_{i-1}}}^{\lambda_{t_i}}e^\lambda\frac{(\lambda-\lambda_{t_{i-1}})^n}{n!}\mathrm{d}\lambda}_{\text{analyticallycomputed (AppendixA)}}+\underbrace{\mathcal{O}(h_i^{k+1})}_{\mathrm{omitted}}}\end{split}$$</span></p><p>对于高阶中的计算高阶导数项，这里和DPM-Solver中一样使用有限差分来近似计算。但是由于三阶以上在largeguidancescale下极不稳定，因此DPM-Solver++只考虑一阶和二阶求解器。对于这种形式的求解器，作者分别命名为<strong>DPM-Solver++1S</strong>和<strong>DPM-Solver++ 2S</strong>。</p><h4 id="从单步求解器到多步求解器">2.2.2.从单步求解器到多步求解器</h4><p>DPM-Solver和其他高阶求解器在单步求解时，使用的是当前时刻<spanclass="math inline"><em>t</em><sub><em>k</em></sub></span>的模型输出及其导数来预测下一个时刻<spanclass="math inline"><em>t</em><sub><em>k</em> − 1</sub></span>的值，这种方法称为单步求解器（one-stepsolver）：</p><p><span class="math display">$$\begin{split}    \mathbf{x}_{k-1}=\Phi(\mathbf{x}_k,t_k,t_{k-1})\end{split}$$</span></p><p>然而正如之前所讨论的，在大的引导尺度下，梯度会变得非常不稳定，DPM-Solver++提出一个多步求解器来解决这个问题。多步求解器（multi-stepsolver）使用前面多个时刻的模型输出及其导数来预测下一个时刻的值，一个典型的二阶多步求解器形式为：</p><p><span class="math display">$$\begin{split}    \mathbf{x}_{k-1}=\Phi(\mathbf{x}_k,t_k,t_{k-1},\mathbf{x}_{k+1},t_{k+1})\end{split}$$</span></p><p>这种方法称为线性多步法（linear multi-stepmethod），在数值积分中有着悠久的历史。具体来说，DPM-Solver++使用了<strong>隐式 Adams-Moulton 方法</strong>和<strong>显式Adams-Bashforth方法</strong>，这些方法可以利用之前步骤的函数值来更精确地预测下一步。</p><h4 id="动态阈值">2.2.3.动态阈值</h4><p>在大引导尺度下，生成样本的像素值可能会超出训练数据的范围，导致图像质量下降。为了解决这个问题，DPM-Solver++引入了动态阈值（dynamicthresholding）技术。其基本思想是，在每一步的采样过程中，DPM-Solver++会先计算一个去噪后的数据预测 <spanclass="math inline"><strong>x</strong><sub><em>θ</em></sub>(<em>t</em>)</span>。如果这个预测的像素值超出了预定的范围（例如[-1,1]），它会根据动态阈值的策略，对这些值进行裁剪或缩放。动态阈值的实现方式通常是对预测结果进行逐元素裁剪，具体步骤如下：</p><ol type="1"><li>计算预测的图像 <spanclass="math inline"><strong>x</strong><sub>pred</sub> = <strong>x</strong><sub><em>θ</em></sub>(<strong>x</strong><sub><em>t</em></sub>, <em>t</em>)</span>。</li><li>找到图像中像素值的绝对最大值 <spanclass="math inline"><em>s</em> = percentile(|<strong>x</strong><sub>pred</sub>|,<em>p</em>)</span>，通常取第99.5 个百分位数。</li><li>对预测图像进行裁剪：<spanclass="math inline"><strong>x</strong><sub>clipped</sub> = <strong>x</strong><sub>pred</sub>/max (<em>s</em>, 1)</span>。</li></ol><h2 id="源码">3.源码</h2><p>关于DPM-Solver和DPM-Solver++的源码在<ahref="https://github.com/LuChengTHU/dpm-solver">https://github.com/LuChengTHU/dpm-solver</a>上可以找到。需要重点关注<code>DPM-Solver</code>类的实现，首先我们可以先看一下主要workflow的<code>sample</code>函数是如何实现的（这里简化了很多逻辑）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">sample</span>(<span class="params">...</span>):</span><br><span class="line">    <span class="comment"># 参数初始化与检查</span></span><br><span class="line">    <span class="comment"># ...</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">if</span> method == <span class="string">&#x27;adaptive&#x27;</span>:</span><br><span class="line">            x = 自适应采样(x, ...)</span><br><span class="line">        <span class="keyword">elif</span> method == <span class="string">&#x27;multistep&#x27;</span>:</span><br><span class="line">            <span class="comment"># 初始化时间步</span></span><br><span class="line">            <span class="comment"># 前order步低阶采样</span></span><br><span class="line">            <span class="comment"># 后续高阶采样</span></span><br><span class="line">        <span class="keyword">elif</span> method <span class="keyword">in</span> [<span class="string">&#x27;singlestep&#x27;</span>, <span class="string">&#x27;singlestep_fixed&#x27;</span>]:</span><br><span class="line">            <span class="comment"># 按顺序单步采样</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError</span><br><span class="line">        <span class="comment"># 可选去噪</span></span><br><span class="line">    <span class="comment"># 返回最终结果或中间结果</span></span><br></pre></td></tr></table></figure><p>可以看到，<code>sample</code>函数内部支持四种采样方法，分别是自适应采样（adaptive）、多步采样（multistep）、单步采样（singlestep）和固定单步采样（singlestep_fixed）。其中多步采样和单步采样分别对应DPM-Solver++和DPM-Solver的实现：</p><ol type="1"><li><strong>自适应采样（adaptive）</strong>：这种方法会根据当前的误差动态调整步长，从而在保证采样质量的同时提高效率。具体来说，它实现了DPM-Solver-12/DPM-Solver-23：<ol type="1"><li>order=2 → 每一步先做一阶，再做二阶，比较两者差值估计局部误差；</li><li>order=3 → 每一步先做二阶，再做三阶，比较差值。</li></ol></li><li><strong>多步采样（multistep）</strong>：即DPM-Solver++的多步求解器实现，支持高阶多步方法：<ol type="1"><li>order=1：退化为 DDIM。</li><li>order=2：先用一步一阶方法 (DDIM) 启动，然后用 Adams-Bashforth-2 /Adams-Moulton-2 的 predictor–corrector 来跑后续步骤。</li><li>order=3：先用 1 步一阶 + 1步二阶启动，再用三阶多步法跑剩余步骤。</li></ol></li><li><strong>单步采样（singlestep_fixed）</strong>：对DPM-Solver++的单步求解器实现，假设等间隔步长，支持一阶、二阶和三阶单步方法：<ol type="1"><li>order=1：退化为 DDIM。</li><li>order=2：使用 DPM-Solver++ 2S 的公式。</li><li>order=3：使用 DPM-Solver++ 3S 的公式。</li></ol></li><li><strong>混合单步采样（singlestep_fixed）</strong>：DPM-Solver++对singlestep_fixed的改进版本，支持任意步长</li></ol><h3 id="多步步长实现">3.1.多步步长实现</h3><p>关于这部分的源码为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">assert</span> steps &gt;= order</span><br><span class="line">timesteps = <span class="variable language_">self</span>.get_time_steps(skip_type=skip_type, t_T=t_T, t_0=t_0, N=steps, device=device)</span><br><span class="line"><span class="keyword">assert</span> timesteps.shape[<span class="number">0</span>] - <span class="number">1</span> == steps</span><br><span class="line"><span class="comment"># Init the initial values.</span></span><br><span class="line">step = <span class="number">0</span></span><br><span class="line">t = timesteps[step]</span><br><span class="line">t_prev_list = [t]</span><br><span class="line">model_prev_list = [<span class="variable language_">self</span>.model_fn(x, t)]</span><br><span class="line"><span class="keyword">if</span> <span class="variable language_">self</span>.correcting_xt_fn <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    x = <span class="variable language_">self</span>.correcting_xt_fn(x, t, step)</span><br><span class="line"><span class="keyword">if</span> return_intermediate:</span><br><span class="line">    intermediates.append(x)</span><br><span class="line"><span class="comment"># Init the first `order` values by lower order multistep DPM-Solver.</span></span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, order):</span><br><span class="line">    t = timesteps[step]</span><br><span class="line">    x = <span class="variable language_">self</span>.multistep_dpm_solver_update(x, model_prev_list, t_prev_list, t, step, solver_type=solver_type)</span><br><span class="line">    <span class="keyword">if</span> <span class="variable language_">self</span>.correcting_xt_fn <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        x = <span class="variable language_">self</span>.correcting_xt_fn(x, t, step)</span><br><span class="line">    <span class="keyword">if</span> return_intermediate:</span><br><span class="line">        intermediates.append(x)</span><br><span class="line">    t_prev_list.append(t)</span><br><span class="line">    model_prev_list.append(<span class="variable language_">self</span>.model_fn(x, t))</span><br><span class="line"><span class="comment"># Compute the remaining values by `order`-th order multistep DPM-Solver.</span></span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(order, steps + <span class="number">1</span>):</span><br><span class="line">    t = timesteps[step]</span><br><span class="line">    <span class="comment"># We only use lower order for steps &lt; 10</span></span><br><span class="line">    <span class="keyword">if</span> lower_order_final <span class="keyword">and</span> steps &lt; <span class="number">10</span>:</span><br><span class="line">        step_order = <span class="built_in">min</span>(order, steps + <span class="number">1</span> - step)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        step_order = order</span><br><span class="line">    x = <span class="variable language_">self</span>.multistep_dpm_solver_update(x, model_prev_list, t_prev_list, t, step_order, solver_type=solver_type)</span><br><span class="line">    <span class="keyword">if</span> <span class="variable language_">self</span>.correcting_xt_fn <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        x = <span class="variable language_">self</span>.correcting_xt_fn(x, t, step)</span><br><span class="line">    <span class="keyword">if</span> return_intermediate:</span><br><span class="line">        intermediates.append(x)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(order - <span class="number">1</span>):</span><br><span class="line">        t_prev_list[i] = t_prev_list[i + <span class="number">1</span>]</span><br><span class="line">        model_prev_list[i] = model_prev_list[i + <span class="number">1</span>]</span><br><span class="line">    t_prev_list[-<span class="number">1</span>] = t</span><br><span class="line">    <span class="comment"># We do not need to evaluate the final model value.</span></span><br><span class="line">    <span class="keyword">if</span> step &lt; steps:</span><br><span class="line">        model_prev_list[-<span class="number">1</span>] = <span class="variable language_">self</span>.model_fn(x, t)</span><br></pre></td></tr></table></figure><p>我们一步步来看，首先是对时间步、模型输出的初始化：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">t = timesteps[<span class="number">0</span>]</span><br><span class="line">t_prev_list = [t]</span><br><span class="line">model_prev_list = [<span class="variable language_">self</span>.model_fn(x, t)]</span><br></pre></td></tr></table></figure><p>初始时刻<spanclass="math inline"><em>t</em><sub>0</sub> = <em>T</em></span>，存储当前时间步和模型输出，这里相当于初始化第一个梯度点。</p><p>在多步方法开始前，需要至少<code>order</code>个点来进行预测，因此需要先用低阶方法初始化前<code>order-1</code>个点：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, order):</span><br><span class="line">    x = <span class="variable language_">self</span>.multistep_dpm_solver_update(x, model_prev_list, t_prev_list, t, step, solver_type)</span><br><span class="line">    ...</span><br><span class="line">    t_prev_list.append(t)</span><br><span class="line">    model_prev_list.append(<span class="variable language_">self</span>.model_fn(x, t))</span><br></pre></td></tr></table></figure><p>所以前几步使用低阶公式（比如Euler方法）来计算，得到前<code>order</code>个点。之后就进入了多步方法的核心循环：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(order, steps + <span class="number">1</span>):</span><br><span class="line">    ...</span><br><span class="line">    x = <span class="variable language_">self</span>.multistep_dpm_solver_update(x, model_prev_list, t_prev_list, t, step_order, solver_type=solver_type)</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(order - <span class="number">1</span>):</span><br><span class="line">        t_prev_list[i] = t_prev_list[i + <span class="number">1</span>]</span><br><span class="line">        model_prev_list[i] = model_prev_list[i + <span class="number">1</span>]</span><br><span class="line">    t_prev_list[-<span class="number">1</span>] = t</span><br><span class="line">    <span class="keyword">if</span> step &lt; steps:</span><br><span class="line">        model_prev_list[-<span class="number">1</span>] = <span class="variable language_">self</span>.model_fn(x, t)</span><br></pre></td></tr></table></figure><p>在每一步中，使用前<code>order</code>个点的模型输出和时间步来预测下一个点，然后更新存储的点列表。这里的<code>multistep_dpm_solver_update</code>函数实现了具体的多步公式（如Adams-Bashforth或Adams-Moulton）。</p><h3 id="单步步长实现">3.2.单步步长实现</h3><p>这部分源码为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> method == <span class="string">&#x27;singlestep&#x27;</span>:</span><br><span class="line">    timesteps_outer, orders = <span class="variable language_">self</span>.get_orders_and_timesteps_for_singlestep_solver(steps=steps, order=order, skip_type=skip_type, t_T=t_T, t_0=t_0, device=device)</span><br><span class="line"><span class="keyword">elif</span> method == <span class="string">&#x27;singlestep_fixed&#x27;</span>:</span><br><span class="line">    K = steps // order</span><br><span class="line">    orders = [order,] * K</span><br><span class="line">    timesteps_outer = <span class="variable language_">self</span>.get_time_steps(skip_type=skip_type, t_T=t_T, t_0=t_0, N=K, device=device)</span><br><span class="line"><span class="keyword">for</span> step, order <span class="keyword">in</span> <span class="built_in">enumerate</span>(orders):</span><br><span class="line">    s, t = timesteps_outer[step], timesteps_outer[step + <span class="number">1</span>]</span><br><span class="line">    timesteps_inner = <span class="variable language_">self</span>.get_time_steps(skip_type=skip_type, t_T=s.item(), t_0=t.item(), N=order, device=device)</span><br><span class="line">    lambda_inner = <span class="variable language_">self</span>.noise_schedule.marginal_lambda(timesteps_inner)</span><br><span class="line">    h = lambda_inner[-<span class="number">1</span>] - lambda_inner[<span class="number">0</span>]</span><br><span class="line">    r1 = <span class="literal">None</span> <span class="keyword">if</span> order &lt;= <span class="number">1</span> <span class="keyword">else</span> (lambda_inner[<span class="number">1</span>] - lambda_inner[<span class="number">0</span>]) / h</span><br><span class="line">    r2 = <span class="literal">None</span> <span class="keyword">if</span> order &lt;= <span class="number">2</span> <span class="keyword">else</span> (lambda_inner[<span class="number">2</span>] - lambda_inner[<span class="number">0</span>]) / h</span><br><span class="line">    x = <span class="variable language_">self</span>.singlestep_dpm_solver_update(x, s, t, order, solver_type=solver_type, r1=r1, r2=r2)</span><br><span class="line">    <span class="keyword">if</span> <span class="variable language_">self</span>.correcting_xt_fn <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        x = <span class="variable language_">self</span>.correcting_xt_fn(x, t, step)</span><br><span class="line">    <span class="keyword">if</span> return_intermediate:</span><br><span class="line">        intermediates.append(x)</span><br></pre></td></tr></table></figure><p>单步方法的实现相对简单，首先根据选择的方法（<code>singlestep</code>或<code>singlestep_fixed</code>）计算时间步和对应的阶数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> method == <span class="string">&#x27;singlestep&#x27;</span>:</span><br><span class="line">    timesteps_outer, orders = <span class="variable language_">self</span>.get_orders_and_timesteps_for_singlestep_solver(steps=steps, order=order, skip_type=skip_type, t_T=t_T, t_0=t_0, device=device)</span><br><span class="line"><span class="keyword">elif</span> method == <span class="string">&#x27;singlestep_fixed&#x27;</span>:</span><br><span class="line">    K = steps // order</span><br><span class="line">    orders = [order,] * K</span><br><span class="line">    timesteps_outer = <span class="variable language_">self</span>.get_time_steps(skip_type=skip_type, t_T=t_T, t_0=t_0, N=K, device=device)</span><br></pre></td></tr></table></figure><p>对于<code>singlestep</code>，调用<code>get_orders_and_timesteps_for_singlestep_solver</code>，根据总步数<code>steps</code> 和目标阶数<code>order</code>，动态决定每一步是1/2/3阶。这就是论文里说的DPM-Solver-fast，组合 1/2/3 阶单步方法，保证用满 <code>steps</code>次函数调用。</p><p>对于<code>singlestep_fixed</code>，固定每次都是 <code>order</code>阶。比如 <code>order=2</code>，那就是纯二阶单步 DPM-Solver，每次用 2个子步近似积分。</p><p>接着遍历每一个大步，对于每个大步，计算对应的内部分割时间步：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> step, order <span class="keyword">in</span> <span class="built_in">enumerate</span>(orders):</span><br><span class="line">    s, t = timesteps_outer[step], timesteps_outer[step + <span class="number">1</span>]</span><br><span class="line">    timesteps_inner = <span class="variable language_">self</span>.get_time_steps(skip_type=skip_type, t_T=s.item(), t_0=t.item(), N=order, device=device)</span><br><span class="line">    lambda_inner = <span class="variable language_">self</span>.noise_schedule.marginal_lambda(timesteps_inner)</span><br><span class="line">    h = lambda_inner[-<span class="number">1</span>] - lambda_inner[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><p>每个大步<spanclass="math inline">[<em>s</em>, <em>t</em>]</span>内部分成<code>order</code>个子时间点。换句话说，三阶公式需要3个子点，二阶需要2个子点。<code>lambda_inner</code>在<spanclass="math inline"><em>λ</em></span>空间的坐标（<spanclass="math inline"><em>λ</em> = log <em>α</em><sub><em>t</em></sub> − log <em>σ</em><sub><em>t</em></sub></span>，论文定义的log-SNR变量）。</p><p>接着计算出单步公式的系数，用于确定子点在区间中的相对位置：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">r1 = <span class="literal">None</span> <span class="keyword">if</span> order &lt;= <span class="number">1</span> <span class="keyword">else</span> (lambda_inner[<span class="number">1</span>] - lambda_inner[<span class="number">0</span>]) / h</span><br><span class="line">r2 = <span class="literal">None</span> <span class="keyword">if</span> order &lt;= <span class="number">2</span> <span class="keyword">else</span> (lambda_inner[<span class="number">2</span>] - lambda_inner[<span class="number">0</span>]) / h</span><br></pre></td></tr></table></figure><p>之后调用单步更新公式，并会有一些额外的修正（比如动态阈值）：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = <span class="variable language_">self</span>.singlestep_dpm_solver_update(x, s, t, order, solver_type=solver_type, r1=r1, r2=r2)</span><br><span class="line"><span class="keyword">if</span> <span class="variable language_">self</span>.correcting_xt_fn <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    x = <span class="variable language_">self</span>.correcting_xt_fn(x, t, step)</span><br><span class="line"><span class="keyword">if</span> return_intermediate:</span><br><span class="line">    intermediates.append(x)</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AIGC </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>动态规划的算法题</title>
      <link href="/2025/09/22/Algo/dp/"/>
      <url>/2025/09/22/Algo/dp/</url>
      
        <content type="html"><![CDATA[<h1 id="动态规划">动态规划</h1><h2 id="应用场景">应用场景</h2><p>动态规划（Dynamic Programming，简称DP）是一种用于解决复杂问题的算法设计方法。它通过将问题分解为更小的子问题，并存储这些子问题的结果以避免重复计算，从而提高效率。动态规划通常适用于具有重叠子问题和最优子结构性质的问题。</p><p><strong>C++模板：</strong></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 示例：二维动态规划</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">uniquePaths</span><span class="params">(<span class="type">int</span> m, <span class="type">int</span> n)</span> </span>&#123;</span><br><span class="line">        vector&lt;vector&lt;<span class="type">int</span>&gt;&gt; <span class="built_in">dp</span>(m, <span class="built_in">vector</span>&lt;<span class="type">int</span>&gt;(n, <span class="number">1</span>));</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt; m; ++i) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">1</span>; j &lt; n; ++j) &#123;</span><br><span class="line">                dp[i][j] = dp[i - <span class="number">1</span>][j] + dp[i][j - <span class="number">1</span>];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> dp[m - <span class="number">1</span>][n - <span class="number">1</span>];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h2 id="例题">例题</h2><h3 id="接雨水">1. 接雨水</h3><p><ahref="https://leetcode.cn/problems/trapping-rain-water/description/">LeetCodeHot100: 42.接雨水</a></p><p><strong>简要描述</strong>：给定 n 个非负整数表示每个宽度为 1的柱子的高度图，计算按此排列的柱子，下雨之后能接多少雨水。</p><p><strong>解题关键</strong>：对于每根柱子来说，能接的雨水量取决于其左侧最高的柱子和右侧最高的柱子中的较小值与当前柱子高度的差值，因此我们需要找到每个位置的左侧最高柱子和右侧最高柱子。但是这样对每个柱子都进行一次遍历，时间复杂度为<spanclass="math inline">𝒪(<em>N</em><sup>2</sup>)</span>。进一步思考，我们可以使用动态规划来预处理每个位置的左侧最高柱子和右侧最高柱子，从而将时间复杂度降低到<span class="math inline">𝒪(<em>N</em>)</span>。</p><p>创建两个长度为n的数组<code>leftMax</code>和<code>rightMax</code>。对于<spanclass="math inline">0 ≤ <em>i</em> &lt; <em>n</em></span>，leftMax[i]表示下标 i 及其左边的位置中，height 的最大高度，rightMax[i] 表示下标 i及其右边的位置中，height的最大高度。显然，leftMax[0]=height[0]，rightMax[n−1]=height[n−1]。两个数组的其余元素的计算如下：</p><ul><li><p>当 <spanclass="math inline">1 ≤ <em>i</em> ≤ <em>n</em> − 1</span>时，leftMax[i]=max(leftMax[i−1],height[i])；</p></li><li><p>当 <spanclass="math inline">0 ≤ <em>i</em> ≤ <em>n</em> − 2</span>时，rightMax[i]=max(rightMax[i+1],height[i])。</p></li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">trap</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; height)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> n = height.<span class="built_in">size</span>();</span><br><span class="line">        <span class="keyword">if</span>(n==<span class="number">0</span>) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        <span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">leftMax</span><span class="params">(n)</span>, <span class="title">rightMax</span><span class="params">(n)</span></span>;</span><br><span class="line">        leftMax[<span class="number">0</span>] = height[<span class="number">0</span>];</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">1</span>; i&lt;n; i++)&#123;</span><br><span class="line">            leftMax[i] = <span class="built_in">max</span>(leftMax[i<span class="number">-1</span>], height[i]);</span><br><span class="line">        &#125;</span><br><span class="line">        rightMax[n<span class="number">-1</span>] = height[n<span class="number">-1</span>];</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=n<span class="number">-2</span>; i&gt;=<span class="number">0</span>; i--)&#123;</span><br><span class="line">            rightMax[i] = <span class="built_in">max</span>(rightMax[i<span class="number">+1</span>], height[i]);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="type">int</span> ans = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>; i&lt;n; i++)&#123;</span><br><span class="line">            ans += <span class="built_in">min</span>(leftMax[i], rightMax[i]) - height[i];</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="打家劫舍">2. 打家劫舍</h3><p><ahref="https://leetcode.cn/problems/house-robber/description/">LeetCodeHot100: 198.打家劫舍</a></p><p><strong>简要描述</strong>：你是一个专业的小偷，计划偷窃沿街的房屋。每间房内都藏有一定的现金，影响你偷窃的唯一制约因素就是相邻的房屋装有相互连通的防盗系统，如果两间相邻的房屋在同一晚上被小偷闯入，系统会自动报警。</p><p><strong>解题关键</strong>：对于每个房屋，我们有两个选择：偷窃或不偷窃。如果我们选择偷窃当前房屋，那么我们不能偷窃前一个房屋；如果我们选择不偷窃当前房屋，那么我们可以选择偷窃或不偷窃前一个房屋。基于这个思路，我们可以定义一个动态规划数组<code>dp</code>，其中 <code>dp[i]</code> 表示到第 <code>i</code>个房屋为止，能够获得的最大金额。那么，状态转移方程为：</p><p><span class="math display">$$\begin{split}    dp[i] &amp; = \max(dp[i-1], dp[i-2] + nums[i]) \\\end{split}$$</span></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">rob</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> n = nums.<span class="built_in">size</span>();</span><br><span class="line">        <span class="keyword">if</span>(n == <span class="number">0</span>) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">if</span>(n == <span class="number">1</span>) <span class="keyword">return</span> nums[<span class="number">0</span>];</span><br><span class="line">        <span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">dp</span><span class="params">(n)</span></span>;</span><br><span class="line">        dp[<span class="number">0</span>] = nums[<span class="number">0</span>];</span><br><span class="line">        dp[<span class="number">1</span>] = <span class="built_in">max</span>(nums[<span class="number">0</span>], nums[<span class="number">1</span>]);</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">2</span>; i &lt; n; i++)&#123;</span><br><span class="line">            dp[i] = <span class="built_in">max</span>(dp[i<span class="number">-1</span>], dp[i<span class="number">-2</span>] + nums[i]);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> dp[n<span class="number">-1</span>];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>但是，我们可以进一步优化空间复杂度，因为 <code>dp[i]</code> 只依赖于<code>dp[i-1]</code> 和<code>dp[i-2]</code>，我们只需要两个变量（<strong>滚动数组</strong>）来存储这两个状态。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">rob</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> n = nums.<span class="built_in">size</span>();</span><br><span class="line">        <span class="keyword">if</span>(n == <span class="number">0</span>) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">if</span>(n == <span class="number">1</span>) <span class="keyword">return</span> nums[<span class="number">0</span>];</span><br><span class="line">        <span class="type">int</span> prev2 = nums[<span class="number">0</span>];</span><br><span class="line">        <span class="type">int</span> prev1 = <span class="built_in">max</span>(nums[<span class="number">0</span>], nums[<span class="number">1</span>]);</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">2</span>; i &lt; n; i++)&#123;</span><br><span class="line">            <span class="type">int</span> curr = <span class="built_in">max</span>(prev1, prev2 + nums[i]);</span><br><span class="line">            prev2 = prev1;</span><br><span class="line">            prev1 = curr;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> prev1;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="完全平方数">3. 完全平方数</h3><p><ahref="https://leetcode.cn/problems/perfect-squares/description/">LeetCodeHot100: 279.完全平方数</a></p><p><strong>简要描述</strong>：给定正整数 n，找到若干个完全平方数（如 1,4, 9, 16, …）使得它们的和等于n，并且使得所需的完全平方数的个数最少。</p><p><strong>解题关键</strong>：我们可以使用动态规划来解决这个问题。定义一个数组<code>dp</code>，其中 <code>dp[i]</code> 表示组成整数 <code>i</code>所需的最少完全平方数的个数。初始时，<code>dp[0] = 0</code>，表示组成整数0 需要 0 个完全平方数。对于每个整数 <code>i</code>，我们遍历所有小于等于<code>sqrt(i)</code> 的整数 <code>j</code>，计算 <code>j*j</code> 并更新<code>dp[i]</code>：</p><p><span class="math display">$$\begin{split}    dp[i] &amp; = \min(dp[i], dp[i - j*j] + 1) \\\end{split}$$</span></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">numSquares</span><span class="params">(<span class="type">int</span> n)</span> </span>&#123;</span><br><span class="line">        <span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">dp</span><span class="params">(n + <span class="number">1</span>, INT_MAX)</span></span>;</span><br><span class="line">        dp[<span class="number">0</span>] = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n; i++)&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="type">int</span> j = <span class="number">1</span>; j * j &lt;= i; j++)&#123;</span><br><span class="line">                dp[i] = <span class="built_in">min</span>(dp[i], dp[i - j * j] + <span class="number">1</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> dp[n];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>对于这道题其实还有一种特殊解法，叫做<strong>四平方和定理</strong>，即任何一个正整数都可以表示为四个整数的平方和。当且仅当<spanclass="math inline"><em>n</em> ≠ 4<sup><em>k</em></sup> × (8<em>m</em> + 7)</span>时，<span class="math inline"><em>n</em></span>可以表示为至多三个整数的平方和。因此当 <spanclass="math inline"><em>n</em> = 4<sup><em>k</em></sup> × (8<em>m</em> + 7)</span>时，可以直接得到答案为4；如果不成立，就只有可能是1、2、3三种情况，对于1的情况则必有<span class="math inline"><em>n</em></span>是完全平方数，2的情况则可以通过枚举 <spanclass="math inline"><em>i</em></span> 来判断 <spanclass="math inline"><em>n</em> − <em>i</em><sup>2</sup></span>是否为完全平方数，从而得到答案为2，否则答案为3。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">numSquares</span><span class="params">(<span class="type">int</span> n)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">while</span> (n % <span class="number">4</span> == <span class="number">0</span>) &#123;</span><br><span class="line">            n /= <span class="number">4</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (n % <span class="number">8</span> == <span class="number">7</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">4</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">isPerfectSquare</span>(n)) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i * i &lt;= n; i++) &#123;</span><br><span class="line">            <span class="keyword">if</span> (<span class="built_in">isPerfectSquare</span>(n - i * i)) &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="number">2</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">3</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="function"><span class="type">bool</span> <span class="title">isPerfectSquare</span><span class="params">(<span class="type">int</span> x)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> y = (<span class="type">int</span>)std::<span class="built_in">sqrt</span>(x);</span><br><span class="line">        <span class="keyword">return</span> y * y == x;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="零钱兑换">4. 零钱兑换</h3><p><ahref="https://leetcode.cn/problems/coin-change/description/">LeetCodeHot100: 322.零钱兑换</a></p><p><strong>简要描述</strong>：给定不同面额的硬币 coins 和一个总金额amount，编写一个函数来计算可以凑成总金额所需的最少的硬币个数。如果没有任何一种硬币组合能组成总金额，返回-1。</p><p><strong>解题关键</strong>：我们可以使用动态规划来解决这个问题。定义一个数组<code>dp</code>，其中 <code>dp[i]</code> 表示组成金额 <code>i</code>所需的最少硬币个数。初始时，<code>dp[0] = 0</code>，表示组成金额 0 需要0 个硬币。对于每个金额 <code>i</code>，我们遍历所有硬币面额<code>coin</code>，如果 <code>i - coin &gt;= 0</code>，则更新<code>dp[i]</code>：</p><p><span class="math display">$$\begin{split}    dp[i] &amp; = \min(dp[i], dp[i - coin] + 1) \\\end{split}$$</span></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">coinChange</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; coins, <span class="type">int</span> amount)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> MAX = amount<span class="number">+1</span>;</span><br><span class="line">        <span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">dp</span><span class="params">(amount<span class="number">+1</span>, MAX)</span></span>;</span><br><span class="line">        dp[<span class="number">0</span>] = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">1</span>;i&lt;=amount;i++)&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="type">int</span> coin:coins)&#123;</span><br><span class="line">                <span class="keyword">if</span>(i&gt;=coin)&#123;</span><br><span class="line">                    dp[i] = <span class="built_in">min</span>(dp[i], <span class="number">1</span> + dp[i-coin]);</span><br><span class="line">                &#125;  </span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> dp[amount] == MAX ? <span class="number">-1</span> : dp[amount];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="单词拆分">5. 单词拆分</h3><p><ahref="https://leetcode.cn/problems/word-break/description/">LeetCodeHot100: 139.单词拆分</a></p><p><strong>简要描述</strong>：给定一个非空字符串 s和一个包含非空单词列表的字典 wordDict，判定 s是否可以被空格拆分为一个或多个在字典中出现的单词。</p><p><strong>解题关键</strong>：我们可以使用动态规划来解决这个问题。定义一个布尔数组<code>dp</code>，其中 <code>dp[i]</code> 表示字符串 <code>s</code> 的前<code>i</code>个字符是否可以被拆分成字典中的单词。初始时，<code>dp[0] = true</code>，表示空字符串可以被拆分。对于每个位置<code>i</code>，我们遍历所有可能的拆分点 <code>j</code>，如果<code>dp[j]</code> 为真且子字符串 <code>s[j:i]</code> 在字典中，则将<code>dp[i]</code> 设为真：</p><p><span class="math display">$$\begin{split}    dp[i] &amp; = dp[j] \land (s[j:i] \in wordDict) \\\end{split}$$</span></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">bool</span> <span class="title">wordBreak</span><span class="params">(string s, vector&lt;string&gt;&amp; wordDict)</span> </span>&#123;</span><br><span class="line">        <span class="function">unordered_set&lt;string&gt; <span class="title">wordSet</span><span class="params">(wordDict.begin(), wordDict.end())</span></span>;</span><br><span class="line">        <span class="type">int</span> n = s.<span class="built_in">size</span>();</span><br><span class="line">        <span class="function">vector&lt;<span class="type">bool</span>&gt; <span class="title">dp</span><span class="params">(n + <span class="number">1</span>, <span class="literal">false</span>)</span></span>;</span><br><span class="line">        dp[<span class="number">0</span>] = <span class="literal">true</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n; i++) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; i; j++) &#123;</span><br><span class="line">                <span class="keyword">if</span> (dp[j] &amp;&amp; wordSet.<span class="built_in">count</span>(s.<span class="built_in">substr</span>(j, i - j))) &#123;</span><br><span class="line">                    dp[i] = <span class="literal">true</span>;</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> dp[n];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="最长递增子序列">6. 最长递增子序列</h3><p><ahref="https://leetcode.cn/problems/longest-increasing-subsequence/description/">LeetCodeHot100: 300.最长递增子序列</a></p><p><strong>简要描述</strong>：给你一个整数数组 nums，找到其中最长严格递增子序列的长度。</p><p><strong>解题关键</strong>：我们可以使用动态规划来解决这个问题。定义一个数组<code>dp</code>，其中 <code>dp[i]</code> 表示以 <code>nums[i]</code>结尾的最长递增子序列的长度。初始时，所有的 <code>dp[i]</code> 都设为1，因为每个元素本身都可以作为一个长度为 1 的递增子序列。对于每个位置<code>i</code>，我们遍历之前的位置 <code>j</code>，如果<code>nums[j] &lt; nums[i]</code>，则更新 <code>dp[i]</code>：</p><p><span class="math display">$$\begin{split}    dp[i] &amp; = \max(dp[i], dp[j] + 1) \\\end{split}$$</span></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">lengthOfLIS</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> n = nums.<span class="built_in">size</span>();</span><br><span class="line">        <span class="keyword">if</span>(n == <span class="number">0</span>) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        <span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">dp</span><span class="params">(n, <span class="number">1</span>)</span></span>;</span><br><span class="line">        <span class="type">int</span> maxLen = <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">1</span>; i &lt; n; i++)&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="type">int</span> j = <span class="number">0</span>; j &lt; i; j++)&#123;</span><br><span class="line">                <span class="keyword">if</span>(nums[j] &lt; nums[i])&#123;</span><br><span class="line">                    dp[i] = <span class="built_in">max</span>(dp[i], dp[j] + <span class="number">1</span>);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            maxLen = <span class="built_in">max</span>(maxLen, dp[i]);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> maxLen;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>事实上这道题还有另一种解法（贪心+二分），首先根据贪心的思想，我们要使上升子序列尽可能长，就需要让子序列上升得尽可能慢，也就是说在保证上升的前提下，尽可能让每个位置的数值小一些。我们可以维护一个数组<code>tails</code>，<code>tails[i]</code> 表示长度为 <code>i</code>的上升子序列的末尾元素的最小值。</p><p>对于每个元素 <code>num</code>，我们使用二分查找在 <code>tails</code>中找到第一个大于等于 <code>num</code> 的位置 <code>index</code>，并将<code>tails[index]</code> 更新为 <code>num</code>。如果 <code>num</code>大于 <code>tails</code> 中的所有元素，则将其添加到 <code>tails</code>的末尾。最终，<code>tails</code> 的长度即为最长递增子序列的长度。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">lengthOfLIS</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">        vector&lt;<span class="type">int</span>&gt; tails;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> num : nums)&#123;</span><br><span class="line">            <span class="keyword">auto</span> it = <span class="built_in">lower_bound</span>(tails.<span class="built_in">begin</span>(), tails.<span class="built_in">end</span>(), num);</span><br><span class="line">            <span class="keyword">if</span>(it == tails.<span class="built_in">end</span>())&#123;</span><br><span class="line">                tails.<span class="built_in">push_back</span>(num);</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                *it = num;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> tails.<span class="built_in">size</span>();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="乘积最大子数组">7. 乘积最大子数组</h3><p><ahref="https://leetcode.cn/problems/maximum-product-subarray/description/">LeetCodeHot100: 152.乘积最大子数组</a></p><p><strong>简要描述</strong>：给你一个整数数组 nums，请你找出数组中乘积最大的连续子数组（该子数组中至少包含一个数字），并返回该子数组所对应的乘积。</p><p><strong>解题关键</strong>：由于数组中可能包含负数，直接使用动态规划计算最大乘积会遇到问题，因为负数会将最大值变为最小值，最小值变为最大值。因此，我们需要<strong>同时维护两个状态：当前的最大乘积和当前的最小乘积</strong>。对于每个元素<code>num</code>，我们有三种选择：</p><ol type="1"><li>将 <code>num</code> 作为新的子数组的开始；</li><li>将 <code>num</code> 乘以当前的最大乘积；</li><li>将 <code>num</code> 乘以当前的最小乘积。</li></ol><p>由于这里每次计算都依赖于前一个状态，我们可以使用滚动数组的思想来优化空间复杂度。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">maxProduct</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> n = nums.<span class="built_in">size</span>();</span><br><span class="line">        <span class="keyword">if</span>(n == <span class="number">0</span>) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        <span class="type">int</span> maxProd = nums[<span class="number">0</span>];</span><br><span class="line">        <span class="type">int</span> minProd = nums[<span class="number">0</span>];</span><br><span class="line">        <span class="type">int</span> result = nums[<span class="number">0</span>];</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">1</span>; i &lt; n; i++)&#123;</span><br><span class="line">            <span class="keyword">if</span>(nums[i] &lt; <span class="number">0</span>)&#123;</span><br><span class="line">                <span class="built_in">swap</span>(maxProd, minProd);</span><br><span class="line">            &#125;</span><br><span class="line">            maxProd = <span class="built_in">max</span>(nums[i], maxProd * nums[i]);</span><br><span class="line">            minProd = <span class="built_in">min</span>(nums[i], minProd * nums[i]);</span><br><span class="line">            result = <span class="built_in">max</span>(result, maxProd);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="分割等和子集">8. 分割等和子集</h3><p><ahref="https://leetcode.cn/problems/partition-equal-subset-sum/description/">LeetCodeHot100: 416.分割等和子集</a></p><p><strong>简要描述</strong>：给你一个 只包含正整数 的 非空 数组 nums。请你判断是否可以将这个数组分割成两个子集，使得两个子集的元素和相等。</p><p><strong>解题关键</strong>：首先计算数组的总和 <code>sum</code>，如果<code>sum</code> 是奇数，则无法分割成两个相等的子集，直接返回false。否则，我们需要找到一个子集，使得其元素和为<code>sum / 2</code>。这可以转化为一个 0-1背包问题。我们定义一个布尔数组 <code>dp</code>，其中 <code>dp[i]</code>表示是否可以凑出和为 <code>i</code>的子集。初始时，<code>dp[0] = true</code>，表示和为 0的子集总是可以凑出的。 对于每个数字 <code>num</code>，我们从后向前遍历<code>dp</code> 数组，在 <code>i&gt;=num</code> 时更新<code>dp[i]</code>（因为当 <code>i&lt;num</code> 时不可能会用到num，此时<code>dp[i]</code> 和之前状态保持一致：</p><p><span class="math display">$$\begin{split}    dp[i] &amp; = dp[i] \lor dp[i - num] \\\end{split}$$</span></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">bool</span> <span class="title">canPartition</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> sum = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> num : nums)&#123;</span><br><span class="line">            sum += num;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span>(sum % <span class="number">2</span> != <span class="number">0</span>) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        <span class="type">int</span> target = sum / <span class="number">2</span>;</span><br><span class="line">        <span class="function">vector&lt;<span class="type">bool</span>&gt; <span class="title">dp</span><span class="params">(target + <span class="number">1</span>, <span class="literal">false</span>)</span></span>;</span><br><span class="line">        dp[<span class="number">0</span>] = <span class="literal">true</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> num : nums)&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="type">int</span> i = target; i &gt;= num; i--)&#123;</span><br><span class="line">                dp[i] = dp[i] || dp[i - num];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> dp[target];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="最长公共子序列">9. 最长公共子序列</h3><p><ahref="https://leetcode.cn/problems/longest-common-subsequence/description/">LeetCodeHot100: 1143.最长公共子序列</a></p><p><strong>简要描述</strong>：给定两个字符串 text1 和text2，返回这两个字符串的最长公共子序列的长度。如果不存在公共子序列，返回0 。</p><p><strong>解题关键</strong>：我们可以使用动态规划来解决这个问题。定义一个二维数组<code>dp</code>，其中 <code>dp[i][j]</code> 表示字符串<code>text1</code> 的前 <code>i</code> 个字符和字符串 <code>text2</code>的前 <code>j</code> 个字符的最长公共子序列的长度。状态转移方程如下：</p><p><span class="math display">$$\begin{split}    dp[i][j] &amp; =    \begin{cases}        dp[i-1][j-1] + 1 &amp; \text{if } text1[i-1] == text2[j-1] \\        \max(dp[i-1][j], dp[i][j-1]) &amp; \text{if } text1[i-1] \neqtext2[j-1]    \end{cases} \\\end{split}$$</span></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">longestCommonSubsequence</span><span class="params">(string text1, string text2)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> m = text<span class="number">1.</span><span class="built_in">size</span>();</span><br><span class="line">        <span class="type">int</span> n = text<span class="number">2.</span><span class="built_in">size</span>();</span><br><span class="line">        vector&lt;vector&lt;<span class="type">int</span>&gt;&gt; <span class="built_in">dp</span>(m + <span class="number">1</span>, <span class="built_in">vector</span>&lt;<span class="type">int</span>&gt;(n + <span class="number">1</span>, <span class="number">0</span>));</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">1</span>; i &lt;= m; i++)&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="type">int</span> j = <span class="number">1</span>; j &lt;= n; j++)&#123;</span><br><span class="line">                <span class="keyword">if</span>(text1[i - <span class="number">1</span>] == text2[j - <span class="number">1</span>])&#123;</span><br><span class="line">                    dp[i][j] = dp[i - <span class="number">1</span>][j - <span class="number">1</span>] + <span class="number">1</span>;</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    dp[i][j] = <span class="built_in">max</span>(dp[i - <span class="number">1</span>][j], dp[i][j - <span class="number">1</span>]);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> dp[m][n];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="编辑距离">10. 编辑距离</h3><p><ahref="https://leetcode.cn/problems/edit-distance/description/">LeetCodeHot100: 72.编辑距离</a></p><p><strong>简要描述</strong>：给你两个单词 word1 和 word2，请你计算出将word1 转换成 word2所使用的最少操作数。你可以对一个单词进行如下三种操作： 1. 插入一个字符2. 删除一个字符 3. 替换一个字符</p><p><strong>解题关键</strong>：我们可以使用动态规划来解决这个问题。定义一个二维数组<code>dp</code>，其中 <code>dp[i][j]</code> 表示将字符串<code>word1</code> 的前 <code>i</code> 个字符转换成字符串<code>word2</code> 的前 <code>j</code> 个字符所需的最少操作数。对于<code>dp[i][j]</code>，如果<code>word1[i-1] == word2[j-1]</code>，则不需要额外操作，<code>dp[i][j] = dp[i-1][j-1]</code>；否则，我们需要考虑分别使用三种操作后的最小值：如果是替换操作，则<code>dp[i-1][j-1] + 1</code>；如果是插入操作，则<code>dp[i][j-1] + 1</code>；如果是删除操作，则<code>dp[i-1][j] + 1</code>。状态转移方程如下：</p><p><span class="math display">$$\begin{split}    dp[i][j] &amp; =    \begin{cases}        dp[i-1][j-1] &amp; \text{if } word1[i-1] == word2[j-1] \\        \min(dp[i-1][j] + 1, dp[i][j-1] + 1, dp[i-1][j-1] + 1) &amp;\text{if } word1[i-1] \neq word2[j-1]    \end{cases} \\\end{split}$$</span></p><p>同时我们还需要维护一下边界条件： 1. 当 <code>word2</code> 为空时，将<code>word1</code> 转换为空所需的操作数为<code>i</code>，即删除所有字符：<code>dp[i][0] = i</code>。 2. 当<code>word1</code> 为空时，将空字符串转换为 <code>word2</code>所需的操作数为<code>j</code>，即插入所有字符：<code>dp[0][j] = j</code>。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">minDistance</span><span class="params">(string word1, string word2)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> m = word<span class="number">1.</span><span class="built_in">size</span>(), n = word<span class="number">2.</span><span class="built_in">size</span>();</span><br><span class="line">        vector&lt;vector&lt;<span class="type">int</span>&gt;&gt; <span class="built_in">dp</span>(m<span class="number">+1</span>, <span class="built_in">vector</span>&lt;<span class="type">int</span>&gt;(n<span class="number">+1</span>));</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;=m;i++)&#123;</span><br><span class="line">            dp[i][<span class="number">0</span>] = i;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> j=<span class="number">0</span>;j&lt;=n;j++)&#123;</span><br><span class="line">            dp[<span class="number">0</span>][j] = j; </span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">1</span>;i&lt;=m;i++)&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="type">int</span> j=<span class="number">1</span>;j&lt;=n;j++)&#123;</span><br><span class="line">                <span class="keyword">if</span>(word1[i<span class="number">-1</span>]==word2[j<span class="number">-1</span>])&#123;</span><br><span class="line">                    dp[i][j] = dp[i<span class="number">-1</span>][j<span class="number">-1</span>];</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">else</span>&#123;</span><br><span class="line">                    dp[i][j] = <span class="built_in">min</span>(dp[i<span class="number">-1</span>][j<span class="number">-1</span>], <span class="built_in">min</span>(dp[i][j<span class="number">-1</span>], dp[i<span class="number">-1</span>][j])) + <span class="number">1</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> dp[m][n];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="目标和">11. 目标和</h3><p><ahref="https://leetcode.cn/problems/target-sum/description/">LeetCode:494.目标和</a></p><p><strong>简要描述</strong>：给你一个整数数组 nums 和一个整数 target。向数组中的每个整数前添加 ‘+’ 或 ‘-’ ，然后串联起所有整数，可以构造一个表达式 ：例如，nums = [2, 1] ，可以在 2 之前添加 ‘+’ ，在 1 之前添加 ‘-’，然后串联得到表达式 “+2-1” 。返回可以通过上述方法构造的、运算结果等于target 的不同 表达式 的数目。</p><p><strong>解题关键</strong>：我们可以将问题转化为一个子集和问题。假设我们将数组分成两个子集S1 和 S2，其中 S1 中的元素前添加 ‘+’，S2 中的元素前添加‘-’。那么我们有以下等式：</p><p><span class="math display">$$\begin{split}    sum(S1) - sum(S2) &amp; = target \\    sum(S1) + sum(S2) &amp; = sum(nums) \\\end{split}$$</span></p><p>通过以上两个等式，我们可以推导出：</p><p><span class="math display">$$\begin{split}    2 * sum(S1) &amp; = target + sum(nums) \\    sum(S1) &amp; = (target + sum(nums)) / 2 \\\end{split}$$</span></p><p>因此，问题转化为在数组中找到和为<code>(target + sum(nums)) / 2</code>的子集的个数。我们可以使用动态规划来解决这个子集和问题，或者说是 0-1背包问题。定义一个数组 <code>dp</code>，其中 <code>dp[i]</code> 表示和为<code>i</code> 的子集的个数。初始时，<code>dp[0] = 1</code>，表示和为 0的子集只有一个，即空集。对于每个数字 <code>num</code>，我们从后向前遍历<code>dp</code> 数组，在 <code>i&gt;=num</code> 时更新<code>dp[i]</code>：</p><p><span class="math display">$$\begin{split}    dp[i] &amp; = dp[i] + dp[i - num] \\\end{split}$$</span></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">findTargetSumWays</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums, <span class="type">int</span> target)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> sum = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> num : nums)&#123;</span><br><span class="line">            sum += num;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span>((target + sum) % <span class="number">2</span> != <span class="number">0</span> || <span class="built_in">abs</span>(target) &gt; sum) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        <span class="type">int</span> subsetSum = (target + sum) / <span class="number">2</span>;</span><br><span class="line">        <span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">dp</span><span class="params">(subsetSum + <span class="number">1</span>, <span class="number">0</span>)</span></span>;</span><br><span class="line">        dp[<span class="number">0</span>] = <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> num : nums)&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="type">int</span> i = subsetSum; i &gt;= num; i--)&#123;</span><br><span class="line">                dp[i] += dp[i - num];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> dp[subsetSum];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="买卖股票的最佳时机-iii">12. 买卖股票的最佳时机 III</h3><p><ahref="https://leetcode.cn/problems/best-time-to-buy-and-sell-stock-iii/description/">LeetCode经典150: 123.买卖股票的最佳时机 III</a></p><p><strong>简要描述</strong>：给定一个数组 prices ，其中 prices[i]是一支给定股票第 i天的价格。设计一个算法来计算你所能获取的最大利润。你最多可以完成 两笔交易。</p><p><strong>解题关键</strong>：我们可以使用动态规划来解决这个问题。分析每一天的状态，我们可以发现也就可能处于5种状态：</p><ol type="1"><li>未进行任何操作；</li><li>第一次买入股票；</li><li>第一次卖出股票；</li><li>第二次买入股票；</li><li>第二次卖出股票。</li></ol><p>对于第一种状态，显然利润为0；对于第二种状态，利润为负的股票价格；对于第三种状态，利润为第一次卖出股票的最大利润；对于第四种状态，利润为第一次卖出股票的最大利润减去第二次买入股票的价格；对于第五种状态，利润为第二次卖出股票的最大利润。因此我们可以定义一个二维数组<code>dp</code>，其中 <code>dp[i][j]</code> 表示第 <code>i</code>天处于状态 <code>j</code> 时的最大利润。状态转移方程如下：</p><p><span class="math display">$$\begin{split}    dp[i][0] &amp; = 0 \\    dp[i][1] &amp; = \max(dp[i-1][1], -prices[i]) \\    dp[i][2] &amp; = \max(dp[i-1][2], dp[i-1][1] + prices[i]) \\    dp[i][3] &amp; = \max(dp[i-1][3], dp[i-1][2] - prices[i]) \\    dp[i][4] &amp; = \max(dp[i-1][4], dp[i-1][3] + prices[i]) \\\end{split}$$</span></p><p>可以发现，每个状态只依赖于前一天的状态，因此我们可以使用滚动数组来优化空间复杂度，并且对于第一种状态不用记录：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">maxProfit</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; prices)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> n = prices.<span class="built_in">size</span>();</span><br><span class="line">        <span class="type">int</span> buy1=-prices[<span class="number">0</span>], sell1=<span class="number">0</span>, buy2=-prices[<span class="number">0</span>], sell2=<span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">1</span>;i&lt;n;i++)&#123;</span><br><span class="line">            buy1 = <span class="built_in">max</span>(buy1, -prices[i]);</span><br><span class="line">            sell1 = <span class="built_in">max</span>(sell1, prices[i]+buy1);</span><br><span class="line">            buy2 = <span class="built_in">max</span>(buy2, sell1-prices[i]);</span><br><span class="line">            sell2 = <span class="built_in">max</span>(sell2, buy2+prices[i]);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> sell2;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 刷题 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>前缀和的算法题</title>
      <link href="/2025/09/21/Algo/prefix_sum/"/>
      <url>/2025/09/21/Algo/prefix_sum/</url>
      
        <content type="html"><![CDATA[<h1 id="前缀和">前缀和</h1><h2 id="应用场景">应用场景</h2><p>前缀和（PrefixSum）是一种用于高效计算数组子区间和的技术。通过预处理数组，构建一个前缀和数组，可以在常数时间内计算任意子区间的和，从而显著提高查询效率。前缀和常用于解决涉及频繁求和操作的问题，如区间和查询、子数组和等。</p><h2 id="例题">例题</h2><h3 id="和为k的子数组">1. 和为K的子数组</h3><p><ahref="https://leetcode.cn/problems/subarray-sum-equals-k/description/">LeetCodeHot100: 560.和为K的子数组</a></p><p><strong>简要描述</strong>：给定一个整数数组和一个整数k，你需要找到该数组中和为 k 的连续的子数组的个数。</p><p><strong>解题关键</strong>：如果使用最简单的做法，就是双重循环得到子数组两端<code>i</code>和<code>j</code>，然后计算子数组<code>nums[i:j]</code>的和，判断是否等于<code>k</code>。这种做法的时间复杂度为<spanclass="math inline">𝒪(<em>N</em><sup>2</sup>)</span>，在数据量较大时效率较低。</p><p>我们可以使用前缀和来解决这个问题。定义前缀和数组<code>prefixSum</code>，其中 <code>prefixSum[i]</code> 表示数组<code>nums</code> 中从索引 <code>0</code> 到索引 <code>i-1</code>的元素之和。这样，<strong>任意子数组 <code>nums[i:j]</code>的和可以表示为<code>prefixSum[j+1] - prefixSum[i]</code></strong>。这里有两个地方可以改进：一是如果我们从左到右遍历数组，我们不需要显式地构建前缀和数组，只需要一个变量来维护当前的前缀和即可；二是我们可以使用哈希表来存储每个前缀和出现的次数，从而在遍历数组时，快速查找是否存在一个前缀和<code>prefixSum[j+1] - k</code>，使得子数组 <code>nums[i:j]</code>的和为 <code>k</code>。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">subarraySum</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums, <span class="type">int</span> k)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> ans = <span class="number">0</span>;</span><br><span class="line">        unordered_map&lt;<span class="type">int</span>, <span class="type">int</span>&gt; num_map;</span><br><span class="line">        <span class="type">int</span> pre = <span class="number">0</span>;</span><br><span class="line">        num_map[<span class="number">0</span>] = <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> &amp;x:nums)&#123;</span><br><span class="line">            pre+=x;</span><br><span class="line">            <span class="keyword">if</span>(num_map.<span class="built_in">find</span>(pre-k)!=num_map.<span class="built_in">end</span>())&#123;</span><br><span class="line">                ans+=num_map[pre-k];</span><br><span class="line">            &#125;</span><br><span class="line">            num_map[pre]++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 刷题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Algorithm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>双指针的算法题</title>
      <link href="/2025/09/21/Algo/two_pointer/"/>
      <url>/2025/09/21/Algo/two_pointer/</url>
      
        <content type="html"><![CDATA[<h1 id="双指针的使用">双指针的使用</h1><h2 id="应用场景">应用场景</h2><p>「双指针」，当我们需要枚举数组中的两个元素时，如果我们发现<strong>随着第一个元素的递增，第二个元素是递减的</strong>，那么就可以使用双指针的方法，将枚举的时间复杂度从<span class="math inline">𝒪(<em>N</em><sup>2</sup>)</span> 减少至 <spanclass="math inline">𝒪(<em>N</em>)</span>。为什么是 <spanclass="math inline">𝒪(<em>N</em>)</span>呢？这是因为在枚举的过程每一步中，「左指针」会向右移动一个位置，而「右指针」会向左移动若干个位置，这个与数组的元素有关，但我们知道它一共会移动的位置数为<spanclass="math inline">𝒪(<em>N</em>)</span>，均摊下来，每次也向左移动一个位置，因此时间复杂度为<span class="math inline">𝒪(<em>N</em>)</span>。</p><hr /><h2 id="例题">例题</h2><h3 id="三数之和">1. 三数之和</h3><p><a href="https://leetcode.cn/problems/3sum/description/">LeetCodeHot100: 15.三数之和</a></p><p><strong>简要描述</strong>：给定一个包含 n 个整数的数组 nums，判断nums 中是否存在三个元素 a，b，c 使得 a + b + c = 0 ?找到所有满足条件且不重复的三元组。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    vector&lt;vector&lt;<span class="type">int</span>&gt;&gt; <span class="built_in">threeSum</span>(vector&lt;<span class="type">int</span>&gt;&amp; nums) &#123;</span><br><span class="line">        <span class="type">int</span> n = nums.<span class="built_in">size</span>();</span><br><span class="line">        <span class="built_in">sort</span>(nums.<span class="built_in">begin</span>(), nums.<span class="built_in">end</span>());</span><br><span class="line">        vector&lt;vector&lt;<span class="type">int</span>&gt;&gt; ans;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> first=<span class="number">0</span>; first&lt;n; first++)&#123;</span><br><span class="line">            <span class="keyword">if</span>(first&gt;<span class="number">0</span> &amp;&amp; nums[first]==nums[first<span class="number">-1</span>])&#123;</span><br><span class="line">                <span class="keyword">continue</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// 如果用传统做法，需要三重循环枚举 second 和 third</span></span><br><span class="line">            <span class="comment">// 但我们发现随着 first 的增加，second 需要增大，而 third 需要减小</span></span><br><span class="line">            <span class="comment">// 因此可以使用双指针遍历后面两层循环将时间复杂度从 O(N^3) 降到 O(N^2)</span></span><br><span class="line">            <span class="type">int</span> target = -nums[first];</span><br><span class="line">            <span class="type">int</span> third = n<span class="number">-1</span>;</span><br><span class="line">            <span class="keyword">for</span>(<span class="type">int</span> second=first<span class="number">+1</span>; second&lt;n; second++)&#123;</span><br><span class="line">                <span class="keyword">if</span>(second&gt;first<span class="number">+1</span> &amp;&amp; nums[second]==nums[second<span class="number">-1</span>])&#123;</span><br><span class="line">                    <span class="keyword">continue</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">while</span>(second&lt;third &amp;&amp; nums[second]+nums[third]&gt;target)&#123;</span><br><span class="line">                    third--;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">if</span>(second==third)&#123;</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">if</span>(nums[second]+nums[third]==target)&#123;</span><br><span class="line">                    ans.<span class="built_in">push_back</span>(&#123;nums[first], nums[second], nums[third]&#125;);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="接雨水">2. 接雨水</h3><p><ahref="https://leetcode.cn/problems/trapping-rain-water/description/">LeetCodeHot100: 42.接雨水</a></p><p><strong>简要描述</strong>：给定 n 个非负整数表示每个宽度为 1的柱子的高度图，计算按此排列的柱子，下雨之后能接多少雨水。</p><p><strong>解题关键</strong>：对于每根柱子来说，能接的雨水量取决于其左侧最高的柱子和右侧最高的柱子中的较小值与当前柱子高度的差值，因此我们需要找到每个位置的左侧最高柱子和右侧最高柱子。可以使用动态规划来预处理每个位置的左侧最高柱子和右侧最高柱子，从而将时间复杂度降低到<spanclass="math inline">𝒪(<em>N</em>)</span>。但是这样需要额外维护两个数组，空间复杂度为<spanclass="math inline">𝒪(<em>N</em>)</span>。进一步思考，我们可以使用双指针来优化空间复杂度。</p><p>维护两个指针<code>left</code>和<code>right</code>，以及两个变量<code>leftMax</code>和<code>rightMax</code>，初始时<code>left=0</code>，<code>right=n−1</code>，<code>leftMax=0</code>，<code>rightMax=0</code>。指针<code>left</code>只会向右移动，指针<code>right</code>只会向左移动，在移动指针的过程中维护两个变量<code>leftMax</code>和<code>rightMax</code>的值。</p><p>当两个指针没有相遇时，进行如下操作：</p><ul><li><p>使用<code>height[left]</code>和<code>height[right]</code>的值更新<code>leftMax</code>和<code>rightMax</code>的值；</p></li><li><p>如果<code>leftMax &lt; rightMax</code>，下标<code>left</code>处能接的雨水量等于<code>leftMax - height[left]</code>，将下标<code>left</code>处能接的雨水量加到能接的雨水总量，然后将<code>left</code>加<code>1</code>（即向右移动一位）；</p></li><li><p>如果<code>leftMax ≥ rightMax</code>，下标<code>right</code>处能接的雨水量等于<code>rightMax - height[right]</code>，将下标<code>right</code>处能接的雨水量加到能接的雨水总量，然后将<code>right</code>减<code>1</code>（即向左移动一位）。</p></li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">trap</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; height)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> n = height.<span class="built_in">size</span>();</span><br><span class="line">        <span class="keyword">if</span>(n==<span class="number">0</span>) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        <span class="type">int</span> left = <span class="number">0</span>, right = n<span class="number">-1</span>;</span><br><span class="line">        <span class="type">int</span> leftMax = <span class="number">0</span>, rightMax = <span class="number">0</span>;</span><br><span class="line">        <span class="type">int</span> ans = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">while</span>(left&lt;right)&#123;</span><br><span class="line">            leftMax = <span class="built_in">max</span>(leftMax, height[left]);</span><br><span class="line">            rightMax = <span class="built_in">max</span>(rightMax, height[right]);</span><br><span class="line">            <span class="keyword">if</span>(leftMax &lt; rightMax)&#123;</span><br><span class="line">                ans += leftMax - height[left];</span><br><span class="line">                left++;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                ans += rightMax - height[right];</span><br><span class="line">                right--;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="相交链表">3. 相交链表</h3><p><ahref="https://leetcode.cn/problems/intersection-of-two-linked-lists/description/">LeetCodeHot100: 160.相交链表</a></p><p><strong>简要描述</strong>：编写一个程序，找到两个单链表相交的起始节点。</p><p><strong>解题关键</strong>：当链表 headA 和 headB都不为空时，创建两个指针 <code>pA</code> 和<code>pB</code>，初始时分别指向两个链表的头节点 headA 和headB，然后将两个指针依次遍历两个链表的每个节点。具体做法如下：</p><ul><li><p>每步操作需要同时更新指针 <code>pA</code> 和<code>pB</code>；</p></li><li><p>如果指针 <code>pA</code> 不为空，则将指针 <code>pA</code>移到下一个节点；如果指针 <code>pB</code> 不为空，则将指针<code>pB</code> 移到下一个节点；</p></li><li><p>如果指针 <code>pA</code> 为空，则将指针 <code>pA</code> 移到链表headB 的头节点；如果指针 <code>pB</code> 为空，则将指针 <code>pB</code>移到链表 headA 的头节点；</p></li><li><p>当指针 <code>pA</code> 和 <code>pB</code>指向同一个节点或者都为空时，返回它们指向的节点或者 null。</p></li></ul><p>如何理解这种做法呢？假设链表 headA 的长度为 <code>a+c</code>，链表headB 的长度为 <code>b+c</code>，其中 <code>c</code>是两个链表相交部分的长度，<code>a</code> 和 <code>b</code> 分别是链表headA 和 headB 独有部分的长度。那么当指针 <code>pA</code> 遍历完链表headA 后，再遍历链表 headB 时，它一共走了 <code>a+c+b</code>步；同理，当指针 <code>pB</code> 遍历完链表 headB 后，再遍历链表 headA时，它一共走了 <code>b+c+a</code> 步。这样一来，两个指针在走过<code>a+c+b</code>步后会同时到达相交节点（如果存在相交节点），或者同时到达链表的末尾null（如果不存在相交节点）。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">ListNode *<span class="title">getIntersectionNode</span><span class="params">(ListNode *headA, ListNode *headB)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(!headA || !headB) <span class="keyword">return</span> <span class="literal">nullptr</span>;</span><br><span class="line">        ListNode* pA = headA;</span><br><span class="line">        ListNode* pB = headB;</span><br><span class="line">        <span class="keyword">while</span>(pA != pB)&#123;</span><br><span class="line">            pA = pA ? pA-&gt;next : headB;</span><br><span class="line">            pB = pB ? pB-&gt;next : headA;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> pA;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 刷题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Algorithm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>C++常用STL结构体以及常用语法点</title>
      <link href="/2025/09/19/Algo/cpp_use/"/>
      <url>/2025/09/19/Algo/cpp_use/</url>
      
        <content type="html"><![CDATA[<h1 id="c常用stl结构体及常用语法点">C++常用STL结构体及常用语法点</h1><h2 id="vector">1. <code>vector</code></h2><p><code>vector</code> 是来自C++STL（标准模板库）的顺序容器，底层就是动态数组，可以自动扩容，可以随机访问（下标访问<span class="math inline">𝒪(1)</span>），支持在末尾高效插入/删除。</p><h3 id="定义方式">定义方式</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line">vector&lt;<span class="type">int</span>&gt; v1;</span><br><span class="line"><span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">v2</span><span class="params">(<span class="number">5</span>)</span></span>; <span class="comment">// 长度为5，初始值为0</span></span><br><span class="line"><span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">v3</span><span class="params">(<span class="number">5</span>,<span class="number">100</span>)</span></span>; <span class="comment">// 长度为5，初始值全为100</span></span><br><span class="line"></span><br><span class="line">string s = <span class="string">&quot;hello&quot;</span>;</span><br><span class="line"><span class="function">vector&lt;<span class="type">char</span>&gt; <span class="title">v4</span><span class="params">(s.begin(), s.end())</span></span>; <span class="comment">// 通过迭代器区间构造</span></span><br></pre></td></tr></table></figure><h3 id="常用用法">常用用法</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">vector&lt;<span class="type">int</span>&gt; v;</span><br><span class="line">v.<span class="built_in">push_back</span>(<span class="number">10</span>); <span class="comment">// 末尾添加10</span></span><br><span class="line">v.<span class="built_in">emplace_back</span>(<span class="number">20</span>); <span class="comment">// 末尾添加20，效率更高，一般和push_back等效</span></span><br><span class="line"></span><br><span class="line">cout &lt;&lt; v[<span class="number">0</span>];</span><br><span class="line">cout &lt;&lt; v.<span class="built_in">at</span>(<span class="number">1</span>); <span class="comment">// 带边界检查的访问，越界会抛异常</span></span><br><span class="line">cout &lt;&lt; v.<span class="built_in">front</span>(); <span class="comment">// 第一个元素</span></span><br><span class="line">cout &lt;&lt; v.<span class="built_in">back</span>(); <span class="comment">// 最后一个元素</span></span><br><span class="line"></span><br><span class="line">v.<span class="built_in">pop_back</span>(); <span class="comment">// 删除末尾元素</span></span><br><span class="line">v.<span class="built_in">clear</span>(); <span class="comment">// 清空所有元素</span></span><br><span class="line"></span><br><span class="line">cout &lt;&lt; v.<span class="built_in">size</span>(); </span><br><span class="line">cout &lt;&lt; v.<span class="built_in">empty</span>();</span><br></pre></td></tr></table></figure><h3 id="二维vector">二维vector</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">vector&lt;vector&lt;<span class="type">int</span>&gt;&gt; <span class="built_in">grid</span>(<span class="number">3</span>, <span class="built_in">vector</span>&lt;<span class="type">int</span>&gt;(<span class="number">4</span>,<span class="number">0</span>)); <span class="comment">// 3行4列的全0矩阵</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 邻接表</span></span><br><span class="line"><span class="type">int</span> n = <span class="number">5</span>;</span><br><span class="line">vector&lt;vector&lt;<span class="type">int</span>&gt;&gt; <span class="built_in">adj</span>(n);   <span class="comment">// n个点</span></span><br><span class="line">adj[<span class="number">0</span>].<span class="built_in">push_back</span>(<span class="number">1</span>);</span><br><span class="line">adj[<span class="number">1</span>].<span class="built_in">push_back</span>(<span class="number">2</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 二维DP</span></span><br><span class="line">vertor&lt;vector&lt;<span class="type">int</span>&gt;&gt; <span class="built_in">dp</span>(n<span class="number">+1</span>, <span class="built_in">vector</span>&lt;<span class="type">int</span>&gt;(m<span class="number">+1</span>, <span class="number">0</span>));</span><br></pre></td></tr></table></figure><h2 id="unordered_map">2. <code>unordered_map</code></h2><p>它是 C++ STL 提供的一个哈希表（HashTable）容器。存储键值对（key-value），查找、插入、删除操作的平均时间复杂度都是<spanclass="math inline">𝒪(1)</span>，底层实现是<strong>哈希表</strong>（hashtable）+ <strong>拉链法/开放寻址法</strong>解决冲突。</p><h3 id="定义方式-1">定义方式</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;unordered_map&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line">unordered_map&lt;<span class="type">int</span>, <span class="type">int</span>&gt; mp;           <span class="comment">// key:int, value:int</span></span><br><span class="line">unordered_map&lt;string, <span class="type">int</span>&gt; wordFreq;  <span class="comment">// key:string, value:int</span></span><br></pre></td></tr></table></figure><h3 id="常见操作">常见操作</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 插入/修改</span></span><br><span class="line">mp[<span class="number">1</span>] = <span class="number">100</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 查找</span></span><br><span class="line"><span class="keyword">if</span> (mp.<span class="built_in">find</span>(<span class="number">2</span>)!=mp.<span class="built_in">end</span>())&#123;</span><br><span class="line">cout &lt;&lt; <span class="string">&quot;key=2,value=&quot;</span> &lt;&lt; mp[<span class="number">2</span>] &lt;&lt; endl;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 遍历</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">auto</span> &amp;p:mp)&#123;</span><br><span class="line">cout &lt;&lt; <span class="string">&quot;key=&quot;</span> &lt;&lt; p.first &lt;&lt; <span class="string">&quot; value=&quot;</span> &lt;&lt; p.second &lt;&lt; endl;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 删除</span></span><br><span class="line">mp.<span class="built_in">erase</span>(<span class="number">2</span>); <span class="comment">// 删除键2</span></span><br><span class="line">mp.<span class="built_in">clear</span>(); <span class="comment">// 清空所有元素</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 判断是否为空</span></span><br><span class="line">cout &lt;&lt; mp.<span class="built_in">size</span>();</span><br><span class="line">cout &lt;&lt; mp.<span class="built_in">empty</span>(); <span class="comment">// (true/false)</span></span><br></pre></td></tr></table></figure><blockquote><p><em>这里在遍历的时候用的是<code>auto &amp;p:mp</code>，为什么不用<code>auto p:mp</code>？</em></p><p>因为<code>auto &amp;p:mp</code>是对集合中每个元素的引用，后者是对每个元素的副本。使用前者可以避免不必要的拷贝开销，提升性能，尤其是当元素类型较大时，创建一个副本会消耗额外的内存和CPU周期。在大多数情况下，特别是处理<code>std::string</code>或自定义类对象时，推荐使用引用遍历以提高效率。</p></blockquote><h3 id="常用场景">常用场景</h3><p>用于快速查找、计数、映射关系。比如<strong>判断数组是否包含重复元素</strong>、<strong>计数器</strong>、<strong>统计单词频率</strong>等。</p><h2 id="unordered_set">3. <code>unordered_set</code></h2><p><code>std::unordered_set</code> 是 C++ STL 提供的一个哈希集合（HashSet）容器。它存储唯一的元素，且不保证元素的顺序。和<code>std::set</code> 不同，<code>unordered_set</code>的底层实现是<strong>哈希表</strong>（hashtable），支持高效的插入、删除和查找操作，平均时间复杂度为 <spanclass="math inline">𝒪(1)</span>。</p><h3 id="定义方式-2">定义方式</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;unordered_set&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line">unordered_set&lt;<span class="type">int</span>&gt; s;           <span class="comment">// 存储int类型的唯一元素</span></span><br><span class="line">unordered_set&lt;string&gt; fruits = &#123;<span class="string">&quot;apple&quot;</span>, <span class="string">&quot;banana&quot;</span>, <span class="string">&quot;orange&quot;</span>&#125;;  <span class="comment">// 存储string类型的唯一元素</span></span><br></pre></td></tr></table></figure><h3 id="常见操作-1">常见操作</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 插入元素</span></span><br><span class="line">s.<span class="built_in">insert</span>(<span class="number">10</span>);</span><br><span class="line">s.<span class="built_in">insert</span>(<span class="number">20</span>);</span><br><span class="line">s.<span class="built_in">insert</span>(<span class="number">10</span>); <span class="comment">// 重复插入无效</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 查找元素</span></span><br><span class="line"><span class="keyword">if</span> (s.<span class="built_in">find</span>(<span class="number">20</span>) != s.<span class="built_in">end</span>()) &#123;</span><br><span class="line">    cout &lt;&lt; <span class="string">&quot;20 exists in the set&quot;</span> &lt;&lt; endl;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 或者用count更高效，返回1或0</span></span><br><span class="line"><span class="keyword">if</span> (s.<span class="built_in">count</span>(<span class="number">20</span>)) &#123;</span><br><span class="line">    cout &lt;&lt; <span class="string">&quot;20 exists in the set&quot;</span> &lt;&lt; endl;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 删除元素</span></span><br><span class="line">s.<span class="built_in">erase</span>(<span class="number">10</span>); <span class="comment">// 删除元素10</span></span><br><span class="line">s.<span class="built_in">clear</span>(); <span class="comment">// 清空所有元素</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 遍历元素</span></span><br><span class="line"><span class="keyword">for</span> (<span class="type">const</span> <span class="keyword">auto</span> &amp;elem : s) &#123;</span><br><span class="line">    cout &lt;&lt; elem &lt;&lt; <span class="string">&quot; &quot;</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="常用场景-1">常用场景</h3><p>用于快速查找唯一元素、去重、集合运算等。比如<strong>判断数组是否包含重复元素</strong>、<strong>求两个数组的交集/并集</strong>等。</p><h2 id="priority_queue">4. <code>priority_queue</code></h2><p><code>std::priority_queue</code> 是 C++ STL提供的一个优先队列容器。它是一种基于堆（heap）数据结构实现的容器，支持高效的插入和删除操作。默认情况下，<code>priority_queue</code>是一个<strong>最大堆（max-heap）</strong>，即每次访问或删除的都是当前队列中最大的元素，其插入和删除最大元素的时间复杂度为<span class="math inline">𝒪(log <em>n</em>)</span>。</p><h3 id="常用场景-2">常用场景</h3><p>常用于需要频繁调取最大值/最小值的场景，如滑动窗口最大值、合并区间、topK 问题等。</p><h3 id="定义方式-3">定义方式</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;queue&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line">priority_queue&lt;<span class="type">int</span>&gt; maxHeap; <span class="comment">// 默认是最大堆</span></span><br><span class="line">priority_queue&lt;<span class="type">int</span>, vector&lt;<span class="type">int</span>&gt;, greater&lt;<span class="type">int</span>&gt;&gt; minHeap; <span class="comment">// 最小堆</span></span><br><span class="line">priority_queue&lt;pair&lt;<span class="type">int</span>, <span class="type">int</span>&gt;&gt; pq; <span class="comment">// 存储pair，按第一个元素排序</span></span><br><span class="line">priority_queue&lt;pair&lt;<span class="type">int</span>, <span class="type">int</span>&gt;, vector&lt;pair&lt;<span class="type">int</span>, <span class="type">int</span>&gt;&gt;, greater&lt;pair&lt;<span class="type">int</span>, <span class="type">int</span>&gt;&gt;&gt; minPq; <span class="comment">// 最小堆，按第一个元素排序</span></span><br></pre></td></tr></table></figure><blockquote><p><em>最小堆是怎么构建的？</em></p><p><code>priority_queue</code> 有三个模板参数： 1. 元素类型（如<code>int</code>、<code>pair&lt;int, int&gt;</code> 等） 2.底层容器类型（默认为 <code>std::vector</code>） 3. 比较函数（默认为<code>std::less&lt;T&gt;</code>，即最大堆）</p><p>比较函数用于定义堆的排序规则。通过将比较函数设置为<code>std::greater&lt;T&gt;</code>，可以将默认的最大堆转换为最小堆。</p></blockquote><h3 id="常用用法-1">常用用法</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">priority_queue&lt;<span class="type">int</span>&gt; pq;</span><br><span class="line">priority_queue&lt;pair&lt;<span class="type">int</span>, <span class="type">int</span>&gt;&gt; pq_pair; <span class="comment">// 按pair的第一个元素排序</span></span><br><span class="line"></span><br><span class="line">pq.<span class="built_in">push</span>(<span class="number">10</span>); <span class="comment">// 插入元素</span></span><br><span class="line">pq_pair.<span class="built_in">emplace</span>(<span class="number">1</span>, <span class="number">100</span>); <span class="comment">// 插入pair(1,100)，相比于使用push可以省去一次拷贝</span></span><br><span class="line"></span><br><span class="line">p = pq.<span class="built_in">top</span>(); <span class="comment">// 返回最大元素</span></span><br><span class="line">pq.<span class="built_in">pop</span>(); <span class="comment">// 删除最大元素</span></span><br><span class="line">cout &lt;&lt; pq.<span class="built_in">size</span>();</span><br><span class="line">cout &lt;&lt; pq.<span class="built_in">empty</span>(); <span class="comment">// (true/false)</span></span><br><span class="line"></span><br><span class="line">id, val = pq_pair.<span class="built_in">top</span>(); <span class="comment">// 返回pair的最大元素</span></span><br></pre></td></tr></table></figure><h2 id="queue">5. <code>queue</code></h2><p><code>std::queue</code> 是 C++ STL提供的一个队列容器。它是一种<strong>先进先出</strong>（FIFO, First InFirstOut）的数据结构，支持在队列的末尾插入元素，在队列的前端删除元素。<code>queue</code>底层通常是通过 <code>deque</code>（双端队列）或<code>list</code>（链表）实现的。</p><h3 id="定义方式-4">定义方式</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;queue&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line">queue&lt;<span class="type">int</span>&gt; q; <span class="comment">// 定义一个存储int类型的队列</span></span><br><span class="line">queue&lt;string&gt; strQueue; <span class="comment">// 定义一个存储string类型的队列</span></span><br></pre></td></tr></table></figure><h3 id="常用操作">常用操作</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">q.<span class="built_in">push</span>(<span class="number">10</span>); <span class="comment">// 在队列末尾插入元素10(入队)</span></span><br><span class="line">q.<span class="built_in">front</span>(); <span class="comment">// 返回队列前端的元素(不删除)</span></span><br><span class="line">q.<span class="built_in">back</span>(); <span class="comment">// 返回队列末尾的元素(不删除)</span></span><br><span class="line">q.<span class="built_in">pop</span>(); <span class="comment">// 删除队列前端的元素(出队)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 没有随机访问q[i]，只能访问队首和队尾</span></span><br><span class="line">cout &lt;&lt; q.<span class="built_in">size</span>(); <span class="comment">// 返回队列中元素的个数</span></span><br></pre></td></tr></table></figure><h2 id="deque">6. <code>deque</code></h2><p><code>std::deque</code> 是 C++ STL提供的一个双端队列容器。它支持在队列的两端高效地插入和删除元素，同时也支持随机访问。<code>deque</code>底层通常是通过一系列连续的内存块（chunk）实现的，因此在两端插入和删除元素的时间复杂度为<span class="math inline">𝒪(1)</span>，而随机访问的时间复杂度为 <spanclass="math inline">𝒪(1)</span>。可以看作是<code>queue</code>和<code>stack</code>的结合体。</p><h3 id="定义方式-5">定义方式</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;deque&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line">deque&lt;<span class="type">int</span>&gt; dq; <span class="comment">// 定义一个存储int类型的双端队列</span></span><br><span class="line">deque&lt;string&gt; strDeque; <span class="comment">// 定义一个存储string类型的双端队列</span></span><br></pre></td></tr></table></figure><h3 id="常用操作-1">常用操作</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">dq.<span class="built_in">push_back</span>(<span class="number">10</span>); <span class="comment">// 在队列末尾插入元素10</span></span><br><span class="line">dq.<span class="built_in">push_front</span>(<span class="number">20</span>); <span class="comment">// 在队列前端插入元素20</span></span><br><span class="line">dq.<span class="built_in">pop_back</span>(); <span class="comment">// 删除队列末尾的元素</span></span><br><span class="line">dq.<span class="built_in">pop_front</span>(); <span class="comment">// 删除队列前端的元素</span></span><br><span class="line"></span><br><span class="line">cout &lt;&lt; dq.<span class="built_in">front</span>(); <span class="comment">// 返回队列前端的元素</span></span><br><span class="line">cout &lt;&lt; dq.<span class="built_in">back</span>(); <span class="comment">// 返回队列末尾的元素</span></span><br><span class="line">cout &lt;&lt; dq[<span class="number">0</span>]; <span class="comment">// 随机访问第一个元素</span></span><br></pre></td></tr></table></figure><h2 id="链表">7. 链表</h2><p>链表是一种常见的数据结构，由一系列节点组成，每个节点包含数据和指向下一个节点的指针。链表的主要优点是插入和删除操作高效，时间复杂度为<span class="math inline">𝒪(1)</span>，但随机访问元素的时间复杂度为<span class="math inline">𝒪(<em>n</em>)</span>。</p><h3 id="自定义单向链表">自定义单向链表</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">Node</span> &#123;</span><br><span class="line">    <span class="type">int</span> val;</span><br><span class="line">    Node* next; <span class="comment">// 指向下一个节点的指针</span></span><br><span class="line">    <span class="built_in">Node</span>(<span class="type">int</span> x) : <span class="built_in">val</span>(x), <span class="built_in">next</span>(<span class="literal">nullptr</span>) &#123;&#125; <span class="comment">// 构造函数</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><blockquote><p><em>为什么要定义构造函数？</em>定义构造函数可以在创建节点时直接初始化节点的值和指针，避免手动赋值的繁琐和错误，提高代码的可读性和安全性。其中<code>Node(int x) : val(x), next(nullptr) &#123;&#125;</code>是一个构造函数的初始化列表语法，表示在创建 <code>Node</code>对象时，使用参数 <code>x</code> 初始化成员变量<code>val</code>，并将成员变量 <code>next</code> 初始化为<code>nullptr</code>。在使用时可以直接这样写： <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Node* node = <span class="keyword">new</span> <span class="built_in">Node</span>(<span class="number">5</span>); <span class="comment">// val=5, next=nullptr</span></span><br></pre></td></tr></table></figure>如果不定义构造函数，创建节点时需要手动赋值，如下所示：<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Node* node = <span class="keyword">new</span> Node; <span class="comment">// 需要手动赋值</span></span><br><span class="line">node-&gt;val = <span class="number">5</span>;</span><br><span class="line">node-&gt;next = <span class="literal">nullptr</span>;</span><br></pre></td></tr></table></figure></p></blockquote><h4 id="常见操作-2">常见操作</h4><p>插入节点（新增节点）：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Node* newNode = <span class="keyword">new</span> <span class="built_in">Node</span>(<span class="number">10</span>); <span class="comment">// 创建新节点</span></span><br><span class="line">newNode-&gt;next = head; <span class="comment">// 将新节点的next指向当前头节点</span></span><br><span class="line">head = newNode; <span class="comment">// 更新头节点为新节点</span></span><br></pre></td></tr></table></figure><p>删除节点：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Node* temp = head; <span class="comment">// 临时指针指向头节点</span></span><br><span class="line">head = head-&gt;next; <span class="comment">// 更新头节点为下一个节点</span></span><br><span class="line"><span class="keyword">delete</span> temp; <span class="comment">// 删除原头节点</span></span><br></pre></td></tr></table></figure><p>删除指定值的节点：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Node* curr = head;</span><br><span class="line">Node* prev = <span class="literal">nullptr</span>;</span><br><span class="line"><span class="keyword">while</span> (curr != <span class="literal">nullptr</span> &amp;&amp; curr-&gt;val != target) &#123;</span><br><span class="line">    prev = curr;</span><br><span class="line">    curr = curr-&gt;next;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> (curr != <span class="literal">nullptr</span>) &#123; <span class="comment">// 找到目标节点</span></span><br><span class="line">    prev-&gt;next = curr-&gt;next; <span class="comment">// 删除目标节点</span></span><br><span class="line">    <span class="keyword">delete</span> curr;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>销毁整个链表：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> (head != <span class="literal">nullptr</span>) &#123;</span><br><span class="line">    Node* temp = head;</span><br><span class="line">    head = head-&gt;next;</span><br><span class="line">    <span class="keyword">delete</span> temp;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="自定义双向链表">自定义双向链表</h3><p>双向链表中的每个节点包含指向前一个节点和后一个节点的指针。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">DNode</span> &#123;</span><br><span class="line">    <span class="type">int</span> val;</span><br><span class="line">    DNode* prev; <span class="comment">// 指向前一个节点的指针</span></span><br><span class="line">    DNode* next; <span class="comment">// 指向下一个节点的指针</span></span><br><span class="line">    <span class="built_in">DNode</span>(<span class="type">int</span> x) : <span class="built_in">val</span>(x), <span class="built_in">prev</span>(<span class="literal">nullptr</span>), <span class="built_in">next</span>(<span class="literal">nullptr</span>) &#123;&#125; <span class="comment">// 构造函数</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h4 id="常见操作-3">常见操作</h4><p>插入节点（新增节点）：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">DNode* newNode = <span class="keyword">new</span> <span class="built_in">DNode</span>(<span class="number">10</span>); <span class="comment">// 创建新节点</span></span><br><span class="line">newNode-&gt;next = head; <span class="comment">// 将新节点的next指向当前头节点</span></span><br><span class="line"><span class="keyword">if</span> (head != <span class="literal">nullptr</span>) &#123;</span><br><span class="line">    head-&gt;prev = newNode; <span class="comment">// 更新当前头节点的prev指向新节点</span></span><br><span class="line">&#125;</span><br><span class="line">head = newNode; <span class="comment">// 更新头节点为新节点</span></span><br></pre></td></tr></table></figure><p>删除指定值的节点：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">DNode* curr = head;</span><br><span class="line"><span class="keyword">while</span> (curr != <span class="literal">nullptr</span> &amp;&amp; curr-&gt;val != target) &#123;</span><br><span class="line">    curr = curr-&gt;next;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> (curr != <span class="literal">nullptr</span>) &#123; <span class="comment">// 找到目标节点</span></span><br><span class="line">    <span class="keyword">if</span> (curr-&gt;prev != <span class="literal">nullptr</span>) &#123;</span><br><span class="line">        curr-&gt;prev-&gt;next = curr-&gt;next; <span class="comment">// 更新前一个节点的next</span></span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        head = curr-&gt;next; <span class="comment">// 更新头节点</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (curr-&gt;next != <span class="literal">nullptr</span>) &#123;</span><br><span class="line">        curr-&gt;next-&gt;prev = curr-&gt;prev; <span class="comment">// 更新后一个节点的prev</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">delete</span> curr; <span class="comment">// 删除目标节点</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>销毁整个链表：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> (head != <span class="literal">nullptr</span>) &#123;</span><br><span class="line">    DNode* temp = head;</span><br><span class="line">    head = head-&gt;next;</span><br><span class="line">    <span class="keyword">delete</span> temp;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="list"><code>list</code></h3><p><code>std::list</code> 是 C++ STL提供的一个双向链表容器。它支持在任意位置高效地插入和删除元素，时间复杂度为<spanclass="math inline">𝒪(1)</span>，但不支持随机访问，访问元素的时间复杂度为<spanclass="math inline">𝒪(<em>n</em>)</span>。其用法和<code>deque</code>类似，但更适合频繁插入和删除操作的场景。</p><h3 id="定义方式-6">定义方式</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;list&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line">list&lt;<span class="type">int</span>&gt; lst; <span class="comment">// 定义一个存储int类型的双向链表</span></span><br><span class="line">list&lt;string&gt; strList; <span class="comment">// 定义一个存储string类型的双向链表</span></span><br></pre></td></tr></table></figure><h3 id="常用操作-2">常用操作</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">lst.<span class="built_in">push_back</span>(<span class="number">10</span>); <span class="comment">// 在链表末尾插入元素10</span></span><br><span class="line">lst.<span class="built_in">push_front</span>(<span class="number">20</span>); <span class="comment">// 在链表前端插入元素20</span></span><br><span class="line">lst.<span class="built_in">pop_back</span>(); <span class="comment">// 删除链表末尾的元素</span></span><br><span class="line">lst.<span class="built_in">pop_front</span>(); <span class="comment">// 删除链表前端的元素</span></span><br><span class="line">cout &lt;&lt; lst.<span class="built_in">front</span>(); <span class="comment">// 返回链表前端的元素</span></span><br><span class="line">cout &lt;&lt; lst.<span class="built_in">back</span>(); <span class="comment">// 返回链表末尾的元素</span></span><br><span class="line">lst.<span class="built_in">remove</span>(<span class="number">10</span>); <span class="comment">// 删除值为10的所有元素</span></span><br><span class="line">cout &lt;&lt; lst.<span class="built_in">size</span>(); <span class="comment">// 返回链表中元素的个数</span></span><br></pre></td></tr></table></figure><h2 id="关于new和delete">8.关于<code>new</code>和<code>delete</code></h2><h3 id="c中的内存管理">C++中的内存管理</h3><p>C++中的内存管理主要分为两种类型：<strong>栈内存</strong>（stackmemory）和<strong>堆内存</strong>（heap memory）。</p><ul><li><strong>栈内存</strong>：由编译器自动管理，函数调用时分配，函数返回时释放。适用于局部变量和函数参数，分配和释放速度快，但空间有限。</li><li><strong>堆内存</strong>：由程序员手动管理，使用<code>new</code>关键字分配，使用<code>delete</code>关键字释放。适用于需要动态分配内存的场景，空间较大，但分配和释放速度较慢，容易出现内存泄漏和碎片化问题。</li></ul><p>比如下面这个程序，<code>createNode()</code>如果不使用<code>new</code>，而是直接定义一个<code>ListNode</code>对象，那么这个对象会被分配在栈上，函数返回后就会被销毁，导致无法在其他函数<code>useNode()</code>中使用：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">ListNode* <span class="title">createNode</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> <span class="built_in">ListNode</span>(<span class="number">10</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">useNode</span><span class="params">(ListNode* node)</span> </span>&#123;</span><br><span class="line">    std::cout &lt;&lt; node-&gt;val &lt;&lt; std::endl;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    ListNode* n = <span class="built_in">createNode</span>();</span><br><span class="line">    <span class="built_in">useNode</span>(n);</span><br><span class="line">    <span class="keyword">delete</span> n;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>new和delete的使用容易造成内存泄漏和悬空指针等问题，因此能不用就不用，其使用场景一般包括：</p><ol type="1"><li>动态数据结构（链表、树、图等），链表节点、树节点、图节点的数量通常运行时才能确定；</li><li>长期存在的对象，比如游戏中的一个玩家、后台服务器中的一个连接对象；</li><li>需要在函数之间共享数据，函数返回的数据需要在调用函数中使用。</li></ol><h3 id="new关键字"><code>new</code>关键字</h3><p><code>new</code>用于在堆上动态分配内存，并<strong>返回指向该内存的指针</strong>。语法如下：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span>* p = <span class="keyword">new</span> <span class="type">int</span>; <span class="comment">// 分配一个int类型的内存</span></span><br><span class="line">*p = <span class="number">10</span>; <span class="comment">// 给分配的内存赋值</span></span><br><span class="line"></span><br><span class="line">ListNode* node = <span class="keyword">new</span> <span class="built_in">ListNode</span>(<span class="number">5</span>); <span class="comment">// 分配一个ListNode类型的内存，并初始化val为5</span></span><br><span class="line">node = node-&gt;next; <span class="comment">// 访问链表节点</span></span><br></pre></td></tr></table></figure><h3 id="delete关键字"><code>delete</code>关键字</h3><p><code>delete</code>用于释放之前使用<code>new</code>分配的内存。语法如下：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">delete</span> p; <span class="comment">// 释放内存</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">delete</span> node; <span class="comment">// 释放链表节点内存</span></span><br></pre></td></tr></table></figure><h2 id="基本数字类型和浮点型数字类型">9.基本数字类型和浮点型数字类型</h2><p>C++中常见的基本数字类型包括整数类型和浮点类型。它们在内存中的大小和表示范围各不相同。</p><h3 id="基本数字类型">基本数字类型</h3><table><colgroup><col style="width: 15%" /><col style="width: 24%" /><col style="width: 41%" /><col style="width: 18%" /></colgroup><thead><tr><th>类型</th><th>字节数（典型）</th><th>范围</th><th>头文件常量</th></tr></thead><tbody><tr><td><strong><code>bool</code></strong></td><td>1 byte</td><td><code>true</code> 或 <code>false</code></td><td>—</td></tr><tr><td><strong><code>char</code></strong></td><td>1 byte</td><td>-128 ~ 127 或 0 ~ 255（取决于是否有符号）</td><td><code>CHAR_MIN</code>, <code>CHAR_MAX</code></td></tr><tr><td><code>signed char</code></td><td>1 byte</td><td>-128 ~ 127</td><td><code>SCHAR_MIN</code>, <code>SCHAR_MAX</code></td></tr><tr><td><code>unsigned char</code></td><td>1 byte</td><td>0 ~ 255</td><td><code>UCHAR_MAX</code></td></tr><tr><td><code>short</code></td><td>2 bytes</td><td>-32,768 ~ 32,767</td><td><code>SHRT_MIN</code>, <code>SHRT_MAX</code></td></tr><tr><td><code>unsigned short</code></td><td>2 bytes</td><td>0 ~ 65,535</td><td><code>USHRT_MAX</code></td></tr><tr><td><strong><code>int</code> </strong></td><td>4 bytes</td><td>-2,147,483,648 ~ 2,147,483,647</td><td><code>INT_MIN</code>, <code>INT_MAX</code></td></tr><tr><td><code>unsigned int</code></td><td>4 bytes</td><td>0 ~ 4,294,967,295</td><td><code>UINT_MAX</code></td></tr><tr><td><strong><code>long</code></strong></td><td>4 bytes（Windows）/ 8 bytes（Linux）</td><td>32位系统同 <code>int</code>，64位系统同 <code>long long</code></td><td><code>LONG_MIN</code>, <code>LONG_MAX</code></td></tr><tr><td><code>unsigned long</code></td><td>同上</td><td>同上</td><td><code>ULONG_MAX</code></td></tr><tr><td><code>long long</code></td><td>8 bytes</td><td>-9,223,372,036,854,775,808 ~ 9,223,372,036,854,775,807</td><td><code>LLONG_MIN</code>, <code>LLONG_MAX</code></td></tr><tr><td><code>unsigned long long</code></td><td>8 bytes</td><td>0 ~ 18,446,744,073,709,551,615</td><td><code>ULLONG_MAX</code></td></tr></tbody></table><p>在使用这些类型时，可以包含头文件 <code>&lt;climits&gt;</code>来获取对应的<code>INT_MIN</code>、<code>INT_MAX</code>等范围常量。</p><h3 id="浮点类型">浮点类型</h3><table><colgroup><col style="width: 17%" /><col style="width: 20%" /><col style="width: 14%" /><col style="width: 18%" /><col style="width: 29%" /></colgroup><thead><tr><th>类型</th><th>字节数</th><th>有效位数</th><th>范围</th><th>头文件常量</th></tr></thead><tbody><tr><td><code>float</code></td><td>4 bytes</td><td>~7 位十进制</td><td>±3.4 × 10^38</td><td><code>FLT_MIN</code>, <code>FLT_MAX</code></td></tr><tr><td><code>double</code></td><td>8 bytes</td><td>~15 位十进制</td><td>±1.7 × 10^308</td><td><code>DBL_MIN</code>, <code>DBL_MAX</code></td></tr><tr><td><code>long double</code></td><td>16 bytes（取决于平台）</td><td><sub>18</sub>19 位十进制</td><td>±1.1 × 10^4932</td><td><code>LDBL_MIN</code>, <code>LDBL_MAX</code></td></tr></tbody></table><p>在使用这些类型时，可以包含头文件 <code>&lt;cfloat&gt;</code>来获取对应的<code>FLT_MIN</code>、<code>FLT_MAX</code>等范围常量。</p><h2 id="访问控制">10. 访问控制</h2><p>C++中的访问控制用于限制类成员（属性和方法）的访问权限，主要有三种访问修饰符：<code>public</code>、<code>protected</code>和 <code>private</code>。</p><ul><li><code>public</code>：公共成员，可以被类的任何对象访问，包括类外部的代码。</li><li><code>protected</code>：受保护成员，可以被类的成员函数和派生类（子类）访问，但不能被类外部的代码访问。</li><li><code>private</code>：私有成员，只能被类的成员函数访问，不能被派生类和类外部的代码访问。</li></ul><h3 id="示例代码">示例代码</h3><ol type="1"><li>基本类成员访问规则（同类内部可访问所有私有成员）：</li></ol><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 示例1：同一个类的成员函数可以访问同类任意对象的 private/protected/public 成员</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">A</span> &#123;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="type">int</span> a = <span class="number">1</span>;</span><br><span class="line"><span class="keyword">protected</span>:</span><br><span class="line">    <span class="type">int</span> b = <span class="number">2</span>;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="type">int</span> c = <span class="number">3</span>;</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">copyFrom</span><span class="params">(<span class="type">const</span> A&amp; other)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 在类的成员函数内部，访问 other 的 private 成员是允许的</span></span><br><span class="line">        a = other.a; <span class="comment">// 合法</span></span><br><span class="line">        b = other.b; <span class="comment">// 合法</span></span><br><span class="line">        c = other.c; <span class="comment">// 合法</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    A x, y;</span><br><span class="line">    x.<span class="built_in">copyFrom</span>(y);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol start="2" type="1"><li>类外访问（谁能访问哪些成员）：</li></ol><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 示例2：类外只能访问 public，protected/private 不可访问</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">B</span> &#123;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="type">int</span> a = <span class="number">1</span>;</span><br><span class="line"><span class="keyword">protected</span>:</span><br><span class="line">    <span class="type">int</span> b = <span class="number">2</span>;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="type">int</span> c = <span class="number">3</span>;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    B obj;</span><br><span class="line">    <span class="comment">// obj.a; // 错误：private，不可访问</span></span><br><span class="line">    <span class="comment">// obj.b; // 错误：protected，不可访问</span></span><br><span class="line">    std::cout &lt;&lt; obj.c &lt;&lt; std::endl; <span class="comment">// 合法：public</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol start="3" type="1"><li>继承访问（派生类访问基类成员）：</li></ol><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 示例3：派生类能访问基类的 protected 和 public，但不能访问 private</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Base</span> &#123;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="type">int</span> priv = <span class="number">1</span>;</span><br><span class="line"><span class="keyword">protected</span>:</span><br><span class="line">    <span class="type">int</span> prot = <span class="number">2</span>;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="type">int</span> pub = <span class="number">3</span>;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Derived</span> : <span class="keyword">public</span> Base &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">f</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">// priv 不可访问：不是同类，也不是 friend</span></span><br><span class="line">        <span class="comment">// int x = priv; // 错误</span></span><br><span class="line">        <span class="type">int</span> y = prot; <span class="comment">// 合法（protected 可被派生类访问）</span></span><br><span class="line">        <span class="type">int</span> z = pub;  <span class="comment">// 合法</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><ol start="4" type="1"><li>友元访问（friend 可以访问 private/protected）：</li></ol><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 示例4：friend 函数 / friend 类</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">C</span> &#123;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="type">int</span> a = <span class="number">10</span>;</span><br><span class="line">    <span class="function"><span class="keyword">friend</span> <span class="type">void</span> <span class="title">accessC</span><span class="params">(<span class="type">const</span> C&amp;)</span></span>; <span class="comment">// 声明 friend 函数</span></span><br><span class="line">    <span class="keyword">friend</span> <span class="keyword">class</span> <span class="title class_">CFriend</span>;          <span class="comment">// 声明 friend 类</span></span><br><span class="line">&#125;;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">accessC</span><span class="params">(<span class="type">const</span> C&amp; obj)</span> </span>&#123;</span><br><span class="line">    std::cout &lt;&lt; obj.a &lt;&lt; std::endl; <span class="comment">// 合法，friend 函数可以访问 private</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CFriend</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">g</span><span class="params">(<span class="type">const</span> C&amp; obj)</span> </span>&#123;</span><br><span class="line">        std::cout &lt;&lt; obj.a &lt;&lt; std::endl; <span class="comment">// 合法，friend 类内也可访问</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h2 id="未定义oj模板">11. 未定义OJ模板</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">long</span> <span class="type">long</span> <span class="title">solve</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">        <span class="type">long</span> <span class="type">long</span> sum = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> x : nums) sum += x;</span><br><span class="line">        <span class="keyword">return</span> sum;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    ios::<span class="built_in">sync_with_stdio</span>(<span class="literal">false</span>);</span><br><span class="line">    cin.<span class="built_in">tie</span>(<span class="literal">nullptr</span>);</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> n;</span><br><span class="line">    cin &gt;&gt; n;</span><br><span class="line"></span><br><span class="line">    <span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">nums</span><span class="params">(n)</span></span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; ++i) &#123;</span><br><span class="line">        cin &gt;&gt; nums[i];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    Solution sol;</span><br><span class="line">    cout &lt;&lt; sol.<span class="built_in">solve</span>(nums) &lt;&lt; <span class="string">&#x27;\n&#x27;</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 刷题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Algorithm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>关于RLHF的一些学习记录</title>
      <link href="/2025/08/22/AI/RLHF/"/>
      <url>/2025/08/22/AI/RLHF/</url>
      
        <content type="html"><![CDATA[<p>参考博客：</p><p><a href="https://zhuanlan.zhihu.com/p/614115887">知乎PPO详解</a> <ahref="https://zhuanlan.zhihu.com/p/677607581">知乎RLHF介绍</a></p><h1 id="lora">1.LoRA</h1><p><strong>LORA</strong>是一种低资源微调大模型方法，出自论文<ahref="https://arxiv.org/abs/2106.09685">LoRA: Low-Rank Adaptation ofLarge LanguageModels</a>，其核心思想是通过在预训练模型的权重矩阵上添加低秩矩阵来实现微调，从而减少需要更新的参数数量。LORA的主要步骤如下：</p><ol type="1"><li>在预训练模型的权重矩阵上添加一个低秩矩阵，该矩阵的秩远低于原始权重矩阵的秩。</li><li>在微调过程中，仅更新低秩矩阵的参数，而保持原始权重矩阵的参数不变。</li><li>通过这种方式，LORA能够在保持模型性能的同时，显著减少需要更新的参数数量，从而降低微调的计算成本。</li></ol><p>具体来说，对于预训练权重矩阵 <spanclass="math inline"><em>W</em><sub>0</sub> ∈ ℝ<sup><em>d</em> × <em>k</em></sup></span>，我们可以用一个低秩分解来表示参数更新<spanclass="math inline">∇<em>W</em></span>：</p><p><span class="math display">$$\begin{split}    \nabla W = \Delta W = A B^T\end{split}$$</span></p><p>其中，<spanclass="math inline"><em>A</em> ∈ ℝ<sup><em>d</em> × <em>r</em></sup></span>，<spanclass="math inline"><em>B</em> ∈ ℝ<sup><em>k</em> × <em>r</em></sup></span>，<spanclass="math inline"><em>r</em></span>是一个远小于<spanclass="math inline"><em>d</em></span>和<spanclass="math inline"><em>k</em></span>的超参数。通过这种方式，LORA能够在保持模型性能的同时，<strong>显著减少需要更新的参数数量（<spanclass="math inline"><em>r</em>(<em>d</em> + <em>k</em>) ≪ <em>d</em><em>k</em></span>），从而降低微调的计算成本</strong>，即：</p><p><span class="math display">$$\begin{split}    W_0+\Delta W=W_0+BA\quad B\in\mathbb{R}^{d\timesr},A\in\mathbb{R}^{r\times k}\quad and\quad r\ll min(d,k)\end{split}$$</span></p><p>一般在Transformer中LoRA最常放在<strong>注意力投影层</strong>（尤其是QV 投影层，很多实现也会对K投影层以及<spanclass="math inline"><em>W</em><sub><em>O</em></sub></span>开启），以及<strong>FFN 的线性层</strong>。</p><h1 id="ppo">2.PPO</h1><h2 id="强化学习">2.1.强化学习</h2><h3 id="基础概念">2.1.1.基础概念</h3><p><strong>目标：</strong>智能体（agent）在环境（env）中，根据状态（s）作决策完成动作（a），得到奖励（r）进入下一个状态，让长期累计奖励最大：</p><p><span class="math display">$$\begin{split}    \max \mathbb{E} \left[ \sum_{t=0}^{T} \gamma^t r_t \right], \quad\text{s.t. } a_t = \pi_\theta(s_t)\end{split}$$</span></p><p>价值函数分为状态价值<spanclass="math inline"><em>V</em><sup><em>π</em></sup>(<em>s</em>)</span>和动作价值<spanclass="math inline"><em>Q</em><sup><em>π</em></sup>(<em>s</em>, <em>a</em>)</span>，分别表示在策略<spanclass="math inline"><em>π</em></span>下，从状态<spanclass="math inline"><em>s</em></span>开始的期望回报和在状态<spanclass="math inline"><em>s</em></span>下采取动作<spanclass="math inline"><em>a</em></span>的期望回报，分别表示为：</p><p><span class="math display">$$\begin{split}    V^\pi(s) &amp;= \mathbb{E} \left[ \sum_{t=0}^{T} \gamma^t r_t \mids_0 = s \right] \\    Q^\pi(s,a) &amp;= \mathbb{E} \left[ \sum_{t=0}^{T} \gamma^t r_t \mids_0 = s, a_0 = a \right]\end{split}$$</span></p><p>此时的强化学习算法可以分为两大类：<strong>基于值函数（Value-Based）的强化学习</strong>和<strong>基于策略（Policy-Based）的强化学习</strong>。</p><p>基于值函数的强化学习算法通过递归求解贝尔曼方程（BellmanEquation）来维护Q值函数，每次选择动作时会贪心地选择当前Q值最大的动作，使得未来累计的奖励最大化，常见的基于值函数的算法有Q-learning和DeepQ-Network（DQN）。由于Q值函数在学习后不会再发生变化，因此每次做出的策略也是确定的，可以理解为确定性策略。</p><p>基于策略的强化学习则直接学习策略本身，通过一组参数<spanclass="math inline"><em>θ</em></span>来表示策略，基于策略的强化学习用参数化概率分布<spanclass="math inline"><em>π</em><sub><em>θ</em></sub>(<em>a</em>|<em>s</em>) = <em>P</em>(<em>a</em>|<em>s</em>; <em>θ</em>)</span>来表示在状态<spanclass="math inline"><em>s</em></span>下采取动作<spanclass="math inline"><em>a</em></span>的概率。常见的基于策略的算法有策略梯度（PolicyGradient）方法和近端策略优化（Proximal PolicyOptimization，PPO）算法。由于采用参数化概率分布来选择策略，基于策略的强化学习会在返回的动作概率列表中对不同的动作进行抽样选择。</p><h3 id="传统策略梯度算法">2.1.2.传统策略梯度算法</h3><p>基于参数化策略的思想，我们的目标就是找到那些可能获得更多奖励的动作，使其对应的概率更大，因此可以定义最大化目标函数<spanclass="math inline"><em>J</em>(<em>θ</em>)</span>如下：</p><p><span class="math display">$$\begin{split}    \max_\theta J(\theta)=\max_\thetaE_{\tau\sim\pi_\theta}R(\tau)=\max_\theta\sum_\tau P(\tau;\theta)R(\tau)\end{split}$$</span></p><p>其中<spanclass="math inline"><em>τ</em></span>是Agent与环境交互产生的状态-动作轨迹<spanclass="math inline"><em>τ</em> = (<em>s</em><sub>0</sub>, <em>a</em><sub>0</sub>, <em>s</em><sub>1</sub>, <em>a</em><sub>1</sub>, …, <em>s</em><sub><em>T</em></sub>, <em>a</em><sub><em>T</em></sub>)</span>，我们的目标则是通过调整<spanclass="math inline"><em>θ</em></span>使得获得更大奖励的轨迹出现的概率更高。其中轨迹<spanclass="math inline"><em>τ</em></span>在策略<spanclass="math inline"><em>π</em><sub><em>θ</em></sub>(<em>a</em> ∣ <em>s</em>)</span>下的概率为<spanclass="math inline">$P(\tau;\theta)=\prod_{t=0}^{T}P(s_{t+1}\mids_t,a_t)\cdot\pi_\theta(a_t\mid s_t)$</span>。</p><p>进一步通过一系列推导，我们可以得到目标函数 <spanclass="math inline"><em>J</em>(<em>θ</em>)</span> 的梯度为：</p><p><span class="math display">$$\begin{split}    \nabla J(\theta) &amp;= \nabla \sum_\tau P(\tau;\theta)R(\tau) \\    &amp;= \sum_\tau \nabla P(\tau;\theta)R(\tau) \\    &amp;= \sum_\tau P(\tau;\theta)\nabla \log P(\tau;\theta)R(\tau) \\    &amp;= \mathbb{E}_{\tau\sim\pi_\theta} \left[ R(\tau) \nabla \logP(\tau;\theta) \right]\end{split}$$</span></p><p>通过轨迹样本进行梯度近似继续推导可以给出</p><p><span class="math display">$$\begin{split}    \nabla_{\theta}J(\theta) &amp;\approx\frac{1}{m}\sum_{i=1}^m\nabla_\theta\logP(\tau^{(i)};\theta)R(\tau^{(i)}) \\    &amp;=\frac{1}{m}\sum_{i=1}^m(\sum_{t^{(i)}=0}^{T^{(i)}}\nabla_\theta\log\pi_\theta(a_{t^{(i)}}\mids_{t^{(i)}}))R(\tau^{(i)}) \\    &amp;\approx\frac{1}{n}\sum_{i=1}^n(\nabla_\theta\log\pi_\theta(a_{t^{(i)}}\mids_{t^{(i)}}))R(t^{(i)})\end{split}$$</span></p><p>由此可以计算出目标函数的梯度，通过策略梯度更新对参数 <spanclass="math inline"><em>θ</em></span> 进行更新：</p><p><span class="math display">$$\begin{split}    \theta\leftarrow\theta+\alpha\nabla_\theta J(\theta)\end{split}$$</span></p><p>但是此时还没有定义策略函数 <spanclass="math inline"><em>π</em><sub><em>θ</em></sub>(<em>a</em>|<em>s</em>)</span>，我们需要对其进行定义。一般有两种常见的策略函数（概率分布）：</p><ol type="1"><li><p>Softmax策略：一般用于离散动作空间，通过对每个动作的Q值进行Softmax变换来得到动作的概率分布：<span class="math display">$$\begin{split}\pi_\theta(a|s) = \frac{e^{Q^\pi(s,a)}}{\sum_{a'} e^{Q^\pi(s,a')}}\end{split}$$</span></p></li><li><p>高斯策略：一般用于连续动作空间，假设动作服从高斯分布： <spanclass="math display">$$\begin{split}\pi_\theta(a|s) = \mathcal{N}(a; \mu_\theta(s), \sigma_\theta(s))\end{split}$$</span></p></li></ol><h2 id="自然策略梯度算法">2.2.自然策略梯度算法</h2><h3 id="传统策略梯度算法的缺陷">2.2.1.传统策略梯度算法的缺陷</h3><p>传统的策略梯度算法中，我们根据目标函数梯度<spanclass="math inline">∇<em>J</em>(<em>θ</em>)</span>和步长<spanclass="math inline"><em>α</em></span> 来更新策略参数<spanclass="math inline"><em>θ</em></span>，但这样的更新过程可能会导致两个常见的问题：</p><ol type="1"><li>过冲（Overshooting）：如果步长<spanclass="math inline"><em>α</em></span>过大，可能会导致策略参数<spanclass="math inline"><em>θ</em></span>更新过头，从而使得策略变得更差。</li><li>下冲（Undershooting）：如果步长<spanclass="math inline"><em>α</em></span>过小，可能会导致策略参数<spanclass="math inline"><em>θ</em></span>更新不足，从而使得策略收敛速度变慢。</li></ol><p>在监督学习中，overshooting问题不会太大，但是在强化学习中，由于环境的复杂性和不确定性，overshooting可能会导致策略陷入到糟糕的正反馈中无法恢复。比如下面这种情况，如果在梯度方向采取的步骤太大，可能会错过最优解并落在低梯度的区域（右图），需要多次迭代才能逃逸或者无法逃逸。</p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/overshooting.png" width="80%" /></p><p>这个问题可以通过限制每次更新步长上限的方式进行缓解，但是这里直接通过限制绝对上限的方式并不合理，更应该考虑<strong>分布对参数变化的敏感度</strong>，而这需要通过引入二阶导数来实现。</p><h3 id="限制策略更新的差异">2.2.2.限制策略更新的差异</h3><p>我们需要表示策略分布之间的差异，而不是参数本身的绝对差异，这可以通过引入<strong>Kullback-Leibler散度（KL散度）</strong>来实现。KL散度可以衡量两个概率分布之间的差异，具体定义为：</p><p><span class="math display">$$\begin{split}    D_{KL}(\pi_\theta || \pi_{\theta'}) = \mathbb{E}_{s \sim d^\pi}\left[ \log \frac{\pi_\theta(a|s)}{\pi_{\theta'}(a|s)} \right]\end{split}$$</span></p><p>在自然策略梯度算法中，我们希望在更新策略时限制KL散度的变化，从而避免过大的更新导致策略性能下降。具体来说，我们可以引入一个约束条件：</p><p><span class="math display">$$\begin{split}    D_{KL}(\pi_\theta || \pi_{\theta'}) \leq \epsilon\end{split}$$</span></p><p>其中<spanclass="math inline"><em>ϵ</em></span>是一个小的正数，表示允许的KL散度上限。通过这种方式，我们可以在更新策略时保持策略分布的相对稳定性，从而提高算法的鲁棒性和收敛速度。使用拉格朗日松弛将原约束转换为惩罚项，可以得到一个更容易求解的表达式：</p><p><span class="math display">$$\begin{split}    \Delta \theta^*=\arg \max _{\Delta \theta} J(\theta+\Delta\theta)-\lambda\left(\mathcal{D}_{\mathrm{KL}}\left(\pi_\theta \|\pi_{\theta+\Delta \theta}\right)-\epsilon\right)\end{split}$$</span></p><p>引入费舍尔信息矩阵（Fisher informationmatrix）后，可以进一步将优化目标化简为：</p><p><span class="math display">$$\begin{split}    \Delta \theta^* \approx \arg \max _{\Delta \theta} \nabla_\thetaJ(\theta)|_{\theta=\theta_{\text {old }}} \cdot \Delta\theta-\frac{1}{2} \lambda\left(\Delta \theta^{\top}F\left(\theta_{\text {old }}\right) \Delta \theta\right)\end{split}$$</span></p><p>通过求解上述优化问题，可以得到自然策略梯度的更新方向：</p><p><span class="math display">$$\begin{split}    \Delta \theta=\sqrt{\frac{2 \epsilon}{\nabla J(\theta)^{\top}F(\theta)^{-1} \nabla J(\theta)}} \tilde{\nabla} J(\theta)\end{split}$$</span></p><p>最终结果在两个方面不同于传统的策略梯度算法：</p><ul><li>考虑到策略对局部变化的敏感性，策略梯度由逆Fisher矩阵校正，而传统的梯度方法假定更新为欧几里得距离。</li><li>更新步长<spanclass="math inline"><em>α</em></span>具有适应梯度和局部敏感性的动态表达式，确保无论参数化如何，策略变化幅度为<spanclass="math inline"><em>ϵ</em></span>。在传统方法中，<spanclass="math inline"><em>α</em></span>通常设置为一些标准值，如0.1或0.01。</li></ul><h2 id="信赖域策略优化算法trpo">2.3.信赖域策略优化算法（TRPO）</h2><p>Trust region policyoptimization（TRPO）算法是现代强化学习的基础，它以自然策略梯度优化为基础，迅速获得普及，成为主流强化学习算法，因为它在经验上比自然策略梯度算法表现得更好、更稳定。尽管此后它已被近端策略优化(PPO) 超越，但它的仍然具有重要的意义。</p><h3 id="自然策略梯度算法的缺陷">2.3.1.自然策略梯度算法的缺陷</h3><ul><li>近似值可能会违反KL约束，从而导致分析得出的步长过大，超出限制要求；</li><li>矩阵的计算时间太长，是<spanclass="math inline">𝒪(<em>n</em><sup>3</sup>)</span>复杂度的运算；</li><li>我们没有检查更新是否真的改进了策略。由于存在大量的近似过程，策略可能并没有优化。</li></ul><h3 id="trpo算法的基本思想">2.3.2.TRPO算法的基本思想</h3><p>TRPO算法的基本思想是通过引入信赖域的概念来限制策略更新的幅度，从而避免过大的更新导致策略性能下降。具体来说，TRPO算法在每次更新时都会求解一个优化问题，目标是最大化策略的期望回报，同时限制KL散度的变化：</p><p><span class="math display">$$\begin{split}    J(\pi_{\theta+\Delta\theta})-J(\pi_\theta)\geq\mathcal{L}_{\pi\theta}(\pi_{\theta+\Delta\theta})-C\mathcal{D}_{KL}^{\max}(\pi_\theta||\pi_{\theta+\Delta\theta})\end{split}$$</span></p><h3 id="具体实现">2.3.3.具体实现</h3><p>TRPO主要有三个改进：</p><ol type="1"><li><p><strong>共轭梯度法</strong>：TRPO使用共轭梯度法来求解优化问题，从而避免了直接计算Hessian矩阵的高昂代价。</p></li><li><p><strong>线性搜索</strong>：TRPO在每次更新时都会进行线性搜索，以找到满足KL散度约束的最优步长。</p></li><li><p><strong>经验重放</strong>：TRPO引入了经验重放机制，通过重用过去的经验来提高样本效率。</p></li></ol><h2 id="ppo-1">2.4.PPO</h2><p>在TRPO的基础上，Schulman等人提出了近端策略优化（Proximal PolicyOptimization，PPO）算法。PPO通过引入一个新的目标函数，进一步简化了TRPO的实现，同时保持了其良好的性能。</p><h3 id="trpo算法的缺陷">2.4.1.TRPO算法的缺陷</h3><ul><li><strong>无法处理大参数矩阵</strong>：尽管使用了共轭梯度法，TRPO仍然难以处理大的Fisher矩阵，即使它们不需要求逆。</li><li><strong>二阶优化很慢</strong>：TRPO的实际实现是基于约束的，需要计算上述Fisher矩阵，这大大减慢了更新过程。此外，我们不能利用一阶随机梯度优化器，例如ADAM。</li><li><strong>TRPO很复杂</strong>：TRPO很难解释、实现和调试。当训练没有产生预期的结果时，确定如何提高性能可能会很麻烦。</li></ul><h3 id="ppo-penalty">2.4.2.PPO Penalty</h3><p>TRPO在理论分析上推导出与KL散度相乘的惩罚项，但在实践中，这种惩罚往往过于严格，只产生非常小的更新。因此，问题是如何可靠地确定缩放参数<spanclass="math inline"><em>β</em></span>，同时避免overshooting：</p><p><span class="math display">$$\begin{split}    \Delta\theta^*=\underset{\Delta\theta}{\operatorname*{\operatorname*{argmax}}}\mathcal{L}_{\theta+\Delta\theta}(\theta+\Delta\theta)-\beta(\mathcal{D}_{\mathrm{KL}}(\pi_\theta\parallel\pi_{\theta+\Delta\theta}))\end{split}$$</span></p><p>难点是很难确定适用于多个问题的某个<spanclass="math inline"><em>β</em></span>值。事实上，即使是同一个问题，随着时间的推移，特征也可能发生变化。我们既不希望<spanclass="math inline"><em>β</em></span>过小，与TRPO一样只产生较小的更新，也不希望<spanclass="math inline"><em>β</em></span>过大，容易出现overshooting问题。</p><p>PPO通过设置目标散度<spanclass="math inline"><em>δ</em></span>的方式解决了这个问题，希望我们的每次更新都位于目标散度附近的某个地方。目标散度应该大到足以显著改变策略，但又应该小到足以使更新稳定。</p><p>具体来说每次更新后，PPO都会检查更新的大小。如果最终更新的散度超过目标散度的1.5倍，则下一次迭代我们将加倍<spanclass="math inline"><em>β</em></span>来加重惩罚。相反，如果更新太小，我们将减半<spanclass="math inline"><em>β</em></span>，从而有效地扩大信任区域。迭代更新的思路与TRPO线搜索有一些相似之处，但PPO搜索是在两个方向上都有效的，而TRPO是单向减小的。</p><h3 id="ppo-clip">2.4.3.PPO Clip</h3><p>与PPO Penalty不同，PPOClip并没有直接使用KL散度作为惩罚项，而是通过限制策略更新的幅度来间接控制KL散度的变化。具体来说，PPOClip引入了一个裁剪函数，将策略更新限制在一个预定义的范围内，从而避免过大的更新导致策略性能下降。</p><p>具体来说，PPO Clip通过以下方式实现裁剪：</p><p><span class="math display">$$\begin{split}    \mathcal{L}_{\pi_\theta}^{CLIP}(\pi_{\theta_k})=\mathbb{E}_{\tau\sim\pi_\theta}\left[\sum_{t=0}^T\left[\min\left(\rho_t(\pi_\theta,\pi_{\theta_k})A_t^{\pi_{\theta_k}},\operatorname{clip}(\rho_t(\pi_\theta,\pi_{\theta_k}),1-\epsilon,1+\epsilon)A_t^{\pi_{\theta_k}}\right)\right]\right]\end{split}$$</span></p><p>其中，<spanclass="math inline"><em>ϵ</em></span>是一个小的正数，表示裁剪的范围。通过这种方式，PPOClip可以有效地限制策略更新的幅度，从而避免过大的更新导致策略性能下降。如下图所示，当<spanclass="math inline"><em>A</em> &gt; 0</span>时，其上界为<spanclass="math inline">1 + <em>ϵ</em></span>，当<spanclass="math inline"><em>A</em> &lt; 0</span>时，其下界为<spanclass="math inline">1 − <em>ϵ</em></span>，从而限制了策略更新的幅度。</p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/PPO_Clip.png" width="80%"/></p><h1 id="rlhf">3.RLHF</h1><h2 id="nlp中的强化学习">3.1.NLP中的强化学习</h2><p>前面介绍了通用强化学习的流程，但是该如何将其对应到NLP任务中呢？换句话说，NLP任务中的状态、动作和奖励分别是什么呢？</p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/RL_NLP.png" width="80%"/></p><p>由上图可以得知这里的状态、动作和奖励分别是：</p><ul><li>状态：当前的文本输入和上下文信息，记为<spanclass="math inline"><em>S</em><sub><em>t</em></sub></span>；</li><li>动作：根据上下文产出的token，记为<spanclass="math inline"><em>A</em><sub><em>t</em></sub></span>，由语言模型生成；</li><li>奖励：根据生成文本的质量和相关性给予的评分，分为即时收益<spanclass="math inline"><em>R</em><sub><em>t</em></sub></span>和总收益<spanclass="math inline"><em>V</em><sub><em>t</em></sub></span>，这两部分分别由另外两个模型生成。</li></ul><p>这里设计的价值函数的表达式为：</p><p><span class="math display">$$\begin{split}    V_t = R_t + \gamma V_{t+1}\end{split}$$</span></p><p>直观理解为：<strong><spanclass="math inline"><em>t</em></span>时刻状态<spanclass="math inline"><em>s</em></span>的总收益 = 身处状态<spanclass="math inline"><em>s</em></span>能带来的即时收益 + 从状态<spanclass="math inline"><em>s</em></span>出发后能带来的未来收益</strong></p><h2 id="rlhf中的四个重要部分">3.2.RLHF中的四个重要部分</h2><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/RLHF_parts.png" width="80%"/></p><p>在RLHF-PPO中，一共有四个主要模型，分别是：</p><ul><li><strong>Actor Model</strong>：演员模型，用于生成动作<spanclass="math inline"><em>A</em><sub><em>t</em></sub></span>；</li><li><strong>Critic Model</strong>：评论家模型，用于评估状态<spanclass="math inline"><em>S</em><sub><em>t</em></sub></span>和动作<spanclass="math inline"><em>A</em><sub><em>t</em></sub></span>的价值，生成总收益<spanclass="math inline"><em>V</em><sub><em>t</em></sub></span>；</li><li><strong>RewardModel</strong>：奖励模型，用于根据生成文本的质量和相关性给予评分，指导ActorModel的学习，生成即时收益<spanclass="math inline"><em>R</em><sub><em>t</em></sub></span>；</li><li><strong>ReferenceModel</strong>：参考模型，用于在RLHF阶段给语言模型增加约束，确保生成文本的质量和一致性。</li></ul><p>其中Actor Model和Critic Model在RLHF阶段是需要训练的，而RewardModel和Reference Model是参数冻结的，RewardModel一般是提前通过人类反馈数据进行微调得到的。</p><h3 id="actor-model">3.2.1.Actor Model</h3><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/Actor_Model.png" width="60%"/></p><p>ActorModel就是我们之前训练的目标语言模型，我们一般用SFT阶段产出的SFT模型对其做初始化。在RLHF训练阶段，我们喂给Actor一条prompt，其生成对应的response，然后我们再将(prompt,response)对作为训练数据，送入后续三个部件组成的“评分系统”得到最终的loss，更新ActorModel的参数。</p><h3 id="reference-model">3.2.2.Reference Model</h3><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/Reference_Model.png" width="80%"/></p><p>ReferenceModel一般也直接用SFT阶段得到的SFT模型作为初始化，在训练阶段其参数是冻结的。我们在RLHF训练过程中希望训练出来的Actor模型<strong>既能达到符合人类喜好的目的，但是又不至于偏离原有的知识和能力</strong>，即希望两个模型的输出分布尽量相似，这里可以用KL散度来衡量这个相似度。</p><p>对Actor来说，我们喂给它一个prompt，它会正常输出对应的response，同时记录下response中每个token的<strong>log_prob</strong>；对Ref来说，我们同样将Prompt送入Ref模型，得到对应的response，并记录下每个token的<strong>ref_log_prob</strong>，那么这两个模型分布的相似度就可以用下面的公式来表示：</p><p><span class="math display">$$\begin{split}    D_{KL}[\operatorname{Actor}(X) \| \operatorname{Ref}(X)]=E_{x \sim\operatorname{Actor}(x)}\left[\log\frac{\operatorname{Actor}(x)}{\operatorname{Ref}(x)}\right]=E_{x \sim\operatorname{Actor}(x)}\left[\text { logProbs } - \text { refLogProbs}\right]\end{split}$$</span></p><h3 id="reward-model">3.2.3.Reward Model</h3><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/Reward_Model.jpeg" width="60%"/></p><p>Reward Model用于计算生成token <spanclass="math inline"><em>A</em><sub><em>t</em></sub></span>的即时收益<spanclass="math inline"><em>R</em><sub><em>t</em></sub></span>，它在RW阶段提前训练，在RLHF过程中，它的参数是冻结的。</p><h3 id="critic-model">3.2.4.Critic Model</h3><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/Critic_Model.jpeg" width="60%"/></p><p>Critic Model用于估计状态-动作对的价值，生成总收益<spanclass="math inline"><em>V</em><sub><em>t</em></sub></span>，在RLHF训练过程中使用rewardmodel作为初始化。</p><h2 id="rlhf的loss计算">3.3.RLHF的loss计算</h2><p>前面已经提到在RLHF训练过程中，Reference和RewardModel的参数是冻结的，只有Actor和CriticModel的参数是需要更新的。下面介绍一下这两个模型的loss计算。</p><h3 id="actor-loss">3.3.1.Actor Loss</h3><p>ActorLoss用于评估Actor是否生成了符合人类喜好的token，这里我们可以给出初始的ActorLoss的表达式：</p><p><span class="math display">$$\begin{split}    \mathcal L_{actor}=-\sum_{t}V_tlogP(A_t|S_t)\end{split}$$</span></p><p>其中<spanclass="math inline"><em>V</em><sub><em>t</em></sub></span>是CriticModel给出的总收益，<spanclass="math inline"><em>l</em><em>o</em><em>g</em><em>P</em>(<em>A</em><sub><em>t</em></sub>|<em>S</em><sub><em>t</em></sub>)</span>是ActorModel生成token <spanclass="math inline"><em>A</em><sub><em>t</em></sub></span>的log概率。这里对这个公式的直观理解为：当<spanclass="math inline"><em>V</em><sub><em>t</em></sub></span>较大时，说明当前生成的token是符合人类喜好的，因此我们希望ActorModel能够以更高概率地生成这样的token，loss应当尽可能要小；反之，当<spanclass="math inline"><em>V</em><sub><em>t</em></sub></span>较小时，我们希望ActorModel能够降低对该token的生成概率，loss应该尽可能大。</p><p>进一步我们引入优势的概念，如果Critic对 <spanclass="math inline"><em>A</em><sub><em>t</em></sub></span>的总收益预测为 <spanclass="math inline"><em>V̂</em><sub><em>t</em></sub> = <em>R</em><sub><em>t</em></sub> + <em>γ</em> * <em>V</em><sub><em>t</em> + 1</sub></span>，但实际执行<span class="math inline"><em>A</em><sub><em>t</em></sub></span>后的总收益是 <spanclass="math inline"><em>V</em><sub><em>t</em></sub></span>，我们就定义<strong>优势</strong>为：</p><p><span class="math display">$$\begin{split}    Adv_t=R_t+\gamma*V_{t+1}-V_t\end{split}$$</span></p><p>将这里的优势代替原<spanclass="math inline"><em>V</em><sub><em>t</em></sub></span>，我们可以得到ActorLoss的改进版本：</p><p><span class="math display">$$\begin{split}    \mathcal L_{actor}=-\sum_{t}Adv_tlogP(A_t|S_t)\end{split}$$</span></p><p>对于收益来说，分为即时收益和未来收益，同样对于优势而言，也可以引入对未来优势的考量，这样我们就可以把<spanclass="math inline"><em>A</em><em>d</em><em>v</em><sub><em>t</em></sub></span>改写为如下形式：</p><p><span class="math display">$$\begin{split}    Adv_t=(R_t+\gamma*V_{t+1}-V_t)+\gamma*\lambda*Adv_{t+1}\end{split}$$</span></p><p>这里的<spanclass="math inline"><em>λ</em></span>是一个超参数，取值范围为<spanclass="math inline">[0, 1]</span>，它控制了未来优势对当前优势的影响程度。当<spanclass="math inline"><em>λ</em> = 0</span>时，表示只考虑当前的即时收益和价值估计；当<spanclass="math inline"><em>λ</em> = 1</span>时，表示完全考虑未来的优势。</p><p>到现在为止，我们这部分只考虑了Reward和Critic两个模型的输出，还没有考虑ReferenceModel的影响。前面提到我们希望Actor Model的输出分布与ReferenceModel尽量相似，因此我们可以在ActorLoss中加入一个KL散度惩罚项，这里我们对<spanclass="math inline"><em>R</em><sub><em>t</em></sub></span>进行改造以引入ReferenceModel：</p><p><span class="math display">$$\begin{split}\left\{\begin{array}{l}R_t=-k l \_c t l *\left(\log \frac{P\left(A_t \mid S_t\right)}{P_{\text{ref }}\left(A_t \mid S_t\right)}\right), t \neq T \\R_t=-k l \_c t l *\left(\log \frac{P\left(A_t \mid S_t\right)}{P_{\text{ref }}\left(A_t \mid S_t\right)}\right)+R_t, t=T\end{array}\right.\end{split}$$</span></p><p>这里为什么只有最后时刻的<spanclass="math inline"><em>R</em><sub><em>t</em></sub></span>被列入考虑了呢，因为在RW训练阶段，我们就是对最后这个位置的token进行打分，即对完整的(prompt,response)进行评估。</p><p>除此之外，我们还引入了PPO-epoch的约束。之所以要引入PPO-epoch，是因为在训练过程中一个batch的经验值将被用于n次模型更新。在强化学习中，收集一个batch的经验是非常耗时的。对应到我们RLHF的例子中，收集一次经验，它要等四个模型做完推理才可以，正是因此，一个batch的经验，只用于计算1次loss，更新1次Actor和Critic模型有些太耗时了。由此我们额外引入了一个KL散度来进行进一步约束，并通过Clip约束和进一步推导可以得到ActorLoss的最终形式：</p><p><span class="math display">$$\begin{split}    \mathcalL_{actor}=-min(Adv_{t}*\frac{P(A_{t}|S_{t})}{P_{old}(A_{t}|S_{t})},Adv_{t}*clip(\frac{P(A_{t}|S_{t})}{P_{old}(A_{t}|S_{t})},0.8,1.2)]\end{split}$$</span></p><p>其中<strong>我们对<spanclass="math inline"><em>A</em><em>d</em><em>v</em><sub><em>t</em></sub></span>进行改造，使其不仅考虑当前时刻的优势，还考虑未来的优势；对<spanclass="math inline"><em>R</em><sub><em>t</em></sub></span>进行改造，使其能够衡量Actor模型是否遵从了Ref模型的约束；重复利用单个batch的经验，同时使用真正产出经验值的Actor分布来约束ppo_epoch中的模型更新，为了控制更新幅度，进一步使用Clip来进行限制</strong>。</p><h3 id="critic-loss">3.3.2.Critic Loss</h3><p>Critic Loss用于评估CriticModel对状态-动作对的价值估计是否准确，这里我们可以给出CriticLoss的初始表达式：</p><p><span class="math display">$$\begin{split}    \mathcal L_{critic}=\sum_{t}\left(V_t - \hat{V}_t\right)^2\end{split}$$</span></p><p>其中<spanclass="math inline"><em>V</em><sub><em>t</em></sub></span>是预估收益，<spanclass="math inline"><em>V̂</em><sub><em>t</em></sub> = <em>R</em><sub><em>t</em></sub> + <em>γ</em> * <em>V</em><sub><em>t</em> + 1</sub></span>是实际收益。同样这里引入优势，可以对实际收益进行改造，得到：</p><p><span class="math display">$$\begin{split}    \hat{V}_t=R_t+\gamma*V_{t+1}=Adv_t+V_t\end{split}$$</span></p><p>类比于Actor Loss，我们也引入PPO-epoch，那么<spanclass="math inline"><em>V</em><sub><em>t</em></sub></span>在PPO-epochs的过程中是不断变化的，所以同样需要对其采取约束，这里不同于<spanclass="math inline"><em>A</em><sub><em>t</em></sub></span>，我们直接在<spanclass="math inline"><em>V</em><sub><em>t</em></sub></span>的基础上设计一个clip范围，然后用这个范围取约束新的<spanclass="math inline"><em>V</em><sub><em>t</em></sub></span>。</p>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PerX2CT记录</title>
      <link href="/2025/06/23/Reconstruction/PerX2CT/"/>
      <url>/2025/06/23/Reconstruction/PerX2CT/</url>
      
        <content type="html"><![CDATA[<p>原论文：<a href="https://arxiv.org/abs/2303.05297">PerspectiveProjection-Based 3D CT Reconstruction from Biplanar X-rays</a></p><h2 id="源码分析">源码分析</h2><h3 id="总模型">1. 总模型</h3><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/PerX2CT.png" width="80%" /></p><p>首先是 <code>INRAEZoomModel</code>类继承自自编码器，看<code>forward</code>逻辑其实很简单，没有用到codebook，就是一个输入input编码成中间feature然后解码为output的过程，最关键的就是其中的编码器部分是如何做到的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 主模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">INRAEZoomModel</span>(<span class="title class_ inherited__">AEModel</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    without codebook, Use CT, Xray</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                 ddconfig,</span></span><br><span class="line"><span class="params">                 lossconfig,</span></span><br><span class="line"><span class="params">                 n_embed,</span></span><br><span class="line"><span class="params">                 embed_dim,</span></span><br><span class="line"><span class="params">                 ckpt_path=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 ignore_keys=[],</span></span><br><span class="line"><span class="params">                 image_key=<span class="string">&quot;image&quot;</span>,</span></span><br><span class="line"><span class="params">                 colorize_nlabels=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 monitor=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 kl_weight=<span class="number">1e-8</span>,</span></span><br><span class="line"><span class="params">                 remap=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 metadata=&#123;&#125;,</span></span><br><span class="line"><span class="params">                 </span>):</span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        <span class="comment"># input (20, 3, 320, 320)</span></span><br><span class="line">        feature = <span class="variable language_">self</span>.encode(<span class="built_in">input</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.gt_key == <span class="variable language_">self</span>.image_key:</span><br><span class="line">            x = <span class="built_in">input</span>[<span class="variable language_">self</span>.image_key]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            x = feature[<span class="variable language_">self</span>.gt_key]</span><br><span class="line"></span><br><span class="line">        feature = feature[<span class="string">&#x27;outputs&#x27;</span>]</span><br><span class="line">        <span class="comment"># (20, 1087, 32, 32)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.use_quant_conv:</span><br><span class="line">            feature = <span class="variable language_">self</span>.quant_conv(feature)</span><br><span class="line">        dec = <span class="variable language_">self</span>.decode(feature)</span><br><span class="line">        <span class="keyword">if</span> dec.shape[<span class="number">1</span>] == <span class="number">1</span>:</span><br><span class="line">            dec = torch.cat([dec] * <span class="number">3</span>, dim=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># output (20, 3, 128, 128)</span></span><br><span class="line">        output_dict = &#123;</span><br><span class="line">            <span class="string">&#x27;outputs&#x27;</span>: dec,</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> output_dict, x</span><br><span class="line"></span><br><span class="line">    ...</span><br></pre></td></tr></table></figure><h3 id="编码器">2.编码器</h3><p>因为在原论文中global的方案（就是上图）效果反而没有local的效果好，因此这里看的是local的实现，也要更简单，相当于没有最上面的<spanclass="math inline"><em>F</em><sub><em>g</em></sub><sup><em>P</em><em>A</em></sup>, <em>F</em><sub><em>g</em></sub><sup><em>L</em><em>a</em><em>t</em></sup></span>直接从编码器得到的输入。编码器的实现主逻辑如下：</p><ul><li>首先要采样点，通过<code>self.get_rays_for_no_rendering()</code>实现；</li><li>接着计算中间最关键的一步，即2D-to-3D模块M，通过<code>self.network_fn.encode()</code>实现；</li><li>最后</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">INREncoderZoomAxisInAlign</span>(nn.Module):</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs: <span class="built_in">dict</span>, full_render_partial_grad=<span class="literal">False</span>, gt_ct=<span class="literal">None</span>, p0=<span class="literal">None</span>, zoom_size=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="keyword">if</span> gt_ct <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="variable language_">self</span>.set_resolution(full_resolution=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            transformed_points, gt_ctslices, axis = <span class="variable language_">self</span>.get_rays_for_no_rendering(inputs, p0, zoom_size)</span><br><span class="line">            nrays_grad_on = <span class="variable language_">self</span>.N_rays_ctslice_grad_on</span><br><span class="line">            <span class="comment">#transformed_points: (batch_size, args.img_res * args.img_res, args.N_samples, 3)</span></span><br><span class="line">            (batch_size, H, W, N_samples, coord_dim) = transformed_points.shape  <span class="comment"># coord_dim : 3 + self.metadata[&#x27;axis_emb_dim&#x27;]</span></span><br><span class="line">            transformed_points = transformed_points.reshape(batch_size, -<span class="number">1</span>, coord_dim)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.use_cond_encoder:</span><br><span class="line">            src_imgs = []</span><br><span class="line">            src_camposes = []</span><br><span class="line">            <span class="comment"># 两个视角 PA，LAT</span></span><br><span class="line">            <span class="keyword">for</span> cond_key <span class="keyword">in</span> <span class="variable language_">self</span>.cond_list:</span><br><span class="line">                src_imgs.append(inputs[cond_key])</span><br><span class="line">                src_camposes.append(inputs[<span class="string">f&quot;<span class="subst">&#123;cond_key&#125;</span>_cam&quot;</span>])</span><br><span class="line">            src_imgs = torch.stack(src_imgs, dim=<span class="number">0</span>)  <span class="comment"># src_images : NS x B x C x H x W</span></span><br><span class="line">            src_camposes = torch.stack(src_camposes, dim=<span class="number">0</span>)  <span class="comment"># src_camposes : (NS x B x 2), 2 is pitch and yaw or None</span></span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>.no_grad_encoder:</span><br><span class="line">                <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                    latent_zs = <span class="variable language_">self</span>.network_fn.encode(src_imgs, src_camposes, transformed_points[..., :<span class="number">3</span>])</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                latent_zs = <span class="variable language_">self</span>.network_fn.encode(src_imgs, src_camposes, transformed_points[..., :<span class="number">3</span>])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            latent_zs = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> gt_ct <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            all_outputs = <span class="variable language_">self</span>.rendering_from_gt((batch_size, H, W, N_samples), transformed_points[..., :<span class="number">3</span>], gt_ct)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            all_outputs = <span class="variable language_">self</span>.run_nerf((batch_size, H, W, N_samples), transformed_points, nrays_grad_on,</span><br><span class="line">                                        latent_zs, full_render_partial_grad=full_render_partial_grad)</span><br><span class="line">            all_outputs[<span class="string">&#x27;outputs&#x27;</span>] = all_outputs[<span class="string">&#x27;outputs&#x27;</span>].permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">            all_outputs[<span class="string">&#x27;outputs&#x27;</span>] = all_outputs[<span class="string">&#x27;outputs&#x27;</span>].reshape(batch_size, -<span class="number">1</span>, H, W)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># all_outputs.keys : &#x27;outputs&#x27;, (optinal: &#x27;dx&#x27;, &#x27;dsigma&#x27;)</span></span><br><span class="line">        all_outputs[<span class="string">&#x27;cropped_ctslice&#x27;</span>] = gt_ctslices</span><br><span class="line">        <span class="keyword">return</span> all_outputs</span><br></pre></td></tr></table></figure><h4 id="采样网格点的生成">2.1.采样网格点的生成</h4><p><code>get_rays_for_no_rendering()</code>函数的作用是，在CT上选择一个视角方向（axial/coronal/sagittal），然后在该方向上进行网格采样，将这些采样点从GT坐标系变换为世界坐标系，最终返回的结果是变换后的采样点坐标+axis编码 以及 与之对应的真实CT切片。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 生成初始相机光线和采样点，并将其转换为特征点和GT切片</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_rays_for_no_rendering</span>(<span class="params">self, inputs: <span class="built_in">dict</span>, p0, zoom_size</span>):</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> transformed_feature_points, gt_ctslices, axis</span><br></pre></td></tr></table></figure><p>创建一个网格采样网络，这里是一个三维的网格，假设网格分辨率为128，那么最后得到的采样点坐标为：<spanclass="math inline">[[0, 0, 0], [0, 0, 1], ..., [0, 0, 127], ..., [127, 127, 127]]</span>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 初始化特征点</span></span><br><span class="line">transformed_points_in_gt = torch.meshgrid(torch.linspace(<span class="number">0</span>, <span class="variable language_">self</span>.ct_res[<span class="number">0</span>] - <span class="number">1</span>, sampling_resolution[<span class="number">0</span>]),  <span class="comment"># 0, 1, ..., 126, 127</span></span><br><span class="line">                                            torch.linspace(<span class="number">0</span>, <span class="variable language_">self</span>.ct_res[<span class="number">1</span>] - <span class="number">1</span>, sampling_resolution[<span class="number">1</span>]),</span><br><span class="line">                                            torch.linspace(<span class="number">0</span>, <span class="variable language_">self</span>.ct_res[<span class="number">2</span>] - <span class="number">1</span>, sampling_resolution[<span class="number">2</span>]))</span><br><span class="line">transformed_points_in_gt = torch.cat((transformed_points_in_gt[<span class="number">0</span>].unsqueeze(-<span class="number">1</span>),</span><br><span class="line">                                        transformed_points_in_gt[<span class="number">1</span>].unsqueeze(-<span class="number">1</span>),</span><br><span class="line">                                        transformed_points_in_gt[<span class="number">2</span>].unsqueeze(-<span class="number">1</span>)), dim=-<span class="number">1</span>)</span><br><span class="line">transformed_points_in_gt = transformed_points_in_gt.unsqueeze(<span class="number">0</span>).cuda()</span><br></pre></td></tr></table></figure><p>第二步则是要将采样点坐标从GT坐标系转换到世界坐标系，这里的转换就是简单的归一化操作，最终转换后的坐标将被归一化到<span class="math inline">[−0.5, 0.5]</span>，最后<code>transformed_points</code>的维度为(batch_size, 128, 128, 128,3)。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">    ...</span><br><span class="line">    <span class="comment"># 将特征点从GT坐标系转换为世界坐标系</span></span><br><span class="line">    transformed_points = <span class="variable language_">self</span>.gt2world_coordinate(transformed_points_in_gt)</span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用于将GT坐标系转换为世界坐标系</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gt2world_coordinate</span>(<span class="params">self, pts_in_gt</span>):</span><br><span class="line">    device = pts_in_gt.get_device()</span><br><span class="line">    pts_in_gt = pts_in_gt - <span class="variable language_">self</span>.center_gt_coord.to(device)</span><br><span class="line">    pts_in_world = (pts_in_gt / <span class="variable language_">self</span>.max_length_gt_coord * <span class="variable language_">self</span>.max_length_world_coord)</span><br><span class="line">    <span class="keyword">return</span> pts_in_world</span><br></pre></td></tr></table></figure><p>接着说明该如何继续根据单个切片得到采样点坐标，看源代码逻辑很复杂，但是在实验过程中代入各参数，其实原代码逻辑可简化为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> b <span class="keyword">in</span> <span class="built_in">range</span>(batch_size):</span><br><span class="line">    file_path = inputs[<span class="string">&#x27;file_path_&#x27;</span>][b].split(<span class="string">&quot;/&quot;</span>)[-<span class="number">1</span>]</span><br><span class="line">    recon_axis, slice_idx = file_path.replace(<span class="string">&quot;.png&quot;</span>, <span class="string">&quot;&quot;</span>).split(<span class="string">&quot;_&quot;</span>)</span><br><span class="line">    slice_idx = <span class="built_in">int</span>(slice_idx)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> recon_axis == <span class="string">&#x27;axial&#x27;</span>:</span><br><span class="line">        <span class="comment"># 切 axial 切片（z=slice_idx）</span></span><br><span class="line">        feature_point = world_grid[<span class="number">0</span>, slice_idx:slice_idx + <span class="number">1</span>, :, :, :]  <span class="comment"># (1, 128, 128, 3)</span></span><br><span class="line">        feature_point = feature_point.permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">3</span>)  <span class="comment"># (128, 128, 1, 3)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">elif</span> recon_axis == <span class="string">&#x27;coronal&#x27;</span>:</span><br><span class="line">        feature_point = world_grid[<span class="number">0</span>, :, slice_idx:slice_idx + <span class="number">1</span>, :, :]  <span class="comment"># (128, 1, 128, 3)</span></span><br><span class="line">        feature_point = feature_point.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>)  <span class="comment"># (128, 128, 1, 3)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">elif</span> recon_axis == <span class="string">&#x27;sagittal&#x27;</span>:</span><br><span class="line">        feature_point = world_grid[<span class="number">0</span>, :, :, slice_idx:slice_idx + <span class="number">1</span>, :]  <span class="comment"># (128, 128, 1, 3)</span></span><br><span class="line">        <span class="comment"># 不需要 permute，shape 本身正确</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 不需要拼接 axis_emb，因为 dim = 0</span></span><br><span class="line">    feature_points.append(feature_point)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 取出对应的 GT 切片</span></span><br><span class="line">    gt_ct = inputs[inputs[<span class="string">&#x27;image_key&#x27;</span>]][b:b+<span class="number">1</span>]  <span class="comment"># (1, 1, H, W)，假设已经是 axial 平面切片</span></span><br><span class="line">    gt_ct = F.interpolate(gt_ct, size=sampling_resolution[:<span class="number">2</span>], mode=<span class="string">&#x27;bilinear&#x27;</span>, align_corners=<span class="literal">True</span>)</span><br><span class="line">    gt_ctslices.append(gt_ct)</span><br><span class="line"></span><br><span class="line">feature_points = torch.stack(feature_points)  <span class="comment"># (B, H, W, 1, 3)</span></span><br><span class="line">gt_ctslices = torch.cat(gt_ctslices, dim=<span class="number">0</span>)  <span class="comment"># (B, 1, H, W) </span></span><br></pre></td></tr></table></figure><p>其实最后得到的就相当于是当前切片对应的归一化后的三维gt坐标，以及对应的ct切片。</p><h4 id="d-to-3d模块">2.2.2D-to-3D模块</h4><p>下面是比较关键的一个模块，即中间的2D-to-3D模块的实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">latent_zs = <span class="variable language_">self</span>.network_fn.encode(src_imgs, src_camposes, transformed_points[..., :<span class="number">3</span>])</span><br></pre></td></tr></table></figure><p>这一部分是通过<code>PerspectiveINRNet</code>类实现的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PerspectiveINRNet</span>(nn.Module):</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">encode</span>(<span class="params">self, src_images, src_camposes, render_pts</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        NS : number of source</span></span><br><span class="line"><span class="string">        src_images : NS x B x C x H x W</span></span><br><span class="line"><span class="string">        src_camposes : (NS x B x 2), 2 is pitch and yaw or None</span></span><br><span class="line"><span class="string">        render_pts: (B, H * W * N_samples, 3)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> src_camposes <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            features = <span class="literal">None</span></span><br><span class="line">            <span class="comment">## src_image: (B x C x H x W), src_campose : (B x 2)</span></span><br><span class="line">            <span class="keyword">for</span> src_image, src_campose <span class="keyword">in</span> <span class="built_in">zip</span>(src_images, src_camposes):</span><br><span class="line">                feature = <span class="variable language_">self</span>.cond_encoder(src_image)  <span class="comment"># (B * C * H * W, for clip, [:, 64, 80, 80])</span></span><br><span class="line">                feature = <span class="variable language_">self</span>.convert_feature_from2d_to3d(feature, src_campose, render_pts)    <span class="comment">### batch x n_rays_steps x feature_ch</span></span><br><span class="line">                features = torch.cat((features, feature), dim=-<span class="number">1</span>) <span class="keyword">if</span> features <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> feature</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            features = <span class="variable language_">self</span>.cond_encoder(src_images)</span><br><span class="line">            features = features.repeat(<span class="number">1</span>, render_pts.shape[<span class="number">1</span>], <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> features <span class="comment">## B x (H * W * N_samples) x (feature_ch * NS)</span></span><br></pre></td></tr></table></figure><p>首先会经过一个编码器（resnet101），将输入的两张X光编码成两个特征图，然后分别将特征图、位姿编码、以及采样点坐标一起输入<code>convert_feature_from2d_to3d()</code>中，最后将两个视角的特征图拼起来。因此这里的关键在于<code>convert_feature_from2d_to3d()</code>的实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">convert_feature_from2d_to3d</span>(<span class="params">self, features, src_campose, render_pts</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    features : (B x C x H x W)</span></span><br><span class="line"><span class="string">    src_camposes : (B x 2), 2 is pitch and yaw  表示每个批次的相机位姿（俯仰角和偏航角）</span></span><br><span class="line"><span class="string">    render_pts: (B, H * W * N_samples, 3)  表示渲染点的3D坐标</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    device = features.get_device()</span><br><span class="line">    batch, n_rays_steps, _ = render_pts.shape</span><br><span class="line"></span><br><span class="line">    ones = torch.ones((batch, n_rays_steps, <span class="number">1</span>))</span><br><span class="line">    render_pts = torch.cat((render_pts, ones.cuda()), dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取俯仰角和偏航角，然后生成相机原点</span></span><br><span class="line">    pitch = src_campose[..., <span class="number">0</span>:<span class="number">1</span>]</span><br><span class="line">    yaw = src_campose[..., <span class="number">1</span>:<span class="number">2</span>]</span><br><span class="line">    camera_origin_input, _, _ = vr.sample_camera_positions(n=batch, r=<span class="number">1</span>, device=device, phi=pitch, theta=yaw)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将相机原点设置为非零值</span></span><br><span class="line">    camera_origin_input[:, <span class="number">0</span>][camera_origin_input[:, <span class="number">0</span>] == <span class="number">0</span>] = <span class="number">1e-5</span></span><br><span class="line">    forward_vector_input = vr.normalize_vecs(-camera_origin_input)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 生成相机到世界坐标系的变换矩阵以及世界到相机坐标系的变换矩阵</span></span><br><span class="line">    cam2world = vr.create_cam2world_matrix(forward_vector_input, camera_origin_input, device=device)</span><br><span class="line">    world2inputcam = torch.inverse(cam2world.<span class="built_in">float</span>())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将渲染点从世界坐标系转换为相机坐标系，并计算其在相机图像平面上的投影坐标</span></span><br><span class="line">    points_in_src = torch.bmm(world2inputcam, render_pts.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>)).permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">    points_uv_in_src = -points_in_src[..., :<span class="number">2</span>] / points_in_src[..., <span class="number">2</span>:<span class="number">3</span>]  <span class="comment">### batch x n_rays_steps x 2</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将投影坐标转换为相机图像坐标</span></span><br><span class="line">    points_xy_in_src = points_uv_in_src / np.tan((<span class="number">2</span> * math.pi * <span class="variable language_">self</span>.cfg[<span class="string">&#x27;fov&#x27;</span>] / <span class="number">360</span>) / <span class="number">2</span>)</span><br><span class="line">    points_xy_in_src = torch.cat([points_xy_in_src[..., <span class="number">0</span>:<span class="number">1</span>], -points_xy_in_src[..., <span class="number">1</span>:<span class="number">2</span>]], -<span class="number">1</span>) <span class="comment">### batch x n_rays_steps x 2</span></span><br><span class="line"></span><br><span class="line">    points_xy_in_src = points_xy_in_src.unsqueeze(<span class="number">2</span>)</span><br><span class="line">    feature_in_src = F.grid_sample(features, points_xy_in_src, mode=<span class="string">&#x27;bilinear&#x27;</span>, align_corners=<span class="literal">True</span>, padding_mode=<span class="string">&#x27;zeros&#x27;</span>).permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">    feature_in_src = feature_in_src.reshape(batch, n_rays_steps, -<span class="number">1</span>)  </span><br><span class="line"></span><br><span class="line">    <span class="comment">### batch x H * W * N_samples x feature_ch</span></span><br><span class="line">    <span class="keyword">return</span> feature_in_src</span><br></pre></td></tr></table></figure><p>该函数的输入为经过encode后的特征图<code>features</code>（B x C x H xW），俯仰角信息<code>src_camposes</code>（B x2）以及采样点的3D点云<code>render_pts</code>（B, H * W * N_samples,3）。所要完成的事情其实是根据俯仰角信息<code>src_camposes</code>得到相机的方向将3D点云的坐标投影到该方向上，进而在特征图上进行采样。</p><h4 id="使用nerf训练">2.3.使用nerf训练</h4><p>得到采样点对应的特征图后，进一步通过nerf来生成，这里的逻辑在<code>run_nerf()</code>函数中，源代码很繁杂，但是大多数为了加速nerf训练用的，但是这里都没有用，因此其主要逻辑可以简化为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">run_nerf</span>(<span class="params">self, org_shape, transformed_points, nrays_grad_on, latent_zs=<span class="literal">None</span>, full_render_partial_grad=<span class="literal">False</span></span>):</span><br><span class="line">    (batch_size, H, W, N_samples) = org_shape</span><br><span class="line"></span><br><span class="line">    <span class="comment"># transformed_points : (batch_size, H * W, 3)</span></span><br><span class="line">    <span class="comment"># latent_zs : (batch_size, H * W, feature_ch * NS)</span></span><br><span class="line">    nerf_inputs = torch.cat((transformed_points, latent_zs), dim=-<span class="number">1</span>)</span><br><span class="line">    ...</span><br><span class="line">    all_outputs = &#123;&#125;</span><br><span class="line">    all_outputs[<span class="string">&#x27;outputs&#x27;</span>] = torch.zeros((batch_size, transformed_points.shape[<span class="number">1</span>], <span class="variable language_">self</span>.output_ch)).to(device)</span><br><span class="line"></span><br><span class="line">    ...</span><br><span class="line">    all_output = <span class="variable language_">self</span>.network_query_fn(nerf_input, <span class="literal">None</span>, <span class="variable language_">self</span>.network_fn)</span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> all_outputs</span><br></pre></td></tr></table></figure><p>这里<code>self.network_query_fn</code>调用的就是一个nerf网络，nerf网络的结构很简单，就是一个MLP网络：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DummyNeRF</span>(nn.Module):</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.input_ch != <span class="variable language_">self</span>.output_ch:</span><br><span class="line">            x = <span class="variable language_">self</span>.linear(x)</span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">&#x27;outputs&#x27;</span>: x&#125;</span><br></pre></td></tr></table></figure><p>该网络将原特征图转为Encode输出的中间特征的维度，之后方便经过Decoder生成最终输出。</p><h3 id="训练-推理">3.训练 &amp; 推理</h3><p>这个训练的时候是针对每张切片进行单独训练的，但是这里是有优化空间的，因为同一个CT的训练使用的编码后的特征图应该是相同的，可以对同个CT的切片一起训练，避免重复计算特征图。</p><p>推理的时候则是输入两张X光，然后分别得到三个方向上的多张切片。</p>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 重建任务 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>服务器配置代理</title>
      <link href="/2025/06/20/AI/agent/"/>
      <url>/2025/06/20/AI/agent/</url>
      
        <content type="html"><![CDATA[<h1 id="服务器配置代理访问外网资源">服务器配置代理，访问外网资源</h1><p>其实大部分在公司或学校的服务器都会受到限制，无法直接访问互联网，但是有时候需要安装或部署的资源在外网，这时候就比较头疼了。</p><p>其实我们也可以通过代理的方式在服务器上访问外网 :)</p><h2 id="本地开启代理">本地开启代理</h2><p>首先开启本地代理（如果本地还不会开代理这个需要自行搜索了），这里以<code>Clash</code>举例：</p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/agent_config1.png" width="60%"/></p><p>并且需要将 允许局域网按钮打开，记住这里的端口号<code>7890</code>。</p><h2 id="查找本地ip地址">查找本地ip地址</h2><p>打开powershell，输入<code>ipconfig</code>，找到无线网络的IPv4地址。</p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/agent_config2.png" width="60%"/></p><h2 id="服务器配置代理">服务器配置代理</h2><p>在服务器终端输入以下指令即可：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> http_proxy=<span class="string">&quot;http://172.27.139.183:7890&quot;</span></span><br><span class="line"><span class="built_in">export</span> https_proxy=<span class="string">&quot;http://172.27.139.183:7890&quot;</span></span><br></pre></td></tr></table></figure><p>其中<code>http://ip:port</code>中的端口号和ip地址分别是第一步和第二步中得到的。</p><p>使用完毕后使用以下指令关闭代理：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">unset</span> http_proxy</span><br><span class="line"><span class="built_in">unset</span> https_proxy</span><br></pre></td></tr></table></figure><h2 id="配置专用代理">配置专用代理</h2><p>使用上述方式配置的代理可以被内网里的所有人在这台服务器上使用，如果你想配置专用代理，可以使用以下方法。</p><p>只把代理开在<code>127.0.0.1</code>上（即本地回环地址上），使用SSH来进行端口转发。具体来说，假设你启动了一个http代理服务监听在端口<code>port</code>上，则在服务器上输入指令：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh -NfR port:127.0.0.1:port username@server_ip</span><br></pre></td></tr></table></figure><p>这个命令用于在远程服务器上开启一个监听<code>port</code>端口的代理服务，并将所有流量通过ssh隧道传输到本地机器上的代理服务。</p><p>之后像之前那样通过<code>export http_proxy=http://127.0.0.1:port &amp;&amp; export https_proxy=http://127.0.0.1:port</code>设置代理就可以了。</p>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>NJU2025春季学期-高级机器学习笔记</title>
      <link href="/2025/06/03/course/AML/"/>
      <url>/2025/06/03/course/AML/</url>
      
        <content type="html"><![CDATA[<h1 id="第一讲绪论">第一讲、绪论</h1><div class="note primary no-icon flat"><p><strong>考点</strong>：</p><p>（<em>2022，2023</em>）机器学习的定义；</p><p>（<em>2021，2023</em>）机器学习的鲁棒性。</p></div><h2 id="机器学习">机器学习</h2><p>机器学习：机器利用数据学习人类经验，不断提高性能的过程</p><p>机器学习的鲁棒性：机器学习中的鲁棒性（Robustness）指的是模型在面对数据扰动、异常样本或分布变化时，仍然能够保持良好性能的能力。换句话说，就是模型对“不完美”输入的容忍能力。</p><h2 id="人工智能">人工智能</h2><p>人工智能的发展阶段：</p><ul><li>1950年，图灵机；</li><li>1956年，达特茅斯（Dartmouth）会议；</li><li>60-80年代，<strong>推理期</strong>；</li><li>80-90年代，<strong>知识期</strong>；</li><li>90年代后，<strong>学习期</strong>。</li></ul><h2 id="重要术语">重要术语</h2><ul><li>监督学习与无监督学习；</li><li>数据集、训练、测试…</li><li>机器学习技术的根本目标是–<strong>使模型具有泛化能力</strong>：应对未见样本的预测能力；</li><li>归纳偏置（InductiveBias）：任何一个有效的机器学习算法必有其偏好；</li><li>NFL（No Free Lunch）原则：一个算法 <spanclass="math inline">𝔏<sub><em>a</em></sub></span>若在某些问题上比另一个算法 <spanclass="math inline">𝔏<sub><em>b</em></sub></span> 好，必存在另一些问题<span class="math inline">𝔏<sub><em>b</em></sub></span> 比 <spanclass="math inline">𝔏<sub><em>a</em></sub></span> 好；</li></ul><hr /><h1 id="第二讲模型评估和选择">第二讲、模型评估和选择</h1><div class="note primary no-icon flat"><p><strong>考点</strong>：</p><p>（<em>2022</em>）评估方法、性能度量、比较检验各解决什么问题；</p><p>（<em>2023</em>）过拟合&amp;欠拟合。</p></div><p><strong>泛化误差 VS 经验误差</strong></p><ul><li>泛化误差：在“未来”（unseen）样本上的误差；</li><li>经验误差：在训练集上的误差，亦称训练误差；</li></ul><p>泛化误差越小越好，但是经验误差并非越小越好（会出现过拟合）。</p><p><strong>过拟合 VS欠拟合</strong>：一个是模型还没有学充分，一个是模型学得过于充分</p><h2 id="评估方法">评估方法</h2><ol type="1"><li>留出法（hold-out）：将数据集划分为训练和测试集；</li><li>交叉验证法（cross-validation）：将数据集分层采样划分为k个大小相似的互斥子集，每次用k-1个子集的并集作为训练集，余下的子集作为测试集，最终返回k个测试结果的均值；</li><li>留一法（LOO）：假设数据集D包含m个样本，若令k=m，则得到留一法。</li></ol><h2 id="性能度量">性能度量</h2><p><strong>回归任务</strong>最常用的性能度量为<strong>均方误差</strong>（MSE）：</p><p><span class="math display">$$\begin{split}    E(f ; D)=\frac{1}{m}\sum_{i=1}^m\left(f\left(\mathbf{x}_i\right)-y_i\right)^2\end{split}$$</span></p><p><strong>分类任务</strong>最常见的性能度量为<strong>错误率（err）和精度（acc）</strong>：</p><p><span class="math display">$$\begin{split}    E(f ; D)=\frac{1}{m} \sum_{i=1}^m\mathbf{I}\left(f\left(\mathbf{x}_i\right) \neq y_i\right)\end{split}$$</span></p><p><span class="math display">$$\begin{aligned}    acc(f ; D)&amp;=\frac{1}{m} \sum_{i=1}^m\mathbf{I}\left(f\left(\mathbf{x}_i\right) = y_i\right) \\    &amp;= 1-E(f ; D)\end{aligned}$$</span></p><p>信息检索和Web搜索中通常使用<strong>查准率（P）和查全率（R）</strong>来衡量正例比率：</p><p><span class="math display">$$\begin{aligned}    P &amp;= \frac{TP}{TP+FP} \\    R &amp;= \frac{TP}{TP+FN}\end{aligned}$$</span></p><blockquote><p>补充：</p><p><strong>TP</strong>（真正例，gt-正 &amp; pred-正）</p><p><strong>TN</strong>（真负例，gt-负 &amp; pred-负）</p><p><strong>FP</strong>（假正例，gt-负 &amp; pred-正）</p><p><strong>FN</strong>（假负例，gt-正 &amp; pred-负）</p></blockquote><p><strong>F1度量</strong>要更为常用：</p><p><span class="math display">$$\begin{split}    F1 = \frac{2PR}{P+R}\end{split}$$</span></p><p><spanclass="math inline"><em>F</em><sub><em>β</em></sub></span>是比F1更一般的形式：</p><p><span class="math display">$$\begin{split}    F_\beta = \frac{(1+\beta^2)PR}{\beta^2P+R}\end{split}$$</span></p><p>当<spanclass="math inline"><em>β</em> = 1</span>时，是标准的F1指标；当<spanclass="math inline"><em>β</em> &gt; 1</span>时，更偏重查全率（R），此时适用于逃犯信息检索任务；当<spanclass="math inline"><em>β</em> &lt; 1</span>时，更偏重查准率（P），此时适用于商品推荐任务；</p><p><strong>ROC曲线</strong>是常用于二分类任务中的工具，其基于<strong>真正例率</strong>（TPR，又叫做召回率）和<strong>假正例率</strong>（FPR）：</p><p><span class="math display">$$\begin{aligned}    TPR &amp;= \frac{TP}{TP+FN} \\    FPR &amp;= \frac{FP}{TN+FP}\end{aligned}$$</span></p><p>通过遍历不同的阈值，每次将大于阈值的作为正例，小于阈值的作为负例，能得到一组TPR和FPR，最终将这些点按序连接可以得到ROC曲线。而<strong>AUC</strong>的值就是ROC曲线下的面积大小。</p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/ROC.png" width="50%" /></p><h2 id="比较检验">比较检验</h2><p>使用T-检验来进行比较检验。</p><h2 id="偏差-方差分解">偏差-方差分解</h2><p>对于回归任务，其泛化误差可通过偏差-方差分解拆解为：</p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/generalError.png" width="80%" /></p><p>泛化性能是由<strong>学习算法的能力</strong>、<strong>数据的充分性</strong>、<strong>学习任务的难度</strong>共同决定的。</p><hr /><h1 id="第三讲线性模型">第三讲、线性模型</h1><div class="note primary no-icon flat"><p><strong>考点</strong>：</p><p>（<em>2021，2022</em>）最小二乘法推导；</p><p>（<em>2023</em>）多元线性回归求闭式解。</p></div><h2 id="回归任务模型">3.1.回归任务模型</h2><h3 id="一元线性回归">一元线性回归</h3><p>对于单一属性的线性回归，其模型可构建为：</p><p><span class="math display">$$\begin{split}    f\left(x\right)=wx_{i}+b\quad\text{使得}f\left(x_{i}\right)\simeqy_{i}\end{split}$$</span></p><p>参数估计方法：<strong>最小二乘法</strong></p><p><span class="math display">$$\begin{aligned}(w^{*},b^{*}) &amp;=\underset{(w,b)}{\operatorname*{\operatorname*{\arg\min}}}\sum_{i=1}^m\left(f\left(x_i\right)-y_i\right)^2\\&amp; =\arg\min_{(w,b)}\sum_{i=1}^m\left(y_i-wx_i-b\right)^2\end{aligned}$$</span></p><p>求解方法：最小化均方误差，分别对 <spanclass="math inline"><em>ω</em></span> 和 <spanclass="math inline"><em>b</em></span> 求导，可得闭式解：</p><p><span class="math display">$$\begin{split}w=\frac{\sum_{i=1}^my_i\left(x_i-\bar{x}\right)}{\sum_{i=1}^mx_i^2-\frac{1}{m}\left(\sum_{i=1}^mx_i\right)^2}\end{split}$$</span></p><p><span class="math display">$$\begin{split}b=\frac{1}{m}\sum_{i=1}^{m}\left(y_{i}-wx_{i}\right)\end{split}$$</span></p><h3 id="多元线性回归">多元线性回归</h3><p>对于多元属性的线性回归（多元线性回归），其中每个样本属性为 <spanclass="math inline"><strong>x</strong><sub><em>i</em></sub> = (<em>x</em><sub><em>i</em>1</sub>; <em>x</em><sub><em>i</em>2</sub>; …; <em>x</em><sub><em>i</em><em>d</em></sub>)  <em>y</em><sub><em>i</em></sub> ∈ <strong>R</strong></span>，其模型可被推广为：</p><p><span class="math display">$$\begin{split}f\left(\mathbf{x}_i\right)=\mathbf{\omega}^\mathrm{T}\mathbf{x}_i+b\text{使得}f\left(\mathbf{x}_i\right)\simeq y_i\end{split}$$</span></p><p>令 <span class="math inline">$\hat{\mathbf{\omega}} =(\mathbf{\omega};b)$</span> ，并构造：</p><p><span class="math display">$$\mathbf{X}=\begin{pmatrix}x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1d} &amp; 1 \\x_{21} &amp; x_{22} &amp; \cdots &amp; x_{2d} &amp; 1 \\\vdots &amp; \vdots &amp; \ddots &amp; \vdots &amp; \vdots \\x_{m1} &amp; x_{m2} &amp; \cdots &amp; x_{md} &amp; 1\end{pmatrix}=\begin{pmatrix}\mathbf{x}_1^\mathrm{T} &amp; 1 \\\mathbf{x}_2^\mathrm{T} &amp; 1 \\\vdots &amp; \vdots \\\mathbf{x}_m^\mathrm{T} &amp; 1\end{pmatrix}$$</span></p><p>那么此时使用最小二乘法可以表述为：</p><p><span class="math display">$$\begin{split}\hat{\mathbf{\omega}}^*=\arg\min_{\hat{\mathbf{\omega}}}\left(\mathbf{y}-\mathbf{X}\hat{\mathbf{\omega}}\right)^\mathrm{T}\left(\mathbf{y}-\mathbf{X}\hat{\mathbf{\omega}}\right).\end{split}$$</span></p><h3 id="广义线性回归模型">广义线性回归模型</h3><p>广义线性回归模型为：</p><p><span class="math display">$$\begin{split}y=g^{-1}\left(\mathbf{\omega}^\mathrm{T}\mathbf{x}+b\right)\end{split}$$</span></p><p>其中<spanclass="math inline"><em>g</em>(⋅)</span>被称为<strong>联系函数</strong>，并满足单调可微的性质。可以理解这里为回归模型引入了非线性的成分。</p><h2 id="二分类任务模型">3.2.二分类任务模型</h2><p>预测值与输出需要找到函数将其联系起来：</p><p><span class="math display">$$\begin{split}z=\mathbf{\omega}^\mathrm{T}\mathbf{x}+b\quad y\in\{0,1\}\end{split}$$</span></p><p>最理想的函数是单位跃迁函数，即预测值大于零判为正例、小于零判为负例，但是<strong>单位跃迁函数不连续也不可导</strong>。因此使用替代函数–<strong>对数几率函数</strong>（logisticfunction）作为联系函数，其单调可微且任意阶可导：</p><p><span class="math display">$$\begin{split}y=\frac{1}{1+e^{-z}}\end{split}$$</span></p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250604130952.png" width="50%" /></p><p>进一步推导可得：</p><p><span class="math display">$$\begin{split}ln\frac{y}{1-y}=\mathbf{\omega}^\mathrm{T}\mathbf{x}+b\end{split}$$</span></p><p>这里的 <span class="math inline">$ln\frac{y}{1-y}$</span>称为几率，反映了 <span class="math inline"><strong>x</strong></span>作为正例的相对可能性。求解使用极大似然法进行求解：</p><p><span class="math display">$$\begin{split}\ell\left(\mathbf{\omega},b\right)=\sum_{i=1}^m\lnp\left(y_i\mid\mathbf{x}_i;\mathbf{\omega}_i,b\right)\end{split}$$</span></p><p><span class="math display">$$\begin{split}p\left(y_i\mid\mathbf{x}_i;\mathbf{\omega}_i,b\right)=y_ip_1\left(\hat{\mathbf{x}}_i;\mathbf{\beta}\right)+\left(1-y_i\right)p_0\left(\hat{\mathbf{x}}_i;\mathbf{\beta}\right)\end{split}$$</span></p><h2 id="多分类任务模型">3.3.多分类任务模型</h2><h3 id="一对一ovo">一对一（OvO）</h3><p>拆分阶段：N个类别两两配对，各自训练二分类分类器。</p><p>测试阶段：新样本提交给所有分类器预测，被预测最多的类别为最终类别。</p><h3 id="一对其他ovr">一对其他（OvR）</h3><p>拆分阶段：某一类作为正例，其余作为反例，各自训练二分类分类器。</p><p>测试阶段：新样本提交给所有分类器，使用置信度最大的类别作为最终类别。</p><h3 id="多对多mvm">多对多（MvM）</h3><p>若干类作为正类，若干类作为反类，使用纠错输出码（Error CorrectingOutput Code，ECOC）进行最终预测：</p><ul><li><strong>编码阶段</strong>：对N个类别做M次划分，每次划分将一部分类别划为正类，一部分划为反类；</li><li><strong>解码阶段</strong>：测试样本交给M个分类器预测；</li><li>最终距离最小的类别为最终类别。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/EOOC.png" width="80%" /></p><h2 id="线性模型的优缺点">3.4.线性模型的优缺点</h2><blockquote><p>优点：</p><p>形式简单、易于建模；</p><p>具有一定的可解释性。</p><p>缺点：</p><p>难以处理非线性问题。</p></blockquote><hr /><h1 id="第四讲支持向量机">第四讲、支持向量机</h1><div class="note primary no-icon flat"><p><strong>考点</strong>：</p><p>（<em>2022，2023</em>）SVM基本型&amp;对偶型推导；</p><p>（<em>2023</em>）核函数。</p></div><p>将训练样本分开的超平面有很多，但是“正中间”的鲁棒性是最好的，泛化能力也最强。</p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/SVM.png" width="60%" /></p><p>SVM优化目标：寻找最大间隔，求解参数 <spanclass="math inline"><strong>ω</strong></span> 和 <spanclass="math inline"><em>b</em></span> 使得 <spanclass="math inline"><em>γ</em></span> 最大。</p><h2 id="基本型和对偶型">4.1.基本型和对偶型</h2><p>优化问题：最大间隔。</p><p><span class="math display">$$\begin{aligned}\underset{\mathbf{\omega}, b}{\arg \max } &amp;\frac{2}{\|\mathbf{\omega}\|} \\\text { s.t. } &amp; y_i\left(\mathbf{\omega}^{\top}\mathbf{x}_i+b\right) \geq 1, i=1,2, \ldots, m .\end{aligned}$$</span></p><p>其可转换为等价问题：凸二次规划问题，这也是<strong>支持向量机的基本型</strong>。</p><p><span class="math display">$$\begin{aligned}\underset{\mathbf{\omega}, b}{\arg \min } &amp;\frac{\|\mathbf{\omega}\|^2}{2} \\\text { s.t. } &amp; y_i\left(\mathbf{\omega}^{\top}\mathbf{x}_i+b\right) \geq 1, i=1,2, \ldots, m .\end{aligned}$$</span></p><p>引入拉格朗日乘子 <spanclass="math inline"><em>α</em><sub><em>i</em></sub> ≥ 0</span>可以得到拉格朗日函数：</p><p><span class="math display">$$\begin{split}L(\mathbf{\omega}, b,\mathbf{\alpha})=\frac{1}{2}\|\mathbf{\omega}\|^2+\sum_{i=1}^m\alpha_i\left(1-y_i\left(\mathbf{\omega}^{\mathrm{T}}\mathbf{x}_i+b\right)\right)\end{split}$$</span></p><p>分别对 <span class="math inline"><strong>ω</strong></span> 和 <spanclass="math inline"><em>b</em></span> 的偏导为零可得：</p><p><span class="math display">$$\begin{split}\mathbf{\omega}=\sum_{i=1}^m\alpha_iy_i\mathbf{x}_i,0=\sum_{i=1}^m\alpha_iy_i\end{split}$$</span></p><p>最后可以得到<strong>支持向量机的对偶型</strong>：</p><p><span class="math display">$$\begin{aligned}\max_{\alpha} &amp;\sum_{i=1}^{m}\alpha_{i}-\frac{1}{2}\sum_{i=1}^{m}\sum_{j=1}^{m}\alpha_{i}\alpha_{j}y_{i}y_{j}\mathbf{x}_{i}^{\mathrm{T}}\mathbf{x}_{j}\\\mathrm{s.t.} &amp;\sum_{i=1}^{m}\alpha_{i}y_{i}=0,\alpha_{i}\geqslant0,\quadi=1,2,\ldots,m\end{aligned}$$</span></p><h2 id="特征空间映射">4.2.特征空间映射</h2><p>若不存在一个能正确划分两类样本的超平面，我们可以将样本从原始空间映射到一个更高维的特征空间，使样本在这个特征空间内线性可分。<em>如果原始空间是有限维（特征数有限），那么一定存在一个高维特征空间是样本可分</em>。</p><p>设样本 <span class="math inline"><strong>x</strong></span>映射后的向量为 <spanclass="math inline"><em>ϕ</em>(<strong>x</strong>)</span>，划分超平面为：</p><p><span class="math display">$$\begin{split}    f(\mathbf{x}) = \mathbf{\omega}^{T}\phi(\mathbf{x})+b\end{split}$$</span></p><p>则原基本型和对偶型可推广为：</p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250604142808.png" width="60%" /></p><p><strong>核函数</strong>：用于绕过显式考虑特征映射、以及计算高维内积的困难。</p><p><span class="math display">$$\begin{split}    \kappa(\mathbf{x}_i,\mathbf{x}_j)=\phi(\mathbf{x}_i)^\mathrm{T}\phi(\mathbf{x}_j)\end{split}$$</span></p><blockquote><p>Mercer定理：若一个对称函数所对应的核矩阵<strong>半正定</strong>，则它就能作为核函数来使用。</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250604143436.png" width="80%" /></p><h2 id="软间隔支持向量机">4.3.软间隔支持向量机</h2><p>软间隔SVM不再假设所有样本都可分，而是引入损失函数，计算每个样本的损失，然后再<strong>最大化间隔和最小化整体损失之间做个合理的折中</strong>：</p><p><span class="math display">$$\begin{aligned}\min_{\mathbf{\omega},b}\frac{1}{2}\|\mathbf{\omega}\|^2+C\sum_{i=1}^ml_{0/1}\left(y_i(\mathbf{\omega}^\top\phi(\mathbf{x}_i)+b)-1\right)\\l_{0/1}=\begin{cases}1 &amp; z&lt;0 \\0 &amp; otherwise &amp;\end{cases}\end{aligned}$$</span></p><p>存在的问题是0/1损失函数非凸非连续，因此这里使用Hinge损失函数来替代0/1损失函数：</p><p><span class="math display">$$\begin{split}\mathrm{Hinge}(y,f(x))=\max(0,1-y\cdot f(x))\end{split}$$</span></p><p>由此可以分别得到软间隔支持向量机的基本型和对偶型：</p><ol type="1"><li>原始问题</li></ol><p><span class="math display">$$\begin{split}\min_{\mathbf{\omega},b}\frac{1}{2}\|\mathbf{\omega}\|^2+C\sum_{i=1}^m\max\left(0,1-y_i(\mathbf{\omega}^\top\phi(\mathbf{x}_i)+b)\right).\end{split}$$</span></p><ol start="2" type="1"><li>对偶问题</li></ol><p><span class="math display">$$\begin{aligned}\operatorname*{min}_{\alpha}&amp;\frac{1}{2}\sum_{i=1}^{m}\sum_{j=1}^{m}\alpha_{i}\alpha_{j}y_{i}y_{j}\phi(\mathbf{x}_{i})^{\top}\phi(\mathbf{x}_{j})-\sum_{i=1}^{m}\alpha_{i}\\\mathrm{s.t.} &amp;\sum_{i=1}^{m}\alpha_{i}y_{i}=0,0\leq\alpha_{i}\leqC,i=1,2,\ldots,m.\end{aligned}$$</span></p><h2 id="svm拓展正则化">4.4.SVM拓展–正则化</h2><p>统计学习模型的更一般形式可以表述为：</p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250604144936.png" width="60%" /></p><p>其中正则化可理解为罚函数，使优化过程趋向于期望目标。</p><hr /><h1 id="第五讲神经网络">第五讲、神经网络</h1><div class="note primary no-icon flat"><p><strong>考点</strong>：</p><p>（<em>2022，2023</em>）多层前馈网络的计算能力、局限性、解决方法。</p></div><h2 id="神经网络历史">5.1.神经网络历史</h2><ul><li>第一阶段（1943~1969）：以感知机为代表；</li><li>第二阶段（1982~2000）：反向传播算法为代表，Hopfield网络；</li><li>第三阶段（2006~迄今）：深度网络为代表。</li></ul><h2 id="神经元模型">5.2.神经元模型</h2><p>M-P神经元模型 <em>[McCulloch and Pitts, 1943]</em></p><ul><li>输入：来自其他 n个神经元传递过来的信号</li><li>处理：通过带权重连接进行传递, 总值与神经元的阈值比较</li><li>输出：通过激活函数得到输出</li></ul><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250604145756.png" width="60%" /></p><blockquote><p>常用激活函数：<strong>Sigmoid函数</strong></p><p><span class="math display">$$\operatorname{sigmoid}(x)=\frac{1}{1+e^{-x}}$$</span></p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250604150040.png" width="30%" /></p></blockquote><h2 id="感知机与多层网络">5.3.感知机与多层网络</h2><p><strong>单层感知机</strong>：由两层神经元组成，输入层接受外界输入信号传递给输出层M-P神经元。</p><p>当两类模式线性可分时，感知机的学习过程一定会收敛，否则其学习过程会发生震荡。因此其无法解决非线性可分问题。</p><p><strong>多层感知机</strong>：输入层和输出层中存在多层神经元，称为隐层或隐含层，其中隐层和输出层神经元都是具有激活功能的功能神经元。</p><ul><li>定义：每两层神经元全互联，不存在同层连接和跨层连接；</li><li>前馈：接受外界输入信号，隐含层与输出层神经元对信号进行加工输出；</li><li>学习：根据训练数据调整神经元的“连接权”以及功能神经元的“阈值”。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250604151125.png" width="50%" /></p><h2 id="误差逆传播算法">5.4.误差逆传播算法</h2><p><strong>误差逆传播算法</strong>（ErrorBackPropagation，反向传播）是最常用的多层前馈神经网络的学习算法：</p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250604152717.png" width="50%" /></p><ol type="1"><li>前向计算：</li></ol><p><span class="math display">$$\begin{split}\mathrm{step1:~}b_h=f(\alpha_h-\gamma_h),\alpha_h=\sum_{i=1}^dv_{ih}x_i\end{split}$$</span></p><p><span class="math display">$$\begin{split}\mathrm{step2:}\quad\hat{y}_j^k=f(\beta_j-\theta_j),\beta_j=\sum_{i=q}^dw_{hj}b_h\end{split}$$</span></p><p><span class="math display">$$\begin{split}\mathrm{step3:~}E_k=\frac{1}{2}\sum_{j=1}^l(\hat{y}_j^k-y_j^k)^2\end{split}$$</span></p><ol start="2" type="1"><li>参数数量：</li></ol><ul><li><p>对于一层到二层的权重 <spanclass="math inline"><em>v</em><sub><em>i</em><em>h</em></sub></span>，总共有 <span class="math inline"><em>d</em> ⋅ <em>q</em></span>个参数需要优化；</p></li><li><p>对于二层到三层的权重 <spanclass="math inline"><em>w</em><sub><em>h</em><em>j</em></sub></span>，总共有 <span class="math inline"><em>q</em> ⋅ <em>l</em></span>个参数需要优化；</p></li><li><p>对于二层的阈值 <spanclass="math inline"><em>θ</em><sub><em>j</em></sub></span> ，总共有<span class="math inline"><em>q</em></span> 个参数需要优化；</p></li><li><p>对于三层的阈值 <spanclass="math inline"><em>γ</em><sub><em>h</em></sub></span> ，总共有<span class="math inline"><em>l</em></span> 个参数需要优化。</p></li></ul><p>因此网络总共有 <spanclass="math inline">(<em>d</em> + <em>l</em> + 1)<em>q</em> + <em>l</em></span>个参数需要优化。</p><ol start="3" type="1"><li>参数优化：</li></ol><p>BP算法基于<strong>梯度下降</strong>策略，以误差率为目标，计算负梯度方向对参数进行调整。</p><p>比如根据之前的前向过程，这里可以对二层的权重参数计算反向传播：</p><p><span class="math display">$$\begin{split}    \Delta w_{hj}=-\eta\frac{\partial E_k}{\partial w_{hj}}\end{split}$$</span></p><p><span class="math display">$$\begin{split}    \frac{\partial E_k}{\partial w_{hj}}=\frac{\partialE_k}{\partial\hat{y}_j^k}\cdot\frac{\partial\hat{y}_j^k}{\partial\beta_j}\cdot\frac{\partial\beta_j}{\partialw_{hj}}\end{split}$$</span></p><p>其中 <span class="math inline"><em>η</em> ∈ (0, 1)</span>是学习率，控制着算法每一轮迭代中的更新步长。</p><blockquote><p>Sigmoid函数在求导时有一个很好的性质：</p><p><span class="math display">$$\begin{split}f^{\prime}\left(x\right)=f\left(x\right)\left(1-f\left(x\right)\right)\end{split}$$</span></p></blockquote><p><strong>BP的不同实现方式</strong>：</p><ul><li>标准BP：每次对单个训练样例更新权值和阈值，单次计算开销小但参数更新频繁，迭代次数多；</li><li>累计BP：最小化整个训练集上的累积误差，一般是读取整个训练集后更新参数，参数更新频率低但计算开销大；</li></ul><p><span class="math display">$$\begin{split}    E=\frac{1}{m}\sum_{k=1}^mE_k\end{split}$$</span></p><p>多层前馈网络具有强大的学习能力，包含足够多神经元的隐层，多层前馈神经网络能以任意精度逼近任意复杂度的连续函数。</p><p>但其也有其局限性：</p><ul><li>由于其强大的表达能力，经常遭遇过拟合；</li><li>如何设置隐藏神经元个数是个难题，实际应用中常使用试错法。</li></ul><p>缓解过拟合的一些策略包括：</p><ul><li>早停：在训练过程中, 若训练误差降低, 但验证误差升高,则停止训练；</li><li>正则化：在误差目标函数中增加一项描述网络复杂程度的成分,防止模型过于复杂，例如连接权值与阈值的平方和。</li></ul><p><span class="math display">$$\begin{split}    E=\lambda\frac{1}{m}\sum_{k=1}^mE_k+(1-\lambda)\sum_iw_i^2\end{split}$$</span></p><h2 id="深度学习">5.5.深度学习</h2><p>深度学习模型是具有很多个隐层的神经网络。一方面，计算能力的大幅提高缓解了训练效率；另一方面，训练数据的大幅增加降低了过拟合风险。因此，以“深度学习”(deep learning) 为代表的复杂模型成为了合适的选择。</p><blockquote><p><strong>复杂模型带来的困难</strong>：</p><p>深度网络难以直接用经典算法（例如BP算法）进行训练，因为误差在多隐层内传播时会出现梯度消失问题（即梯度迅速为0），难以收敛到稳定状态。</p></blockquote><p><strong>训练方法</strong>：</p><ul><li>预训练+微调：将大量参数进行分组，局部先找到较好的设置，然后再基于局部较优的结果进行全局寻优。</li><li>权共享：一组神经元使用相同的连接权值，权共享策略在卷积神经网络（CNN）中发挥了重要作用。</li></ul><hr /><h1 id="第六讲决策树">第六讲、决策树</h1><div class="note primary no-icon flat"><p><strong>考点</strong>：</p><p>（<em>2023</em>）决策树最优划分的两个准则及其偏好；</p><p>（<em>2022</em>）决策树过拟合原因及其解决方案。</p></div><h2 id="基本流程">6.1.基本流程</h2><p>策略：<strong>分而治之</strong></p><p>三种停止条件：</p><ul><li>当前节点包含的样本全属于同一类别，无需划分；</li><li>当前属性集为空，或是所有样本在所有属性上取值相同，无法划分；</li><li>当前节点包含的样本集为空，不能划分。</li></ul><h2 id="划分算法">6.2.划分算法</h2><p>直觉上，决策树的分支结点所包含的样本应尽可能属于同一类别，即结点的<strong>纯度</strong>越高越好。</p><h3 id="信息增益id3决策树">信息增益（ID3决策树）</h3><p>信息熵（informationentropy）是度量样本集合纯度最常用的一种指标，假定当前样本集合 <spanclass="math inline"><em>D</em></span> 中第 <spanclass="math inline"><em>k</em></span> 类样本所占的比例为 <spanclass="math inline"><em>p</em><sub><em>k</em></sub>(<em>k</em> = 1, 2, ..., |<em>y</em>|)</span>，则<span class="math inline"><em>D</em></span> 的信息熵定义为：</p><p><span class="math display">$$\begin{split}    \mathrm{Ent}(D)=-\sum_{k=1}^{|y|}p_k\log_2p_k\end{split}$$</span></p><p><strong><span class="math inline">Ent(<em>D</em>)</span>的值越小，<span class="math inline"><em>D</em></span>的纯度越高</strong>。因此我们可以通过计算划分前后信息熵的差值来判断是否是最优划分，即计算<strong>信息增益</strong>：</p><p><span class="math display">$$\begin{split}    \mathrm{Gain}(D,a)=\mathrm{Ent}(D)-\sum_{v=1}^V\frac{|D^v|}{|D|}\mathrm{Ent}(D^v)\end{split}$$</span></p><p>这里的 <span class="math inline"><em>v</em></span> 表示目前属性 <spanclass="math inline"><em>a</em></span>可能的取值，每一步划分时选择的属性就是最大的信息增益对应的属性，即ID3算法：</p><p><span class="math display">$$\begin{split}    a_{*}=\underset{a\inA}{\operatorname*{\operatorname*{\arg\max}}}\mathrm{Gain}(D,a)\end{split}$$</span></p><h3 id="增益率c4.5决策树">增益率（C4.5决策树）</h3><p>事实上，信息增益准则对可取值数目较多的属性有所偏好（比如气球的种类可能只有氢气球和氦气球等，但是颜色有很多种，这样计算颜色的信息增益一般都很大，因为平均到每个颜色的样本数相较要少、显得纯度更高），为减少这种偏好可能带来的不良影响，C4.5算法使用<strong>增益率</strong>（gainratio）来选择最优划分属性：</p><p><span class="math display">$$\begin{split}    \mathrm{Gain_ratio}(D,a)=\frac{\mathrm{Gain}(D,a)}{\mathrm{IV}(a)}\end{split}$$</span></p><p><span class="math display">$$\begin{split}    \mathrm{IV}(a)=-\sum_{v=1}^V\frac{|D^v|}{|D|}\mathrm{log}_2\frac{|D^v|}{|D|}\end{split}$$</span></p><p>相当于是给信息增益做了一步规范化，<spanclass="math inline">IV(<em>a</em>)</span> 称为属性 <spanclass="math inline"><em>a</em></span>的固有值。一般属性的可能取值数目越多，其固有值越大。</p><h2 id="剪枝处理">6.3.剪枝处理</h2><p>决策树决策分支过多，以致于把训练集自身的一些特点当做所有数据都具有的一般性质，因此可能会导致<strong>过拟合</strong>。通常的解决办法是预留一部分数据用作“验证集”以进行性能评估，并采取剪枝策略来提升泛化性能。</p><h3 id="预剪枝">预剪枝</h3><p>思路：<strong>边建树，边剪枝</strong>。决策树生成过程中，对每个结点在划分前先进行估计，若当前结点的划分不能带来决策树泛化性能提升，则停止划分并将当前结点记为叶结点，其类别标记为训练样例数最多的类别。</p><blockquote><p><em>优缺点</em>：</p><p>优点：降低过拟合风险，显著减少训练时间和测试时间开销</p><p>缺点：存在欠拟合风险，基于贪心本质，有些分支当前划分虽然不能提升泛化性能，但在其基础上的后续划分可能导致性能提高</p></blockquote><h3 id="后剪枝">后剪枝</h3><p>思路：<strong>先建树，后剪枝</strong>。决策树生成后，自底向上对每个结点进行考察，若当前结点的划分导致在验证集上的精度下降，则将其剪除。</p><blockquote><p><em>优缺点</em>：</p><p>优点：比预剪枝保留了更多分支，欠拟合风险小；</p><p>缺点：训练时间开销大，后剪枝过程是在生成完全决策树之后进行的，需要自底向上对所有非叶结点逐一考察；其训练时间要远大于预剪枝决策树。</p></blockquote><h2 id="多变量决策树">6.4.多变量决策树</h2><p>非叶结点不再是仅仅针对某个属性，而是对属性的线性组合。</p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250604205006093.png" width="50%" /></p><hr /><h1 id="第七讲贝叶斯分类器">第七讲、贝叶斯分类器</h1><div class="note primary no-icon flat"><p><strong>考点</strong>：</p><p>（<em>2018</em>）最优贝叶斯分类器及贝叶斯风险；</p><p>（<em>2019，2022，2023</em>）生成式&amp;判别式模型的区别。</p></div><h2 id="贝叶斯决策论">7.1.贝叶斯决策论</h2><p>给定 <span class="math inline"><em>N</em></span> 个类别，令 <spanclass="math inline"><em>λ</em><sub><em>i</em><em>j</em></sub></span>表示将第 <span class="math inline"><em>j</em></span> 类样本误分类为第<span class="math inline"><em>i</em></span>类所产生的损失，则基于后验概率将样本 <spanclass="math inline"><em>x</em></span> 分到第 <spanclass="math inline"><em>i</em></span> 类的条件风险为：</p><p><span class="math display">$$\begin{split}    R(c_i\mid\mathbf{x})=\sum_{j=1}^N\lambda_{ij}P(c_j\mid x)\end{split}$$</span></p><p><strong>贝叶斯判定准则</strong>（Bayes Decision Rule）：</p><p><span class="math display">$$\begin{split}    h^*(x)=\underset{c\in\mathcal{Y}}{\operatorname*{\arg\min}}R(c\mid\mathbf{x})\end{split}$$</span></p><p><span class="math inline"><em>h</em><sup>*</sup>(<em>x</em>)</span>称为贝叶斯最优分类器，其总体风险称为贝叶斯风险，反映了学习性能的理论上限。</p><p>机器学习需要实现的是基于有限的训练样本尽可能准确地估计出后验概率：</p><p><span class="math display">$$\begin{split}    P(c\mid x)=\frac{P(c)P(x\mid c)}{P(x)}\end{split}$$</span></p><blockquote><p><strong>判别式 VS 生成式</strong>：</p><p>1.判别式（discriminative）模型</p><p>直接对 <spanclass="math inline"><em>P</em>(<em>c</em> ∣ <em>x</em>)</span>建模，代表方法：决策树、BP神经网络、SVM</p><p>2.生成式（generative）模型</p><p>先对联合概率分布 <spanclass="math inline"><em>P</em>(<em>x</em>, <em>c</em>)</span>建模，再由此获得 <spanclass="math inline"><em>P</em>(<em>c</em> ∣ <em>x</em>)</span>，代表方法：贝叶斯分类器</p></blockquote><h2 id="朴素贝叶斯">7.2.朴素贝叶斯</h2><p>主要障碍是所有属性上的联合概率难以从有限训练样本估计获得。因此我们不妨<strong>假定所有属性之间相互独立</strong>，可以得到：</p><p><span class="math display">$$\begin{split}    P(c\mid\mathbf{x})=\frac{P(c)P(\mathbf{x}\midc)}{P(\mathbf{x})}=\frac{P(c)}{P(\mathbf{x})}\prod_{i=1}^dP(x_i\mid c)\end{split}$$</span></p><p>由于 <span class="math inline"><em>P</em>(<strong>x</strong>)</span>对所有类别相同，由此可以得到：</p><p><span class="math display">$$\begin{split}    h_{nb}(x)=\arg\max_{c\in\mathcal{Y}}P(c)\prod_{i=1}^dP(x_i\mid c)\end{split}$$</span></p><p>接下来分两步计算上式：</p><ul><li>估计 <span class="math inline"><em>P</em>(<em>c</em>)</span></li></ul><p><span class="math display">$$\begin{split}    P(c)=\frac{|D_c|}{|D|}\end{split}$$</span></p><ul><li>估计 <spanclass="math inline"><em>P</em>(<strong>x</strong> ∣ <em>c</em>)</span></li></ul><p>对离散属性，有：</p><p><span class="math display">$$\begin{split}    P(x_i\mid c)=\frac{|D_{c,x_i}|}{|D_c|}\end{split}$$</span></p><p>对连续属性，使用概率密度函数：</p><p><span class="math display">$$\begin{split}    p\left(x_i \mid c\right)=\frac{1}{\sqrt{2 \pi} \sigma_{c, i}} \exp\left(-\frac{\left(x_i-\mu_{c, i}\right)^2}{2 \sigma_{c, i}^2}\right)\end{split}$$</span></p><p>这个思路有一个问题，一旦有一个属性值在训练集中没有在某个类别中出现过，则由频率估计概率，该属性值的概率为0，直接连乘会导致整个<spanclass="math inline"><em>P</em>(<strong>x</strong> ∣ <em>c</em>)</span>为0。因此为了避免这种情况，这里使用<strong>拉普拉斯修正</strong>：</p><p><span class="math display">$$\begin{split}    \hat{P}(c)=\frac{|D_c|+1}{|D|+N},\quad\hat{P}(x_i\midc)=\frac{|D_{c,x_i}|+1}{|D_c|+N_i}\end{split}$$</span></p><p>其中 <span class="math inline"><em>N</em></span> 表示训练集 <spanclass="math inline"><em>D</em></span> 中可能的类别数，<spanclass="math inline"><em>N</em><sub><em>i</em></sub></span> 表示第 <spanclass="math inline"><em>i</em></span> 个属性可能的取值数。</p><h2 id="半朴素贝叶斯">7.3.半朴素贝叶斯</h2><p>朴素贝叶斯分类器的<strong>属性独立性假设</strong>在现实情况中往往难以成立，因此半朴素贝叶斯分类器（Semi-naïveBayesClassifier）会适当考虑一部分属性间的相互依赖关系。其中最常用的策略是<strong>独依赖估计</strong>（One-DependentEstimator，ODE），即假设每个属性在类别之外最多仅依赖一个其他属性：</p><p><span class="math display">$$\begin{split}    P(c\mid x)\propto P(c)\prod_{i=1}^dP(x_i\mid c,\boxed{pa_i})\end{split}$$</span></p><p>这个其他属性 <spanclass="math inline"><em>p</em><em>a</em><sub><em>i</em></sub></span>也叫做 <span class="math inline"><em>x</em><sub><em>i</em></sub></span>的<strong>父属性</strong>，那么该如何确定父属性呢？</p><ul><li>SPODE（Super-ParentODE）：假设所有属性依赖于同一属性<strong>超父</strong>，通过交叉验证等模型选择方法来确定超父属性；</li><li>TAN（Tree Augmented naïveBayes）：以属性间的条件互信息为边的权重，构建完全图利用最大带权生成树算法，进保留相关属性间的依赖性。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250605142948145.png" width="70%" /></p><p><strong>AODE</strong>（Averaged One-DependentEstimator）尝试将每个属性作为超父来构建SPODE，最后将拥有足够训练数据支撑的SPODE集成起来作为最终结果：</p><p><span class="math display">$$\begin{split}    P(c\mid\mathbf{x})\propto\sum_{\begin{array}{c}i=1 \quad |D_{x_i}|\geqslant m^{\prime}\end{array}}^dP(c,x_i)\prod_{j=1}^dP(x_j\mid c,x_i)\end{split}$$</span></p><p>其中 <span class="math inline"><em>m</em><sup>′</sup></span>为阈值常数，用于保留拥有足量训练数据的SPODE。同样AODE在计算时也使用了拉普拉斯修正：</p><p><span class="math display">$$\begin{split}    \hat{P}(c,x_i)=\frac{|D_{c,x_i}|+1}{|D|+N_i},\quad\hat{P}(x_j\midc,x_i)=\frac{|D_{c,x_i,x_j}|+1}{|D_{c,x_i}|+N_j}\end{split}$$</span></p><blockquote><p>高阶依赖：ODE <span class="math inline">→</span> kDE</p><p>明显障碍：随着 k 的增加，估计 <spanclass="math inline"><em>P</em>(<em>x</em><sub><em>j</em></sub> ∣ <em>c</em>, <em>p</em><em>a</em><sub><em>i</em></sub>)</span>所需的样本数将以指数级增加。</p></blockquote><h2 id="贝叶斯网">7.4.贝叶斯网</h2><p>了解即可。</p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250605144759105.png" width="70%" /></p><hr /><h1 id="第八讲集成学习">第八讲、集成学习</h1><h2 id="个体与集成">8.1.个体与集成</h2><p>集成学习（ensemble learning）通过多学习器来提升性能：</p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250605145420227.png" width="50%" /></p><p>关键：<strong>好而不同</strong></p><p>假设基分类器的错误率相互独立，为 <spanclass="math inline"><em>P</em>(<em>h</em><sub><em>i</em></sub>(<strong>x</strong>) ≠ <em>f</em>(<strong>x</strong>)) = <em>ϵ</em></span>，则由Hoeffding不等式可得，如果我们从一个有限范围内独立采样了n个样本，那么样本均值偏离期望值超过 <spanclass="math inline"><em>ϵ</em></span>的概率会以指数速度下降（关于样本数量n）。即在一定条件下，随着集成分类器数目的增加，集成的错误率将指数级下降，最终趋向于0。</p><p>但是这里有一个关键假设，就是基学习器的误差相互独立，然而在现实生活中个体学习器来自同一个问题，显然不可能完全独立。因此如何产生好而不同的个体学习器是集成学习研究的核心。</p><p>集成学习大致可分为两大类：串行 VS 并行。</p><h2 id="boosting">8.2.Boosting</h2><p>特征：</p><ul><li>每次调整训练数据的样本分布</li><li>串行生成</li><li>个体学习器间存在强依赖关系</li></ul><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250605152544770.png" width="70%" /></p><p>其中Boosting算法中最重要的是<strong>AdaBoost</strong>算法：</p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250605153817270.png" width="70%" /></p><p>总的来说，其流程可以总结为：</p><ul><li>初始化训练数据的权值分布 <spanclass="math inline">$D_1=\frac{1}{m}$</span>，其中m代表有m个样本数据；</li><li>训练弱分类器 <spanclass="math inline"><em>h</em><sub><em>i</em></sub></span>，如果某个训练样本被弱分类器准确分类，则在构造下一个训练集时其对应的权值会减小；相反，如果某个训练样本被错误分类，其权值则会增大，相当于在下一次训练时加大对其的注意力。最后整个训练过程迭代<span class="math inline"><em>T</em></span> 次，最终有 <spanclass="math inline"><em>T</em></span> 个弱分类器；</li><li>最终各个弱分类器被组合成一个强分类器，其中分类误差小的分类器其权值被加大，使其起到更大的决定性作用。</li></ul><h2 id="bagging与随机森林">8.3.Bagging与随机森林</h2><h3 id="bagging">Bagging</h3><p>Bagging算法首先随机采样原数据集 <spanclass="math inline"><em>D</em></span> 得到不同分布的数据集 <spanclass="math inline"><em>D</em><sub>1</sub>, <em>D</em><sub>2</sub>, ..., <em>D</em><sub><em>T</em></sub></span>用于给不同的基学习器学习（由于不存在依赖关系，这一步可以并行执行），最终得到<span class="math inline"><em>T</em></span>个弱学习器，使用<strong>投票法</strong>来结合弱学习器得到最终的强学习器。</p><p>特征：</p><ul><li>个体学习器不存在强依赖关系</li><li>并行化生成</li><li>自助采样法</li></ul><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250605162320683.png" width="50%" /></p><p>可进一步使用包外估计，即仅考虑那些未使用样本<spanclass="math inline"><em>x</em></span>训练的基学习器在<spanclass="math inline"><em>x</em></span>上的预测：</p><p><span class="math display">$$\begin{split}    H^{oob}(\mathbf{x})=\arg\max_{y\in\mathcal{Y}}\sum_{t=1}^T\mathbb{I}(h_t(\mathbf{x})=y)\cdot\mathbb{I}(\mathbf{x}\notinD_t)\end{split}$$</span></p><h3 id="随机森林">随机森林</h3><p>随机森林（RandomForest）是一个包含多个<strong>决策树</strong>的分类器，其输出类别由个别树输出的类别的<strong>众数</strong>而定。</p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250605163249.png" width="80%" /></p><p>构造一个随机决策树的过程：</p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250605163737563.png" width="50%" /></p><h2 id="结合策略">8.4.结合策略</h2><ul><li>平均法/加权平均法：最基本的方法</li><li>投票法：<ul><li>绝对多数投票法：一个分类结果占据半数优势时才会选择它；</li><li>相对多数投票法：选择票数最多的结果；</li><li>加权投票法：相对多数的加权版本。</li></ul></li><li>Stacking学习法</li></ul><h2 id="多样性">8.5.多样性</h2><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250605165432245.png" width="50%" /></p><p>基学习器学得越准确以及越多样性（好而不同），集成学习越好。</p><p>常见的增强个体学习器多样性的方法：</p><ol type="1"><li><strong>数据样本扰动</strong>：通常基于采样法的不同，比如Bagging的自助采样、AdaBoost的序列采样</li></ol><blockquote><p>数据样本扰动对“不稳定基学习器”很有效：</p><p><strong>不稳定基学习器</strong>包括：决策树、神经网络等；</p><p><strong>稳定基学习器</strong>包括：线性学习器、支持向量机、朴素贝叶斯、k近邻等。</p></blockquote><ol start="2" type="1"><li><strong>输入属性扰动</strong>：每次随机选择一部分输入数据维度进行学习</li></ol><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250605171633933.png" width="50%" /></p><ol start="3" type="1"><li><p><strong>输出属性扰动</strong>：输出表示扰动</p><ul><li>翻转法：随机改变输入样本的标记</li><li>输出调剂法：分类输出改为回归输出得到分类器</li><li>ECOC法：多类任务分解为一系列两类任务来求解</li></ul></li><li><p><strong>算法参数扰动</strong>：采用负相关法，强制要求个体神经网络采用不同的参数</p></li></ol><hr /><h1 id="第九讲聚类算法">第九讲、聚类算法</h1><div class="note primary no-icon flat"><p><strong>考点</strong>：</p><p>（<em>2018</em>）三种算法的举例；</p><p>（<em>2018</em>）距离度量的四个性质并证明；</p><p>（<em>2022</em>）K-means过程，k值如何选；</p><p>（<em>2023</em>）密度聚类和层次聚类的代表算法、关键假设。</p></div><h2 id="聚类定义">9.1.聚类定义</h2><p>目标：将数据样本划分为若干个通常不相交的簇（cluster）</p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250605172510980.png" width="70%" /></p><h2 id="性能度量-1">9.2.性能度量</h2><p>聚类性能度量，也称为聚类有效性指标，需要做到<strong>簇内相似度</strong>（intra-clustersimilarity）<strong>高且簇间相似度</strong>（inter-clustersimilarity）<strong>低</strong>。</p><ul><li>外部指标：将聚类结果与某个<em>参考模型</em>进行比较</li></ul><blockquote><p><strong>补充</strong>：</p><p>定义：TP–同属一个类并被聚在一起的样本对数、FP–不同类但被聚在一起的样本对数、FN–同类但被分在不同簇、TN–不同类且被分开</p><p>1.<strong>Jaccard系数</strong>：衡量两个集合的相似度</p><p><span class="math display">$$\begin{split}  J=\frac{TP}{TP+FP+FN}\end{split}$$</span></p><p>2.<strong>FM 指数</strong>：P和R的几何平均</p><p><span class="math display">$$\begin{split}  \mathrm{FM}=\sqrt{\frac{TP}{TP+FP}\cdot\frac{TP}{TP+FN}}\end{split}$$</span></p><p>3.<strong>Rand指数</strong>：表示聚类结果中有多少比例的样本对被正确分类</p><p><span class="math display">$$\begin{split}  \mathrm{Rand}=\frac{TP+TN}{TP+TN+FP+FN}\end{split}$$</span></p></blockquote><ul><li>内部指标：直接考察聚类结果，无参考模型</li></ul><blockquote><p><strong>补充</strong>：</p><p>1.<strong>DB指数</strong>：衡量簇内部和外部距离的指标</p><p><span class="math display">$$\begin{split}  \mathrm{DB}=\frac{1}{k}\sum_{i=1}^k\max_{j\neqi}\left(\frac{s_i+s_j}{d_{ij}}\right)\end{split}$$</span></p><p>其中 <span class="math inline"><em>s</em><sub><em>i</em></sub></span>表示第 <span class="math inline"><em>i</em></span>个簇的内部平均距离，<spanclass="math inline"><em>d</em><sub><em>i</em><em>j</em></sub></span>表示第 <span class="math inline"><em>i</em></span> 和 <spanclass="math inline"><em>j</em></span>簇之间的中心距离。该指标越小越好，表示簇内部越紧凑、簇间分离越好。</p><p>2.<strong>Dunn指数</strong>：衡量簇内部和外部距离的指标</p><p><span class="math display">$$\begin{split}  \mathrm{Dunn}=\frac{\min_{i\neq j}d(C_i,C_j)}{\max_k\delta(C_k)}\end{split}$$</span></p><p>分子表示不同簇之间的最小距离，分母则指代单个簇内部的最大直径，越大越好</p></blockquote><h2 id="距离计算">9.3.距离计算</h2><p>距离度量的意义：聚类来自于分组，分组来自于合理度量，度量来自于距离，因此距离对聚类有很本质的作用。</p><p>需满足的基本性质：</p><ul><li><strong>非负性</strong>：<spanclass="math inline">dist (<strong>x</strong><sub><em>i</em></sub>, <strong>x</strong><sub><em>j</em></sub>) ≥ 0</span>；</li><li><strong>同一性</strong>：<spanclass="math inline">dist (<strong>x</strong><sub><em>i</em></sub>, <strong>x</strong><sub><em>j</em></sub>) = 0当且仅当<strong>x</strong><sub><em>i</em></sub> = <strong>x</strong><sub><em>j</em></sub></span>；</li><li><strong>对称性</strong>：<spanclass="math inline">dist (<strong>x</strong><sub><em>i</em></sub>, <strong>x</strong><sub><em>j</em></sub>) = dist (<strong>x</strong><sub><em>j</em></sub>, <strong>x</strong><sub><em>i</em></sub>)</span>；</li><li><strong>直递性</strong>：<spanclass="math inline">dist (<strong>x</strong><sub><em>i</em></sub>, <strong>x</strong><sub><em>j</em></sub>) &lt; dist (<strong>x</strong><sub><em>i</em></sub>, <strong>x</strong><sub><em>k</em></sub>) + dist (<strong>x</strong><sub><em>k</em></sub>, <strong>x</strong><sub><em>j</em></sub>)</span>.</li></ul><p>常用的距离形式：<strong>闵可夫斯基距离</strong>（MinkowskiDistance）</p><p><span class="math display">$$\begin{split}    \mathrm{dist}_{\mathrm{mk}}(\boldsymbol{x}_{i},\boldsymbol{x}_{j})=\left(\sum_{u=1}^{n}\left|x_{iu}-x_{ju}\right|^{p}\right)^{\frac{1}{p}}\end{split}$$</span></p><blockquote><p>当 <span class="math inline"><em>p</em> = 2</span>时，又称为欧氏距离（Euclidean distance）；</p><p>当 <span class="math inline"><em>p</em> = 1</span>时，又称为曼哈顿距离（Manhattan distance）。</p></blockquote><p>对于无序（non-ordinal）属性，可使用VDM（Value DifferenceMetric）：</p><p><span class="math display">$$\begin{split}    VDM_p(a,b)=\sum_{i=1}^k\left|\frac{m_{u,a,i}}{m_{u,a}}-\frac{m_{u,b,i}}{m_{u,b}}\right|^p\end{split}$$</span></p><p>上式表示属性 <span class="math inline"><em>u</em></span> 上两个离散值<span class="math inline"><em>a</em></span> 和 <spanclass="math inline"><em>b</em></span> 之间的VDM距离，其中 <spanclass="math inline"><em>k</em></span> 为样本簇数，<spanclass="math inline"><em>m</em><sub><em>u</em>, <em>a</em></sub></span>表示属性<spanclass="math inline"><em>u</em></span>上取值为<spanclass="math inline"><em>a</em></span>的样本数，<spanclass="math inline"><em>m</em><sub><em>u</em>, <em>a</em>, <em>i</em></sub></span>表示第<spanclass="math inline"><em>i</em></span>个样本簇中在属性<spanclass="math inline"><em>u</em></span>上取值为<spanclass="math inline"><em>a</em></span>的样本数。</p><p>对于混合属性，可使用MinkovDM：</p><p><span class="math display">$$\begin{split}    \mathrm{MinkovDM}_p(\boldsymbol{x}_i,\boldsymbol{x}_j)=\left(\sum_{u=1}^{n_c}|x_{iu}-x_{ju}|^p+\sum_{u=n_c+1}^n\mathrm{VDM}_p(x_{iu},x_{ju})\right)^{\frac{1}{p}}\end{split}$$</span></p><h2 id="聚类算法">9.4.聚类算法</h2><p>常见聚类方法可分为以下几类：</p><ul><li><strong>原型聚类</strong>：有簇中心的聚类方法。先对原型初始化，然后对原型进行迭代更新求解。<em>代表算法：K-means、LVQ、高斯混合</em>；</li><li><strong>密度聚类</strong>：划分为多个等价类，未必有簇中心。从样本密度的角度来考察样本之间的可连接性，并基于可连接样本不断扩展聚类簇。<em>代表算法：DBSCAN、OPTICS、DENCLUE</em>；</li><li><strong>层次聚类</strong>：在不同层次对数据集进行划分，从而形成树形聚类结构。<em>代表算法：AGNES、DIANA</em>。</li></ul><h3 id="原型聚类">原型聚类</h3><ol type="1"><li><p><strong>K-means（K均值聚类）</strong>：每个簇中心以该簇中所有样本点的均值表示</p><ul><li>Step1: 随机选取 k 个样本点作为簇中心；</li><li>Step2: 将其他样本点根据其与簇中心的距离，划分给最近的簇；</li><li>Step3: 更新各簇的均值向量，将其作为新的簇中心；</li><li>Step4: 若所有簇中心未发生改变，则停止；否则执行 Step 2。</li></ul></li></ol><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250605195805353.png" width="70%" /></p><ol start="2" type="1"><li><strong>学习向量量化（LVQ）</strong>：试图找到一组原型向量来刻画聚类结构，但数据样本带有类别标记，通过聚类来形成类别的子类结构，每个聚类对应于类别的一个子类</li></ol><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250605204347631.png" width="50%" /></p><ol start="3" type="1"><li><strong>高斯混合聚类（Gaussian Mixture Clustering,GMM）</strong>：采用高斯概率分布来表达聚类模型，簇中心=均值，簇半径=方差</li></ol><blockquote><p>n维样本空间中的随机向量x若服从高斯分布，则其概率密度函数为：</p><p><span class="math display">$$\begin{split}    p(\boldsymbol{x})=\frac{1}{(2\pi)^{\frac{n}{2}}|\boldsymbol{\Sigma}|^{\frac{1}{2}}}e^{-\frac{1}{2}(\boldsymbol{x}-\boldsymbol{\mu})^{\mathrm{T}}\boldsymbol{\Sigma}^{-1}(\boldsymbol{x}-\boldsymbol{\mu})}\end{split}$$</span></p></blockquote><p>假设样本由下面这个高斯混合分布生成：</p><p><span class="math display">$$\begin{split}    p_{\mathcal{M}}(\boldsymbol{x})=\sum_{i=1}^k\alpha_i\cdotp(\boldsymbol{x}\mid\boldsymbol{\mu}_i,\boldsymbol{\Sigma}_i)\end{split}$$</span></p><p>其中可以根据 <spanclass="math inline"><em>α</em><sub>1</sub>, <em>α</em><sub>2</sub>, ..., <em>α</em><sub><em>k</em></sub></span>定义的先验分布选择高斯混合成分，<spanclass="math inline"><em>α</em><sub><em>i</em></sub></span> 即为选择第<span class="math inline"><em>i</em></span>个混合成分的概率。之后根据被选择的混合成分的概率密度函数进行采样，从而生成相应的样本。</p><p>由此可以得到，样本 <spanclass="math inline"><strong>x</strong><sub><strong>j</strong></sub></span>由第 <span class="math inline"><em>i</em></span>个高斯混合成分生成的后验概率为：</p><p><span class="math display">$$\begin{split}    \gamma_{ji} =p_{\mathcal{M}}(z_{j}=i\mid\boldsymbol{x}_{j})=\frac{P(z_{j}=i)\cdotp_{\mathcal{M}}(\boldsymbol{x}_{j}\midz_{j}=i)}{p_{\mathcal{M}}(\boldsymbol{x}_{j})}=\frac{\alpha_{i}\cdotp(\boldsymbol{x}_{j}\mid\boldsymbol{\mu}_{i},\boldsymbol{\Sigma}_{i})}{\sum_{l=1}^{k}\alpha_{l}\cdotp(\boldsymbol{x}_{j}\mid\boldsymbol{\mu}_{l},\boldsymbol{\Sigma}_{l})}\end{split}$$</span></p><p>求解时同样使用极大似然估计来进行参数估计：</p><p><span class="math display">$$\begin{split}    LL(D)=\ln\left(\prod_{j=1}^mp_\mathcal{M}(x_j)\right)=\sum_{j=1}^m\ln\left(\sum_{i=1}^k\alpha_i\cdotp(\boldsymbol{x}_j\mid\boldsymbol{\mu}_i,\boldsymbol{\Sigma}_i)\right)\end{split}$$</span></p><p>参数估计完毕后，使用<strong>EM算法</strong>类似K-means求解聚类。</p><blockquote><p><strong>补充：EM算法</strong></p><p>EM算法（Expectation-MaximizationAlgorithm）是一种迭代优化算法，用于含有隐变量的模型中，常用于估计最大似然参数（MaximumLikelihood Estimation, MLE）或最大后验概率（MAP）。</p><p><strong>核心思想</strong>：EM 算法在估计过程中交替执行两步</p><p>1.<em>E 步（期望步）</em>：在当前参数下，估计隐变量的分布；</p><p>2.<em>M步（最大化步）</em>：在估计出的隐变量分布下，最大化对数似然函数，更新参数。</p><p>在这里求解GMM时的场景则是：</p><p><strong>E步：</strong>根据当前的参数计算每个样本属于每个高斯成分的后验概率<spanclass="math inline"><em>γ</em><sub><em>j</em><em>i</em></sub></span>；（隐变量）</p><p><strong>M步：</strong>更新模型参数 <spanclass="math inline">{(<em>α</em><sub><em>i</em></sub>, <strong>μ</strong><sub><em>i</em></sub>, <strong>Σ</strong><sub><em>i</em></sub>) ∣ 1 ≤ <em>i</em> ≤ <em>k</em>}</span>。（更新参数）</p></blockquote><h3 id="密度聚类">密度聚类</h3><p><strong>DBSCAN</strong>（Density-Based Spatial Clustering ofApplications withNoise）是一种基于密度的聚类算法，非常适合处理具有任意形状的簇、噪声点和离群点的数据集。其基本假设是：<strong>簇是由足够密集的点组成的区域</strong>，它通过考察每个点在其邻域中的“密度”来决定是否属于某个簇。</p><p><strong>关键定义</strong>：</p><ul><li><span class="math inline"><em>ϵ</em></span>-邻域（EpsilonNeighborhood）：以点<spanclass="math inline"><em>p</em></span>为中心、半径为<spanclass="math inline"><em>ϵ</em></span>的圆形区域；</li><li>MinPts（最小点数）：构成一个密集区域所需的最小点数</li></ul><table><colgroup><col style="width: 50%" /><col style="width: 50%" /></colgroup><thead><tr><th style="text-align: left;"><strong>类型</strong></th><th style="text-align: left;"><strong>定义</strong></th></tr></thead><tbody><tr><td style="text-align: left;">核心点（core point）</td><td style="text-align: left;">自己的<spanclass="math inline"><em>ϵ</em></span>-邻域内至少包含MinPts个点（包括自己）</td></tr><tr><td style="text-align: left;">边界点（border point）</td><td style="text-align: left;">自己不是核心点，但是在某个核心点的<spanclass="math inline"><em>ϵ</em></span>-邻域内</td></tr><tr><td style="text-align: left;">噪声点（noise point）</td><td style="text-align: left;">既不是核心点，也不是边界点</td></tr></tbody></table><p><strong>算法流程</strong>：</p><ol type="1"><li><p>从数据集中任取一个未访问的点 p：</p><ul><li>如果 p 是核心点，以 p 为起点，扩展一个簇；</li><li>如果 p 是边界点或噪声点，则忽略；</li></ul></li><li><p>对所有点重复此过程，直到所有点都被访问；</p></li><li><p>最终将所有密度相连的点归为一类，噪声点单独标记为 -1。</p></li></ol><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250605220026372.png" width="60%" /></p><p>如上图，虚线表示<spanclass="math inline"><em>ϵ</em></span>-邻域，MinPts为3，首先我们可以判断<span class="math inline"><em>x</em><sub>1</sub></span>是核心点，之后通过迭代过程判断所有点之间的密度相连关系即可构建出当前MinPts对应的聚类结果。</p><blockquote><p><strong>优缺点</strong>：</p><p>优点：能识别任意形状的簇（环形、月牙形）；能识别噪声点（离群点）；不需要向K-means一样指定簇数。</p><p>缺点：对参数敏感（<spanclass="math inline"><em>ϵ</em></span>、MinPts）；需要满足密度假设；如果不同簇的密度有较大差异，表现会很差。</p></blockquote><h3 id="层次聚类">层次聚类</h3><p><strong>AGNES</strong>(AGglomerativeNESting)：自底向上，从最细的粒度开始（单个样本），逐渐合并相似的簇，直到最粗的簇（一个簇）。关键假设：能够产生不同粒度的聚类结果。</p><p>算法流程：</p><ul><li>Step1: 将每个样本点作为一个簇；</li><li>Step2: 合并最近的两个簇；</li><li>Step3: 若所有样本点都存在于一个簇中，则停止；否则转到 Step2。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250605220904037.png" width="60%" /></p><hr /><h1 id="第十讲降维与度量学习">第十讲、降维与度量学习</h1><div class="note primary no-icon flat"><p><strong>考点</strong>：</p><p>（<em>2018，2019，2021</em>）PCA的最近重构性和最大可分性；</p><p>（<em>2019</em>）马氏距离表达式；</p><p>（<em>2022</em>）维度灾难是什么，ISOMAP、LLE；</p><p>（<em>2019</em>）特征脸是什么。</p></div><h2 id="k-近邻学习">10.1.K-近邻学习</h2><p>基本思路：近朱者赤，近墨者黑。使用投票法/平均法进行确定样本的类别，<strong>懒惰学习</strong>的代表。</p><blockquote><p><strong>懒惰学习</strong>：事先没有分类器，输入测试样本才开始准备分类器</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250606125620540.png" width="60%" /></p><p>由上述例子可以看出，K-近邻的关键在于<strong>k值的选取</strong>和<strong>距离的计算</strong>。</p><p>给定测试样本<spanclass="math inline"><em>x</em></span>，若其最近邻样本为<spanclass="math inline"><em>z</em></span>，则最近邻分类器出错的概率为（<spanclass="math inline"><em>x</em></span>和<spanclass="math inline"><em>z</em></span>类别标记不同）：</p><p><span class="math display">$$\begin{aligned}P(err) &amp;=1-\sum_{c\in\mathcal{Y}}P(c\mid\boldsymbol{x})P(c\mid\boldsymbol{z}) \\&amp; \simeq1-\sum_{c\in\mathcal{Y}}P^2(c\mid\boldsymbol{x}) \\&amp; \leq1-P^2(c^*\mid\boldsymbol{x}) \\&amp; =\begin{pmatrix}1+P\left(c^{*}\mid\boldsymbol{x}\right)\end{pmatrix}\begin{pmatrix}1-P\left(c^{*}\mid\boldsymbol{x}\right)\end{pmatrix} \\&amp; \leq2\times\begin{pmatrix}1-P(c^{*}\mid\boldsymbol{x})\end{pmatrix}.\end{aligned}$$</span></p><p><em>最近邻分类器的泛化错误率不会超过贝叶斯最优分类器错误率的两倍！</em></p><p>K-近邻的问题：<strong>真实问题中很难准确地找到k-近邻</strong>。</p><p>密采样的假设：样本的每个 <spanclass="math inline"><em>ϵ</em></span>-邻域内都有近邻。</p><blockquote><p>考虑一个20维的例子，若近邻的距离阈值设为<spanclass="math inline">10<sup>−3</sup></span>。单纯考虑一个维度下的k-近邻，那么在这一维的单位空间至少要<spanclass="math inline">10<sup>3</sup></span>个样本才能满足密采样的假设。当考虑20个维度时，此时单位空间至少需要<spanclass="math inline">10<sup>3 × 20</sup></span>个样本才能满足密采样条件。</p><p>若是一张<span class="math inline">300 × 233</span>的彩色图像，将其按<span class="math inline"><em>h</em> × <em>w</em> × <em>c</em></span>的顺序展平，最终得到 <span class="math inline">300 × 233 × 3</span>的向量，该维度有209700，则更加灾难。</p></blockquote><p><strong>维度灾难</strong>：高维空间给距离计算带来很大的麻烦。更严重的是，当样本变得稀疏时，k近邻会不准。</p><p>数据样本虽是高维的，但与学习任务密切相关的也许仅是某个低维空间，即高维空间中的一个低维嵌入（embedding）。</p><h2 id="低维嵌入">10.2.低维嵌入</h2><p><strong>MDS</strong>（MultidimensionalScaling，多维尺度分析）是一种降维方法，它的主要目标是：寻找一个低维子空间，使得距离和样本原有距离近似不变。</p><p>核心思路：寻找低维子空间尽量保持样本内积不变。已知样本的距离矩阵，使用特征值分解来求解内积矩阵（内积保距）。</p><p><strong>算法过程</strong>：</p><ol type="1"><li>从距离矩阵 <span class="math inline"><em>D</em></span> 构造内积矩阵<spanclass="math inline"><em>B</em> = <em>X</em><em>X</em><sup>⊤</sup></span>：</li></ol><p>给定距离矩阵 <spanclass="math inline"><em>D</em> = [<em>d</em><sub><em>i</em><em>j</em></sub>]</span>，可以先构造平方距离矩阵：</p><p><span class="math display">$$\begin{split}    D^{(2)} = \|d_{ij}\|^2\end{split}$$</span></p><p>然后构造<strong>中心化矩阵</strong>：</p><p><span class="math display">$$\begin{split}    H = I - \frac{1}{n}\mathbf{1}\mathbf{1}^\top\end{split}$$</span></p><p>其中 <span class="math inline"><em>I</em></span> 是 <spanclass="math inline"><em>n</em> × <em>n</em></span> 单位矩阵，<spanclass="math inline"><strong>1</strong></span>是全1列向量。用它对平方距离矩阵进行双中心化，得到<strong>内积矩阵</strong>：</p><p><span class="math display">$$\begin{split}    B = - \frac{1}{2}HD^{(2)}H\end{split}$$</span></p><blockquote><p><strong><em>Q：为什么需要中心化？</em></strong></p><p>你只有成对的距离信息，但没有坐标。为了推导坐标，就必须：</p><p>假设这些点的<strong>整体几何中心是原点</strong>（即数据集中心化），否则你推导出的内积会带有“偏移量”误差。</p><p><strong><em>Q：而中心化矩阵是如何推导的？</em></strong></p><p>对欧几里得距离有：</p><p><span class="math display">$$\begin{split}  d_{ij}^2=\|x_i-x_j\|^2=x_i^\top x_i-2x_i^\top x_j+x_j^\top x_j\end{split}$$</span> <span class="math display">$$\begin{split}  B=XX^\top\Rightarrow B_{ij}=x_i^\top x_j\end{split}$$</span></p><p>设矩阵的对角线元素为 <spanclass="math inline"><em>b</em><sub><em>i</em><em>i</em></sub> = <em>x</em><sub><em>i</em></sub><sup>⊤</sup><em>x</em><sub><em>i</em></sub></span>，则有：</p><p><span class="math display">$$\begin{split}  d_{ij}^2=b_{ii}+b_{jj}-2b_{ij}\end{split}$$</span></p><p>由公式：</p><p><span class="math display">$$\begin{split}  b_{ij}=-\frac{1}{2}\left(d_{ij}^2-\bar{d}_{i\cdot}^2-\bar{d}_{\cdotj}^2+\bar{d}_{\cdot\cdot}^2\right)\end{split}$$</span></p><p>这个转换其实就等价于双重中心化操作：</p><p><span class="math display">$$\begin{split}  B = - \frac{1}{2}HD^{(2)}H\end{split}$$</span></p></blockquote><ol start="2" type="1"><li>对 <span class="math inline"><em>B</em></span> 作特征值分解：</li></ol><p><span class="math display">$$\begin{split}    B=V\Lambda V^\top\end{split}$$</span></p><ol start="3" type="1"><li>选前 k 个最大的特征值和对应的特征向量，计算坐标：</li></ol><p><span class="math display">$$\begin{split}    X=V_k\Lambda_k^{1/2}\end{split}$$</span></p><p>此时 <spanclass="math inline"><em>X</em> ∈ ℝ<sup><em>n</em> × <em>k</em></sup></span>就是降维后每个样本的k-维坐标。</p><h2 id="流形学习">10.3.流形学习</h2><h3 id="isomap">ISOMAP</h3><p><strong>ISOMAP</strong>其实是对 MDS的非线性扩展版本，适用于流形学习问题。它的目标是：<strong>保留数据在流形上的“测地线距离（GeodesicDistance）”结构，而不是原始空间中的欧几里得距离。</strong>举个不太恰当的例子，比如一张弯曲的纸上的两个点，之前的MDS相当于计算的是三维空间中的距离（三维欧氏距离），而ISOMAP考虑的是流形上的路径距离（两个点在纸面上的距离，测地线距离）。</p><p>而在算法设计上，ISOMAP使用最短路径算法来确定任意两点的测地线距离：</p><ol type="1"><li><strong>构建邻接图</strong>：对每个点连接它的k-近邻或<spanclass="math inline"><em>ϵ</em></span>邻域，构建无向图 <spanclass="math inline"><em>G</em></span> ，边权值为欧氏距离；</li><li><strong>使用Dijkstra或Floyd求解任意两点的最短距离</strong>：最终得到测地线距离矩阵<spanclass="math inline"><em>D</em><sup><em>g</em><em>e</em><em>o</em></sup></span>；</li><li><strong>使用测地线距离执行MDS</strong>：相当于将原距离矩阵替换为测地线距离，其余步骤不变。</li></ol><h3 id="lle">LLE</h3><p><strong>LLE</strong>（Locally LinearEmbedding）同样用于非线性降维，该算法假设高维空间中的数据样本在一个低维流形上，且在局部邻域内近似线性。因此我们可以在每个点的邻域中用线性组合重建该点，然后在低维空间中找到映射，使这些“重建关系”得以保留。</p><p>算法步骤：</p><ol type="1"><li><p><strong>找到每个点的k近邻</strong>：<spanclass="math inline">{<em>x</em><sub><em>i</em><sub>1</sub></sub>, <em>x</em><sub><em>i</em><sub>2</sub></sub>, ..., <em>x</em><sub><em>i</em><sub><em>k</em></sub></sub>}</span>；</p></li><li><p><strong>学习局部重构权重</strong>：</p><p>对每个点 <spanclass="math inline"><em>x</em><sub><em>i</em></sub></span>，求一组权重<spanclass="math inline"><em>w</em><sub><em>i</em><em>j</em></sub></span>，使其满足：<span class="math display">$$\begin{split}     \boldsymbol{x}_i\approx\sum_{j\in\mathcal{N}(i)}w_{ij}\boldsymbol{x}_j\end{split}$$</span></p><p>得到<strong>优化目标</strong>： <span class="math display">$$\begin{split}     \min_{w_{ij}}\sum_i\left\|\boldsymbol{x}_i-\sum_{j\in\mathcal{N}(i)}w_{ij}\boldsymbol{x}_j\right\|^2\end{split}$$</span></p><p>同时又约束： <span class="math display">$$\begin{split}     \sum_{j\in\mathcal{N}(i)}w_{ij}=1\end{split}$$</span></p><p>该过程对每个<spanclass="math inline"><em>x</em><sub><em>i</em></sub></span>独立进行，每个<spanclass="math inline"><em>x</em><sub><em>i</em></sub></span>有自己的一组权重。</p></li><li><p><strong>在低维空间中保留重构权重</strong>：</p><p>目标是寻找低维表示<spanclass="math inline"><em>y</em><sub><em>i</em></sub> ∈ ℝ<sup><em>b</em></sup></span>，使得同样的权重<spanclass="math inline"><em>w</em><sub><em>i</em><em>j</em></sub></span>仍能重构它：<span class="math display">$$\begin{split}     \min_{\boldsymbol{y}_1,\ldots,\boldsymbol{y}_n}\sum_i\left\|\boldsymbol{y}_i-\sum_{j\in\mathcal{N}(i)}w_{ij}\boldsymbol{y}_j\right\|^2\end{split}$$</span></p><p>这个优化问题通过将其写成矩阵形式，然后通过特征值分解求解，这里就不具体展开了。</p></li></ol><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250609205841865.png" width="40%" /></p><h2 id="度量学习">10.4.度量学习</h2><p>之前介绍的距离度量有以下几种：</p><ul><li>欧氏距离</li><li>曼哈顿距离</li><li>测地距离</li></ul><p>之前说过距离度量对于降维很重要，不同的任务需要选择不同的距离度量，能否直接学习一个合适的距离度量呢？这里使用<strong>马氏距离（MahalanobisDistance）</strong>来参数化学习距离度量：</p><p><span class="math display">$$\begin{split}    \mathrm{dist}_{\mathrm{mah}}^2(x_i,x_j)=(x_i-x_j)^\mathrm{T}\mathrm{M}(x_i-x_j)=\|x_i-x_j\|_\mathrm{M}^2\end{split}$$</span></p><p>其中 <span class="math inline">M</span>称为度量矩阵，它是一个半正定对称矩阵，距离度量学习相当于就是要学习 <spanclass="math inline">M</span>。欧氏距离的一个问题就是–各个方向都同等重要，这里引入度量矩阵则能使降维侧重于某些维度。</p><h3 id="距离度量学习nca">距离度量学习–NCA</h3><p>NCA（Neighborhood ComponentAnalysis）常用于近邻分类器（KNN），而近邻分类器在进行判别时通常使用多数投票法，这里NCA使用概率投票法（其实本质差不多）。对于任意样本<span class="math inline"><em>x</em><sub><em>j</em></sub></span>，它对<span class="math inline"><em>x</em><sub><em>i</em></sub></span>分类结果影响的概率为：</p><p><span class="math display">$$\begin{split}    p_{ij}=\frac{\exp\left(-\left\|\boldsymbol{x}_i-\boldsymbol{x}_j\right\|_\mathbf{M}^2\right)}{\sum_l\exp\left(-\left\|\boldsymbol{x}_i-\boldsymbol{x}_l\right\|_\mathbf{M}^2\right)}\end{split}$$</span></p><p>其实很好理解，对于一个点，离它越近的点越相似，两个点的标签越可能是一样的。那么对于点<spanclass="math inline"><em>x</em><sub><em>i</em></sub></span>，其分类正确的概率为：</p><p><span class="math display">$$\begin{split}    p_i=\sum_{j:y_j=y_i}p_{ij}\end{split}$$</span></p><p>NCA的目标就是<strong>最大化所有样本的正确分类概率之和</strong>：</p><p><span class="math display">$$\begin{split}    \mathcal{L}(A)=\sum_ip_i=\sum_i\sum_{j:y_j=y_i}p_{ij}\end{split}$$</span></p><h3 id="距离度量学习lmnn">距离度量学习–LMNN</h3><p><strong>LMNN（Large Margin Nearest Neighbor）</strong>是另一种监督式度量学习方法，与 NCA 类似，但优化目标和策略不同。LMNN的核心思想是：学习一个距离度量，使得 K近邻分类器（KNN）在训练集上分类间隔更大，让同类点靠得近，不同类点被“推远”。</p><p>具体来说，其目标函数包含<strong>拉进</strong>和<strong>推远</strong>两项：</p><p><span class="math display">$$\begin{split}    \mathcal{L}(M)=\sum_{(i,j)\in\mathcal{N}}d_L(x_i,x_j)^2+\mu\sum_{(i,j,l)}\xi_{ijl}\end{split}$$</span></p><p><span class="math display">$$\begin{split}    \xi_{ijl}=\begin{bmatrix}1+d_L(x_i,x_j)^2-d_L(x_i,x_l)^2\end{bmatrix}_+\end{split}$$</span></p><p>其中第一项中的 <span class="math inline">𝒩</span>表示所有的<strong>同类K近邻对</strong>（targetneighbor），是我们希望拉近的，所以希望距离越小越好； 而第二项中的 <spanclass="math inline"><em>ξ</em><sub><em>i</em><em>j</em><em>l</em></sub></span>是<strong>Hinge损失</strong>项，用于惩罚违反<strong>margin规则</strong>的异类点，其中<span class="math inline"><em>x</em><sub><em>l</em></sub></span> 表示与<span class="math inline"><em>x</em><sub><em>i</em></sub></span>标签不同的样本。</p><p>那么这里的margin规则是什么意思呢？它表示惩罚那些“离得太近的异类点”，在这里我们希望<span class="math inline"><em>x</em><sub><em>i</em></sub></span>与异类点 <spanclass="math inline"><em>x</em><sub><em>l</em></sub></span>的距离至少比同类点之间的距离远 1个单位（margin），如果没有达到这个要求就需要惩罚，所以这里使用Hinge损失。</p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250606161257888.png" width="90%" /></p><h2 id="pca">10.5.PCA</h2><p>PCA（Principal ComponentAnalysis，主成分分析）是一种经典的无监督降维方法，其目标是：<strong>在保证尽可能保留原始数据方差信息的前提下，将高维数据投影到低维空间。</strong></p><p>正交属性空间中的样本点，如何找到一个超平面使得所有样本都能被恰当表达呢？显然我们希望该超平面具备以下性质：</p><ul><li><strong>最近重构性</strong>：样本点到这个超平面的距离都足够近；</li><li><strong>最大可分性</strong>：样本点在这个超平面上的投影都尽可能分开。</li></ul><p>那么可以通过以上两种目标进行推导：</p><ol type="1"><li><p>基于最近重构性推导：</p><ul><li>对样本中心化：<spanclass="math inline">$\bar{x}=\frac{1}{n}\sum_{i=1}^nx_i,\quadX_{\mathrm{centered}}=X-\bar{x}$</span></li><li>假定投影变换后得到的新坐标系为<spanclass="math inline"><em>w</em><sub><em>i</em></sub></span>， 其中<spanclass="math inline"><em>w</em><sub><em>i</em></sub></span>是标准正交基向量，即：</li></ul></li></ol><p><span class="math display">$$\begin{split}||\boldsymbol{w}_i||_2=1,\boldsymbol{w}_i^\mathrm{T}\boldsymbol{w}_j=0(i\neqj)\end{split}$$</span></p><ul><li>舍弃原坐标系中的部分维度，假设维度降低到<spanclass="math inline"><em>d</em><sup>′</sup> &lt; <em>d</em></span>，则样本点在低维坐标系中的投影为：</li></ul><p><span class="math display">$$\begin{split}    \boldsymbol{z}_i=(z_{i1};z_{i2};\ldots;z_{id^{\prime}})\quadz_{ij}=\boldsymbol{w}_j^\mathrm{T}\boldsymbol{x}_i\end{split}$$</span></p><ul><li>基于 <spanclass="math inline"><em>z</em><sub><em>i</em></sub></span> 来重构 <spanclass="math inline"><em>x</em><sub><em>i</em></sub></span>，可以得到：<spanclass="math inline">$\hat{\boldsymbol{x}}_i=\sum_{j=1}^{d^{\prime}}z_{ij}\boldsymbol{w}_j$</span>。</li><li>由此得到原样本点和基于投影重构的样本点之间的距离为：</li></ul><p><span class="math display">$$\begin{aligned}\sum_{i=1}^m\left\|\sum_{j=1}^{d^{\prime}}z_{ij}\boldsymbol{w}_j-\boldsymbol{x}_i\right\|_2^2&amp;=\sum_{i=1}^m\boldsymbol{z}_i^\mathrm{T}\boldsymbol{z}_i-2\sum_{i=1}^m\boldsymbol{z}_i^\mathrm{T}\mathbf{W}^\mathrm{T}\boldsymbol{x}_i+\mathrm{const}\\&amp;\propto-\mathrm{tr}\left(\mathbf{W}^\mathrm{T}\left(\sum_{i=1}^mx_ix_i^\mathrm{T}\right)\mathbf{W}\right).\end{aligned}$$</span></p><ul><li>由此可得基于最近重构性的优化目标为：</li></ul><p><span class="math display">$$\begin{aligned}&amp;\min_{\mathbf{W}}\quad-\operatorname{tr}(\mathbf{W}^\mathrm{T}\mathbf{X}\mathbf{X}^\mathrm{T}\mathbf{W})\\&amp; \mathrm{s.t.}\quad\mathbf{W}^\mathrm{T}\mathbf{W}=\mathbf{I}.\end{aligned}$$</span></p><ol start="2" type="1"><li>基于最大可分性推导（更好理解）：</li></ol><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250606164844991.png" width="50%" /></p><ul><li>样本点<spanclass="math inline"><em>x</em><sub><em>i</em></sub></span>在新空间中超平面上的投影是<spanclass="math inline"><em>W</em><sup>⊤</sup><em>x</em><sub><em>i</em></sub></span>，若所有样本点的投影能尽可能分开，则应该使得投影后样本点的方差最大化<strong>（计算协方差矩阵）</strong>：</li></ul><p><span class="math display">$$\begin{split}    \sum_i\mathbf{W}^\mathrm{T}x_ix_i^\mathrm{T}\mathbf{W}\end{split}$$</span></p><ul><li>由此可以得到基于最大可分性的优化目标为：</li></ul><p><span class="math display">$$\begin{split}    \max_{\mathbf{W}}\quad\mathrm{tr}(\mathbf{W}^\mathrm{T}\mathbf{X}\mathbf{X}^\mathrm{T}\mathbf{W})\mathrm{s.t.}\quad\mathbf{W}^\mathrm{T}\mathbf{W}=\mathbf{I}.\end{split}$$</span></p><p>可以发现两种思路都是等价的。因此求解思路就是对协方差矩阵做<strong>特征值分解</strong>：</p><p><span class="math display">$$\begin{split}    C=\frac{1}{n}X^\top X\in\mathbb{R}^{D\times D}\end{split}$$</span></p><p><span class="math display">$$\begin{split}    C=U\Lambda U^\top\end{split}$$</span></p><p>其中 <span class="math inline"><em>U</em></span>的列是特征向量，表示主成分方向；<spanclass="math inline"><em>Λ</em></span>是对角矩阵，对应的特征值表示方向的方差大小。选取前 <spanclass="math inline"><em>d</em><sup>′</sup></span>个最大特征值对应的特征向量，组成<spanclass="math inline"><em>U</em><sub><em>d</em><sup>′</sup></sub></span>，最后将原始数据投影到这些方向上即可得到降维后的数据：</p><p><span class="math display">$$\begin{split}    Z=X_{\mathrm{centered}}\cdot U_{d’}\end{split}$$</span></p><p>PCA是最常用的降维方法，其在不同领域有不同的称谓，例如在人脸识别中该技术称为“特征脸”（eigenface），将前<spanclass="math inline"><em>d</em><sup>′</sup></span>个特征值对应的特征向量还原为图像可以得到：</p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250609203020531.png" width="60%" /></p><hr /><h1 id="第十一讲特征选择和稀疏学习">第十一讲、特征选择和稀疏学习</h1><div class="note primary no-icon flat"><p><strong>考点</strong>：</p><p>（<em>2018，2019，2021，2022，2023</em>）子集搜索和子集评估的三种方法（或者说特征选择的三种方法）；</p><p>（<em>2019</em>）L1范数为什么可以得到稀疏解。</p></div><h2 id="特征选择">11.1.特征选择</h2><h3 id="特征">特征</h3><p>特征：用于描述物体的属性，分为以下几类：</p><ul><li>相关特征：对当前学习任务有用的属性；</li><li>无关特征：与当前学习任务无关的属性；</li><li>冗余特征：其所包含的信息能由其他特征推演出来。</li></ul><h3 id="特征选择-1">特征选择</h3><p>目的：从给定的特征集合中选出任务相关特征的子集，不丢失重要特征。</p><ul><li>减轻维度灾难：在少量属性上构建模型；</li><li>减低学习难度：保留关键信息。</li></ul><p>可行方法：存在两个关键环节–<strong>子集搜索</strong>和<strong>子集评价</strong>。</p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250606170706055.png" width="60%" /></p><p><strong>子集搜索</strong></p><p>用贪心策略选择包含重要信息的特征子集。</p><ol type="1"><li>前向搜索：最优子集初始为空集，特征集合初始时包括所有给定特征；</li><li>后向搜索：从完整的特征集合开始，逐渐减少特征；</li><li>双向搜索：每一轮逐渐增加相关特征，同时减少无关特征。</li></ol><p><strong>子集评价</strong></p><p>选定的特征子集确定了对数据集的一个划分，样本自身的标签对应着对数据集的真实划分。通过估算这两个划分的差异，就能对特征子集进行评价，与真实划分的差异越小，说明当前特征子集越好。</p><p>这里可用信息增益来进行子集评价：特征子集 <spanclass="math inline"><em>A</em></span> 上的取值将原数据集 <spanclass="math inline"><em>D</em></span> 分为 <spanclass="math inline"><em>V</em></span> 份，每一份用 <spanclass="math inline"><em>D</em><sup><em>v</em></sup></span>表示，则特征子集 <span class="math inline"><em>A</em></span>的信息增益为</p><p><span class="math display">$$\begin{split}    \mathrm{Gain}(A)=\mathrm{Ent}(D)-\sum_{v=1}^V\frac{|D^v|}{|D|}\mathrm{Ent}(D^v)\end{split}$$</span></p><h4 id="过滤式">过滤式</h4><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250606192533368.png" width="60%" /></p><p>先用特征选择过程过滤原始数据，再用过滤后的特征来训练模型，<strong>特征选择过程与后续学习器无关</strong>。</p><p><strong>Relief</strong>方法：一个好的特征应当在同类样本之间相似，而在异类样本之间差异大。也就是说，某特征能明显区分一个样本与其“近邻异类”的差异，同时保持“近邻同类”的相似性，那这个特征就是有用的。</p><p>相关定义：</p><ul><li>猜中近邻（near-hit）：<spanclass="math inline"><em>x</em><sub><em>i</em></sub></span>的同类样本中的最近邻 <spanclass="math inline"><em>x</em><sub><em>i</em>, <em>n</em><em>h</em></sub></span>；</li><li>猜错近邻（near-miss）：<spanclass="math inline"><em>x</em><sub><em>i</em></sub></span>的异类样本中的最近邻 <spanclass="math inline"><em>x</em><sub><em>i</em>, <em>n</em><em>m</em></sub></span>。</li></ul><p>算法步骤（以二分类为例）：</p><ol type="1"><li>初始化每个特征的权重为0：</li></ol><p><span class="math display">$$\begin{split}    W_j=0\quad \text{for all } j=1,2,...,d\end{split}$$</span></p><ol start="2" type="1"><li><p>对于每个样本 <spanclass="math inline"><em>x</em><sub><em>i</em></sub></span>：</p><ul><li>找到该样本的猜中近邻<spanclass="math inline"><em>H</em><sub><em>i</em></sub></span>和猜错近邻<spanclass="math inline"><em>M</em><sub><em>i</em></sub></span></li><li>对每个特征 <spanclass="math inline"><em>f</em><sub><em>j</em></sub></span>，更新其权重，其中<spanclass="math inline">diff(<em>a</em>, <em>b</em>) = |<em>a</em> − <em>b</em>|</span>（或其他距离度量），<spanclass="math inline"><em>x</em><sub><em>i</em></sub><sup><em>j</em></sup></span>表示第<spanclass="math inline"><em>i</em></span>个样本的第<spanclass="math inline"><em>j</em></span>个特征值：</li></ul></li></ol><p><span class="math display">$$\begin{split}    W_j=0\quad \text{for all } j=1,2,...,d\end{split}$$</span></p><ol start="3" type="1"><li>最终得到每个特征的权重（也叫做相关统计量），权重越高说明该特征对分类越有用。</li></ol><p>Relief方法的时间开销随采样次数以及原始特征数线性增长，运行效率高。</p><h4 id="包裹式">包裹式</h4><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250606194820037.png" width="60%" /></p><p>包裹式选择把<strong>最终使用的学习器性能</strong>作为特征子集的评价准则。</p><p><strong>LVW</strong>（Las VegasWrapper）是一种随机化的特征选择方法，属于 wrapper 方法（包装器方法），由Liu 和 Motoda 在 1998年提出。随机采样特征子集，使用学习器对其评估准确率，如果新子集更好，就接受它为当前最优解。与Relief 评分式方法不同，LVW 是基于模型性能来“试验”出优质特征组合的。</p><p>算法步骤：重复以下步骤直到cnt达到阈值<spanclass="math inline"><em>T</em></span></p><ol type="1"><li>随机生成一个特征子集 <spanclass="math inline"><em>S</em> ⊂ <em>F</em></span>，通常会随机保留一部分特征；</li><li>使用分类器L在训练集上评估S的准确率 <spanclass="math inline"><em>a</em><em>c</em><em>c</em><sub><em>S</em></sub></span>；</li><li>若性能更好但子集更小：<ul><li>更新最优子集： <spanclass="math inline"><em>S</em><sub>best</sub> ← <em>S</em></span></li><li>更新最优准确率： <spanclass="math inline"><em>a</em><em>c</em><em>c</em><sub>best</sub> ← <em>a</em><em>c</em><em>c</em><sub><em>S</em></sub></span></li><li>计数器归零：cnt = 0</li></ul></li><li>否则计数器加1： <span class="math inline">cnt ← cnt + 1</span></li></ol><p>从最终学习器性能来看，包裹式特征选择比过滤式特征选择更好，但需多次训练学习器，计算开销通常比过滤式特征选择大得多。</p><h4 id="嵌入式">嵌入式</h4><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250606200232624.png" width="60%" /></p><p>嵌入式特征选择是将特征选择过程与学习器训练过程融为一体，两者在同一优化过程中完成，<strong>在学习器训练过程中自动地进行特征选择</strong>。</p><p>考虑最简单的线性回归–<strong>岭回归</strong>（ridgeregression），它在普通的线性回归问题（平方误差损失）上加上了L2正则化项防止过拟合：</p><p><span class="math display">$$\begin{split}    \operatorname*{min}_{\boldsymbol{w}}\sum_{i=1}^{m}(y_{i}-\boldsymbol{w}^{\top}\boldsymbol{x}_{i})^{2}+\lambda\|\boldsymbol{w}\|_{2}^{2}\end{split}$$</span></p><p>将L2正则化项替换为L1正则化项，可以得到<strong>LASSO</strong>（LeastAbsolute Shrinkage and SelectionOperator，最小绝对收缩与选择算子）：</p><p><span class="math display">$$\begin{split}    \operatorname*{min}_{\boldsymbol{w}}\sum_{i=1}^{m}(y_{i}-\boldsymbol{w}^{\top}\boldsymbol{x}_{i})^{2}+\lambda\|\boldsymbol{w}\|_{1}\end{split}$$</span></p><p>使用L1范式 LASSO 有一个特殊性质，几何上的 L1范数形成的是一个<strong>菱形约束区域</strong>，它与损失函数的等高线接触时，往往在轴（即某个<spanclass="math inline"><em>w</em><sub><em>j</em></sub> = 0</span>）上交点，导致某些权重直接为零，从而达到特征选择的目的。</p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250606200938379.png" width="70%" /></p><p>LASSO没有封闭解，常用PGD（Proximal GradientDescend，近端梯度下降）法求解。</p><h2 id="稀疏表示">11.2.稀疏表示</h2><p>稀疏表示的优势：文本数据线性可分、存储高效</p><p><em>能否将稠密表示的数据集转化为稀疏表示，使其享受上述优势？</em></p><p><strong>字典学习</strong>：为普通稠密表达的样本找到合适的字典，将样本转化为稀疏表示，用尽可能少的原子（字典中的“词条”）线性组合来近似表示数据。</p><p>给定数据矩阵 <spanclass="math inline"><strong>X</strong> ∈ ℝ<sup><em>n</em> × <em>d</em></sup></span>，我们需要得到一个字典矩阵<spanclass="math inline"><strong>D</strong> ∈ ℝ<sup><em>k</em> × <em>d</em></sup></span>（k为字典的大小），一个稀疏编码矩阵<spanclass="math inline"><strong>Z</strong> ∈ ℝ<sup><em>n</em> × <em>k</em></sup></span>，使得：</p><p><span class="math display">$$\begin{split}    X\approx ZD\end{split}$$</span></p><p>其中 <span class="math inline"><em>Z</em></span>中每一行都是稀疏的（多数值为0）。通常其优化形式为：</p><p><span class="math display">$$\begin{split}    \min_{Z,D}\|X-ZD\|_F^2+\lambda\sum_{i=1}^n\|z_i\|_1\end{split}$$</span></p><p>其中第一项表示矩阵误差，使用Frobenius范数（也可以用L2范数），第二项表示对编码矩阵非零元素个数的惩罚项。</p><p><strong>矩阵补全</strong>：从得到的部分信号,基于压缩感知的思想恢复出完整信号。其基本目标是，给定一个部分观测的矩阵（很多元素缺失），预测其缺失部分的值。通常假设<strong>矩阵具有低秩（Low-Rank）结构</strong>，即大部分信息可以由少数几个主因子表示。</p><p>最基本的优化形式可以表示为：</p><p><span class="math display">$$\begin{aligned}\min_X\operatorname{rank}(X),\quad\operatorname{s.t.}X_{ij}=M_{ij},\forall(i,j)\in\Omega\end{aligned}$$</span></p><p>其中<spanclass="math inline"><em>Ω</em></span>表示观测到的条目集合，优化目标就是恢复出的矩阵中对应元素应该与已观测到的对应元素相同，但是要尽可能保证矩阵的秩越小越好。但这是一个NP难问题，因此使用<strong>核范数</strong>（nuclearnorm，即矩阵的奇异值之和）代替秩来进行凸优化：</p><p><span class="math display">$$\begin{aligned}\min_X\|X\|_*\quad\mathrm{s.t.}X_{ij}=M_{ij},\forall(i,j)\in\Omega\end{aligned}$$</span></p><p>通常使用半正定规划（SDP，Semi-Definite Programming）求解。</p><hr /><h1 id="第十二讲半监督学习">第十二讲、半监督学习</h1><div class="note primary no-icon flat"><p><strong>考点</strong>：</p><p>（<em>2018</em>）TSVM；</p><p>（<em>2019，2022，2023</em>）图半监督学习推导中的D和能量函数、闭式解。</p></div><p>监督学习的局限：需要海量的该质量标注样本，现实世界需要获取巨大人力物力。</p><p>半监督学习：希望同时使用<strong>有标记样本</strong>和<strong>未标记样本</strong>构建泛化性能良好的模型。</p><h2 id="未标记样本">12.1.未标记样本</h2><p>未标记样本的潜在假设：</p><ul><li><strong>聚类假设</strong>（ClusteringAssumption）：假设数据存在簇结构，即同一簇的样本属于同一类别；</li><li><strong>流形假设</strong>（ManifoldAssumption）：假设数据分布在一个流形结构上，邻近的样本具有相似的输出值。</li></ul><h2 id="生成式方法">12.2.生成式方法</h2><p>回顾原型聚类中的GMM，我们使用一个高斯混合模型来建模生成概率分布。这里探讨在半监督下的GMM如何进行：</p><p>从最大化后验概率出发：</p><p><span class="math display">$$\begin{aligned}f(x)=\arg\max_{j\in\mathcal{Y}}p(y=j\mid x)\end{aligned}$$</span></p><p>由于该项很难求，因此我们引入隐变量<spanclass="math inline"><em>Θ</em></span>，将其改写为：</p><p><span class="math display">$$\begin{aligned}p(y=j\mid x)=\sum_{i=1}^kp(y=j,\Theta=i\midx)=\sum_{i=1}^kp(y=j\mid\Theta=i,x)\cdotp(\Theta=i\mid x)\end{aligned}$$</span></p><p>最终得到对数似然函数为：</p><p><span class="math display">$$\begin{aligned}\ln p(D_l\cup D_u) &amp; =\sum_{(x_j,y_j)\inD_l}\ln\left(\sum_{i=1}^k\alpha_ip(x_j\mid\mu_i,\Sigma_i)\cdotp(y_j\mid\Theta=i,x_j)\right)\\&amp; +\sum_{x_j\inD_u}\ln\left(\sum_{i=1}^k\alpha_ip(x_j\mid\mu_i,\Sigma_i)\right)\end{aligned}$$</span></p><p>其中第一项是对有标签数据的对数似然，即使有了标签<spanclass="math inline"><em>y</em><sub><em>j</em></sub></span>，仍然考虑混合成分<spanclass="math inline"><em>Θ</em></span>，这是因为<spanclass="math inline"><em>y</em><sub><em>j</em></sub></span>与<spanclass="math inline"><em>Θ</em></span>不一定一一对应，尤其当一个类别可能由多个高斯成分生成。第二项则是无标签数据的对数似然，只能使用<spanclass="math inline"><em>x</em><sub><em>j</em></sub></span>本身。之后和之前介绍GMM一样，使用EM算法求解，将高斯混合模型换成混合专家模型、朴素贝叶斯模型等,，可推出其他生成式半监督方法。</p><blockquote><p>优缺点：</p><p>优点：简单、易于实现</p><p>缺点：模型假设必须准确，即假设的生成式模型必须与真实数据分布吻合</p></blockquote><h2 id="半监督svm">12.3.半监督SVM</h2><p><strong>TSVM</strong>（Transductive Support Vector Machine）是另一种经典的半监督学习方法，但和 GMM不同，它属于判别式方法，不建模数据分布，而是直接优化分类决策边界。其核心思想是在少量标注样本的基础上，利用未标注数据来帮助调整分类边界，使其落在类簇之间（clusterassumption / low-density separation assumption）。</p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250606213437535.png" width="50%" /></p><p>回顾标准SVM的优化目标，可以总结为：</p><p><span class="math display">$$\begin{split}\min_{w,b,\xi}\frac{1}{2}\|w\|^2+C\sum_i\xi_i\quad\text{subject to}y_i(w^Tx_i+b)\geq1-\xi_i\end{split}$$</span></p><p>即在保证分类正确（或有一定松弛）的条件下，最大化间隔<spanclass="math inline">$\frac{1}{\|w\|}$</span>。而TSVM的想法是，不仅保证有标签数据分类结果良好，<strong>无标签数据还需要远离决策边界</strong>，使得边界落在两个类别的低密度区。具体来说TSVM的目标函数可总结为：</p><p><span class="math display">$$\begin{aligned}\min_{\boldsymbol{w},b,\hat{\boldsymbol{y}},\boldsymbol{\xi}} &amp;\frac{1}{2}\|\boldsymbol{w}\|_2^2+C_l\sum_{i=1}^l\xi_i+C_u\sum_{i=l+1}^m\xi_i\\\mathrm{s.t.} &amp;y_{i}(\boldsymbol{w}^{\top}\boldsymbol{x}_{i}+b)\geq1-\xi_{i},i=1,\ldots,l,\\&amp;\hat{y}_{i}(\boldsymbol{w}^{\top}\boldsymbol{x}_{i}+b)\geq1-\xi_{i},i=l+1,\ldots,m,\\&amp; \xi_{i}\geq0,i=1,\ldots,m,\end{aligned}$$</span></p><p>其算法过程可总结为：</p><ol type="1"><li>用有标签样本 <spanclass="math inline"><em>D</em><sub><em>l</em></sub></span> 训练一个标准SVM；</li><li>用 SVM 对无标记样本 D_u 中样本进行预测，得到初始伪标签 <spanclass="math inline"><em>ȳ</em><sub><em>i</em></sub> ∈ {−1, +1}</span>；</li><li>初始化惩罚项权重，<spanclass="math inline"><em>C</em><sub><em>u</em></sub> &lt; <em>C</em><sub><em>l</em></sub></span>，其中无标签样本的惩罚项很小，以避免伪标签不准时影响模型太大；</li><li>之后逐步迭代优化求解TSVM的优化问题（求解TSVM的目标函数），具体来说一般是交替优化SVM；</li><li>在这期间会检查为标签是否严重违反了边界合理性，即 <spanclass="math inline"><em>y</em>̂<sub><em>i</em></sub><em>y</em>̂<sub><em>j</em></sub> &lt; 0, <em>ξ</em><sub><em>i</em></sub> &gt; 0, <em>ξ</em><sub><em>j</em></sub> &gt; 0, <em>ξ</em><sub><em>i</em></sub> + <em>ξ</em><sub><em>j</em></sub> &gt; 2</span>，这表示这些伪标签可能是错的，我们可以尝试交换伪标签看看能否减少目标函数值；</li><li>逐步提升无标签样本的惩罚力度：<spanclass="math inline"><em>C</em><sub><em>u</em></sub> = <em>m</em><em>i</em><em>n</em>(2<em>C</em><sub><em>u</em></sub>, <em>C</em><sub><em>l</em></sub>)</span>。</li></ol><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250607104946619.png" width="80%" /></p><p>但在未标记样本进行标记指派及调整的过程中，可能会出现类别不平衡问题（某类的样本远多于另一类），为了减轻类别不平衡所造成的不利影响，可将优化目标中的<spanclass="math inline"><em>C</em><sub><em>u</em></sub></span>拆分为<spanclass="math inline"><em>C</em><sub><em>u</em></sub><sup>+</sup></span>与<spanclass="math inline"><em>C</em><sub><em>u</em></sub><sup>−</sup></span>：</p><p><span class="math display">$$\begin{split}    C_u^+=\frac{u_-}{u_+}C_u^-\end{split}$$</span></p><h2 id="图半监督学习">12.4.图半监督学习</h2><p>给定一个数据集，我们可以将其映射为一个图，数据集中的每个样本对应图中的一个结点，若两个样本之间的相似度很高，则对应结点之间存在一条边，边的强度正比于样本间的相似度。我们将有标记样本对应的结点看作<strong>染过色</strong>，未标记样本所对应的结点<strong>尚未染色</strong>，于是半监督学习可以看作是图染色问题。</p><p>算法过程（以二分类为例）：</p><ol type="1"><li>基于数据集 <spanclass="math inline"><em>D</em><sub><em>l</em></sub> ∪ <em>D</em><sub><em>u</em></sub></span>构建一个图 <spanclass="math inline"><em>G</em> = (<em>V</em>, <em>E</em>)</span>。<ul><li>其中结点集为：</li></ul></li></ol><p><span class="math display">$$\begin{split}    V=\{x_1,...,x_l,x_{l+1},...,x_{l+u}\}\end{split}$$</span></p><ul><li>边集<spanclass="math inline"><em>E</em></span>可表示为一个亲和矩阵（affinitymatrix），基于高斯函数定义为：</li></ul><p><span class="math display">$$\left.\mathbf{W}_{ij}=\left\{\begin{array}{cc}\exp\left(\frac{-\|x_i-x_j\|_2^2}{2\sigma^2}\right), &amp;\mathrm{if}\quad i\neq j; \\\\0 ,&amp; \mathrm{otherwise};\end{array}\right.\right.$$</span></p><ol start="2" type="1"><li>假定从图 <spanclass="math inline"><em>G</em> = (<em>V</em>, <em>E</em>)</span>将学得一个实值函数 <spanclass="math inline"><em>f</em> : <em>V</em> → ℝ</span>，对每个样本输出一个分类概率得分。直观上相似的样本应该具有相似的标记，于是可以得到关于<spanclass="math inline"><em>f</em></span>的<strong>能量函数</strong>：</li></ol><p><span class="math display">$$\begin{aligned}&amp;E(f)={\frac{1}{2}}\sum_{i=1}^{m}\sum_{j=1}^{m}{\boldsymbol{W}_{ij}}(f({\boldsymbol{x}_{i}})-f({\boldsymbol{x}_{j}}))^{2}\\&amp;=\frac{1}{2}\left(\sum_{i=1}^{m}d_{i}f^{2}(\boldsymbol{x}_{i})+\sum_{j=1}^{m}d_{j}f^{2}(\boldsymbol{x}_{j})-2\sum_{i=1}^{m}\sum_{j=1}^{m}\boldsymbol{W}_{ij}f(\boldsymbol{x}_{i})f(\boldsymbol{x}_{j})\right)\\&amp; =f^{T}(D-W)\boldsymbol{f}\end{aligned}$$</span></p><p><span class="math display">$$\begin{split}    D=\mathrm{diag}(d_i),d_i=\sum_{j=1}^{l+u}(W_{ij})\end{split}$$</span></p><ol start="3" type="1"><li>采用分块矩阵的表示方式：</li></ol><p><span class="math display">$$\begin{split}    E(f)=(\boldsymbol{f}_l^\top\boldsymbol{f}_u^\top)\left(\begin{bmatrix}\mathbf{D}_{ll} &amp; \mathbf{0}_{lu} \\\mathbf{0}_{ul} &amp; \mathbf{D}_{uu}\end{bmatrix}-\begin{bmatrix}\mathbf{W}_{ll} &amp; \mathbf{W}_{lu} \\\mathbf{W}_{ul} &amp; \mathbf{W}_{uu}\end{bmatrix}\right)\begin{bmatrix}f_l \\\mathbf{f}_u\end{bmatrix} \\=\boldsymbol{f}_l^\top(\mathbf{D}_l-\mathbf{W}_{ll})\boldsymbol{f}_l-2\boldsymbol{f}_u^\top\mathbf{W}_{ul}\boldsymbol{f}_l+\boldsymbol{f}_u^\top(\mathbf{D}_{uu}-\mathbf{W}_{uu})\boldsymbol{f}_u.\end{split}$$</span></p><p>通过<span class="math inline">$\frac{\partial E(f)}{\partialf_u}=0$</span>，可以得到：</p><p><span class="math display">$$\begin{aligned}f_{u} &amp;=(D_{uu}(\boldsymbol{I}-\boldsymbol{D}_{uu}^{-1}\boldsymbol{W}_{uu}))^{-1}\boldsymbol{W}_{ul}\boldsymbol{f}_{l}\\&amp;=(\boldsymbol{I}-\boldsymbol{D}_{uu}^{-1}\boldsymbol{W}_{uu})^{-1}\boldsymbol{D}_{uu}^{-1}\boldsymbol{W}_{ul}\boldsymbol{f}_{l}\\&amp;=(\boldsymbol{I}-\boldsymbol{P}_{uu})^{-1}\boldsymbol{P}_{ul}\boldsymbol{f}_l\end{aligned}$$</span></p><p><span class="math display">$$\begin{split}    P=D^{-1}W\end{split}$$</span></p><blockquote><p>优缺点：</p><p>图半监督学习方法在概念上相当清晰，易于通过对所涉矩阵运算的分析来探索算法性质</p><p>但存储开销高，且构图过程仅能考虑训练样本集，难以判知新样本在图中的位置</p></blockquote><h2 id="基于分歧的方法">12.5.基于分歧的方法</h2><p>基于分歧的方法（disagreement-basedmethods）使用多学习器，而学习器之间的分歧对未标记数据的利用很重要。协同训练（co-training）是基于分歧方法的重要代表，其最初针对多视图（multi-view）数据设计，</p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250607134045187.png" width="50%" /></p><p>协同训练很好地利用了多视角的相容互补性，但它需要假设<strong>数据拥有两个充分且条件独立的视图</strong>。因此此后出现了一些能在单视图数据上使用的变体算法，可以使用<em>不同的学习算法</em>、或<em>不同的数据采样</em>、或<em>不同的参数设置</em>来产生不同的学习器，也能有效利用未标记样本提升性能。</p><p>后续研究发现此类算法事实上无需数据拥有多视图，仅需弱学习器之间具有显著的分歧（差异），即可通过相互提供伪标记样本的方式来提高泛化性能。</p><blockquote><p>优缺点：</p><p>基于分歧的方法只需采用合适的基学习器，就能较少受到模型假设、损失函数非凸性和数据规模等问题的影响，学习方法简单有效、理论基础相对坚实、适用范围广泛。</p><p>但为了使用这类方法，需要生成具有显著分歧、性能尚可的多个学习器，但当有标记样本很少、尤其数据不具有多视图时，很难实现这一点。</p></blockquote><h2 id="半监督聚类">12.6.半监督聚类</h2><p>聚类是一种典型的无监督学习任务，但有时我们往往能获得一些额外的监督信息，这时我们可以使用半监督聚类（semi-supervisedclustering）来使用监督信息以获得更好的聚类效果。</p><p>聚类任务中获得的监督信息大致有两种类型：</p><ol type="1"><li><strong>必连（must-link）与勿连（cannot-link）约束</strong>：前者是指样本必属于同一个簇，后者是指样本必不属于同一个簇；</li><li><strong>有少量的有标记样本</strong>。</li></ol><h3 id="约束k均值">约束k均值</h3><p><strong>约束k均值</strong>（constrainedk-means）算法是利用第一类监督信息的代表，该算法是k-means算法的扩展，它在聚类过程中要确保必连和勿连关系集合中的约束得以满足，否则将返回错误提示。</p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250607135958008.png" width="60%" /></p><h3 id="约束种子k均值">约束种子k均值</h3><p><strong>约束种子k均值</strong>（constrained seedk-means）算法使用少量有标记样本，即假设少量有标记样本属于<spanclass="math inline"><em>k</em></span>个聚类簇。这样的监督信息利用起来很容易：直接将有标签样本作为种子，用他们初始化k均值算法的k个聚类中心，在聚类粗迭代更新过程中不改变种子样本的簇隶属关系，这样就得到了约束种子k均值算法。</p><h1 id="考题回忆">2025考题回忆</h1><ol type="1"><li>什么是机器学习？目前机器学习的稳健性存在问题，请举出一两个例子来说明。</li><li>说明什么是过拟合和欠拟合，说明二者产生的原因并以具体模型为例子说明如何避免。</li><li>推导带正则化项的线性回归闭式解，并说明线性模型的优缺点。</li><li>决策树的两种选择类型，并说明偏好。</li><li>SVM的目标？并用基本型推导至对偶型。引入核函数，说明核函数的好处。</li><li>PCA有两种性质，选择一种说明，并利用它推导PCA。</li><li>生成式和判别式模型的区别？并举例子。</li><li>密度聚类和层次聚类的具体代表算法？说明步骤并说明各自依赖什么假设。</li><li>多层前馈网络的学习能力？局限与解决办法。</li><li>图半监督学习的能量函数？推导闭式解。</li><li>LVM是哪种特征选择？说明三种特征选择的区别。</li></ol><p>简单来说就是基本全是原题。@…@</p>]]></content>
      
      
      <categories>
          
          <category> 研究生课程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 课程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MAR的细节</title>
      <link href="/2025/05/20/AIGC/MAR/"/>
      <url>/2025/05/20/AIGC/MAR/</url>
      
        <content type="html"><![CDATA[<p><strong>系列文章：</strong></p><ol class="series-items"><li><a href="/2025/05/20/AIGC/MAR/" title="MAR的细节">MAR的细节</a></li></ol><blockquote><p>原论文：<ahref="https://proceedings.neurips.cc/paper_files/paper/2024/hash/66e226469f20625aaebddbe47f0ca997-Abstract-Conference.html">AutoregressiveImage Generation without Vector Quantization</a></p><p>源码：<ahref="https://github.com/LTH14/mar">https://github.com/LTH14/mar</a></p></blockquote><p>同时参考了<ahref="https://zhuanlan.zhihu.com/p/711930343">知乎大佬的博客</a>辅助理解。</p><h1 id="背景">1.背景</h1><p>自回归模型是目前自然语言处理中生成模型的基础，这些模型使用输入的前一个词预测序列中下一个词，考虑到语言的离散性，模型的输入和输出都表示在离散空间中，这种流行的做法使得大家普遍认为自回归模型就一定要使用到离散表征（其中在图像生成中通常使用VQ）。因此本工作旨在解决这个问题，即<strong>自回归模型是否一定要与向量量化（VQ）相结合</strong>？</p><p>本文注意到，其实自回归强调的”predicting next tokens based on previousones”跟值是离散还是连续无关，真正重要的其实是得到每个token的概率分布，使用离散值表示能很好使用类别分布（CategoriesDistribution）建模得到，但是这并不代表这就是必要的，如果能找到另一种建模方式，就可以不使用VQ来进行自回归生成。</p><blockquote><p><strong>补充</strong>：为什么要代替VQ呢</p><p>VQTokenizer训练不稳定，在训练过程中由于使用了<strong>argmin</strong>这种不可导的计算，导致在梯度回传的过程中是直接将量化向量梯度复制给encode向量的，这个梯度不准确，因此往往导致不准确的训练。</p></blockquote><p>本文尝试在连续值域上使用扩散模型来为每个token的概率分布建模，为每个token预测一个向量<spanclass="math inline"><em>z</em></span>，作为去噪网络的条件嵌入，输出下一个token的概率分布<spanclass="math inline"><em>p</em>(<em>x</em>|<em>z</em>)</span>。不仅如此，本文进一步将AR模型和掩码生成模型结合在一起，提出了掩码自回归模型（MAR）。</p><h1 id="method">2.Method</h1><h2 id="回顾离散tokens的做法">2.1.回顾离散tokens的做法</h2><p>在自回归生成任务中，如果采用一个离散的tokenizer来预测下一个<spanclass="math inline"><em>x</em></span>，那么<spanclass="math inline"><em>x</em></span>将被表示为一个索引 <spanclass="math inline">0 ≤ <em>x</em> &lt; <em>K</em></span>用于在词汇表（大小为<spanclass="math inline"><em>K</em></span>）中查找对应的表示。此时自回归模型将生成一个<spanclass="math inline"><em>D</em></span>维的向量<spanclass="math inline"><em>z</em></span> ，之后将通过与<spanclass="math inline"><em>K</em></span>分类矩阵 <spanclass="math inline"><em>W</em> ∈ ℝ<sup><em>K</em> × <em>D</em></sup></span>相乘得到属于每个分类的概率 <spanclass="math inline"><em>p</em>(<em>x</em>|<em>z</em>) = <em>s</em><em>o</em><em>f</em><em>t</em><em>m</em><em>a</em><em>x</em>(<em>W</em><em>z</em>)</span>，取最大概率的词汇得到 <span class="math inline"><em>x</em></span>。</p><p>回顾这个做法，其中有两点很重要：</p><ul><li>需要有一个能衡量预测分布和真实分布之间差异的<strong>损失函数</strong>；</li><li>在推理时有一个能从分布 <spanclass="math inline"><em>x</em> ∼ <em>p</em>(<em>x</em>|<em>z</em>)</span>中采样的<strong>采样器</strong>。</li></ul><p>由此可以发现，其实自回归模型不一定要用离散表征（VQ），对概率分布的建模才是最重要的。</p><h2 id="diffusion-loss">2.2.Diffusion Loss</h2><h3 id="loss-function">2.2.1.Loss Function</h3><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/20250521143240.png" width="50%"/></p><p>本文使用了扩散模型来建模每个token的概率分布 <spanclass="math inline"><em>p</em>(<em>x</em>|<em>z</em>)</span>，而用自回归模型来生成每个token的条件向量 <spanclass="math inline"><em>z</em></span>（如上图），具体来说本文使用的DiffusionLoss可表述为：</p><p><span class="math display">$$\begin{split}    \mathcal{L}(z,x)=\mathbb{E}_{\varepsilon,t}\left[\left\|\varepsilon-\varepsilon_\theta(x_t|t,z)\right\|^2\right].\end{split}$$</span></p><p>其中需要注意的是，这里的噪声预测模型<spanclass="math inline"><em>ε</em><sub><em>θ</em></sub></span>使用的是一个小型的MLP，所以计算开销不会过大。这里的条件<spanclass="math inline"><em>z</em></span>是用自回归网络 <spanclass="math inline"><em>z</em> = <em>f</em>(⋅)</span>生成的，并且由于使用连续表示，这里 <spanclass="math inline"><em>z</em></span>的梯度也是可以通过上面的损失函数进行传播的，所以使用DiffusionLoss也能同步训练自回归网络 <spanclass="math inline"><em>f</em>(⋅)</span> 。</p><p>在训练时，作者提到了一个小trick，Diffusion Loss中的期望值 <spanclass="math inline">𝔼<sub><em>ε</em>, <em>t</em></sub></span>是对时间步<spanclass="math inline"><em>t</em></span>的期望，因此为了训练的稳定，需要在同一个给定的<spanclass="math inline"><em>z</em></span>下对时间步<spanclass="math inline"><em>t</em></span>采样多次，而不需要重新计算<spanclass="math inline"><em>z</em></span>，在实验中设置<spanclass="math inline"><em>t</em></span>地重复采样次数为4。</p><h3 id="采样">2.2.2.采样</h3><p>这里的采样就是扩散模型中的采样过程，即：</p><p><span class="math display">$$\begin{split}    x_{t-1}=\frac{1}{\sqrt{\alpha_t}}\left(x_t-\frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}_t}}\varepsilon_\theta(x_t|t,z)\right)+\sigma_t\delta.\end{split}$$</span></p><p>同时，参考openAI的CG（2021，Diffusion BeatsGan）中提出的温度控制，这里引入了温度因子<spanclass="math inline"><em>τ</em></span>来控制采样的随机性（多样性）。具体来说，本文采用的方式是对噪声方差<spanclass="math inline"><em>σ</em><sub><em>t</em></sub><em>δ</em></span>乘上<spanclass="math inline"><em>τ</em></span>来控制采样的多样性。</p><h2 id="引入自回归模型">2.3.引入自回归模型</h2><p>对于一个tokens序列<spanclass="math inline">{<em>x</em><sup>1</sup>, <em>x</em><sup>2</sup>, ..., <em>x</em><sup><em>n</em></sup>}</span>（以某种规定的顺序排序），自回归模型的建模可以表示为：</p><p><span class="math display">$$\begin{split}    p\left(x^1, \ldots, x^n\right)=\prod_{i=1}^n p\left(x^i \mid x^1,\ldots, x^{i-1}\right)\end{split}$$</span></p><p>这里定义的<spanclass="math inline"><em>x</em><sup><em>i</em></sup></span>可以定义在连续空间中，并且其概率分布是用扩散模型来建模的。而条件向量<spanclass="math inline"><em>z</em><sup><em>i</em></sup></span>则是通过一个自回归网络生成的：<spanclass="math inline"><em>z</em><sup><em>i</em></sup> = <em>f</em>(<em>x</em><sup>1</sup>, …, <em>x</em><sup><em>i</em> − 1</sup>)</span>。</p><h2 id="masked-generative-models">2.4.Masked Generative Models</h2><h3 id="mae">2.4.1.MAE</h3><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/bi-attention.png" width="50%"/></p><p>目前大多数的自回归生成采用<strong>因果注意力</strong>（CasualAttention）作为自回归架构（即用前面的预测后面的），但是这种单向的约束其实并不完全符合自回归的定义（即用已知tokens预测未知tokens），因此作者认为应该选择<strong>双向注意力</strong>（BidirectionalAttention）作为自回归架构。进一步说，作者选择了<strong>MAE</strong>（MaskedAutoencoder）实现双向注意力，基于未Masked的tokens预测一批masked的tokens。</p><h3 id="随机排序">2.4.2.随机排序</h3><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/MAR_random_order.png" width="50%"/></p><p>本文使用<strong>随机排序</strong>定义token序列，这其实也符合MAE的思想，每个样本对应的tokens序列都是随机的。但在解码阶段，MAE添加了额外的positionalembedding来指定要预测的token。</p><p>并且在掩码生成建模中，模型会基于已知的tokens生成一系列tokens而不是仅仅预测一个token，即<spanclass="math inline"><em>p</em>({<em>x</em><sup><em>i</em></sup>, <em>x</em><sup><em>i</em> + 1</sup>..., <em>x</em><sup><em>j</em></sup>} ∣ <em>x</em><sup>1</sup>, ..., <em>x</em><sup><em>i</em> − 1</sup>)</span>，其中 <spanclass="math inline"><em>i</em> ≤ <em>j</em></span>。相当于每步预测时同步预测好几个tokens，这样可以加速生成过程，作者称其为”nextset-of-tokens prediction”。</p><h1 id="实现细节">3.实现细节</h1><h2 id="diffusion-loss-1">3.1.Diffusion Loss</h2><p>作者使用的扩散模型基于iDDPM，使用余弦噪声调度方案，并可以引入<spanclass="math inline">ℒ<sub><em>v</em><em>l</em><em>b</em></sub></span>，训练过程使用CFG（Classifier-freeGuidance）进行训练。噪声预测网络使用多层ResBlock进行构建，条件向量<spanclass="math inline"><em>z</em></span>和<spanclass="math inline"><em>t</em></span>是通过AdaLN嵌入（具体可参考DiT）来进行条件控制的。</p><h2 id="ar-and-mar">3.2.AR and MAR</h2><p>作者使用的tokenizer来自于LDM，包括VQ-16和KL-16，Transformer的架构参考ViT，因果注意力的实现来自于GPT。</p><h2 id="总结">3.3.总结</h2><p>具体来说，MAR的实现细节可分为两个阶段：</p><h3 id="训练阶段">3.3.1.训练阶段</h3><ul><li><strong>From pixel tolatent</strong>：首先对于输入图像，其实需要先过一层预训练的VAE从像素空间（pixel）转换为隐空间（latent）；</li><li><strong>Patchify</strong>：接着和ViT类似将其划分为patches，从<spanclass="math inline">ℝ<sup><em>b</em>, <em>c</em>, <em>h</em>, <em>w</em></sup></span>reshape为<spanclass="math inline">ℝ<sup><em>b</em>, <em>l</em>, <em>d</em></sup></span>，这就是定义在<strong>连续空间上的imagetokens</strong>；</li><li><strong>Masking</strong>：得到tokens后，需要mask一部分tokens进行训练，这里作者定义的maskingratio范围为 <spanclass="math inline">[0.7, 1]</span>。需要注意的是，在同一个batch中的maskingratio是相同的，但是采样的masked tokens是不同的；</li><li><strong>MAE</strong>：在编码解码之前，需要为tokens加上<strong>位置编码</strong>，maskedtokens表示为<code>[m]</code>，同时为了避免unmasked的tokens序列过短，作者在tokens序列开头扩充了64个<code>[cls]</code>tokens，并且这个类别tokens也可以适应CFG的训练；</li><li><strong>Diffusion</strong>：根据输入的gt latent <spanclass="math inline"><em>x</em><sub>0</sub></span> 和MAE解码出来的条件<span class="math inline"><em>z</em></span>可以训练扩散模型，具体来说采样一个时间步<spanclass="math inline"><em>t</em></span>和一个噪音<spanclass="math inline"><em>ε</em></span>，可以得到<spanclass="math inline"><em>x</em><sub><em>t</em></sub></span>，通过噪声预测网络得到预测噪声<spanclass="math inline"><em>ε</em><sub><em>θ</em></sub></span>，通过计算<spanclass="math inline"><em>ε</em></span>和<spanclass="math inline"><em>ε</em><sub><em>θ</em></sub></span>的MSE损失进行训练。</li></ul><h3 id="推理阶段">3.3.2.推理阶段</h3><ul><li><strong>Mask</strong>：在推理阶段，作者设置了<strong>余弦衰减的maskratio调度曲线</strong>（从1.0到0），并设置64步采样。i.e.假设指定的随机token顺序为<spanclass="math inline">[10, 18, 23, 8, 39, 28, 30, 60...]</span>，那么生成的顺序可以是第一步根据<spanclass="math inline">[[<em>c</em><em>l</em><em>s</em>], ..., [<em>c</em><em>l</em><em>s</em>]]</span>生成<spanclass="math inline">[10, 18, 23]</span>，第二步根据<spanclass="math inline">[[<em>c</em><em>l</em><em>s</em>], ..., [<em>c</em><em>l</em><em>s</em>], 10, 18, 23]</span>生成<spanclass="math inline">[8, 39, 28, 30]</span>，之后mask放开得越来越多，生成的token也越来越多；</li><li><strong>CFG</strong>：由于在训练时使用Classifier-freeGuidance进行训练，因此MAR可以实现无条件生成和条件生成。在条件生成时，给定条件类别label，会生成<strong>classembedding</strong>，同时与<strong>fakelatent</strong>进行拼接，这是因为CFG在进行条件生成时需要同时预测条件噪声与无条件噪声（<spanclass="math inline">∇<sub><em>x</em><sub><em>t</em></sub></sub>log <em>P</em>(<em>x</em><sub><em>t</em></sub>|<em>y</em>) = <em>λ</em>∇<sub><em>x</em><sub><em>t</em></sub></sub>log <em>P</em>(<em>x</em><sub><em>t</em></sub>|<em>y</em>) + (1 − <em>λ</em>)∇<sub><em>x</em><sub><em>t</em></sub></sub>log <em>P</em>(<em>x</em><sub><em>t</em></sub>)</span>），然后作为64个<code>[cls]</code>喂给MAEEncoder；</li><li><strong>DiffusionSample</strong>：每步MAE生成后，根据对应要生成tokens的条件信息<spanclass="math inline"><em>z</em></span>，使用Diffusion的逆向过程进行采样即可；</li><li><strong>From latent topixel</strong>：生成所有tokens后，此时其维度为<spanclass="math inline">ℝ<sup><em>b</em> × <em>l</em> × <em>d</em></sup></span>，首先将其unpatchify回图像维度，然后通过预训练VAE的Decoder回到原Pixel空间。</li></ul><h1 id="源码">4.源码</h1><p>mar的模型定义在<code>models\mar.py</code>中，从<code>forward</code>函数中入手：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MAR</span>(nn.Module):</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, imgs, labels</span>):</span><br><span class="line">        <span class="comment"># class embed</span></span><br><span class="line">        class_embedding = <span class="variable language_">self</span>.class_emb(labels)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># patchify and mask (drop) tokens</span></span><br><span class="line">        x = <span class="variable language_">self</span>.patchify(imgs)</span><br><span class="line">        gt_latents = x.clone().detach()</span><br><span class="line">        orders = <span class="variable language_">self</span>.sample_orders(bsz=x.size(<span class="number">0</span>))</span><br><span class="line">        mask = <span class="variable language_">self</span>.random_masking(x, orders)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># mae encoder</span></span><br><span class="line">        x = <span class="variable language_">self</span>.forward_mae_encoder(x, mask, class_embedding)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># mae decoder</span></span><br><span class="line">        z = <span class="variable language_">self</span>.forward_mae_decoder(x, mask)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># diffloss</span></span><br><span class="line">        loss = <span class="variable language_">self</span>.forward_loss(z=z, target=gt_latents, mask=mask)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><h2 id="patchify-mask">4.1.Patchify &amp; Mask</h2><p>第一步对类别的embedding，源码直接使用<code>nn.embedding</code>来处理，即<code>self.class_emb = nn.Embedding(class_num, encoder_embed_dim)</code>。第二步<code>patchify</code>的做法很直观，就是和ViT一样将其划分为patches：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">patchify</span>(<span class="params">self, x</span>):</span><br><span class="line">    bsz, c, h, w = x.shape</span><br><span class="line">    p = <span class="variable language_">self</span>.patch_size</span><br><span class="line">    h_, w_ = h // p, w // p</span><br><span class="line"></span><br><span class="line">    x = x.reshape(bsz, c, h_, p, w_, p)</span><br><span class="line">    x = torch.einsum(<span class="string">&#x27;nchpwq-&gt;nhwcpq&#x27;</span>, x)</span><br><span class="line">    x = x.reshape(bsz, h_ * w_, c * p ** <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> x  <span class="comment"># [n, l, d]</span></span><br></pre></td></tr></table></figure><p>接着后续的mask部分分两步，首先使用<code>sample_orders()</code>打乱顺序，然后使用<code>random_masking()</code>来生成tokenmask，这里的mask ratio是通过截断正态分布生成的，范围在<spanclass="math inline">[0.75, 1]</span>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">sample_orders</span>(<span class="params">self, bsz</span>):</span><br><span class="line">    <span class="comment"># generate a batch of random generation orders</span></span><br><span class="line">    orders = []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(bsz):</span><br><span class="line">        order = np.array(<span class="built_in">list</span>(<span class="built_in">range</span>(<span class="variable language_">self</span>.seq_len)))</span><br><span class="line">        np.random.shuffle(order)</span><br><span class="line">        orders.append(order)</span><br><span class="line">    orders = torch.Tensor(np.array(orders)).cuda().long()</span><br><span class="line">    <span class="keyword">return</span> orders</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">random_masking</span>(<span class="params">self, x, orders</span>):</span><br><span class="line">    <span class="comment"># generate token mask</span></span><br><span class="line">    bsz, seq_len, embed_dim = x.shape</span><br><span class="line">    mask_rate = <span class="variable language_">self</span>.mask_ratio_generator.rvs(<span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">    num_masked_tokens = <span class="built_in">int</span>(np.ceil(seq_len * mask_rate))</span><br><span class="line">    mask = torch.zeros(bsz, seq_len, device=x.device)</span><br><span class="line">    mask = torch.scatter(mask, dim=-<span class="number">1</span>, index=orders[:, :num_masked_tokens],</span><br><span class="line">                         src=torch.ones(bsz, seq_len, device=x.device))</span><br><span class="line">    <span class="keyword">return</span> mask</span><br></pre></td></tr></table></figure><h2 id="mae-encode-decode">4.2.MAE Encode &amp; Decode</h2><p>首先是Encoder部分：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward_mae_encoder</span>(<span class="params">self, x, mask, class_embedding</span>):</span><br><span class="line">    x = <span class="variable language_">self</span>.z_proj(x)</span><br><span class="line">    bsz, seq_len, embed_dim = x.shape</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 提前预留出64个[cls]tokens的位置，并且也为mask向量预留出同样的位置</span></span><br><span class="line">    x = torch.cat([torch.zeros(bsz, <span class="variable language_">self</span>.buffer_size, embed_dim, device=x.device), x], dim=<span class="number">1</span>)</span><br><span class="line">    mask_with_buffer = torch.cat([torch.zeros(x.size(<span class="number">0</span>), <span class="variable language_">self</span>.buffer_size, device=x.device), mask], dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># CFG训练 此处的fake_latent表示无条件生成</span></span><br><span class="line">    <span class="keyword">if</span> <span class="variable language_">self</span>.training:</span><br><span class="line">        drop_latent_mask = torch.rand(bsz) &lt; <span class="variable language_">self</span>.label_drop_prob</span><br><span class="line">        drop_latent_mask = drop_latent_mask.unsqueeze(-<span class="number">1</span>).cuda().to(x.dtype)</span><br><span class="line">        class_embedding = drop_latent_mask * <span class="variable language_">self</span>.fake_latent + (<span class="number">1</span> - drop_latent_mask) * class_embedding</span><br><span class="line"></span><br><span class="line">    x[:, :<span class="variable language_">self</span>.buffer_size] = class_embedding.unsqueeze(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 引入可学习的位置编码</span></span><br><span class="line">    x = x + <span class="variable language_">self</span>.encoder_pos_embed_learned</span><br><span class="line">    x = <span class="variable language_">self</span>.z_proj_ln(x)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 只保留unmask的部分</span></span><br><span class="line">    x = x[(<span class="number">1</span>-mask_with_buffer).nonzero(as_tuple=<span class="literal">True</span>)].reshape(bsz, -<span class="number">1</span>, embed_dim)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># transformer encoder</span></span><br><span class="line">    <span class="keyword">if</span> <span class="variable language_">self</span>.grad_checkpointing <span class="keyword">and</span> <span class="keyword">not</span> torch.jit.is_scripting():</span><br><span class="line">        <span class="keyword">for</span> block <span class="keyword">in</span> <span class="variable language_">self</span>.encoder_blocks:</span><br><span class="line">            x = checkpoint(block, x)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">for</span> block <span class="keyword">in</span> <span class="variable language_">self</span>.encoder_blocks:</span><br><span class="line">            x = block(x)</span><br><span class="line">    x = <span class="variable language_">self</span>.encoder_norm(x)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><p>接着是Decoder部分：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward_mae_decoder</span>(<span class="params">self, x, mask</span>):</span><br><span class="line">    x = <span class="variable language_">self</span>.decoder_embed(x)</span><br><span class="line">    mask_with_buffer = torch.cat([torch.zeros(x.size(<span class="number">0</span>), <span class="variable language_">self</span>.buffer_size, device=x.device), mask], dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 和Encode一样填充前面的[cls]</span></span><br><span class="line">    mask_tokens = <span class="variable language_">self</span>.mask_token.repeat(mask_with_buffer.shape[<span class="number">0</span>], mask_with_buffer.shape[<span class="number">1</span>], <span class="number">1</span>).to(x.dtype)</span><br><span class="line">    x_after_pad = mask_tokens.clone()</span><br><span class="line">    x_after_pad[(<span class="number">1</span> - mask_with_buffer).nonzero(as_tuple=<span class="literal">True</span>)] = x.reshape(x.shape[<span class="number">0</span>] * x.shape[<span class="number">1</span>], x.shape[<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># decoder position embedding</span></span><br><span class="line">    x = x_after_pad + <span class="variable language_">self</span>.decoder_pos_embed_learned</span><br><span class="line"></span><br><span class="line">    <span class="comment"># apply Transformer blocks</span></span><br><span class="line">    <span class="keyword">if</span> <span class="variable language_">self</span>.grad_checkpointing <span class="keyword">and</span> <span class="keyword">not</span> torch.jit.is_scripting():</span><br><span class="line">        <span class="keyword">for</span> block <span class="keyword">in</span> <span class="variable language_">self</span>.decoder_blocks:</span><br><span class="line">            x = checkpoint(block, x)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">for</span> block <span class="keyword">in</span> <span class="variable language_">self</span>.decoder_blocks:</span><br><span class="line">            x = block(x)</span><br><span class="line">    x = <span class="variable language_">self</span>.decoder_norm(x)</span><br><span class="line"></span><br><span class="line">    x = x[:, <span class="variable language_">self</span>.buffer_size:]</span><br><span class="line">    x = x + <span class="variable language_">self</span>.diffusion_pos_embed_learned</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><h2 id="diffusion-loss-2">4.3.Diffusion Loss</h2><p>关于Diffusion Loss的计算，这里使用的是iDDPM的计算，即需要加上<spanclass="math inline">ℒ<sub><em>v</em><em>l</em><em>b</em></sub></span>，并使用预测方差，具体的diffusion部分可参考<ahref="https://litchi-lee.github.io/2025/05/08/AIGC/iDDPM/">iDDPM</a>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MAR</span>(nn.Module):</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward_loss</span>(<span class="params">self, z, target, mask</span>):</span><br><span class="line">        bsz, seq_len, _ = target.shape</span><br><span class="line">        target = target.reshape(bsz * seq_len, -<span class="number">1</span>).repeat(<span class="variable language_">self</span>.diffusion_batch_mul, <span class="number">1</span>)</span><br><span class="line">        z = z.reshape(bsz*seq_len, -<span class="number">1</span>).repeat(<span class="variable language_">self</span>.diffusion_batch_mul, <span class="number">1</span>)</span><br><span class="line">        mask = mask.reshape(bsz*seq_len).repeat(<span class="variable language_">self</span>.diffusion_batch_mul)</span><br><span class="line">        loss = <span class="variable language_">self</span>.diffloss(z=z, target=target, mask=mask)</span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DiffLoss</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, target, z, mask=<span class="literal">None</span></span>):</span><br><span class="line">        t = torch.randint(<span class="number">0</span>, <span class="variable language_">self</span>.train_diffusion.num_timesteps, (target.shape[<span class="number">0</span>],), device=target.device)</span><br><span class="line">        model_kwargs = <span class="built_in">dict</span>(c=z)</span><br><span class="line">        loss_dict = <span class="variable language_">self</span>.train_diffusion.training_losses(<span class="variable language_">self</span>.net, target, t, model_kwargs)</span><br><span class="line">        loss = loss_dict[<span class="string">&quot;loss&quot;</span>]</span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            loss = (loss * mask).<span class="built_in">sum</span>() / mask.<span class="built_in">sum</span>()</span><br><span class="line">        <span class="keyword">return</span> loss.mean()</span><br></pre></td></tr></table></figure><p>这里不太一样的是噪声预测网络的实现，本文使用的是一个简单的MLP，条件则是通过AdaLN进行控制的（类似于<ahref="https://litchi-lee.github.io/2025/05/08/AIGC/DIT/">DiT</a>）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleMLPAdaLN</span>(nn.Module):</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, t, c</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Apply the model to an input batch.</span></span><br><span class="line"><span class="string">        :param x: an [N x C] Tensor of inputs.</span></span><br><span class="line"><span class="string">        :param t: a 1-D batch of timesteps.</span></span><br><span class="line"><span class="string">        :param c: conditioning from AR transformer.</span></span><br><span class="line"><span class="string">        :return: an [N x C] Tensor of outputs.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        x = <span class="variable language_">self</span>.input_proj(x)</span><br><span class="line">        t = <span class="variable language_">self</span>.time_embed(t)</span><br><span class="line">        c = <span class="variable language_">self</span>.cond_embed(c)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将时间步信息和条件信息融合</span></span><br><span class="line">        y = t + c</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.grad_checkpointing <span class="keyword">and</span> <span class="keyword">not</span> torch.jit.is_scripting():</span><br><span class="line">            <span class="keyword">for</span> block <span class="keyword">in</span> <span class="variable language_">self</span>.res_blocks:</span><br><span class="line">                x = checkpoint(block, x, y)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">for</span> block <span class="keyword">in</span> <span class="variable language_">self</span>.res_blocks:</span><br><span class="line">                x = block(x, y)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.final_layer(x, y)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ResBlock</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        channels</span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.channels = channels</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.in_ln = nn.LayerNorm(channels, eps=<span class="number">1e-6</span>)</span><br><span class="line">        <span class="variable language_">self</span>.mlp = nn.Sequential(</span><br><span class="line">            nn.Linear(channels, channels, bias=<span class="literal">True</span>),</span><br><span class="line">            nn.SiLU(),</span><br><span class="line">            nn.Linear(channels, channels, bias=<span class="literal">True</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.adaLN_modulation = nn.Sequential(</span><br><span class="line">            nn.SiLU(),</span><br><span class="line">            nn.Linear(channels, <span class="number">3</span> * channels, bias=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, y</span>):</span><br><span class="line">        shift_mlp, scale_mlp, gate_mlp = <span class="variable language_">self</span>.adaLN_modulation(y).chunk(<span class="number">3</span>, dim=-<span class="number">1</span>)</span><br><span class="line">        h = modulate(<span class="variable language_">self</span>.in_ln(x), shift_mlp, scale_mlp)</span><br><span class="line">        h = <span class="variable language_">self</span>.mlp(h)</span><br><span class="line">        <span class="keyword">return</span> x + gate_mlp * h</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AIGC </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>随手记录一些常用知识点</title>
      <link href="/2025/05/10/AI/tricks/"/>
      <url>/2025/05/10/AI/tricks/</url>
      
        <content type="html"><![CDATA[<h1 id="正向kl散度-vs-反向kl散度">1.正向KL散度 VS 反向KL散度</h1><p>正向KL散度（Forward KL divergence）和反向KL散度（Reverse KLdivergence）是衡量两个概率分布差异的常用方法，这两个散度在使用时的侧重优化目标是有些许差异的。</p><p>设有两个概率分布<spanclass="math inline"><em>P</em>(<em>x</em>), <em>Q</em>(<em>x</em>)</span>，其中前者为真实分布，后者为近似分布。则<strong>正向KL散度</strong>的定义为：</p><p><span class="math display">$$\begin{split}    D_{KL}(P\|Q) = \sum_x P(x) \log\frac{P(x)}{Q(x)}\end{split}$$</span></p><blockquote><p>正向KL的特点是：<strong>模式覆盖</strong>（mode covering）</p><p>把真实分布<spanclass="math inline"><em>P</em>(<em>x</em>)</span>作为基准，衡量Q与P的差异，换句话说其目标是让近似分布<span class="math inline"><em>Q</em>(<em>x</em>)</span> 逼近真实分布<span class="math inline"><em>P</em>(<em>x</em>)</span> 。正向KL会惩罚P很大但是Q很小的情况，所以它会更<strong>倾向于让Q覆盖P的所有区域</strong>，所以正向KL也叫做模式覆盖，其更容易产生更分散的分布。</p></blockquote><p><strong>反向KL散度</strong>的定义为：</p><p><span class="math display">$$\begin{split}    D_{KL}(Q\|P) = \sum_x Q(x) \log\frac{Q(x)}{P(x)}\end{split}$$</span></p><blockquote><p>反向KL的特点是：<strong>模式检索</strong>（mode seeking）</p><p>它把近似分布<spanclass="math inline"><em>Q</em>(<em>x</em>)</span>作为基准，惩罚P很小但是Q很大的情况，更<strong>倾向于让Q集中在P的高概率区域</strong>，但是不一定能覆盖P的全部区域，更为激进会更容易产生模式坍塌（modecollapse）。</p></blockquote><p>举个例子来说，假设P是一个双峰分布，而Q是一个单峰分布：</p><ul><li>正向KL：为了减小KL散度，Q会尽量去覆盖两个峰；</li><li>反向KL：Q更有可能选择只拟合其中一个概率更大的单峰，而忽略另一个。</li></ul><h1 id="ema">2.EMA</h1><p>EMA（指数移动平均）是<strong>Exponential MovingAverage</strong>的缩写，是一种对数据序列进行平滑的统计方法，它给予最近的数据更大的权重，而旧数据的权重指数级衰减。常用于模型参数平滑、损失平滑等，给定某个变量（比如损失、权重）在时间<spanclass="math inline"><em>t</em></span>的值为<spanclass="math inline"><em>x</em><sub><em>t</em></sub></span>，则EMA的更新公式为：</p><p><span class="math display">$$\begin{split}    EMA_t = \alpha \cdot x_t + (1-\alpha) \cdot EMA_{t-1}\end{split}$$</span></p><p>其中 <span class="math inline"><em>α</em> ∈ (0, 1)</span>是平滑系数（也叫做衰减率），<span class="math inline"><em>α</em></span>越小EMA更新越平稳，相反<spanclass="math inline"><em>α</em></span>越大EMA更新对当前值更敏感。在实际使用中，通常设为0.99或0.999。</p>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DiT的细节</title>
      <link href="/2025/05/08/AIGC/DIT/"/>
      <url>/2025/05/08/AIGC/DIT/</url>
      
        <content type="html"><![CDATA[<p><strong>系列文章：</strong></p><ol class="series-items"><li><a href="/2025/03/28/AIGC/overview_DM/" title="扩散模型的发展（简略版）">扩散模型的发展（简略版）</a></li><li><a href="/2025/04/15/AIGC/DDPM/" title="DDPM总结">DDPM总结</a></li><li><a href="/2025/04/17/AIGC/DDIM/" title="DDIM总结">DDIM总结</a></li><li><a href="/2025/04/27/AIGC/LDM/" title="LDM的细节">LDM的细节</a></li><li><a href="/2025/05/08/AIGC/DIT/" title="DiT的细节">DiT的细节</a></li><li><a href="/2025/05/08/AIGC/iDDPM/" title="iDDPM总结">iDDPM总结</a></li><li><a href="/2026/02/27/AIGC/JiT/" title="PixelDiT & JiT总结">PixelDiT & JiT总结</a></li><li><a href="/2026/02/27/AIGC/Flow_Matching/" title="Flow Matching简要介绍">Flow Matching简要介绍</a></li></ol><blockquote><p>原论文： <ahref="http://openaccess.thecvf.com/content/ICCV2023/html/Peebles_Scalable_Diffusion_Models_with_Transformers_ICCV_2023_paper.html">Scalablediffusion models with transformers</a></p><p>源码： <ahref="https://github.com/facebookresearch/DiT">https://github.com/facebookresearch/DiT</a></p></blockquote><h1 id="背景介绍">1.背景介绍</h1><p>DiT使用LDM的架构来生成图片，但是它<strong>将通常使用的UNet架构替换为了Transformer架构</strong>，作者发现这样做不仅可以实现高质量的生成，并且由于使用了Transformer作为噪声预测网络，该方法的ScalingLaws也拟合得很好。</p><p>DDPM首次提出使用UNet作为DM的骨架，是因为UNet在像素级自回归模型和条件GAN中表现都很出色，后续的很多工作也延续了这一架构。DiT通过一系列实验表明其实UNet的归纳偏置对扩散模型的性能其实并不重要，因此扩散模型完全有能力从最近的Transformer架构中获益，通过继承其他领域的实践经验和训练方法，保留可扩展性、鲁棒性和效率等特性。</p><h1 id="method">2.Method</h1><h2 id="序言">2.1.序言</h2><h3 id="diffusion">2.1.1.Diffusion</h3><p>对于DDPM的前向过程，具体过程可参考<ahref="https://litchi-lee.github.io/2025/04/15/AIGC/DDPM/">DDPM</a>，可以总结为：</p><p><span class="math display">$$\begin{split}    q(x_t \mid x_0) =\mathcal{N}\bigl(x_t;\;\sqrt{\bar\alpha_t}\,x_0,\;(1-\bar\alpha_t)\,I\bigr).\end{split}$$</span></p><p>Diffusion Model的训练目标是要拟合出逆向过程的概率分布：</p><p><span class="math display">$$\begin{split}    p_\theta(x_{t-1} \mid x_t) = \mathcal{N}\bigl(\mu_\theta(x_t),\Sigma_\theta(x_t)\bigr).\end{split}$$</span></p><p>逆向过程的训练则是通过最小化 <spanclass="math inline"><em>x</em><sub>0</sub></span>的对数似然的变分下界进行，其中第一项表示<strong>重建项（reconstructionterm）</strong>，第二项表示<strong>一致项（consistencyterm）</strong>：</p><p><span class="math display">$$\begin{split}    \mathcal{L}(\theta)=-p(x_0|x_1)+\sum_t\mathcal{D}_{KL}(q^*(x_{t-1}|x_t,x_0)||p_\theta(x_{t-1}|x_t))\end{split}$$</span></p><p>通过进一步重参数化推导，并使用噪声预测网络<spanclass="math inline"><em>ϵ</em><sub><em>t</em></sub><em>h</em><em>e</em><em>t</em><em>a</em></span>，最终上述损失可以重新组织为对预测噪声和真实采样噪声间的均方误差：</p><p><span class="math display">$$\begin{split}    L_{\mathrm{simple}}=E_{t,x_0,\epsilon}\left[||\epsilon-\epsilon_\theta(x_t,t)||^2\right]\end{split}$$</span></p><p>在DDPM中使用不可学习的方差 <spanclass="math inline"><em>Σ</em><sub><em>θ</em></sub>(<em>x</em><sub><em>t</em></sub>)</span>，但是后续iDDPM对这一部分进行了优化，为了训练可学习的方差，额外引入了<spanclass="math inline">ℒ<sub><em>v</em><em>l</em><em>b</em></sub></span>，具体可参考<ahref="https://litchi-lee.github.io/2025/05/08/AIGC/iDDPM/">iDDPM</a>：</p><p><span class="math display">$$\begin{split}    L_{\mathrm{hybrid}}=L_{\mathrm{simple}}+\lambda L_{\mathrm{vlb}}\end{split}$$</span></p><h3 id="cfg">2.1.2.CFG</h3><p>条件扩散模型需要输入额外的条件信息作为输入（比如类标签<spanclass="math inline"><em>c</em></span>），通常使用<strong>Classifier-FreeGuidance</strong>的方式进行训练。具体来说，由贝叶斯公式可以推导：<spanclass="math inline">∇<sub><em>x</em></sub>log <em>p</em>(<em>c</em> ∣ <em>x</em>) ∝ ∇<sub><em>x</em></sub>log <em>p</em>(<em>x</em> ∣ <em>c</em>) − ∇<sub><em>x</em></sub>log <em>p</em>(<em>x</em>)</span>，代入ClassifierGuidance中公式：<spanclass="math inline">∇<sub><em>x</em></sub>log <em>p</em>(<em>x</em> ∣ <em>c</em>) = <em>s</em> ⋅ ∇<sub><em>x</em></sub>log <em>p</em>(<em>c</em> ∣ <em>x</em>) + ∇<sub><em>x</em></sub>log <em>p</em>(<em>x</em>)</span>，即可得到CFG的计算公式：</p><p><span class="math display">$$\begin{gather}\begin{split}    \hat{\epsilon}_{\theta}(x_{t},c)&amp;=\epsilon_{\theta}(x_{t},\emptyset)+s\cdot\nabla_{x}\log p(x|c) \\    &amp;\propto\epsilon_{\theta}(x_{t},\emptyset)+s\cdot(\epsilon_{\theta}(x_{t},c)-\epsilon_{\theta}(x_{t},\emptyset))\end{split}\end{gather}$$</span></p><p>其中当 <span class="math inline"><em>c</em> = ∅</span>时表示无条件生成，通过调节超参数<spanclass="math inline"><em>s</em></span>的值，可以控制条件引导的强度。</p><h3 id="ldm">2.1.3.LDM</h3><p>在高分辨率像素空间（PixelSpace）中训练扩散模型计算开销是非常大的，Latent DiffusionModels通过两阶段处理来解决这个问题：首先学习一个自编码器将图像压缩到潜在空间（LatentSpace）表征中，之后在这个潜在空间中训练扩散模型，具体可参考<ahref="https://litchi-lee.github.io/2025/04/27/AIGC/LDM/">LDM</a>。</p><p>在这里DiT也使用LDM的思路，即在潜在空间中训练扩散模型。</p><h2 id="diffusion-transformer">2.2.Diffusion Transformer</h2><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/Patchify.png" width="50%" /></p><p>如上图所示，DiT基于ViT来构建Transformer，即通过对图像进行<strong>Patchify</strong>来生成图像tokens，对于DiT的输入<spanclass="math inline"><em>z</em> ∈ ℝ<sup><em>b</em>, <em>c</em>, <em>h</em>, <em>w</em></sup></span>，经过Patchify后输出 <spanclass="math inline"><em>z</em><sup>′</sup> ∈ ℝ<sup><em>b</em>, <em>T</em>, <em>d</em></sup></span>。其中patch size参数<spanclass="math inline"><em>p</em></span>用于调节patch的大小，可以用于控制Patchify精度。</p><p>Patchify得到tokens后，就可以送入transformerblock中进行序列生成了（在这里相当于是生成噪音tokens）。但是还有一个问题，条件信息<spanclass="math inline"><em>y</em></span>和时间步信息<spanclass="math inline"><em>t</em></span>该如何嵌入到transformer中控制条件生成呢？</p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/DiT_0.png" /></p><p>如上图所示，这里DiT进行了四种架构的尝试。</p><h3 id="in-context-conditioning">2.2.1.In-Context Conditioning</h3><p>直接将条件信息<span class="math inline"><em>c</em></span>和<spanclass="math inline"><em>t</em></span>的embedding作为两个额外的tokens添加到输入tokens中，这和原ViT使用<code>[cls]</code>的方法是一样的，因此都不用对ViT的结构进行修改。</p><h3 id="cross-attention-block">2.2.2.Cross-Attention Block</h3><p>使用Cross-Attention需要额外增加一个多头交叉注意力模块来嵌入条件，条件输入时直接将两个条件信息的embedding拼接起来输入，但是这种做法会增加很多计算量。</p><h3 id="adaln-block">2.2.3.AdaLN Block</h3><blockquote><p><strong>补充：AdaLN</strong></p><p>首先对于普通的LN，通常的做法是：</p><p><span class="math display">$$ LayerNorm(x) =\frac{x-\mu}{\sigma}\cdot\gamma + \beta $$</span></p><p>其中<spanclass="math inline"><em>μ</em>, <em>σ</em></span>是输入<spanclass="math inline"><em>x</em></span>的均值和标准差，<spanclass="math inline"><em>γ</em>, <em>β</em></span>则分别是缩放因子和偏移因子，表示缩放和偏移。</p><p>而AdaLN中，模型不再使用固定的<spanclass="math inline"><em>γ</em></span>和<spanclass="math inline"><em>β</em></span>，而是通过外部输入条件<spanclass="math inline"><em>y</em></span>来生成与输入<spanclass="math inline"><em>x</em></span>相同维度的缩放参数和偏移参数。具体来说，通常的做法是让<spanclass="math inline"><em>y</em></span>通过一层Linear网络得到<strong>三个参数<code>shift</code>、<code>scale</code>和<code>gate</code></strong>，其中<code>shift</code>和<code>scale</code>分别用于代替<spanclass="math inline"><em>β</em></span>和<spanclass="math inline"><em>γ</em></span>，而<code>gate</code>是一个可学习的残差门控参数，用于在ResBlock中控制多大程度保留这里的隐层输出<spanclass="math inline"><em>h</em></span>，即：</p><p><span class="math display">$$ h=\frac{x-\mu}{\sigma}\cdot scale +shift $$</span></p><p>接着使用门控参数<code>gate</code>控制多大程度保留隐层输出：</p><p><span class="math display">$$\begin{split}x'=x+gate\cdot h\end{split}$$</span></p></blockquote><p>DiT将原TransformerBlock中的LN层替换为了adaLN层，并通过MLP将输入条件embedding转换为AdaLN层的缩放和偏移参数<spanclass="math inline"><em>γ</em>, <em>β</em></span>。在前三个方法中，AdaLN方法所带来的额外计算量是最小的。</p><h3 id="adaln-zero-block">2.2.4.AdaLN-Zero Block</h3><p>原来的AdaLN方法可以表述为：</p><p><span class="math display">$$\begin{split}    \text{AdaLN}(x)=\gamma(c)\cdot\frac{x-\mu}{\sigma}+\beta(c)\end{split}$$</span></p><p>但是后续的研究发现使用Zero-Initializing能加速模型的训练，该设计的含义即初始化时让调节参数<span class="math inline">(<em>γ</em>, <em>β</em>)</span>为零向量，使模型在无条件控制下初始行为等价于不使用条件信息。这种做法的好处是在训练初期不会因为条件噪声干扰模型主干网络的训练，其过程可以表述为：</p><p><span class="math display">$$\begin{split}    \text{AdaLN-Zero}(x)=(1+\Delta\gamma(c))\cdot\frac{x-\mu}{\sigma}+\Delta\beta(c)\end{split}$$</span></p><p>除此之外，DiT还使用了额外的一个缩放参数 <spanclass="math inline"><em>α</em></span>对残差连接进行处理。经过实验，作者发现该方法的效果是最好的，因此DiT最后使用<strong>AdaLN-Zero</strong>来进行条件控制。</p><h1 id="源码">3.源码</h1><h2 id="训练">3.1.训练</h2><p>训练时的workflow主要定义在<code>train.py</code>中，每个epoch需要完成的流程为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(args.epochs):</span><br><span class="line">    sampler.set_epoch(epoch)</span><br><span class="line">    logger.info(<span class="string">f&quot;Beginning epoch <span class="subst">&#123;epoch&#125;</span>...&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> loader:</span><br><span class="line">        x = x.to(device)</span><br><span class="line">        y = y.to(device)</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            <span class="comment"># Map input images to latent space + normalize latents:</span></span><br><span class="line">            x = vae.encode(x).latent_dist.sample().mul_(<span class="number">0.18215</span>)</span><br><span class="line">        t = torch.randint(<span class="number">0</span>, diffusion.num_timesteps, (x.shape[<span class="number">0</span>],), device=device)</span><br><span class="line">        model_kwargs = <span class="built_in">dict</span>(y=y)</span><br><span class="line">        loss_dict = diffusion.training_losses(model, x, t, model_kwargs)</span><br><span class="line">        loss = loss_dict[<span class="string">&quot;loss&quot;</span>].mean()</span><br><span class="line">        opt.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        opt.step()</span><br><span class="line">        update_ema(ema, model.module)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Log loss values:</span></span><br><span class="line">        running_loss += loss.item()</span><br><span class="line">        log_steps += <span class="number">1</span></span><br><span class="line">        train_steps += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> train_steps % args.log_every == <span class="number">0</span>:</span><br><span class="line">            <span class="comment"># Measure training speed:</span></span><br><span class="line">            torch.cuda.synchronize()</span><br><span class="line">            end_time = time()</span><br><span class="line">            steps_per_sec = log_steps / (end_time - start_time)</span><br><span class="line">            <span class="comment"># Reduce loss history over all processes:</span></span><br><span class="line">            avg_loss = torch.tensor(running_loss / log_steps, device=device)</span><br><span class="line">            dist.all_reduce(avg_loss, op=dist.ReduceOp.SUM)</span><br><span class="line">            avg_loss = avg_loss.item() / dist.get_world_size()</span><br><span class="line">            logger.info(<span class="string">f&quot;(step=<span class="subst">&#123;train_steps:07d&#125;</span>) Train Loss: <span class="subst">&#123;avg_loss:<span class="number">.4</span>f&#125;</span>, Train Steps/Sec: <span class="subst">&#123;steps_per_sec:<span class="number">.2</span>f&#125;</span>&quot;</span>)</span><br><span class="line">            <span class="comment"># Reset monitoring variables:</span></span><br><span class="line">            running_loss = <span class="number">0</span></span><br><span class="line">            log_steps = <span class="number">0</span></span><br><span class="line">            start_time = time()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Save DiT checkpoint:</span></span><br><span class="line">        <span class="keyword">if</span> train_steps % args.ckpt_every == <span class="number">0</span> <span class="keyword">and</span> train_steps &gt; <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">if</span> rank == <span class="number">0</span>:</span><br><span class="line">                checkpoint = &#123;</span><br><span class="line">                    <span class="string">&quot;model&quot;</span>: model.module.state_dict(),</span><br><span class="line">                    <span class="string">&quot;ema&quot;</span>: ema.state_dict(),</span><br><span class="line">                    <span class="string">&quot;opt&quot;</span>: opt.state_dict(),</span><br><span class="line">                    <span class="string">&quot;args&quot;</span>: args</span><br><span class="line">                &#125;</span><br><span class="line">                checkpoint_path = <span class="string">f&quot;<span class="subst">&#123;checkpoint_dir&#125;</span>/<span class="subst">&#123;train_steps:07d&#125;</span>.pt&quot;</span></span><br><span class="line">                torch.save(checkpoint, checkpoint_path)</span><br><span class="line">                logger.info(<span class="string">f&quot;Saved checkpoint to <span class="subst">&#123;checkpoint_path&#125;</span>&quot;</span>)</span><br><span class="line">            dist.barrier()</span><br></pre></td></tr></table></figure><p>其中最重要的逻辑在中间部分：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> x, y <span class="keyword">in</span> loader:</span><br><span class="line">    x = x.to(device)</span><br><span class="line">    y = y.to(device)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="comment"># Map input images to latent space + normalize latents:</span></span><br><span class="line">        x = vae.encode(x).latent_dist.sample().mul_(<span class="number">0.18215</span>)</span><br><span class="line">    t = torch.randint(<span class="number">0</span>, diffusion.num_timesteps, (x.shape[<span class="number">0</span>],), device=device)</span><br><span class="line">    model_kwargs = <span class="built_in">dict</span>(y=y)</span><br><span class="line">    loss_dict = diffusion.training_losses(model, x, t, model_kwargs)</span><br></pre></td></tr></table></figure><p>其主要workflow可以总结为以下几步：</p><ul><li>得到输入图像<span class="math inline"><em>x</em></span>和条件<spanclass="math inline"><em>y</em></span>，对<spanclass="math inline"><em>x</em></span>进行VAE编码将其变换到latent空间中，并额外引入了一个<strong>经验性缩放常数</strong>（这里是SD训练时用到的经验性常数0.18215，因为这里使用的是SD中的VAE），将输入图像在latent空间中的值变换到<spanclass="math inline">[−1, 1]</span>；</li><li>随机采样一个时间步<span class="math inline"><em>t</em></span>，<spanclass="math inline"><em>t</em> ∈ [0, <em>n</em><em>u</em><em>m</em>_<em>s</em><em>t</em><em>e</em><em>p</em><em>s</em>]</span>；</li><li>将输入图像<span class="math inline"><em>x</em></span>、条件<spanclass="math inline"><em>y</em></span>和时间步<spanclass="math inline"><em>t</em></span>输入扩散模型进行训练。</li></ul><h3 id="diffusion-1">3.1.1.Diffusion</h3><p>由于作者这里使用的是可学习方差，因此这一部分的代码主要来自iDDPM，我们主要关注这里的loss是如何计算出来的。这里使用的是<strong>混合损失</strong>，即：</p><p><span class="math display">$$\begin{split}    L_{\mathrm{hybrid}}=L_{\mathrm{mse}}+\lambda L_{\mathrm{vlb}}\end{split}$$</span></p><p>其中<spanclass="math inline"><em>λ</em> = 0.001</span>，用于防止<spanclass="math inline"><em>L</em><sub>vlb</sub></span>过大影响MSE损失。同时，在预测模型输出的时候也会同步输出预测方差，<code>model_output, model_var_values = th.split(model_output, C, dim=1)</code>，这里的输出会多一层。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">GaussianDiffusion</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">training_losses</span>(<span class="params">self, model, x_start, t, model_kwargs=<span class="literal">None</span>, noise=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="keyword">if</span> model_kwargs <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            model_kwargs = &#123;&#125;  <span class="comment"># 无条件</span></span><br><span class="line">        <span class="keyword">if</span> noise <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            noise = th.randn_like(x_start)  <span class="comment"># 采样噪声</span></span><br><span class="line">        <span class="comment"># 前向过程得到x_t</span></span><br><span class="line">        x_t = <span class="variable language_">self</span>.q_sample(x_start, t, noise=noise)</span><br><span class="line"></span><br><span class="line">        terms = &#123;&#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment"># KL散度损失</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.loss_type == LossType.KL <span class="keyword">or</span> <span class="variable language_">self</span>.loss_type == LossType.RESCALED_KL:</span><br><span class="line">            terms[<span class="string">&quot;loss&quot;</span>] = <span class="variable language_">self</span>._vb_terms_bpd(</span><br><span class="line">                model=model,</span><br><span class="line">                x_start=x_start,</span><br><span class="line">                x_t=x_t,</span><br><span class="line">                t=t,</span><br><span class="line">                clip_denoised=<span class="literal">False</span>,</span><br><span class="line">                model_kwargs=model_kwargs,</span><br><span class="line">            )[<span class="string">&quot;output&quot;</span>]</span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>.loss_type == LossType.RESCALED_KL:</span><br><span class="line">                terms[<span class="string">&quot;loss&quot;</span>] *= <span class="variable language_">self</span>.num_timesteps</span><br><span class="line">        <span class="comment"># MSE损失</span></span><br><span class="line">        <span class="keyword">elif</span> <span class="variable language_">self</span>.loss_type == LossType.MSE <span class="keyword">or</span> <span class="variable language_">self</span>.loss_type == LossType.RESCALED_MSE:</span><br><span class="line">            model_output = model(x_t, t, **model_kwargs)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 可学习方差</span></span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>.model_var_type <span class="keyword">in</span> [</span><br><span class="line">                ModelVarType.LEARNED,</span><br><span class="line">                ModelVarType.LEARNED_RANGE,</span><br><span class="line">            ]:</span><br><span class="line">                B, C = x_t.shape[:<span class="number">2</span>]</span><br><span class="line">                <span class="keyword">assert</span> model_output.shape == (B, C * <span class="number">2</span>, *x_t.shape[<span class="number">2</span>:])</span><br><span class="line">                model_output, model_var_values = th.split(model_output, C, dim=<span class="number">1</span>)</span><br><span class="line">                frozen_out = th.cat([model_output.detach(), model_var_values], dim=<span class="number">1</span>)</span><br><span class="line">                terms[<span class="string">&quot;vb&quot;</span>] = <span class="variable language_">self</span>._vb_terms_bpd(</span><br><span class="line">                    model=<span class="keyword">lambda</span> *args, r=frozen_out: r,</span><br><span class="line">                    x_start=x_start,</span><br><span class="line">                    x_t=x_t,</span><br><span class="line">                    t=t,</span><br><span class="line">                    clip_denoised=<span class="literal">False</span>,</span><br><span class="line">                )[<span class="string">&quot;output&quot;</span>]</span><br><span class="line">                <span class="keyword">if</span> <span class="variable language_">self</span>.loss_type == LossType.RESCALED_MSE:</span><br><span class="line">                    <span class="comment"># 乘上一个很小的系数防止影响MSE损失</span></span><br><span class="line">                    terms[<span class="string">&quot;vb&quot;</span>] *= <span class="variable language_">self</span>.num_timesteps / <span class="number">1000.0</span></span><br><span class="line"></span><br><span class="line">            target = &#123;</span><br><span class="line">                ModelMeanType.PREVIOUS_X: <span class="variable language_">self</span>.q_posterior_mean_variance(</span><br><span class="line">                    x_start=x_start, x_t=x_t, t=t</span><br><span class="line">                )[<span class="number">0</span>],</span><br><span class="line">                ModelMeanType.START_X: x_start,</span><br><span class="line">                ModelMeanType.EPSILON: noise,</span><br><span class="line">            &#125;[<span class="variable language_">self</span>.model_mean_type]</span><br><span class="line">            <span class="keyword">assert</span> model_output.shape == target.shape == x_start.shape</span><br><span class="line">            <span class="comment"># 计算噪声的MSE损失</span></span><br><span class="line">            terms[<span class="string">&quot;mse&quot;</span>] = mean_flat((target - model_output) ** <span class="number">2</span>)</span><br><span class="line">            <span class="keyword">if</span> <span class="string">&quot;vb&quot;</span> <span class="keyword">in</span> terms:</span><br><span class="line">                terms[<span class="string">&quot;loss&quot;</span>] = terms[<span class="string">&quot;mse&quot;</span>] + terms[<span class="string">&quot;vb&quot;</span>]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                terms[<span class="string">&quot;loss&quot;</span>] = terms[<span class="string">&quot;mse&quot;</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> NotImplementedError(<span class="variable language_">self</span>.loss_type)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> terms</span><br></pre></td></tr></table></figure><h3 id="dit">3.1.2.DiT</h3><p>DiT中噪声预测网络定义在<code>DiT</code>类中，对于训练过程，输入图像<spanclass="math inline"><em>x</em> ∈ ℝ<sup><em>b</em>, <em>c</em>, <em>h</em>, <em>w</em></sup></span>、时间步条件 <span class="math inline"><em>t</em></span> 和类别条件<span class="math inline"><em>c</em></span>，输出预测噪声<spanclass="math inline"><em>ϵ</em><sub><em>θ</em></sub></span>（对于iDDPM还会输出预测方差<spanclass="math inline"><em>Σ</em><sub><em>θ</em></sub></span>）。三个输入首先都会通过一层embedder转换成相应的嵌入向量，然后将两个条件向量相加送入DiTBlock进行噪声预测，最后将输出unpatchify为原形状。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DiT</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, t, y</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Forward pass of DiT.</span></span><br><span class="line"><span class="string">        x: (N, C, H, W) tensor of spatial inputs (images or latent representations of images)</span></span><br><span class="line"><span class="string">        t: (N,) tensor of diffusion timesteps</span></span><br><span class="line"><span class="string">        y: (N,) tensor of class labels</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        x = (</span><br><span class="line">            <span class="variable language_">self</span>.x_embedder(x) + <span class="variable language_">self</span>.pos_embed</span><br><span class="line">        )  <span class="comment"># (N, T, D), where T = H * W / patch_size ** 2</span></span><br><span class="line">        t = <span class="variable language_">self</span>.t_embedder(t)  <span class="comment"># (N, D)</span></span><br><span class="line">        y = <span class="variable language_">self</span>.y_embedder(y, <span class="variable language_">self</span>.training)  <span class="comment"># (N, D)</span></span><br><span class="line">        <span class="comment"># 直接将两个嵌入相加</span></span><br><span class="line">        c = t + y  <span class="comment"># (N, D)</span></span><br><span class="line">        <span class="keyword">for</span> block <span class="keyword">in</span> <span class="variable language_">self</span>.blocks:</span><br><span class="line">            x = block(x, c)  <span class="comment"># (N, T, D)</span></span><br><span class="line">        x = <span class="variable language_">self</span>.final_layer(x, c)  <span class="comment"># (N, T, patch_size ** 2 * out_channels)</span></span><br><span class="line">        x = <span class="variable language_">self</span>.unpatchify(x)  <span class="comment"># (N, out_channels, H, W)</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><p>下面我们将具体看一看各自的<code>embedder</code>和<code>DiT Block</code>是如何实现的。</p><h4 id="timestepembedder">TimestepEmbedder</h4><p>这个类用于将时间步 <span class="math inline"><em>t</em></span>编码成为向量表示。对于一个标量输入<spanclass="math inline"><em>t</em></span>，首先该embedder会对该时间步计算<strong>正余弦位置编码</strong>（类似于传统transformer中的位置编码），即：</p><p><span class="math display">$$\begin{split}    \omega_i = \frac{1}{(\text{max_period})^{\frac{i}{[D / 2]}}}, \quadi=0,1,\ldots,\left\lfloor \frac{D}{2} \right\rfloor - 1\end{split}$$</span></p><p><span class="math display">$$\begin{split}    \theta_i = t \cdot \omega_i\end{split}$$</span></p><p><span class="math display">$$\begin{split}    \text{embedding}(t) = \left[ \cos(\theta_0), \cos(\theta_1), \ldots,\cos(\theta_{H-1}), \sin(\theta_0), \sin(\theta_1), \ldots,\sin(\theta_{H-1}) \right]\end{split}$$</span></p><p>之后再经过一层MLP将其转换为输出的维度：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TimestepEmbedder</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, hidden_size, frequency_embedding_size=<span class="number">256</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.mlp = nn.Sequential(</span><br><span class="line">            nn.Linear(frequency_embedding_size, hidden_size, bias=<span class="literal">True</span>),</span><br><span class="line">            nn.SiLU(),</span><br><span class="line">            nn.Linear(hidden_size, hidden_size, bias=<span class="literal">True</span>),</span><br><span class="line">        )</span><br><span class="line">        <span class="variable language_">self</span>.frequency_embedding_size = frequency_embedding_size</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">timestep_embedding</span>(<span class="params">t, dim, max_period=<span class="number">10000</span></span>):</span><br><span class="line">        half = dim // <span class="number">2</span></span><br><span class="line">        freqs = torch.exp(</span><br><span class="line">            -math.log(max_period)</span><br><span class="line">            * torch.arange(start=<span class="number">0</span>, end=half, dtype=torch.float32)</span><br><span class="line">            / half</span><br><span class="line">        ).to(device=t.device)</span><br><span class="line">        args = t[:, <span class="literal">None</span>].<span class="built_in">float</span>() * freqs[<span class="literal">None</span>]</span><br><span class="line">        embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> dim % <span class="number">2</span>:</span><br><span class="line">            embedding = torch.cat(</span><br><span class="line">                [embedding, torch.zeros_like(embedding[:, :<span class="number">1</span>])], dim=-<span class="number">1</span></span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">return</span> embedding</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, t</span>):</span><br><span class="line">        t_freq = <span class="variable language_">self</span>.timestep_embedding(t, <span class="variable language_">self</span>.frequency_embedding_size)</span><br><span class="line">        t_emb = <span class="variable language_">self</span>.mlp(t_freq)</span><br><span class="line">        <span class="keyword">return</span> t_emb</span><br></pre></td></tr></table></figure><h3 id="labelembedder">LabelEmbedder</h3><p>这一部分包含了训练时CFG的逻辑，即在训练时有一定概率会扔掉类别信息：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LabelEmbedder</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_classes, hidden_size, dropout_prob</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        use_cfg_embedding = dropout_prob &gt; <span class="number">0</span></span><br><span class="line">        <span class="variable language_">self</span>.embedding_table = nn.Embedding(</span><br><span class="line">            num_classes + use_cfg_embedding, hidden_size</span><br><span class="line">        )</span><br><span class="line">        <span class="variable language_">self</span>.num_classes = num_classes</span><br><span class="line">        <span class="variable language_">self</span>.dropout_prob = dropout_prob</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">token_drop</span>(<span class="params">self, labels, force_drop_ids=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Drops labels to enable classifier-free guidance.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> force_drop_ids <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            drop_ids = (</span><br><span class="line">                torch.rand(labels.shape[<span class="number">0</span>], device=labels.device) &lt; <span class="variable language_">self</span>.dropout_prob</span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            drop_ids = force_drop_ids == <span class="number">1</span></span><br><span class="line">        labels = torch.where(drop_ids, <span class="variable language_">self</span>.num_classes, labels)</span><br><span class="line">        <span class="keyword">return</span> labels</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, labels, train, force_drop_ids=<span class="literal">None</span></span>):</span><br><span class="line">        use_dropout = <span class="variable language_">self</span>.dropout_prob &gt; <span class="number">0</span></span><br><span class="line">        <span class="keyword">if</span> (train <span class="keyword">and</span> use_dropout) <span class="keyword">or</span> (force_drop_ids <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>):</span><br><span class="line">            labels = <span class="variable language_">self</span>.token_drop(labels, force_drop_ids)</span><br><span class="line">        embeddings = <span class="variable language_">self</span>.embedding_table(labels)</span><br><span class="line">        <span class="keyword">return</span> embeddings</span><br></pre></td></tr></table></figure><h3 id="dit-block">DiT Block</h3><p>这一部分是DiT模型的核心模块，首先条件<spanclass="math inline"><em>c</em></span>会通过线性层得到<spanclass="math inline"><em>γ</em><sub>1</sub>, <em>β</em><sub>1</sub>, <em>α</em><sub>1</sub>, <em>γ</em><sub>2</sub>, <em>β</em><sub>2</sub>, <em>α</em><sub>2</sub></span>，然后通过AdaLN-Zero对中间层进行归一化，分别经过Attention层和Forward层：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">modulate</span>(<span class="params">x, shift, scale</span>):</span><br><span class="line">    <span class="keyword">return</span> x * (<span class="number">1</span> + scale.unsqueeze(<span class="number">1</span>)) + shift.unsqueeze(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DiTBlock</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, hidden_size, num_heads, mlp_ratio=<span class="number">4.0</span>, **block_kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.norm1 = nn.LayerNorm(hidden_size, elementwise_affine=<span class="literal">False</span>, eps=<span class="number">1e-6</span>)</span><br><span class="line">        <span class="variable language_">self</span>.attn = Attention(</span><br><span class="line">            hidden_size, num_heads=num_heads, qkv_bias=<span class="literal">True</span>, **block_kwargs</span><br><span class="line">        )</span><br><span class="line">        <span class="variable language_">self</span>.norm2 = nn.LayerNorm(hidden_size, elementwise_affine=<span class="literal">False</span>, eps=<span class="number">1e-6</span>)</span><br><span class="line">        mlp_hidden_dim = <span class="built_in">int</span>(hidden_size * mlp_ratio)</span><br><span class="line">        approx_gelu = <span class="keyword">lambda</span>: nn.GELU(approximate=<span class="string">&quot;tanh&quot;</span>)</span><br><span class="line">        <span class="variable language_">self</span>.mlp = Mlp(</span><br><span class="line">            in_features=hidden_size,</span><br><span class="line">            hidden_features=mlp_hidden_dim,</span><br><span class="line">            act_layer=approx_gelu,</span><br><span class="line">            drop=<span class="number">0</span>,</span><br><span class="line">        )</span><br><span class="line">        <span class="variable language_">self</span>.adaLN_modulation = nn.Sequential(</span><br><span class="line">            nn.SiLU(), nn.Linear(hidden_size, <span class="number">6</span> * hidden_size, bias=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, c</span>):</span><br><span class="line">        <span class="comment"># 得到 \gamma_1, \beta_1, \alpha_1, \gamma_2, \beta_2, \alpha_2</span></span><br><span class="line">        shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp = (</span><br><span class="line">            <span class="variable language_">self</span>.adaLN_modulation(c).chunk(<span class="number">6</span>, dim=<span class="number">1</span>)</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># Attention层</span></span><br><span class="line">        x = x + gate_msa.unsqueeze(<span class="number">1</span>) * <span class="variable language_">self</span>.attn(</span><br><span class="line">            modulate(<span class="variable language_">self</span>.norm1(x), shift_msa, scale_msa)</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># forward层</span></span><br><span class="line">        x = x + gate_mlp.unsqueeze(<span class="number">1</span>) * <span class="variable language_">self</span>.mlp(</span><br><span class="line">            modulate(<span class="variable language_">self</span>.norm2(x), shift_mlp, scale_mlp)</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><h2 id="推理">3.2.推理</h2><p>模型训练完毕后，采样一张图像的主要逻辑如下：</p><ul><li>生成在latent空间下（经过VAEencode后）的高斯噪声<code>z = torch.randn(n, 4, latent_size, latent_size, device=device)</code>，输入条件类别信息<span class="math inline"><em>y</em></span> 用于条件生成；</li><li>生成同样大小的无条件输入用于CFG，并将其与原条件 <spanclass="math inline"><em>z</em></span> 和 <spanclass="math inline"><em>y</em></span>拼接在一起：<code>z = torch.cat([z, z], 0), y = torch.cat([y, y_null], 0)</code>；</li><li>将输入 <span class="math inline"><em>z</em></span> 和条件 <spanclass="math inline"><em>c</em></span> 送入Diffusion采样；</li><li>最终将输出通过VAE Decode重新转换为像素空间。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Load model:</span></span><br><span class="line">latent_size = args.image_size // <span class="number">8</span></span><br><span class="line">model = DiT_models[args.model](</span><br><span class="line">    input_size=latent_size,</span><br><span class="line">    num_classes=args.num_classes</span><br><span class="line">).to(device)</span><br><span class="line"><span class="comment"># Auto-download a pre-trained model or load a custom DiT checkpoint from train.py:</span></span><br><span class="line">ckpt_path = args.ckpt <span class="keyword">or</span> <span class="string">f&quot;DiT-XL-2-<span class="subst">&#123;args.image_size&#125;</span>x<span class="subst">&#123;args.image_size&#125;</span>.pt&quot;</span></span><br><span class="line">state_dict = find_model(ckpt_path)</span><br><span class="line">model.load_state_dict(state_dict)</span><br><span class="line">model.<span class="built_in">eval</span>()  <span class="comment"># important!</span></span><br><span class="line">diffusion = create_diffusion(<span class="built_in">str</span>(args.num_sampling_steps))</span><br><span class="line">vae = AutoencoderKL.from_pretrained(<span class="string">f&quot;stabilityai/sd-vae-ft-<span class="subst">&#123;args.vae&#125;</span>&quot;</span>).to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Labels to condition the model with (feel free to change):</span></span><br><span class="line">class_labels = [<span class="number">207</span>, <span class="number">360</span>, <span class="number">387</span>, <span class="number">974</span>, <span class="number">88</span>, <span class="number">979</span>, <span class="number">417</span>, <span class="number">279</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create sampling noise:</span></span><br><span class="line">n = <span class="built_in">len</span>(class_labels)</span><br><span class="line">z = torch.randn(n, <span class="number">4</span>, latent_size, latent_size, device=device) <span class="comment"># [N, C, H, W]</span></span><br><span class="line">y = torch.tensor(class_labels, device=device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Setup classifier-free guidance:</span></span><br><span class="line">z = torch.cat([z, z], <span class="number">0</span>)</span><br><span class="line">y_null = torch.tensor([<span class="number">1000</span>] * n, device=device)</span><br><span class="line">y = torch.cat([y, y_null], <span class="number">0</span>)</span><br><span class="line">model_kwargs = <span class="built_in">dict</span>(y=y, cfg_scale=args.cfg_scale)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sample images:</span></span><br><span class="line">samples = diffusion.p_sample_loop(</span><br><span class="line">    model.forward_with_cfg, z.shape, z, clip_denoised=<span class="literal">False</span>, model_kwargs=model_kwargs, progress=<span class="literal">True</span>, device=device</span><br><span class="line">)</span><br><span class="line">samples, _ = samples.chunk(<span class="number">2</span>, dim=<span class="number">0</span>)  <span class="comment"># Remove null class samples</span></span><br><span class="line">samples = vae.decode(samples / <span class="number">0.18215</span>).sample</span><br></pre></td></tr></table></figure><p>以下是DiT内部专门用于推理时CFG的forward：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DiT</span>(nn.Module):</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward_with_cfg</span>(<span class="params">self, x, t, y, cfg_scale</span>):</span><br><span class="line">        half = x[: <span class="built_in">len</span>(x) // <span class="number">2</span>]</span><br><span class="line">        combined = torch.cat([half, half], dim=<span class="number">0</span>)</span><br><span class="line">        model_out = <span class="variable language_">self</span>.forward(combined, t, y)</span><br><span class="line"></span><br><span class="line">        eps, rest = model_out[:, :<span class="number">3</span>], model_out[:, <span class="number">3</span>:]</span><br><span class="line">        cond_eps, uncond_eps = torch.split(eps, <span class="built_in">len</span>(eps) // <span class="number">2</span>, dim=<span class="number">0</span>)</span><br><span class="line">        half_eps = uncond_eps + cfg_scale * (cond_eps - uncond_eps)</span><br><span class="line">        eps = torch.cat([half_eps, half_eps], dim=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> torch.cat([eps, rest], dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AIGC </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>iDDPM总结</title>
      <link href="/2025/05/08/AIGC/iDDPM/"/>
      <url>/2025/05/08/AIGC/iDDPM/</url>
      
        <content type="html"><![CDATA[<p><strong>系列文章：</strong></p><ol class="series-items"><li><a href="/2025/03/28/AIGC/overview_DM/" title="扩散模型的发展（简略版）">扩散模型的发展（简略版）</a></li><li><a href="/2025/04/15/AIGC/DDPM/" title="DDPM总结">DDPM总结</a></li><li><a href="/2025/04/17/AIGC/DDIM/" title="DDIM总结">DDIM总结</a></li><li><a href="/2025/04/27/AIGC/LDM/" title="LDM的细节">LDM的细节</a></li><li><a href="/2025/05/08/AIGC/DIT/" title="DiT的细节">DiT的细节</a></li><li><a href="/2025/05/08/AIGC/iDDPM/" title="iDDPM总结">iDDPM总结</a></li><li><a href="/2026/02/27/AIGC/JiT/" title="PixelDiT & JiT总结">PixelDiT & JiT总结</a></li><li><a href="/2026/02/27/AIGC/Flow_Matching/" title="Flow Matching简要介绍">Flow Matching简要介绍</a></li></ol><blockquote><p>原论文： <ahref="https://proceedings.mlr.press/v139/nichol21a.html">Improveddenoising diffusion probabilistic models</a></p><p>源码： <ahref="https://github.com/openai/improved-diffusion">https://github.com/openai/improved-diffusion</a></p></blockquote><h1 id="背景介绍">背景介绍</h1><p>虽然DDPMs可以生成很高质量的图像，但是在log似然指标上还是无法超过自回归模型（VAEs），DDPMs是否能真正学习到数据分布还有待验证。同时DDPM在采样效率上也存在大问题，该如何减少采样所需的步长也有待研究。</p><p>本文提出了结合原目标函数和variationallower-bound（VLB）目标函数的<strong>混合目标函数</strong>，能实现更好的log似然指标，并使用<strong>重要性采样</strong>实现更平滑的训练。并且本文发现使用预训练好的混合损失模型能使用更少的步数实现高质量的生成。除此之外，还提出了一些其他的改进，进一步提升DDPM的生成质量。</p><h1 id="一些改进ddpms的tricks">一些改进DDPMs的tricks</h1><h2 id="可学习方差">可学习方差</h2><p>在DDPM中，对于逆向过程的后验概率可以用神经网络估计为：</p><p><span class="math display">$$\begin{split}    p_\theta\left(x_{t-1} \mid x_t\right):=\mathcal{N}\left(x_{t-1} ;\mu_\theta\left(x_t, t\right), \Sigma_\theta\left(x_t, t\right)\right)\end{split}$$</span></p><p>其中方差部分DDPM设定为固定方差（不可学习），即 <spanclass="math inline"><em>Σ</em><sub><em>θ</em></sub>(<em>x</em><sub><em>t</em></sub>, <em>t</em>) = <em>σ</em><sub><em>t</em></sub><sup>2</sup><strong>I</strong></span>。并且DDPM发现当 <spanclass="math inline"><em>σ</em><sub><em>t</em></sub><sup>2</sup></span>设定为 <span class="math inline"><em>β</em><sub><em>t</em></sub></span>和 <span class="math inline">$\tilde{\beta_t}$</span>时效果其实相差不大，其中 <spanclass="math inline"><em>β</em><sub><em>t</em></sub> := 1 − <em>α</em><sub><em>t</em></sub></span>，<spanclass="math inline">$\tilde{\beta}_t:=\frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t}\beta_t$</span>。但实际这两个参数在时间步逐渐增长的过程中也在逐渐逼近（如下图），最终基本重合。</p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/iDDPM_beta.png" width="50%"/></p><blockquote><p><strong>补充</strong>：</p><p>为什么这里要讨论 <spanclass="math inline"><em>β</em><sub><em>t</em></sub></span> 和 <spanclass="math inline"><em>β̃</em><sub><em>t</em></sub></span>呢，这两个值有什么特殊意义吗？事实上，这里的 <spanclass="math inline"><em>β</em><sub><em>t</em></sub></span> 和 <spanclass="math inline"><em>β̃</em><sub><em>t</em></sub></span>分别是<strong>扩散模型采样方差的上下界</strong>，详细证明出自<ahref="http://proceedings.mlr.press/v37/sohl-dickstein15.html">DPM原论文</a></p></blockquote><p>因此既然方差的上下界最终都相差不大，所以其实 <spanclass="math inline"><em>σ</em><sub><em>t</em></sub></span> 无论选择<span class="math inline"><em>β</em><sub><em>t</em></sub></span> 还是<span class="math inline"><em>β̃</em><sub><em>t</em></sub></span>其实对采样的质量没有多大的影响，这使得模型预测时会更多的考虑均值 <spanclass="math inline"><em>μ</em><sub><em>θ</em></sub>(<em>x</em><sub><em>t</em></sub>, <em>t</em>)</span>而不是方差 <spanclass="math inline"><em>Σ</em><sub><em>θ</em></sub>(<em>x</em><sub><em>t</em></sub>, <em>t</em>)</span>，这也许也是DDPM直接固定方差的原因。</p><p>既然我们都得到了方差的上下界，就可以直接使用简单的插值预测方差，即：</p><p><span class="math display">$$\begin{split}\Sigma_\theta\left(x_t, t\right)=\exp \left(v \log \beta_t+(1-v) \log\tilde{\beta}_t\right)\end{split}$$</span></p><p>这里使用log形式是为了进一步拉大上下界的差距（<spanclass="math inline"><em>β</em><sub><em>t</em></sub></span> 和 <spanclass="math inline"><em>β̃</em><sub><em>t</em></sub></span>都处于0-1），方便模型更好的预测。这里没有对<spanclass="math inline"><em>v</em></span>进一步约束，按理来说为保证上下界应该约束为0-1，但是在实验中表明学习的方差还是能处在上下界之间的。</p><p>此时由于原DDPM的损失函数是不包含方差的，因此本文还引入了变分下界损失函数<spanclass="math inline">ℒ<sub><em>v</em><em>l</em><em>b</em></sub></span>，并使用<span class="math inline"><em>λ</em></span>来进一步约束。具体来说，所有损失可以表达为：</p><p><span class="math display">$$\begin{gather}\begin{split}L_{\mathrm{simple}}&amp;=E_{t,x_0,\epsilon}\left[||\epsilon-\epsilon_\theta(x_t,t)||^2\right]\\\\L_{\mathrm{vlb}} &amp;:=L_0+L_1+...+L_{T-1}+L_T \\L_{0} &amp;:=-\log p_\theta(x_0|x_1) \\L_{t-1} &amp;:=D_{KL}(q(x_{t-1}|x_t,x_0)\parallel p_\theta(x_{t-1}|x_t))\\L_T &amp;:=D_{KL}(q(x_T|x_0)\parallel p(x_T)) \\\\L_{\mathrm{hybrid}}&amp;=L_{\mathrm{simple}}+\lambda L_{\mathrm{vlb}}\end{split}\end{gather}$$</span></p><h2 id="改进噪声强度曲线">改进噪声强度曲线</h2><p>在DDPM中，噪声强度调度是<strong>线性变化</strong>的，但是这会导致前向过程中的后段图像变得过于加噪（如下图，第一排是原DDPM线性噪声强度，第二排是iDDPM），并且<span class="math inline"><em>ᾱ</em><sub><em>t</em></sub></span>过快的趋于0导致图片信息损失得太快。</p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/iDDPM_noise.png" width="80%" /></p><p>因此iDDPM设计了<strong>余弦噪声强度调度</strong>方案：</p><p><span class="math display">$$\begin{split}\bar{\alpha}_t=\frac{f(t)}{f(0)},f(t)=\cos\left(\frac{t/T+s}{1+s}\cdot\frac{\pi}{2}\right)^2\end{split}$$</span></p><p>其中极小偏移量<spanclass="math inline"><em>s</em></span>的引入是为了防止 <spanclass="math inline"><em>β</em><sub><em>t</em></sub></span> 在 <spanclass="math inline"><em>t</em> = 0</span> 附近过小，在实验中设置 <spanclass="math inline"><em>s</em> = 0.008</span>。最终噪声调度变化曲线如下图，可以做到基本拟合线性下降，并且在初始和结束阶段变化幅度较小。</p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/iDDPM_loss.png" width="50%" /></p><h2 id="基于重要性的采样">基于重要性的采样</h2><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/iDDPM_noise_schedule.png" width="50%" /></p><p>iDDPM本来是想直接优化 <spanclass="math inline"><em>L</em><sub><em>v</em><em>l</em><em>b</em></sub></span>来实现最好的log似然指标，但是在实际实验中发现 <spanclass="math inline"><em>L</em><sub><em>v</em><em>l</em><em>b</em></sub></span>很难直接优化（如上图），作者推测可能是由于 <spanclass="math inline"><em>L</em><sub><em>v</em><em>l</em><em>b</em></sub></span>的梯度比 <spanclass="math inline"><em>L</em><sub><em>h</em><em>y</em><em>b</em><em>r</em><em>i</em><em>d</em></sub></span>更为 noisier。通过实验注意到不同时期的 <spanclass="math inline"><em>L</em><sub><em>v</em><em>l</em><em>b</em></sub></span>有很大区别（如下图），在 <span class="math inline"><em>t</em></span>比较小的时候loss下降得更快，这也说明这时候的网络更“需要”训练，使用简单的均匀时间步采样可能不太合适。</p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/iDDPM_log.png" width="50%" /></p><p>因此作者提出了<strong>重要性采样</strong>方法，看下面的公式可能一时之间有些难以理解，其实定性理解就是高损失的时间步（重要的去噪阶段）被赋予更高的采样概率，而低损失的时间步被减少采样。</p><p><span class="math display">$$\begin{split}L_{\mathrm{vlb}}=E_{t\simp_{t}}\left[\frac{L_{t}}{p_{t}}\right],\mathrm{where~}p_{t}\propto\sqrt{E[L_{t}^{2}]}\mathrm{~and~}\sump_{t}=1\end{split}$$</span></p><h2 id="加速采样">加速采样</h2><p>这一部分和DDIM都差不多使用子序列采样，不同主要在于 <spanclass="math inline"><em>σ</em><sub><em>t</em></sub><sup>2</sup> = <em>β̃</em><sub><em>t</em></sub></span>，具体可以看看<ahref="https://litchi-lee.github.io/2025/04/17/AIGC/DDIM/">DDIM的工作</a>。</p><h1 id="源码">源码</h1><h2 id="噪声强度调度">噪声强度调度</h2><p>对于iDDPM使用的余弦噪声强度调度，这里源码中使用<code>improved_diffusion\gaussian_diffusion.py</code>中的<code>get_named_beta_schedule</code>方法得到<spanclass="math inline"><em>β</em><sub><em>t</em></sub></span>，对于余弦策略，代码定义的<code>lambda t: math.cos((t + 0.008) / 1.008 * math.pi / 2) ** 2</code>和原论文中的公式保持一致。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_named_beta_schedule</span>(<span class="params">schedule_name, num_diffusion_timesteps</span>):</span><br><span class="line">    <span class="keyword">if</span> schedule_name == <span class="string">&quot;linear&quot;</span>:</span><br><span class="line">        scale = <span class="number">1000</span> / num_diffusion_timesteps</span><br><span class="line">        beta_start = scale * <span class="number">0.0001</span></span><br><span class="line">        beta_end = scale * <span class="number">0.02</span></span><br><span class="line">        <span class="keyword">return</span> np.linspace(</span><br><span class="line">            beta_start, beta_end, num_diffusion_timesteps, dtype=np.float64</span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">elif</span> schedule_name == <span class="string">&quot;cosine&quot;</span>:</span><br><span class="line">        <span class="keyword">return</span> betas_for_alpha_bar(</span><br><span class="line">            num_diffusion_timesteps,</span><br><span class="line">            <span class="keyword">lambda</span> t: math.cos((t + <span class="number">0.008</span>) / <span class="number">1.008</span> * math.pi / <span class="number">2</span>) ** <span class="number">2</span>,</span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError(<span class="string">f&quot;unknown beta schedule: <span class="subst">&#123;schedule_name&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">betas_for_alpha_bar</span>(<span class="params">num_diffusion_timesteps, alpha_bar, max_beta=<span class="number">0.999</span></span>):</span><br><span class="line">    betas = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_diffusion_timesteps):</span><br><span class="line">        t1 = i / num_diffusion_timesteps</span><br><span class="line">        t2 = (i + <span class="number">1</span>) / num_diffusion_timesteps</span><br><span class="line">        betas.append(<span class="built_in">min</span>(<span class="number">1</span> - alpha_bar(t2) / alpha_bar(t1), max_beta))</span><br><span class="line">    <span class="keyword">return</span> np.array(betas)</span><br></pre></td></tr></table></figure><h2 id="重要性采样">重要性采样</h2><p>这里源码实现了两种采样，一个就是原DDPM的均匀采样，另一种就是iDDPM使用的重要性采样。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">create_named_schedule_sampler</span>(<span class="params">name, diffusion</span>):</span><br><span class="line">    <span class="keyword">if</span> name == <span class="string">&quot;uniform&quot;</span>:</span><br><span class="line">        <span class="keyword">return</span> UniformSampler(diffusion)</span><br><span class="line">    <span class="keyword">elif</span> name == <span class="string">&quot;loss-second-moment&quot;</span>:</span><br><span class="line">        <span class="keyword">return</span> LossSecondMomentResampler(diffusion)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError(<span class="string">f&quot;unknown schedule sampler: <span class="subst">&#123;name&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p>这两种方式都继承自<code>ScheduleSampler</code>类，需要采样时间步时使用<code>ScheduleSampler.sample</code>方法采样时间步，需要实现<code>ScheduleSampler.weights</code>方法计算每个时间步的重要性权重，进而计算出每个时间步的采样概率，根据概率采样时间步。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ScheduleSampler</span>(<span class="title class_ inherited__">ABC</span>):</span><br><span class="line"><span class="meta">    @abstractmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">weights</span>(<span class="params">self</span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">sample</span>(<span class="params">self, batch_size, device</span>):</span><br><span class="line">        <span class="comment"># 得到时间步权重，计算采样概率</span></span><br><span class="line">        w = <span class="variable language_">self</span>.weights()</span><br><span class="line">        p = w / np.<span class="built_in">sum</span>(w)</span><br><span class="line"></span><br><span class="line">        indices_np = np.random.choice(<span class="built_in">len</span>(p), size=(batch_size,), p=p)</span><br><span class="line">        indices = th.from_numpy(indices_np).long().to(device)</span><br><span class="line">        weights_np = <span class="number">1</span> / (<span class="built_in">len</span>(p) * p[indices_np])</span><br><span class="line">        weights = th.from_numpy(weights_np).<span class="built_in">float</span>().to(device)</span><br><span class="line">        <span class="keyword">return</span> indices, weights</span><br></pre></td></tr></table></figure><h3 id="均匀采样实现">均匀采样实现</h3><p>均匀采样就很简单了，直接每个时间步权重设置为1即可。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">UniformSampler</span>(<span class="title class_ inherited__">ScheduleSampler</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, diffusion</span>):</span><br><span class="line">        <span class="variable language_">self</span>.diffusion = diffusion</span><br><span class="line">        <span class="variable language_">self</span>._weights = np.ones([diffusion.num_timesteps])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">weights</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>._weights</span><br></pre></td></tr></table></figure><h3 id="重要性采样实现">重要性采样实现</h3><p>主要需要关注 <code>LossSecondMomentResampler</code>类的实现，其中有两个变量需要关注，分别是记录时间步 <spanclass="math inline"><em>t</em></span> 最近 <code>history_per_term</code>次的 loss 列表 <code>self._loss_history</code> 和记录时间步 <spanclass="math inline"><em>t</em></span> 时的历史记录次数<code>self._loss_counts</code>。在更新时，调用<code>update_with_all_losses</code> 方法，采用FIFO的更新方式，记录最新的 <code>history_per_term</code>次loss。而在计算重要性权重时，额外引入了 <code>self.uniform_prob</code>以防止某些时间步概率为0，增强鲁棒性。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LossAwareSampler</span>(<span class="title class_ inherited__">ScheduleSampler</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">update_with_local_losses</span>(<span class="params">self, local_ts, local_losses</span>):</span><br><span class="line">        batch_sizes = [</span><br><span class="line">            th.tensor([<span class="number">0</span>], dtype=th.int32, device=local_ts.device)</span><br><span class="line">            <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(dist.get_world_size())</span><br><span class="line">        ]</span><br><span class="line">        dist.all_gather(</span><br><span class="line">            batch_sizes,</span><br><span class="line">            th.tensor([<span class="built_in">len</span>(local_ts)], dtype=th.int32, device=local_ts.device),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Pad all_gather batches to be the maximum batch size.</span></span><br><span class="line">        batch_sizes = [x.item() <span class="keyword">for</span> x <span class="keyword">in</span> batch_sizes]</span><br><span class="line">        max_bs = <span class="built_in">max</span>(batch_sizes)</span><br><span class="line"></span><br><span class="line">        timestep_batches = [th.zeros(max_bs).to(local_ts) <span class="keyword">for</span> bs <span class="keyword">in</span> batch_sizes]</span><br><span class="line">        loss_batches = [th.zeros(max_bs).to(local_losses) <span class="keyword">for</span> bs <span class="keyword">in</span> batch_sizes]</span><br><span class="line">        dist.all_gather(timestep_batches, local_ts)</span><br><span class="line">        dist.all_gather(loss_batches, local_losses)</span><br><span class="line">        timesteps = [</span><br><span class="line">            x.item() <span class="keyword">for</span> y, bs <span class="keyword">in</span> <span class="built_in">zip</span>(timestep_batches, batch_sizes) <span class="keyword">for</span> x <span class="keyword">in</span> y[:bs]</span><br><span class="line">        ]</span><br><span class="line">        losses = [x.item() <span class="keyword">for</span> y, bs <span class="keyword">in</span> <span class="built_in">zip</span>(loss_batches, batch_sizes) <span class="keyword">for</span> x <span class="keyword">in</span> y[:bs]]</span><br><span class="line">        <span class="variable language_">self</span>.update_with_all_losses(timesteps, losses)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @abstractmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">update_with_all_losses</span>(<span class="params">self, ts, losses</span>):</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LossSecondMomentResampler</span>(<span class="title class_ inherited__">LossAwareSampler</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, diffusion, history_per_term=<span class="number">10</span>, uniform_prob=<span class="number">0.001</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.diffusion = diffusion</span><br><span class="line">        <span class="variable language_">self</span>.history_per_term = history_per_term</span><br><span class="line">        <span class="variable language_">self</span>.uniform_prob = uniform_prob</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 初始化时间步 t 最近 history_per_term 次的loss</span></span><br><span class="line">        <span class="variable language_">self</span>._loss_history = np.zeros(</span><br><span class="line">            [diffusion.num_timesteps, history_per_term], dtype=np.float64</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># 初始化当前时间步 t 的历史记录次数</span></span><br><span class="line">        <span class="variable language_">self</span>._loss_counts = np.zeros([diffusion.num_timesteps], dtype=np.<span class="built_in">int</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">weights</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="variable language_">self</span>._warmed_up():</span><br><span class="line">            <span class="keyword">return</span> np.ones([<span class="variable language_">self</span>.diffusion.num_timesteps], dtype=np.float64)</span><br><span class="line">        weights = np.sqrt(np.mean(<span class="variable language_">self</span>._loss_history**<span class="number">2</span>, axis=-<span class="number">1</span>))</span><br><span class="line">        weights /= np.<span class="built_in">sum</span>(weights)</span><br><span class="line">        weights *= <span class="number">1</span> - <span class="variable language_">self</span>.uniform_prob</span><br><span class="line">        weights += <span class="variable language_">self</span>.uniform_prob / <span class="built_in">len</span>(weights)</span><br><span class="line">        <span class="keyword">return</span> weights</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">update_with_all_losses</span>(<span class="params">self, ts, losses</span>):</span><br><span class="line">        <span class="keyword">for</span> t, loss <span class="keyword">in</span> <span class="built_in">zip</span>(ts, losses):</span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>._loss_counts[t] == <span class="variable language_">self</span>.history_per_term:</span><br><span class="line">                <span class="comment"># Shift out the oldest loss term.</span></span><br><span class="line">                <span class="variable language_">self</span>._loss_history[t, :-<span class="number">1</span>] = <span class="variable language_">self</span>._loss_history[t, <span class="number">1</span>:]</span><br><span class="line">                <span class="variable language_">self</span>._loss_history[t, -<span class="number">1</span>] = loss</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="variable language_">self</span>._loss_history[t, <span class="variable language_">self</span>._loss_counts[t]] = loss</span><br><span class="line">                <span class="variable language_">self</span>._loss_counts[t] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_warmed_up</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> (<span class="variable language_">self</span>._loss_counts == <span class="variable language_">self</span>.history_per_term).<span class="built_in">all</span>()</span><br></pre></td></tr></table></figure><h3 id="可学习方差-1">可学习方差</h3><p>具体代码细节可在 <code>GaussianDiffusion.p_mean_variance</code>方法中查看，可以看到若指定方差是可学习的，则此时模型输出两部分，分别预测噪声和方差<code>assert model_output.shape == (B, C * 2, *x.shape[2:])</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">GaussianDiffusion</span>:</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">p_mean_variance</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self, model, x, t, clip_denoised=<span class="literal">True</span>, denoised_fn=<span class="literal">None</span>, model_kwargs=<span class="literal">None</span></span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="keyword">if</span> model_kwargs <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            model_kwargs = &#123;&#125;</span><br><span class="line"></span><br><span class="line">        B, C = x.shape[:<span class="number">2</span>]</span><br><span class="line">        <span class="keyword">assert</span> t.shape == (B,)</span><br><span class="line">        model_output = model(x, <span class="variable language_">self</span>._scale_timesteps(t), **model_kwargs)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.model_var_type <span class="keyword">in</span> [ModelVarType.LEARNED, ModelVarType.LEARNED_RANGE]:</span><br><span class="line">            <span class="comment"># 模型输出预测噪声和预测方差两部分</span></span><br><span class="line">            <span class="keyword">assert</span> model_output.shape == (B, C * <span class="number">2</span>, *x.shape[<span class="number">2</span>:])</span><br><span class="line">            model_output, model_var_values = th.split(model_output, C, dim=<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>.model_var_type == ModelVarType.LEARNED:</span><br><span class="line">                model_log_variance = model_var_values</span><br><span class="line">                model_variance = th.exp(model_log_variance)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                min_log = _extract_into_tensor(</span><br><span class="line">                    <span class="variable language_">self</span>.posterior_log_variance_clipped, t, x.shape</span><br><span class="line">                )</span><br><span class="line">                max_log = _extract_into_tensor(np.log(<span class="variable language_">self</span>.betas), t, x.shape)</span><br><span class="line">                <span class="comment"># The model_var_values is [-1, 1] for [min_var, max_var].</span></span><br><span class="line">                frac = (model_var_values + <span class="number">1</span>) / <span class="number">2</span></span><br><span class="line">                model_log_variance = frac * max_log + (<span class="number">1</span> - frac) * min_log</span><br><span class="line">                model_variance = th.exp(model_log_variance)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            model_variance, model_log_variance = &#123;</span><br><span class="line">                <span class="comment"># for fixedlarge, we set the initial (log-)variance like so</span></span><br><span class="line">                <span class="comment"># to get a better decoder log likelihood.</span></span><br><span class="line">                ModelVarType.FIXED_LARGE: (</span><br><span class="line">                    np.append(<span class="variable language_">self</span>.posterior_variance[<span class="number">1</span>], <span class="variable language_">self</span>.betas[<span class="number">1</span>:]),</span><br><span class="line">                    np.log(np.append(<span class="variable language_">self</span>.posterior_variance[<span class="number">1</span>], <span class="variable language_">self</span>.betas[<span class="number">1</span>:])),</span><br><span class="line">                ),</span><br><span class="line">                ModelVarType.FIXED_SMALL: (</span><br><span class="line">                    <span class="variable language_">self</span>.posterior_variance,</span><br><span class="line">                    <span class="variable language_">self</span>.posterior_log_variance_clipped,</span><br><span class="line">                ),</span><br><span class="line">            &#125;[<span class="variable language_">self</span>.model_var_type]</span><br><span class="line">            model_variance = _extract_into_tensor(model_variance, t, x.shape)</span><br><span class="line">            model_log_variance = _extract_into_tensor(model_log_variance, t, x.shape)</span><br><span class="line">        ...</span><br></pre></td></tr></table></figure><p>更深地扒一扒，可以发现 UNet的输出随是否可学习方差而改变（<code>out_channels=(3 if not learn_sigma else 6)</code>）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">create_model</span>(<span class="params">...</span>)</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> UNetModel(</span><br><span class="line">        in_channels=<span class="number">3</span>,</span><br><span class="line">        model_channels=num_channels,</span><br><span class="line">        out_channels=(<span class="number">3</span> <span class="keyword">if</span> <span class="keyword">not</span> learn_sigma <span class="keyword">else</span> <span class="number">6</span>),</span><br><span class="line">        ...</span><br><span class="line">    )</span><br></pre></td></tr></table></figure><p>关于损失的计算，这里需要额外计算 <spanclass="math inline"><em>L</em><sub><em>v</em><em>l</em><em>b</em></sub></span>，其中<code>_vb_terms_bpd</code>方法用于计算 <spanclass="math inline"><em>K</em><em>L</em>(<em>q</em>(<em>x</em><sub><em>t</em> − 1</sub>|<em>x</em><sub><em>t</em></sub>, <em>x</em><sub>0</sub>)||<em>p</em>(<em>x</em><sub><em>t</em> − 1</sub>|<em>x</em><sub><em>t</em></sub>))</span>，并且实验中为了减少对MSE损失的影响，对KL损失乘上了0.001的权重（可以说是影响非常小了）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">GaussianDiffusion</span>:</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_vb_terms_bpd</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self, model, x_start, x_t, t, clip_denoised=<span class="literal">True</span>, model_kwargs=<span class="literal">None</span></span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        true_mean, _, true_log_variance_clipped = <span class="variable language_">self</span>.q_posterior_mean_variance(</span><br><span class="line">            x_start=x_start, x_t=x_t, t=t</span><br><span class="line">        )</span><br><span class="line">        out = <span class="variable language_">self</span>.p_mean_variance(</span><br><span class="line">            model, x_t, t, clip_denoised=clip_denoised, model_kwargs=model_kwargs</span><br><span class="line">        )</span><br><span class="line">        kl = normal_kl(</span><br><span class="line">            true_mean, true_log_variance_clipped, out[<span class="string">&quot;mean&quot;</span>], out[<span class="string">&quot;log_variance&quot;</span>]</span><br><span class="line">        )</span><br><span class="line">        kl = mean_flat(kl) / np.log(<span class="number">2.0</span>)</span><br><span class="line"></span><br><span class="line">        decoder_nll = -discretized_gaussian_log_likelihood(</span><br><span class="line">            x_start, means=out[<span class="string">&quot;mean&quot;</span>], log_scales=<span class="number">0.5</span> * out[<span class="string">&quot;log_variance&quot;</span>]</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">assert</span> decoder_nll.shape == x_start.shape</span><br><span class="line">        decoder_nll = mean_flat(decoder_nll) / np.log(<span class="number">2.0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># At the first timestep return the decoder NLL,</span></span><br><span class="line">        <span class="comment"># otherwise return KL(q(x_&#123;t-1&#125;|x_t,x_0) || p(x_&#123;t-1&#125;|x_t))</span></span><br><span class="line">        output = th.where((t == <span class="number">0</span>), decoder_nll, kl)</span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">&quot;output&quot;</span>: output, <span class="string">&quot;pred_xstart&quot;</span>: out[<span class="string">&quot;pred_xstart&quot;</span>]&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">training_losses</span>(<span class="params">self, model, x_start, t, model_kwargs=<span class="literal">None</span>, noise=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="keyword">if</span> model_kwargs <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            model_kwargs = &#123;&#125;</span><br><span class="line">        <span class="keyword">if</span> noise <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            noise = th.randn_like(x_start)</span><br><span class="line">        x_t = <span class="variable language_">self</span>.q_sample(x_start, t, noise=noise)</span><br><span class="line"></span><br><span class="line">        terms = &#123;&#125;</span><br><span class="line"></span><br><span class="line">        ...</span><br><span class="line">        <span class="keyword">elif</span> <span class="variable language_">self</span>.loss_type == LossType.MSE <span class="keyword">or</span> <span class="variable language_">self</span>.loss_type == LossType.RESCALED_MSE:</span><br><span class="line">            model_output = model(x_t, <span class="variable language_">self</span>._scale_timesteps(t), **model_kwargs)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>.model_var_type <span class="keyword">in</span> [</span><br><span class="line">                ModelVarType.LEARNED,</span><br><span class="line">                ModelVarType.LEARNED_RANGE,</span><br><span class="line">            ]:</span><br><span class="line">                B, C = x_t.shape[:<span class="number">2</span>]</span><br><span class="line">                <span class="keyword">assert</span> model_output.shape == (B, C * <span class="number">2</span>, *x_t.shape[<span class="number">2</span>:])</span><br><span class="line">                model_output, model_var_values = th.split(model_output, C, dim=<span class="number">1</span>)</span><br><span class="line">                <span class="comment"># Learn the variance using the variational bound, but don&#x27;t let</span></span><br><span class="line">                <span class="comment"># it affect our mean prediction.</span></span><br><span class="line">                frozen_out = th.cat([model_output.detach(), model_var_values], dim=<span class="number">1</span>)</span><br><span class="line">                terms[<span class="string">&quot;vb&quot;</span>] = <span class="variable language_">self</span>._vb_terms_bpd(</span><br><span class="line">                    model=<span class="keyword">lambda</span> *args, r=frozen_out: r,</span><br><span class="line">                    x_start=x_start,</span><br><span class="line">                    x_t=x_t,</span><br><span class="line">                    t=t,</span><br><span class="line">                    clip_denoised=<span class="literal">False</span>,</span><br><span class="line">                )[<span class="string">&quot;output&quot;</span>]</span><br><span class="line">                <span class="keyword">if</span> <span class="variable language_">self</span>.loss_type == LossType.RESCALED_MSE:</span><br><span class="line">                    <span class="comment"># Divide by 1000 for equivalence with initial implementation.</span></span><br><span class="line">                    <span class="comment"># Without a factor of 1/1000, the VB term hurts the MSE term.</span></span><br><span class="line">                    terms[<span class="string">&quot;vb&quot;</span>] *= <span class="variable language_">self</span>.num_timesteps / <span class="number">1000.0</span></span><br><span class="line"></span><br><span class="line">            target = &#123;</span><br><span class="line">                ModelMeanType.PREVIOUS_X: <span class="variable language_">self</span>.q_posterior_mean_variance(</span><br><span class="line">                    x_start=x_start, x_t=x_t, t=t</span><br><span class="line">                )[<span class="number">0</span>],</span><br><span class="line">                ModelMeanType.START_X: x_start,</span><br><span class="line">                ModelMeanType.EPSILON: noise,</span><br><span class="line">            &#125;[<span class="variable language_">self</span>.model_mean_type]</span><br><span class="line">            <span class="keyword">assert</span> model_output.shape == target.shape == x_start.shape</span><br><span class="line">            terms[<span class="string">&quot;mse&quot;</span>] = mean_flat((target - model_output) ** <span class="number">2</span>)</span><br><span class="line">            <span class="keyword">if</span> <span class="string">&quot;vb&quot;</span> <span class="keyword">in</span> terms:</span><br><span class="line">                terms[<span class="string">&quot;loss&quot;</span>] = terms[<span class="string">&quot;mse&quot;</span>] + terms[<span class="string">&quot;vb&quot;</span>]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                terms[<span class="string">&quot;loss&quot;</span>] = terms[<span class="string">&quot;mse&quot;</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> NotImplementedError(<span class="variable language_">self</span>.loss_type)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> terms</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AIGC </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LDM的细节</title>
      <link href="/2025/04/27/AIGC/LDM/"/>
      <url>/2025/04/27/AIGC/LDM/</url>
      
        <content type="html"><![CDATA[<p><strong>系列文章：</strong></p><ol class="series-items"><li><a href="/2025/03/28/AIGC/overview_DM/" title="扩散模型的发展（简略版）">扩散模型的发展（简略版）</a></li><li><a href="/2025/04/15/AIGC/DDPM/" title="DDPM总结">DDPM总结</a></li><li><a href="/2025/04/17/AIGC/DDIM/" title="DDIM总结">DDIM总结</a></li><li><a href="/2025/04/27/AIGC/LDM/" title="LDM的细节">LDM的细节</a></li><li><a href="/2025/05/08/AIGC/DIT/" title="DiT的细节">DiT的细节</a></li><li><a href="/2025/05/08/AIGC/iDDPM/" title="iDDPM总结">iDDPM总结</a></li><li><a href="/2026/02/27/AIGC/JiT/" title="PixelDiT & JiT总结">PixelDiT & JiT总结</a></li><li><a href="/2026/02/27/AIGC/Flow_Matching/" title="Flow Matching简要介绍">Flow Matching简要介绍</a></li></ol><blockquote><p>原论文：<ahref="http://openaccess.thecvf.com/content/CVPR2022/html/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.html">High-resolutionimage synthesis with latent diffusion models</a></p><p>源码：<ahref="https://github.com/CompVis/latent-diffusion">https://github.com/CompVis/latent-diffusion</a></p></blockquote><h1 id="背景介绍">1.背景介绍</h1><p>影像生成是当时计算需求最大的领域之一，尤其是复杂场景的高分辨率合成。这对于自回归架构（AR）来说可能需要数十亿参数，同时GANs所取得的成果大多局限于有限变化的数据，不容易扩展到复杂的多模态分布建模。扩散模型的进步在多个任务下取得了SOTA，却不会像GANs一样表现出模式崩溃或者训练不稳定性，也无需像AR模型那样涉及数十亿个参数。即便如此，DMs仍是计算高需求的。</p><blockquote><p>likelihood-based model学习过程可分为两个阶段：</p><ul><li>感知压缩阶段：消除高频细节，学习语义信息</li><li>生成模型阶段：学习该语义压缩信息</li></ul></blockquote><p>本文的目标就是找到一个合适的表示空间，保持相同的语义信息但是更计算高效。</p><h2 id="两阶段图像生成">1.1.两阶段图像生成</h2><h3 id="vqvae">1.1.1.VQVAE</h3><p><strong>VQVAE</strong>使用自回归模型学习离散潜空间的先验表达，它与AE最大的不同在于AE编码的特征向量是连续的，但是VQVAE编码的特征向量是离散的，即视觉码本（VisualCodebook）或视觉字典（Visual Dictionary）。</p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/屏幕截图 2025-04-27 142255.png" width="80%"/></p><h3 id="vqgan">1.1.2.VQGAN</h3><p><strong>VQGAN</strong>是一个改良版的VQVAE，相比于VQVAE，其有3点改进：</p><ul><li>将传统CNN改为Transformer来捕捉较远像素之间的依赖关系</li><li>额外增加了一个PatchGAN作为判别器，并在训练时加入判别损失</li><li>使用感知损失代替传统的L2损失</li></ul><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/屏幕截图 2025-04-27 143922.png" width="60%"/></p><h1 id="method">2.Method</h1><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/LDM_0.png" width="60%"/></p><h2 id="感知图像压缩">2.1.感知图像压缩</h2><p>Perceptual ImageCompression模块使用一个预训练好的VQGAN将原始输入图像进行感知压缩。具体来说，对于给定的输入空间<spanclass="math inline"><em>x</em> ∈ ℝ<sup><em>H</em> × <em>W</em> × 3</sup></span>，编码器 <span class="math inline">ℰ</span> 将 <spanclass="math inline"><em>x</em></span> 编码到隐空间表示 <spanclass="math inline"><em>z</em> = ℰ(<em>x</em>)</span> ，其中 <spanclass="math inline"><em>z</em> ∈ ℝ<sup><em>h</em> × <em>w</em> × <em>c</em></sup></span>。</p><p>为了避免潜空间的方差过大，LDM使用了两种不同的正则化方法：KL-reg和VQ-reg。其中KL-reg相当于在潜空间上施加了KL惩罚，VQ-reg的作用则是使用向量量化层（VectorQuantization Layer）将特征归一化为码本中最近的那个特征。</p><p>对于VQ-reg的计算可以分解为以下步骤：</p><ol type="1"><li>特征reshape：将输入数据 <spanclass="math inline"><em>z</em><sub><em>e</em></sub> ∈ ℝ<sup><em>n</em> × <em>h</em> × <em>w</em> × <em>d</em></sup></span>进行维度合并，得到 <spanclass="math inline"><em>n</em> × <em>h</em> × <em>w</em></span> 个长度为<span class="math inline"><em>d</em></span> 的特征向量；</li><li>映射码本：对于每个特征向量，计算其与码本中<spanclass="math inline"><em>k</em></span>个向量的距离，选择距离最近的特征作为索引，最终得到经过码本查找的<spanclass="math inline"><em>n</em> × <em>h</em> × <em>w</em></span> 个长度为<span class="math inline"><em>d</em></span> 的特征向量；</li><li>还原reshape：对特征再次进行reshape，得到 <spanclass="math inline"><em>z</em><sub><em>q</em></sub> ∈ ℝ<sup><em>n</em> × <em>h</em> × <em>w</em> × <em>d</em></sup></span>；</li><li><strong>复制梯度</strong>：由于在进行选择码本索引时使用了 <spanclass="math inline"><em>a</em><em>r</em><em>g</em><em>m</em><em>i</em><em>n</em></span>，因此造成了梯度回传不连续，因此这里直接将<span class="math inline"><em>z</em><sub><em>q</em></sub></span>的导数复制到 <spanclass="math inline"><em>z</em><sub><em>e</em></sub></span>。</li></ol><h2 id="ldm">2.2.LDM</h2><p>得到压缩后的图像潜空间表示后，可以直接在隐空间上进行前向过程和去噪过程了，由于隐空间特征的大小要比图像空间小很多，因此LDM的推理速度要快很多。</p><p><span class="math display">$$\begin{split}    L_{L D M}:=\mathbb{E}_{\mathcal{E}(x), \epsilon \sim\mathcal{N}(0,1), t}\left[\left\|\epsilon-\epsilon_\theta\left(z_t,t\right)\right\|_2^2\right]\end{split}$$</span></p><h2 id="条件控制">2.3.条件控制</h2><p>去噪过程的条件引入往往是将条件信息 <spanclass="math inline"><em>y</em></span> 和时间步条件 <spanclass="math inline"><em>t</em></span> 一同加入噪声预测器 <spanclass="math inline"><em>ϵ</em><sub><em>θ</em></sub>(<em>z</em><sub><em>t</em></sub>, <em>t</em>, <em>y</em>)</span>，针对不同的生成任务，条件<spanclass="math inline"><em>y</em></span>可能是文本信息、语义图等。</p><p>对于文本信息，LDM引入了一个条件编码器 <spanclass="math inline"><em>τ</em><sub><em>θ</em></sub></span> 来将条件信息<span class="math inline"><em>y</em></span> 统一编码为中间特征 <spanclass="math inline"><em>τ</em><sub><em>θ</em></sub>(<em>y</em>) ∈ ℝ<sup><em>M</em> × <em>d</em><sub><em>r</em></sub></sup></span>，之后通过交叉注意力映射到UNet的中间层。</p><p><span class="math display">$$\begin{split}    L_{L D M}:=\mathbb{E}_{\mathcal{E}(x), \epsilon \sim\mathcal{N}(0,1), t}\left[\left\|\epsilon-\epsilon_\theta\left(z_t, t,\tau_\theta(y)\right)\right\|_2^2\right]\end{split}$$</span></p><p>对于语义图等条件，则可直接将特征图拼接到UNet中完成。</p><h1 id="rdm">3.RDM</h1><blockquote><p>原论文：<a href="http://arxiv.org/abs/2204.11824">Semi-ParametricNeural Image Synthesis</a></p></blockquote><p>这里简单介绍了一下RDM训练范式，因为LDM提供的源码可通过RDM进行推理。</p><h2 id="背景">3.1.背景</h2><p>当时的训练范式需要巨量的计算资源和训练时间，受到<strong>检索增强NLP（RAG）</strong>发展的启发，RDM质疑目前将不同训练样例直接转化为巨量可训练参数的方式，并提出为一个小的生成模型配备一个大型图像数据库的方式。</p><pre><code>注：RAG（Retrieval-Augmented Generation）是一种使LLM在生成回答时读取外部信息库的技术，可以理解为在生成内容之前，先从外部数据库中检索出相关信息作为参考。</code></pre><p>在训练过程中，它会通过近邻查找访问数据库，无需从头学习数据，而是通过检索到的视觉特征合成新的场景。这一方法不仅提升了生成性能，也大幅度降低了参数数量和训练开销。并且这种方式也独立于所使用的生成模型，允许我们使用retrieval-augmenteddiffusion（RDM），也能使用retrieval-augmentedautoregressive（RARM）。</p><h2 id="方法">3.2.方法</h2><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/RDM.png" width="80%"/></p><h3 id="总体介绍">3.2.1.总体介绍</h3><p>和传统全参数生成模型不同，我们定义了一个半参数生成模型 <spanclass="math inline"><em>p</em><sub><em>θ</em>, 𝒟, <em>ξ</em><sub><em>k</em></sub></sub>(<em>x</em>)</span>，其中 <span class="math inline"><em>θ</em></span> 是可训练参数而 <spanclass="math inline">𝒟, <em>ξ</em><sub><em>k</em></sub></span>是不可训练的模型模块，<spanclass="math inline">𝒟 = {<em>y</em><sub><em>i</em></sub>}<sub><em>i</em> = 1</sub><sup><em>N</em></sup></span>是一个大小为 <span class="math inline"><em>N</em></span>的固定图像数据库，与训练图像 <span class="math inline">𝒳</span>不相关，<span class="math inline"><em>ξ</em><sub><em>k</em></sub></span>则表示基于查询 <span class="math inline"><em>x</em></span>的采样策略，即 <spanclass="math inline"><em>ξ</em><sub><em>k</em></sub> : <em>x</em>, 𝒟 ↦ ℳ<sub>𝒟</sub><sup>(<em>k</em>)</sup></span>。</p><p>由此可以发现，<spanclass="math inline"><em>ξ</em><sub><em>k</em></sub>(<em>x</em>, 𝒟)</span>的选择很重要，以便模型能从检索数据库中提取到有用的信息。通常的做法是，考虑一个查询图像<spanclass="math inline"><em>x</em> ∈ ℝ<sup><em>H</em><sub><em>x</em></sub> × <em>W</em><sub><em>x</em></sub> × 3</sup></span>，<spanclass="math inline"><em>ξ</em><sub><em>k</em></sub>(<em>x</em>, 𝒟)</span>通过给定距离函数 <spanclass="math inline"><em>d</em>{<em>x</em>, ⋅}</span> ，返回 <spanclass="math inline"><em>k</em></span> 个最近邻的集合。得到检索的图像样本<spanclass="math inline"><em>y</em> ∈ ℳ<sub>𝒟</sub><sup>(<em>k</em>)</sup></span>后，再通过一个固定的预训练图像编码器 <spanclass="math inline"><em>ϕ</em></span>将高维度图像投射到低维流形。即：</p><p><span class="math display">$$\begin{split}    p_{\theta,\mathcal D,\xi_k}(x) =p_\theta(x|\{\phi(y)|y\in\xi_k(x,\mathcal D)\}).\end{split}$$</span></p><h3id="半参数图像生成模型的两种实例">3.2.2.半参数图像生成模型的两种实例</h3><p>在训练过程中，假设训练数据集 <spanclass="math inline">𝒳 = {<em>x</em><sub><em>i</em></sub>}<sub><em>i</em> = 1</sub><sup><em>M</em></sup></span>满足分布 <span class="math inline"><em>p</em>(<em>x</em>)</span>，我们使用的采样策略是使用CLIP在图像特征空间中计算余弦相似度来选取 <spanclass="math inline"><em>k</em></span> -近邻，同样后续也通过CLIP编码器<spanclass="math inline"><em>ϕ</em> = <em>ϕ</em><sub><em>C</em><em>L</em><em>I</em><em>P</em></sub></span>得到图像表征。</p><ol type="1"><li>Retrieval-Augmented DiffusionModels（RDM）使用LDMs作为底层图像生成模型，即：</li></ol><p><span class="math display">$$\begin{split}    \min_\theta\mathcal{L}=\mathbb{E}_{p(x),z\thicksimE(x),\epsilon\thicksim\mathcal{N}(0,1),t}\left[\|\epsilon-\epsilon_\theta(z_t,t,\left.\{\phi_{\mathbf{CLIP}}(y)\midy\in\xi_k(x,\mathcal{D})\}\right)\|_2^2\right].\end{split}$$</span></p><ol start="2" type="1"><li>Retrieval-Augmented AutoregressiveModels（RARM）使用VQGAN作为底层图像生成模型，即：</li></ol><p><span class="math display">$$\begin{split}    \min_\theta\mathcal{L}=-\mathbb{E}_{p(x),z_q\simE(x)}{\left[\sum_i\log p(z_q^{(i)}\midz_q^{(&lt;i)},\mathrm{~}\{\phi_{\mathbf{CLIP}}(y)\midy\in\xi_k(x,\mathcal{D})\})\right]}\end{split}$$</span></p><h3 id="推理过程">3.2.3.推理过程</h3><p>在推理过程中，<spanclass="math inline">𝒟, <em>ξ</em><sub><em>k</em></sub></span>是可以随着下游任务的要求而改变的，比如根据应用需求扩大或者缩减数据库<span class="math inline">𝒟</span> 、或是直接跳过检索提供条件集 <spanclass="math inline"><em>ϕ</em><sub><strong>C</strong><strong>L</strong><strong>I</strong><strong>P</strong></sub>(<em>y</em>)</span>。这也允许模型完成训练过程中未涉及到的其他任务，比如text-prompt或是class-labeled。</p><p>比如对于text-to-image任务，既可以通过text prompt <spanclass="math inline"><em>c</em><sub><em>t</em><em>e</em><em>x</em><em>t</em></sub></span>使用CLIP生成特征图，然后通过该特征图检索<spanclass="math inline"><em>k</em></span>-近邻作为条件生成；也可以直接通过<spanclass="math inline"><em>c</em><sub><em>t</em><em>e</em><em>x</em><em>t</em></sub></span>生成的特征图作为条件生成。</p><h1 id="源码">4.源码</h1><h2 id="dm模型">4.1.DM模型</h2><p>LDM定义的模型在 <code>ldm\models\diffusion\ddpm.py</code>中，因为使用pytorch-lighting进行训练，所以看起来很复杂，其实只需要关注模型结构部分。其中在模型<code>forward</code>部分，先对每个batch随机选取一个时间步进行训练，然后对条件输入进行处理，将其转为对应的中间特征，最终返回的是<code>p_losses</code>得到的损失。</p><p>而在<code>p_losses</code>中，则先随机初始化一个高斯噪声，通过前向过程<code>q_sample</code>得到<span class="math inline"><em>x</em><sub><em>t</em></sub></span>，接着将 <spanclass="math inline"><em>x</em><sub><em>t</em></sub></span> 、时间步<span class="math inline"><em>t</em></span> 、条件 <spanclass="math inline"><em>c</em></span>一同输入噪声预测网络中，得到预测噪声，之后通过l2损失计算loss。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, c, *args, **kwargs</span>):</span><br><span class="line">    t = torch.randint(<span class="number">0</span>, <span class="variable language_">self</span>.num_timesteps, (x.shape[<span class="number">0</span>],), device=<span class="variable language_">self</span>.device).long()</span><br><span class="line">    <span class="keyword">if</span> <span class="variable language_">self</span>.model.conditioning_key <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">assert</span> c <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.cond_stage_trainable:</span><br><span class="line">            c = <span class="variable language_">self</span>.get_learned_conditioning(c)</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.shorten_cond_schedule:  <span class="comment"># <span class="doctag">TODO:</span> drop this option</span></span><br><span class="line">            tc = <span class="variable language_">self</span>.cond_ids[t].to(<span class="variable language_">self</span>.device)</span><br><span class="line">            c = <span class="variable language_">self</span>.q_sample(x_start=c, t=tc, noise=torch.randn_like(c.<span class="built_in">float</span>()))</span><br><span class="line">    <span class="keyword">return</span> <span class="variable language_">self</span>.p_losses(x, c, t, *args, **kwargs)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">p_losses</span>(<span class="params">self, x_start, cond, t, noise=<span class="literal">None</span></span>):</span><br><span class="line">    noise = default(noise, <span class="keyword">lambda</span>: torch.randn_like(x_start))</span><br><span class="line">    x_noisy = <span class="variable language_">self</span>.q_sample(x_start=x_start, t=t, noise=noise)</span><br><span class="line">    model_output = <span class="variable language_">self</span>.apply_model(x_noisy, t, cond)</span><br><span class="line"></span><br><span class="line">    ...</span><br><span class="line">    loss = <span class="variable language_">self</span>.get_loss(model_out, target, mean=<span class="literal">False</span>).mean(dim=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss, loss_dict</span><br></pre></td></tr></table></figure><h3 id="前向过程">4.1.1.前向过程</h3><p>这里的前向过程与DDPM是保持一致的：</p><p><span class="math display">$$\begin{split}    q(x_t \mid x_0) =\mathcal{N}\bigl(x_t;\;\sqrt{\bar\alpha_t}\,x_0,\;(1-\bar\alpha_t)\,I\bigr).\end{split}$$</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">q_sample</span>(<span class="params">self, x_start, t, noise=<span class="literal">None</span></span>):</span><br><span class="line">    noise = default(noise, <span class="keyword">lambda</span>: torch.randn_like(x_start))</span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">        extract_into_tensor(<span class="variable language_">self</span>.sqrt_alphas_cumprod, t, x_start.shape) * x_start</span><br><span class="line">        + extract_into_tensor(<span class="variable language_">self</span>.sqrt_one_minus_alphas_cumprod, t, x_start.shape)</span><br><span class="line">        * noise</span><br><span class="line">    )</span><br></pre></td></tr></table></figure><h3 id="条件控制噪声预测">4.1.2.条件控制噪声预测</h3><p>这里噪声预测和DDPM一样还是使用UNet，不同在于为了引入条件控制加入了交叉注意力来嵌入条件信息。需要注意的是，时间步信息和条件信息的嵌入方式是不同的，时间步信息<span class="math inline"><em>t</em></span>是直接拼接到UNet每一层中的特征图，而条件信息 <spanclass="math inline"><em>c</em></span>则是通过每一层的交叉注意力进行嵌入的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, timesteps=<span class="literal">None</span>, context=<span class="literal">None</span>, y=<span class="literal">None</span>,**kwargs</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Apply the model to an input batch.</span></span><br><span class="line"><span class="string">    :param x: an [N x C x ...] Tensor of inputs.</span></span><br><span class="line"><span class="string">    :param timesteps: a 1-D batch of timesteps.</span></span><br><span class="line"><span class="string">    :param context: conditioning plugged in via crossattn</span></span><br><span class="line"><span class="string">    :param y: an [N] Tensor of labels, if class-conditional.</span></span><br><span class="line"><span class="string">    :return: an [N x C x ...] Tensor of outputs.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">assert</span> (y <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>) == (</span><br><span class="line">        <span class="variable language_">self</span>.num_classes <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line">    ), <span class="string">&quot;must specify y if and only if the model is class-conditional&quot;</span></span><br><span class="line">    hs = []</span><br><span class="line">    t_emb = timestep_embedding(timesteps, <span class="variable language_">self</span>.model_channels, repeat_only=<span class="literal">False</span>)</span><br><span class="line">    emb = <span class="variable language_">self</span>.time_embed(t_emb)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="variable language_">self</span>.num_classes <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">assert</span> y.shape == (x.shape[<span class="number">0</span>],)</span><br><span class="line">        emb = emb + <span class="variable language_">self</span>.label_emb(y)</span><br><span class="line"></span><br><span class="line">    h = x.<span class="built_in">type</span>(<span class="variable language_">self</span>.dtype)</span><br><span class="line">    <span class="keyword">for</span> module <span class="keyword">in</span> <span class="variable language_">self</span>.input_blocks:</span><br><span class="line">        h = module(h, emb, context)</span><br><span class="line">        hs.append(h)</span><br><span class="line">    h = <span class="variable language_">self</span>.middle_block(h, emb, context)</span><br><span class="line">    <span class="keyword">for</span> module <span class="keyword">in</span> <span class="variable language_">self</span>.output_blocks:</span><br><span class="line">        h = th.cat([h, hs.pop()], dim=<span class="number">1</span>)</span><br><span class="line">        h = module(h, emb, context)</span><br><span class="line">    h = h.<span class="built_in">type</span>(x.dtype)</span><br><span class="line">    <span class="keyword">if</span> <span class="variable language_">self</span>.predict_codebook_ids:</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.id_predictor(h)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.out(h)</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/unet_trans.png"/></p><h3 id="采样过程推理">4.1.3.采样过程（推理）</h3><p>可以直接用 DDIM 进行采样，大大减少推理所需时间。这部分源码可参考<ahref="https://litchi-lee.github.io/2025/04/17/AIGC/DDIM/">DDIM的细节</a>。</p><h2 id="感知图像压缩-1">4.2.感知图像压缩</h2><p>在<code>get_input</code>函数中，输入batch，将返回压缩后的图像和处理后的条件信息，这里我们先关注输入是如何进行压缩的，条件信息的处理将在后续重点讲解。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LatentDiffusion</span>(<span class="title class_ inherited__">DDPM</span>):</span><br><span class="line">    ...</span><br><span class="line"><span class="meta">    @torch.no_grad()</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_input</span>(<span class="params">self, batch, k, return_first_stage_outputs=<span class="literal">False</span>, force_c_encode=<span class="literal">False</span>, cond_key=<span class="literal">None</span>, return_original_cond=<span class="literal">False</span>, bs=<span class="literal">None</span></span>):</span><br><span class="line">        x = <span class="built_in">super</span>().get_input(batch, k)</span><br><span class="line">        <span class="keyword">if</span> bs <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            x = x[:bs]</span><br><span class="line">        x = x.to(<span class="variable language_">self</span>.device)</span><br><span class="line">        <span class="comment"># 感知图像压缩</span></span><br><span class="line">        encoder_posterior = <span class="variable language_">self</span>.encode_first_stage(x)</span><br><span class="line">        z = <span class="variable language_">self</span>.get_first_stage_encoding(encoder_posterior).detach()</span><br><span class="line">        ...</span><br><span class="line">        out = [z, c]</span><br><span class="line">        ...</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line">    </span><br><span class="line"><span class="meta">    @torch.no_grad()</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">encode_first_stage</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">hasattr</span>(<span class="variable language_">self</span>, <span class="string">&quot;split_input_params&quot;</span>):</span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>.split_input_params[<span class="string">&quot;patch_distributed_vq&quot;</span>]:</span><br><span class="line">            ...</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">return</span> <span class="variable language_">self</span>.first_stage_model.encode(x)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="variable language_">self</span>.first_stage_model.encode(x)</span><br></pre></td></tr></table></figure><p>进一步深入，可以发现输入是通过<code>self.first_stage_model.encode(x)</code>这个接口进行压缩的，源码中实现了两种自编码器用于encode，分别是VQVAE和VAE（在<code>ldm\models\autoencoder.py</code>中），但是由于原论文使用VQVAE，因此这里主要扒一扒VQVAE部分的源码。</p><h3 id="encode部分">4.2.1.encode部分</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">VQModel</span>(pl.LightningModule):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,...</span>)</span><br><span class="line">        ...</span><br><span class="line">        <span class="variable language_">self</span>.encoder = Encoder(**ddconfig)</span><br><span class="line">        <span class="variable language_">self</span>.quant_conv = torch.nn.Conv2d(ddconfig[<span class="string">&quot;z_channels&quot;</span>], embed_dim, <span class="number">1</span>)</span><br><span class="line">        <span class="variable language_">self</span>.quantize = VectorQuantizer(n_embed, embed_dim, beta=<span class="number">0.25</span>, remap=remap, sane_index_shape=sane_index_shape)</span><br><span class="line">    </span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">encode</span>(<span class="params">self, x</span>):</span><br><span class="line">        h = <span class="variable language_">self</span>.encoder(x)</span><br><span class="line">        h = <span class="variable language_">self</span>.quant_conv(h)</span><br><span class="line">        quant, emb_loss, info = <span class="variable language_">self</span>.quantize(h)</span><br><span class="line">        <span class="keyword">return</span> quant, emb_loss, info</span><br></pre></td></tr></table></figure><p>首先是第一部分Encoder部分，这一部分主要是一系列的下采样，最终得到中间特征图<spanclass="math inline"><em>h</em></span>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Encoder</span>(nn.Module):</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">    <span class="comment"># timestep embedding</span></span><br><span class="line">    temb = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># downsampling</span></span><br><span class="line">    hs = [<span class="variable language_">self</span>.conv_in(x)]</span><br><span class="line">    <span class="keyword">for</span> i_level <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.num_resolutions):</span><br><span class="line">        <span class="keyword">for</span> i_block <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.num_res_blocks):</span><br><span class="line">            h = <span class="variable language_">self</span>.down[i_level].block[i_block](hs[-<span class="number">1</span>], temb)</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.down[i_level].attn) &gt; <span class="number">0</span>:</span><br><span class="line">                h = <span class="variable language_">self</span>.down[i_level].attn[i_block](h)</span><br><span class="line">            hs.append(h)</span><br><span class="line">        <span class="keyword">if</span> i_level != <span class="variable language_">self</span>.num_resolutions-<span class="number">1</span>:</span><br><span class="line">            hs.append(<span class="variable language_">self</span>.down[i_level].downsample(hs[-<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># middle</span></span><br><span class="line">    h = hs[-<span class="number">1</span>]</span><br><span class="line">    h = <span class="variable language_">self</span>.mid.block_1(h, temb)</span><br><span class="line">    h = <span class="variable language_">self</span>.mid.attn_1(h)</span><br><span class="line">    h = <span class="variable language_">self</span>.mid.block_2(h, temb)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># end</span></span><br><span class="line">    h = <span class="variable language_">self</span>.norm_out(h)</span><br><span class="line">    h = nonlinearity(h)</span><br><span class="line">    h = <span class="variable language_">self</span>.conv_out(h)</span><br><span class="line">    <span class="keyword">return</span> h</span><br></pre></td></tr></table></figure><p>之后经过<code>quant_conv</code>将<spanclass="math inline"><em>h</em></span>的维度转为<code>embed_dim</code>，经过VectorQuantizer转为离散化量化向量。具体来说，先将原特征图reshape为<spanclass="math inline">(<em>b</em> * <em>h</em> * <em>w</em> * <em>c</em>, <em>d</em>)</span>的向量，方便与码本中的向量计算欧氏距离，然后选择最小距离的向量作为量化向量。之后计算量化损失（codebookloss+commitment loss）：</p><p><span class="math display">$$\begin{split}    \mathcal L_{vq} = \beta\|z_q - z.detach\| + \|z_q.detach - z\|\end{split}$$</span></p><p>由于中间操作涉及到取最小值，会阻断梯度的回传，因此额外加了一步<code>z_q = z + (z_q - z).detach()</code>用于保留梯度。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">VectorQuantizer2</span>(nn.Module):</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, z, temp=<span class="literal">None</span>, rescale_logits=<span class="literal">False</span>, return_logits=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="keyword">assert</span> temp <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> temp==<span class="number">1.0</span>, <span class="string">&quot;Only for interface compatible with Gumbel&quot;</span></span><br><span class="line">        <span class="keyword">assert</span> rescale_logits==<span class="literal">False</span>, <span class="string">&quot;Only for interface compatible with Gumbel&quot;</span></span><br><span class="line">        <span class="keyword">assert</span> return_logits==<span class="literal">False</span>, <span class="string">&quot;Only for interface compatible with Gumbel&quot;</span></span><br><span class="line">        <span class="comment"># reshape z -&gt; (batch, height, width, channel) and flatten</span></span><br><span class="line">        z = rearrange(z, <span class="string">&#x27;b c h w d -&gt; b h w d c&#x27;</span>).contiguous()</span><br><span class="line">        <span class="comment"># 展平为 (b * h * w * c, d) 的二维矩阵</span></span><br><span class="line">        z_flattened = z.view(-<span class="number">1</span>, <span class="variable language_">self</span>.e_dim)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算每个向量与codebook中所有向量的欧式距离平方 </span></span><br><span class="line">        <span class="comment"># (z - e)^2 = z^2 + e^2 - 2ze</span></span><br><span class="line">        <span class="comment"># 输出形状为 (b * h * w * c, n_codes)</span></span><br><span class="line">        d = torch.<span class="built_in">sum</span>(z_flattened ** <span class="number">2</span>, dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>) + \</span><br><span class="line">            torch.<span class="built_in">sum</span>(<span class="variable language_">self</span>.embedding.weight**<span class="number">2</span>, dim=<span class="number">1</span>) - <span class="number">2</span> * \</span><br><span class="line">            torch.einsum(<span class="string">&#x27;bd,dn-&gt;bn&#x27;</span>, z_flattened, rearrange(<span class="variable language_">self</span>.embedding.weight, <span class="string">&#x27;n d -&gt; d n&#x27;</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 为每个输入向量选出距离最小的codebook向量的下标</span></span><br><span class="line">        min_encoding_indices = torch.argmin(d, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 得到量化后的向量</span></span><br><span class="line">        z_q = <span class="variable language_">self</span>.embedding(min_encoding_indices).view(z.shape)</span><br><span class="line">        perplexity = <span class="literal">None</span></span><br><span class="line">        min_encodings = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 量化损失 = commit loss + codebook loss</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="variable language_">self</span>.legacy:</span><br><span class="line">            loss = <span class="variable language_">self</span>.beta * torch.mean((z_q.detach()-z)**<span class="number">2</span>) + \</span><br><span class="line">                   torch.mean((z_q - z.detach()) ** <span class="number">2</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> z_q.<span class="built_in">min</span>() &lt; <span class="variable language_">self</span>.lower <span class="keyword">or</span> z_q.<span class="built_in">max</span>() &gt; <span class="variable language_">self</span>.upper:</span><br><span class="line">                loss = torch.mean((z_q.detach() - z) ** <span class="number">2</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                loss = torch.mean((z_q.detach()-z)**<span class="number">2</span>) + <span class="variable language_">self</span>.beta * \</span><br><span class="line">                           torch.mean((z_q - z.detach()) ** <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># preserve gradients</span></span><br><span class="line">        z_q = z + (z_q - z).detach()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># reshape back to match original input shape</span></span><br><span class="line">        z_q = rearrange(z_q, <span class="string">&#x27;b h w d c -&gt; b c h w d&#x27;</span>).contiguous()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.remap <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            min_encoding_indices = min_encoding_indices.reshape(z.shape[<span class="number">0</span>],-<span class="number">1</span>) <span class="comment"># add batch axis</span></span><br><span class="line">            min_encoding_indices = <span class="variable language_">self</span>.remap_to_used(min_encoding_indices)</span><br><span class="line">            min_encoding_indices = min_encoding_indices.reshape(-<span class="number">1</span>,<span class="number">1</span>) <span class="comment"># flatten</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.sane_index_shape:</span><br><span class="line">            min_encoding_indices = min_encoding_indices.reshape(</span><br><span class="line">                z_q.shape[<span class="number">0</span>], z_q.shape[<span class="number">2</span>], z_q.shape[<span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> z_q, loss, (perplexity, min_encodings, min_encoding_indices)</span><br></pre></td></tr></table></figure><h3 id="decode部分">4.2.2.decode部分</h3><p>decode部分和encode一样，也是直接调用定义好的自编码器类内部的<code>decode</code>方法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LatentDiffusion</span>(<span class="title class_ inherited__">DDPM</span>):</span><br><span class="line">    ...</span><br><span class="line"><span class="meta">    @torch.no_grad()</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">decode_first_stage</span>(<span class="params">self, z, predict_cids=<span class="literal">False</span>, force_not_quantize=<span class="literal">False</span></span>):</span><br><span class="line">        ...</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(<span class="variable language_">self</span>.first_stage_model, VQModelInterface):</span><br><span class="line">                <span class="keyword">return</span> <span class="variable language_">self</span>.first_stage_model.decode(</span><br><span class="line">                    z, force_not_quantize=predict_cids <span class="keyword">or</span> force_not_quantize</span><br><span class="line">                )</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">return</span> <span class="variable language_">self</span>.first_stage_model.decode(z)</span><br></pre></td></tr></table></figure><p>和encode过程类似，不过相当于是反过来了，先过一层<code>post_quant_conv</code>恢复原特征维度，然后通过若干层上采样恢复原输入图像的尺寸。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">VQModel</span>(pl.LightningModule):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,...</span>)</span><br><span class="line">        ...</span><br><span class="line">        <span class="variable language_">self</span>.post_quant_conv = torch.nn.Conv2d(embed_dim, ddconfig[<span class="string">&quot;z_channels&quot;</span>], <span class="number">1</span>)</span><br><span class="line">        <span class="variable language_">self</span>.decoder = Decoder(**ddconfig)</span><br><span class="line"></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">decode</span>(<span class="params">self, quant</span>):</span><br><span class="line">        quant = <span class="variable language_">self</span>.post_quant_conv(quant)</span><br><span class="line">        dec = <span class="variable language_">self</span>.decoder(quant)</span><br><span class="line">        <span class="keyword">return</span> dec</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Decoder</span>(nn.Module):</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, z</span>):</span><br><span class="line">        <span class="comment">#assert z.shape[1:] == self.z_shape[1:]</span></span><br><span class="line">        <span class="variable language_">self</span>.last_z_shape = z.shape</span><br><span class="line"></span><br><span class="line">        <span class="comment"># timestep embedding</span></span><br><span class="line">        temb = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># z to block_in</span></span><br><span class="line">        h = <span class="variable language_">self</span>.conv_in(z)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># middle</span></span><br><span class="line">        h = <span class="variable language_">self</span>.mid.block_1(h, temb)</span><br><span class="line">        h = <span class="variable language_">self</span>.mid.attn_1(h)</span><br><span class="line">        h = <span class="variable language_">self</span>.mid.block_2(h, temb)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># upsampling</span></span><br><span class="line">        <span class="keyword">for</span> i_level <span class="keyword">in</span> <span class="built_in">reversed</span>(<span class="built_in">range</span>(<span class="variable language_">self</span>.num_resolutions)):</span><br><span class="line">            <span class="keyword">for</span> i_block <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.num_res_blocks+<span class="number">1</span>):</span><br><span class="line">                h = <span class="variable language_">self</span>.up[i_level].block[i_block](h, temb)</span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.up[i_level].attn) &gt; <span class="number">0</span>:</span><br><span class="line">                    h = <span class="variable language_">self</span>.up[i_level].attn[i_block](h)</span><br><span class="line">            <span class="keyword">if</span> i_level != <span class="number">0</span>:</span><br><span class="line">                h = <span class="variable language_">self</span>.up[i_level].upsample(h)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># end</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.give_pre_end:</span><br><span class="line">            <span class="keyword">return</span> h</span><br><span class="line"></span><br><span class="line">        h = <span class="variable language_">self</span>.norm_out(h)</span><br><span class="line">        h = nonlinearity(h)</span><br><span class="line">        h = <span class="variable language_">self</span>.conv_out(h)</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.tanh_out:</span><br><span class="line">            h = torch.tanh(h)</span><br><span class="line">        <span class="keyword">return</span> h</span><br></pre></td></tr></table></figure><h2 id="条件信息控制">4.3.条件信息控制</h2><p>之前在DM部分其实已经讲过条件特征是如何嵌入到扩散模型中的，这里来扒一扒在源码中是如何处理各类条件信息的，以及如何将其编码为统一的条件特征。可以看到条件信息都是通过<code>self.get_learned_conditioning</code>这个方法进行编码的，而在<code>self.get_learned_conditioning</code>中条件信息又是统一调用<code>self.cond_stage_model</code>中的方法进行编码的，因此我们将重点看一看各类条件信息编码器模型（定义在<code>ldm\modules\encoders\modules.py</code>中）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LatentDiffusion</span>(<span class="title class_ inherited__">DDPM</span>):</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, c, *args, **kwargs</span>):</span><br><span class="line">        ...</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.model.conditioning_key <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">assert</span> c <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>.cond_stage_trainable:</span><br><span class="line">                c = <span class="variable language_">self</span>.get_learned_conditioning(c)</span><br><span class="line">            ...</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.p_losses(x, c, t, *args, **kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_learned_conditioning</span>(<span class="params">self, c</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.cond_stage_forward <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">hasattr</span>(<span class="variable language_">self</span>.cond_stage_model, <span class="string">&#x27;encode&#x27;</span>) <span class="keyword">and</span> <span class="built_in">callable</span>(</span><br><span class="line">                <span class="variable language_">self</span>.cond_stage_model.encode</span><br><span class="line">            ):</span><br><span class="line">                c = <span class="variable language_">self</span>.cond_stage_model.encode(c)</span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">isinstance</span>(c, DiagonalGaussianDistribution):</span><br><span class="line">                    c = c.mode()</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                c = <span class="variable language_">self</span>.cond_stage_model(c)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">assert</span> <span class="built_in">hasattr</span>(<span class="variable language_">self</span>.cond_stage_model, <span class="variable language_">self</span>.cond_stage_forward)</span><br><span class="line">            c = <span class="built_in">getattr</span>(<span class="variable language_">self</span>.cond_stage_model, <span class="variable language_">self</span>.cond_stage_forward)(c)</span><br><span class="line">        <span class="keyword">return</span> c</span><br></pre></td></tr></table></figure><h3 id="类别编码器">4.3.1.类别编码器</h3><p>用于将<strong>类别信息</strong>转换为embedding向量，适用于分类条件生成任务。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ClassEmbedder</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, embed_dim, n_classes=<span class="number">1000</span>, key=<span class="string">&#x27;class&#x27;</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.key = key</span><br><span class="line">        <span class="variable language_">self</span>.embedding = nn.Embedding(n_classes, embed_dim)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, batch, key=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="keyword">if</span> key <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            key = <span class="variable language_">self</span>.key</span><br><span class="line">        <span class="comment"># this is for use in crossattn</span></span><br><span class="line">        c = batch[key][:, <span class="literal">None</span>]</span><br><span class="line">        c = <span class="variable language_">self</span>.embedding(c)</span><br><span class="line">        <span class="keyword">return</span> c</span><br></pre></td></tr></table></figure><h3 id="文本编码器">4.3.2.文本编码器</h3><h4 id="transformer">transformer</h4><p>普通的transformer编码器，输入<strong>TokenIDs</strong>，输出其embedding向量表示。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TransformerEmbedder</span>(<span class="title class_ inherited__">AbstractEncoder</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Some transformer encoder layers&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n_embed, n_layer, vocab_size, max_seq_len=<span class="number">77</span>, device=<span class="string">&quot;cuda&quot;</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.device = device</span><br><span class="line">        <span class="variable language_">self</span>.transformer = TransformerWrapper(num_tokens=vocab_size, max_seq_len=max_seq_len,</span><br><span class="line">                                              attn_layers=Encoder(dim=n_embed, depth=n_layer))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, tokens</span>):</span><br><span class="line">        tokens = tokens.to(<span class="variable language_">self</span>.device)  <span class="comment"># meh</span></span><br><span class="line">        z = <span class="variable language_">self</span>.transformer(tokens, return_embeddings=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> z</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">encode</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>(x)</span><br></pre></td></tr></table></figure><h4 id="bert">Bert</h4><p>输入<strong>原始文本</strong>，先使用BERT的Tokenizer将其转为TokenIDs，然后再调用<code>BertEmbedder</code>将其转为embedding向量。其中Tokenizer来自huggingface无法训练，后者可训练。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BERTTokenizer</span>(<span class="title class_ inherited__">AbstractEncoder</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; Uses a pretrained BERT tokenizer by huggingface. Vocab size: 30522 (?)&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, device=<span class="string">&quot;cuda&quot;</span>, vq_interface=<span class="literal">True</span>, max_length=<span class="number">77</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizerFast  <span class="comment"># <span class="doctag">TODO:</span> add to reuquirements</span></span><br><span class="line">        <span class="variable language_">self</span>.tokenizer = BertTokenizerFast.from_pretrained(<span class="string">&quot;bert-base-uncased&quot;</span>)</span><br><span class="line">        <span class="variable language_">self</span>.device = device</span><br><span class="line">        <span class="variable language_">self</span>.vq_interface = vq_interface</span><br><span class="line">        <span class="variable language_">self</span>.max_length = max_length</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, text</span>):</span><br><span class="line">        batch_encoding = <span class="variable language_">self</span>.tokenizer(text, truncation=<span class="literal">True</span>, max_length=<span class="variable language_">self</span>.max_length, return_length=<span class="literal">True</span>,</span><br><span class="line">                                        return_overflowing_tokens=<span class="literal">False</span>, padding=<span class="string">&quot;max_length&quot;</span>, return_tensors=<span class="string">&quot;pt&quot;</span>)</span><br><span class="line">        tokens = batch_encoding[<span class="string">&quot;input_ids&quot;</span>].to(<span class="variable language_">self</span>.device)</span><br><span class="line">        <span class="keyword">return</span> tokens</span><br><span class="line"></span><br><span class="line"><span class="meta">    @torch.no_grad()</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">encode</span>(<span class="params">self, text</span>):</span><br><span class="line">        tokens = <span class="variable language_">self</span>(text)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="variable language_">self</span>.vq_interface:</span><br><span class="line">            <span class="keyword">return</span> tokens</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span>, <span class="literal">None</span>, [<span class="literal">None</span>, <span class="literal">None</span>, tokens]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">decode</span>(<span class="params">self, text</span>):</span><br><span class="line">        <span class="keyword">return</span> text</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BERTEmbedder</span>(<span class="title class_ inherited__">AbstractEncoder</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Uses the BERT tokenizr model and add some transformer encoder layers&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n_embed, n_layer, vocab_size=<span class="number">30522</span>, max_seq_len=<span class="number">77</span>,</span></span><br><span class="line"><span class="params">                 device=<span class="string">&quot;cuda&quot;</span>,use_tokenizer=<span class="literal">True</span>, embedding_dropout=<span class="number">0.0</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.use_tknz_fn = use_tokenizer</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.use_tknz_fn:</span><br><span class="line">            <span class="variable language_">self</span>.tknz_fn = BERTTokenizer(vq_interface=<span class="literal">False</span>, max_length=max_seq_len)</span><br><span class="line">        <span class="variable language_">self</span>.device = device</span><br><span class="line">        <span class="variable language_">self</span>.transformer = TransformerWrapper(num_tokens=vocab_size, max_seq_len=max_seq_len,</span><br><span class="line">                                              attn_layers=Encoder(dim=n_embed, depth=n_layer),</span><br><span class="line">                                              emb_dropout=embedding_dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, text</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.use_tknz_fn:</span><br><span class="line">            tokens = <span class="variable language_">self</span>.tknz_fn(text)<span class="comment">#.to(self.device)</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            tokens = text</span><br><span class="line">        z = <span class="variable language_">self</span>.transformer(tokens, return_embeddings=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> z</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">encode</span>(<span class="params">self, text</span>):</span><br><span class="line">        <span class="comment"># output of length 77</span></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>(text)</span><br></pre></td></tr></table></figure><h4 id="clip">CLIP</h4><p>输入<strong>原始文本</strong>，使用OpenAICLIP的tokenizer以及文本编码器，生成文本的embedding表示，参数冻结无法训练。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">FrozenCLIPTextEmbedder</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Uses the CLIP transformer encoder for text.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, version=<span class="string">&#x27;ViT-L/14&#x27;</span>, device=<span class="string">&quot;cuda&quot;</span>, max_length=<span class="number">77</span>, n_repeat=<span class="number">1</span>, normalize=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.model, _ = clip.load(version, jit=<span class="literal">False</span>, device=<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">        <span class="variable language_">self</span>.device = device</span><br><span class="line">        <span class="variable language_">self</span>.max_length = max_length</span><br><span class="line">        <span class="variable language_">self</span>.n_repeat = n_repeat</span><br><span class="line">        <span class="variable language_">self</span>.normalize = normalize</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">freeze</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="variable language_">self</span>.model = <span class="variable language_">self</span>.model.<span class="built_in">eval</span>()</span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> <span class="variable language_">self</span>.parameters():</span><br><span class="line">            param.requires_grad = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, text</span>):</span><br><span class="line">        tokens = clip.tokenize(text).to(<span class="variable language_">self</span>.device)</span><br><span class="line">        z = <span class="variable language_">self</span>.model.encode_text(tokens)</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.normalize:</span><br><span class="line">            z = z / torch.linalg.norm(z, dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> z</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">encode</span>(<span class="params">self, text</span>):</span><br><span class="line">        z = <span class="variable language_">self</span>(text)</span><br><span class="line">        <span class="keyword">if</span> z.ndim==<span class="number">2</span>:</span><br><span class="line">            z = z[:, <span class="literal">None</span>, :]</span><br><span class="line">        z = repeat(z, <span class="string">&#x27;b 1 d -&gt; b k d&#x27;</span>, k=<span class="variable language_">self</span>.n_repeat)</span><br><span class="line">        <span class="keyword">return</span> z</span><br></pre></td></tr></table></figure><h3 id="图像编码器">4.3.3.图像编码器</h3><h4 id="图像缩放器">图像缩放器</h4><p>输入<strong>图像特征图</strong>，对其进行空间尺寸的调整（下采样或上采样以及维度映射），输出调整尺寸后的图像特征图。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SpatialRescaler</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                 n_stages=<span class="number">1</span>,</span></span><br><span class="line"><span class="params">                 method=<span class="string">&#x27;bilinear&#x27;</span>,</span></span><br><span class="line"><span class="params">                 multiplier=<span class="number">0.5</span>,</span></span><br><span class="line"><span class="params">                 in_channels=<span class="number">3</span>,</span></span><br><span class="line"><span class="params">                 out_channels=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 bias=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.n_stages = n_stages</span><br><span class="line">        <span class="keyword">assert</span> <span class="variable language_">self</span>.n_stages &gt;= <span class="number">0</span></span><br><span class="line">        <span class="keyword">assert</span> method <span class="keyword">in</span> [<span class="string">&#x27;nearest&#x27;</span>,<span class="string">&#x27;linear&#x27;</span>,<span class="string">&#x27;bilinear&#x27;</span>,<span class="string">&#x27;trilinear&#x27;</span>,<span class="string">&#x27;bicubic&#x27;</span>,<span class="string">&#x27;area&#x27;</span>]</span><br><span class="line">        <span class="variable language_">self</span>.multiplier = multiplier</span><br><span class="line">        <span class="variable language_">self</span>.interpolator = partial(torch.nn.functional.interpolate, mode=method)</span><br><span class="line">        <span class="variable language_">self</span>.remap_output = out_channels <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.remap_output:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&#x27;Spatial Rescaler mapping from <span class="subst">&#123;in_channels&#125;</span> to <span class="subst">&#123;out_channels&#125;</span> channels after resizing.&#x27;</span>)</span><br><span class="line">            <span class="variable language_">self</span>.channel_mapper = nn.Conv2d(in_channels,out_channels,<span class="number">1</span>,bias=bias)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        <span class="keyword">for</span> stage <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.n_stages):</span><br><span class="line">            x = <span class="variable language_">self</span>.interpolator(x, scale_factor=<span class="variable language_">self</span>.multiplier)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.remap_output:</span><br><span class="line">            x = <span class="variable language_">self</span>.channel_mapper(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">encode</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>(x)</span><br></pre></td></tr></table></figure><h4 id="clip-1">CLIP</h4><p>输入<strong>图像特征</strong>，使用OpenAICLIP图像编码器，将其转为图像对应的embedding向量，参数冻结同样无法训练。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">FrozenClipImageEmbedder</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Uses the CLIP image encoder.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">            self,</span></span><br><span class="line"><span class="params">            model,</span></span><br><span class="line"><span class="params">            jit=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">            device=<span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available(<span class="params"></span>) <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>,</span></span><br><span class="line"><span class="params">            antialias=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">        </span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.model, _ = clip.load(name=model, device=device, jit=jit)</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.antialias = antialias</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.register_buffer(<span class="string">&#x27;mean&#x27;</span>, torch.Tensor([<span class="number">0.48145466</span>, <span class="number">0.4578275</span>, <span class="number">0.40821073</span>]), persistent=<span class="literal">False</span>)</span><br><span class="line">        <span class="variable language_">self</span>.register_buffer(<span class="string">&#x27;std&#x27;</span>, torch.Tensor([<span class="number">0.26862954</span>, <span class="number">0.26130258</span>, <span class="number">0.27577711</span>]), persistent=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">preprocess</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># normalize to [0,1]</span></span><br><span class="line">        x = kornia.geometry.resize(x, (<span class="number">224</span>, <span class="number">224</span>),</span><br><span class="line">                                   interpolation=<span class="string">&#x27;bicubic&#x27;</span>,align_corners=<span class="literal">True</span>,</span><br><span class="line">                                   antialias=<span class="variable language_">self</span>.antialias)</span><br><span class="line">        x = (x + <span class="number">1.</span>) / <span class="number">2.</span></span><br><span class="line">        <span class="comment"># renormalize according to clip</span></span><br><span class="line">        x = kornia.enhance.normalize(x, <span class="variable language_">self</span>.mean, <span class="variable language_">self</span>.std)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># x is assumed to be in range [-1,1]</span></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.model.encode_image(<span class="variable language_">self</span>.preprocess(x))</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AIGC </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>重建任务常用指标</title>
      <link href="/2025/04/26/AIGC/metrics/"/>
      <url>/2025/04/26/AIGC/metrics/</url>
      
        <content type="html"><![CDATA[<h1 id="psnr">PSNR</h1><p>峰值信噪比（Peak Signal-to-NoiseRatio，PSNR）是<strong>像素级误差指标</strong>，用于衡量重建图像与参考图像之间的像素误差，值越大表示图像越接近参考图像，单位dB。</p><p><span class="math display">$$  \begin{split}    PSNR = 10 \cdot log_{10}(\frac{MAX^2}{MSE})  \end{split}$$</span></p><p>其中 <span class="math inline">$MSE = \frac{1}{n} \sum(x-\hat{x})^2$</span> 。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">psnr = <span class="number">20</span> * np.log10(PIXEL_MAX / np.sqrt(np.power(img1 - img2, <span class="number">2</span>)))</span><br></pre></td></tr></table></figure><p>或者在pytorch中，可以写作：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_psnr</span>(<span class="params">pred, target, max_val=<span class="number">1.0</span></span>):</span><br><span class="line">    mse = F.mse_loss(pred, target)</span><br><span class="line">    psnr = <span class="number">10</span> * torch.log10(max_val ** <span class="number">2</span> / mse)</span><br><span class="line">    <span class="keyword">return</span> psnr.item()</span><br></pre></td></tr></table></figure><h1 id="ssim">SSIM</h1><p>结构相似性（Structure SimilarityIndex，SSIM），衡量图像在亮度、对比度、结构上的相似性，范围为 <spanclass="math inline">[−1, 1]</span> ，越接近1表示越相似。</p><p><span class="math display">$$\begin{split}    \mathrm{SSIM}(x,y)=\frac{(2\mu_x\mu_y+C_1)(2\sigma_{xy}+C_2)}{(\mu_x^2+\mu_y^2+C_1)(\sigma_x^2+\sigma_y^2+C_2)}\end{split}$$</span></p><p>在pytorch中，直接调用：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> piq <span class="keyword">import</span> ssim</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_ssim</span>(<span class="params">pred, target</span>):</span><br><span class="line">    <span class="keyword">return</span> ssim(pred, target, data_range=<span class="number">1.0</span>).item()</span><br></pre></td></tr></table></figure><p><strong>MS-SSIM</strong>（Multi-ScaleSSIM）是SSIM的扩展版本，更鲁棒细致。在pytorch中同样可以直接调包：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> piq <span class="keyword">import</span> multi_scale_ssim</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_ms_ssim</span>(<span class="params">pred, target</span>):</span><br><span class="line">    <span class="keyword">return</span> multi_scale_ssim(pred, target, data_range=<span class="number">1.0</span>).item()</span><br></pre></td></tr></table></figure><h1 id="lpips">LPIPS</h1><p><strong>LPIPS</strong>(Learned Perceptual Image PatchSimilarity)基于深度网络（如Alexnet或者VGG）提取特征后计算相似性，更强调感知上的差距，范围为<span class="math inline">[−1, 1]</span>，越小越好。在pytorch中可直接调用：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> lpips</span><br><span class="line">loss_fn = lpips.LPIPS(net=<span class="string">&#x27;alex&#x27;</span>)  <span class="comment"># 可选: alex / vgg / squeeze</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_lpips</span>(<span class="params">pred, target</span>):</span><br><span class="line">    <span class="keyword">return</span> loss_fn(pred, target).item()</span><br></pre></td></tr></table></figure><h1 id="hfen">HFEN</h1><p><strong>HFEN</strong>（High-Frequency ErrorNorm）用于评估图像中的高频信息是否被重建出来。具体来说，它将图像通过<strong>LoG</strong>（Laplacianof Gaussian）滤波器提取高频信息，然后计算误差。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">hfen</span>(<span class="params">pred, target</span>):</span><br><span class="line">    <span class="comment"># 使用 LoG 近似（可以用更精确的方法替代）</span></span><br><span class="line">    laplacian_kernel = torch.tensor([[[[<span class="number">0</span>,  <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">                                       [<span class="number">1</span>, -<span class="number">4</span>, <span class="number">1</span>],</span><br><span class="line">                                       [<span class="number">0</span>,  <span class="number">1</span>, <span class="number">0</span>]]]], dtype=pred.dtype, device=pred.device)</span><br><span class="line">    pred_hf = F.conv2d(pred, laplacian_kernel, padding=<span class="number">1</span>)</span><br><span class="line">    target_hf = F.conv2d(target, laplacian_kernel, padding=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> F.mse_loss(pred_hf, target_hf).item()</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AIGC </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DDIM总结</title>
      <link href="/2025/04/17/AIGC/DDIM/"/>
      <url>/2025/04/17/AIGC/DDIM/</url>
      
        <content type="html"><![CDATA[<p><strong>系列文章：</strong></p><ol class="series-items"><li><a href="/2025/03/28/AIGC/overview_DM/" title="扩散模型的发展（简略版）">扩散模型的发展（简略版）</a></li><li><a href="/2025/04/15/AIGC/DDPM/" title="DDPM总结">DDPM总结</a></li><li><a href="/2025/04/17/AIGC/DDIM/" title="DDIM总结">DDIM总结</a></li><li><a href="/2025/04/27/AIGC/LDM/" title="LDM的细节">LDM的细节</a></li><li><a href="/2025/05/08/AIGC/DIT/" title="DiT的细节">DiT的细节</a></li><li><a href="/2025/05/08/AIGC/iDDPM/" title="iDDPM总结">iDDPM总结</a></li><li><a href="/2026/02/27/AIGC/JiT/" title="PixelDiT & JiT总结">PixelDiT & JiT总结</a></li><li><a href="/2026/02/27/AIGC/Flow_Matching/" title="Flow Matching简要介绍">Flow Matching简要介绍</a></li></ol><h1 id="论文回顾">论文回顾</h1><blockquote><p>原论文：<a href="http://arxiv.org/abs/2010.02502">Denoising DiffusionImplicit Models</a></p></blockquote><h2 id="背景">背景</h2><p>近期的降噪过程要么是基于朗之万动力学（NCSN）或是基于加噪过程的逆过程（DDPM），但是这些方法都有很严重的缺陷–需要很多次迭代来生成高质量的图像。本文提出了DDIM（DenoisingDiffusion ImplicitModels），将<strong>原基于马尔可夫假设的DDPM推广到了非马尔科夫过程的DDPM</strong>，加速采样过程。</p><h2 id="非马尔可夫前向过程的变分推理">非马尔可夫前向过程的变分推理</h2><p>回顾DDPM中的目标函数，可以将其写作（其中<spanclass="math inline"><em>γ</em><sub><em>t</em></sub></span>是一些常数项）：</p><p><span class="math display">$$    \begin{split}        L_{\gamma} :=  \sum_{t=1}^T \gamma_t \mathbb E_{q(x_t|x_0)}[\left\lVert\epsilon_t  - {\hat\epsilon}_{\theta}(x_t, t)\right\rVert_2^2], \epsilon_t \sim \mathcal{N}(0,\textit{I})    \end{split}$$</span></p><p>可以发现，其实<spanclass="math inline"><em>L</em><sub><em>γ</em></sub></span>只依赖于边缘分布<spanclass="math inline"><em>q</em>(<em>x</em><sub><em>t</em></sub>|<em>x</em><sub>0</sub>)</span>而不直接依赖于联合分布<spanclass="math inline"><em>q</em>(<em>x</em><sub>1 : <em>T</em></sub>|<em>x</em><sub>0</sub>)</span>，换句话说只要<spanclass="math inline"><em>q</em>(<em>x</em><sub><em>t</em></sub>|<em>x</em><sub>0</sub>)</span>已知并且满足高斯分布的形式，那么就可以用DDPM预测噪音的目标函数训练模型。进一步说，只要<strong>我们保证<spanclass="math inline"><em>q</em>(<em>x</em><sub><em>t</em></sub>|<em>x</em><sub>0</sub>)</span>的形式不变，那么我们就可以复用训好的DDPM，然后定义新的采样过程</strong>。</p><h3 id="非马尔可夫前向过程">非马尔可夫前向过程</h3><p>（1）考虑一个非马尔可夫过程，定义为（与DDPM不同）：</p><p><span class="math display">$$\begin{split}q_\sigma(\mathbf{x}_{1:T}|\mathbf{x}_0):=q_\sigma(\mathbf{x}_T|\mathbf{x}_0)\prod_{t=2}^Tq_\sigma(\mathbf{x}_{t-1}|\mathbf{x}_t,\mathbf{x}_0)\end{split}$$</span></p><blockquote><p>注意在DDPM中，马尔可夫前向过程为： <span class="math display">$$\begin{split}q(\mathbf{x}_{1:T}|\mathbf{x}_0):=\prod_{t=1}^Tq(\mathbf{x}_{t}|\mathbf{x}_{t-1})\end{split}$$</span></p></blockquote><p>（2）并且定义（与DDPM一致）：</p><p><span class="math display">$$\begin{split}q_\sigma(\mathbf{x}_T|\mathbf{x}_0)=\mathcal{N}(\sqrt{\alpha_T}\mathbf{x}_0,(1-\alpha_T)\mathbf{I})\end{split}$$</span></p><div class="note warning flat"><p>要注意这里的符号和DDPM中不太一样，这里的<spanclass="math inline"><em>α</em><sub><em>T</em></sub></span>相当于DDPM中的<spanclass="math inline"><em>ᾱ</em><sub><em>T</em></sub></span></p></div><p>（3）对<span class="math inline"><em>t</em> &gt; 1</span>定义：</p><p><span class="math display">$$\begin{split}q_\sigma(\mathbf{x}_{t-1}|\mathbf{x}_t,\mathbf{x}_0)=\mathcal{N}\left(\sqrt{\alpha_{t-1}}\mathbf{x}_0+\sqrt{1-\alpha_{t-1}-\sigma_t^2}\cdot\frac{\mathbf{x}_t-\sqrt{\alpha_t}\mathbf{x}_0}{\sqrt{1-\alpha_t}},\sigma_t^2\mathbf{I}\right).\end{split}$$</span></p><p>只要满足上述三个定义，就可以确保之前所说的<spanclass="math inline"><em>q</em>(<em>x</em><sub><em>t</em></sub>|<em>x</em><sub>0</sub>)</span>的形式不改变（原论文附录证明），即对任意<spanclass="math inline"><em>t</em></span>，满足：</p><p><span class="math display">$$\begin{split}q_\sigma(\mathbf{x}_{t}|\mathbf{x}_0)=\mathcal{N}\left(\sqrt{\alpha_t}\mathbf{x}_0,(1-\alpha_t)\mathbf{I}\right).\end{split}$$</span></p><p>由此可以根据贝叶斯公式得到：</p><p><span class="math display">$$\begin{split}q_\sigma(\mathbf{x}_t|\mathbf{x}_{t-1},\mathbf{x}_0)=\frac{q_\sigma(\mathbf{x}_{t-1}|\mathbf{x}_t,\mathbf{x}_0)q_\sigma(\mathbf{x}_t|\mathbf{x}_0)}{q_\sigma(\mathbf{x}_{t-1}|\mathbf{x}_0)}\end{split}$$</span></p><p>此时这里的前向过程不再是马尔可夫过程，<spanclass="math inline"><em>σ</em></span>的大小控制着前向过程的随机性，当<spanclass="math inline"><em>σ</em> → 0</span>时，就是一个确定性过程，即对<spanclass="math inline"><em>t</em></span>一旦确定<spanclass="math inline"><em>x</em><sub>0</sub></span>和<spanclass="math inline"><em>x</em><sub><em>t</em></sub></span>，<spanclass="math inline"><em>x</em><sub><em>t</em> − 1</sub></span>也随之确定。</p><h3 id="采样过程">采样过程</h3><p>回顾DDPM的采样过程，我们使用一个噪声预测模型<spanclass="math inline"><em>ϵ</em><sub><em>θ</em></sub></span>来预测<spanclass="math inline"><em>ϵ</em><sub><em>t</em></sub></span>，即<spanclass="math inline">$\mathbf{x}_t=\sqrt{\alpha_t}\mathbf{x}_0+\sqrt{1-\alpha_t}\epsilon_\theta$</span>，由此我们可以根据<spanclass="math inline"><em>x</em><sub><em>t</em></sub></span>预测<spanclass="math inline"><em>x</em><sub>0</sub></span>：</p><p><span class="math display">$$\begin{split}f_\theta^{(t)}(\mathbf{x}_t):=(\mathbf{x}_t-\sqrt{1-\alpha_t}\cdot\epsilon_\theta^{(t)}(\mathbf{x}_t))/\sqrt{\alpha_t}.\end{split}$$</span></p><p>进一步得到采样过程的后验公式：</p><p><span class="math display">$$p_\theta^{(t)}(\mathbf{x}_{t-1}|\mathbf{x}_t)=\begin{cases}\mathcal{N}(f_\theta^{(1)}(\mathbf{x}_1),\sigma_1^2\mathbf{I}) &amp;\mathrm{if~}t=1 \\q_\sigma(\mathbf{x}_{t-1}|\mathbf{x}_t,f_\theta^{(t)}(\mathbf{x}_t))&amp; \text{otherwise,}\end{cases}$$</span></p><p>由此可以得到非马尔可夫过程的扩散过程。</p><h2 id="从广义生成过程中采样">从广义生成过程中采样</h2><h3 id="扩散隐模型的降噪过程">扩散隐模型的降噪过程</h3><p>进一步带入公式求解，可以<strong>从<spanclass="math inline"><em>x</em><sub><em>t</em></sub></span>得到<spanclass="math inline"><em>x</em><sub><em>t</em> − 1</sub></span>的求解</strong>：</p><p><span class="math display">$$\begin{split}x_{t-1}=\sqrt{\alpha_{t-1}}\underbrace{\left(\frac{x_t-\sqrt{1-\alpha_t}\epsilon_\theta^{(t)}(x_t)}{\sqrt{\alpha_t}}\right)}_{\text{“predicted}\mathbf{x}_0\text{”}}+\underbrace{\sqrt{1-\alpha_{t-1}-\sigma_t^2}\cdot\epsilon_\theta^{(t)}(\mathbf{x}_t)}_{\text{“direction}\,\text{pointing}\,\text{to}\,\mathbf{x}_t\text{”}}+\underbrace{\sigma_t\epsilon_t}_{\text{random}\,\text{noise}}\end{split}$$</span></p><p>由上述公式可以发现，在给定<spanclass="math inline"><em>x</em><sub><em>t</em></sub></span>和<spanclass="math inline"><em>ϵ</em><sub><em>θ</em></sub></span>时，此时只有<spanclass="math inline"><em>σ</em><sub><em>t</em></sub></span>会导致随机性，构成不同的生成过程。我们不妨讨论一下这里<spanclass="math inline"><em>σ</em><sub><em>t</em></sub></span>的取值，原文考虑了2种情况：</p><ol type="1"><li>当<spanclass="math inline">$\sigma_t=\sqrt{(1-\alpha_{t-1})/(1-\alpha_t)}\sqrt{1-\alpha_t/\alpha_{t-1}}$</span>时，此时生成过程退化为<strong>DDPM</strong>（推导略）；</li><li>当<spanclass="math inline"><em>σ</em><sub><em>t</em></sub> = 0</span>时，此时该生成过程为确定性过程，我们将这个生成过程命名为<strong>DDIM</strong>。</li></ol><p>在实验过程中，使用参数<spanclass="math inline"><em>η</em></span>来进行控制随机性生成：</p><p><span class="math display">$$\begin{split}\sigma_{\tau_i}(\eta)=\eta\sqrt{(1-\alpha_{\tau_{i-1}})/(1-\alpha_{\tau_i})}\sqrt{1-\alpha_{\tau_i}/\alpha_{\tau_{i-1}}}\end{split}$$</span></p><h3 id="加速采样过程">加速采样过程</h3><p>由于只要满足<spanclass="math inline"><em>q</em>(<em>x</em><sub><em>t</em></sub>|<em>x</em><sub>0</sub>)</span>的形式不改变，我们就可以复用DDPM训好的模型，而DDPM在前向和反向过程中都需要迭代<spanclass="math inline"><em>T</em></span>步，我们不妨来考虑一下步长小于<spanclass="math inline"><em>T</em></span>的前向过程。</p><p>考虑一个从原始序列 <spanclass="math inline">[1, …, <em>T</em>]</span> 采样得到的长度为 <spanclass="math inline"><em>S</em></span> 的子序列 <spanclass="math inline">[<em>τ</em><sub>1</sub>, …, <em>τ</em><sub><em>S</em></sub>]</span>，此时前向过程 <spanclass="math inline">[<em>x</em><sub><em>τ</em><sub>1</sub></sub>, …, <em>x</em><sub><em>τ</em><sub><em>S</em></sub></sub>]</span>同样满足 <spanclass="math inline">$q({x}_{\tau_i}|{x}_0)=\mathcal{N}({x}_t;\sqrt{\alpha_{\tau_i}}{x}_0,(1-\alpha_{\tau_i}){I})$</span>。此时对于采样过程便可以同样使用子序列进行采样达到加速采样的效果。</p><h1 id="源码">源码</h1><p>来源于LDM（<ahref="https://github.com/CompVis/latent-diffusion">https://github.com/CompVis/latent-diffusion</a>），和原论文可能有出入，但是核心实现是一样的。</p><p>DDIM代码主要逻辑在<code>ldm\models\diffusion\ddim.py</code>中，在训练时使用DDPM进行训练，推理时使用DDIM进行采样来加速。在DDIM中使用<code>sample</code>函数作为接口进行采样，其中比较重要的参数包括<strong>DDIM采样步数<spanclass="math inline"><em>S</em></span>、随机化生成参数<spanclass="math inline"><em>η</em></span></strong>、条件信息<spanclass="math inline"><em>c</em></span>（适配条件生成）、无条件引导参数（适配Classifier-freeguidance）等，我们其实主要关注前两个参数即可，这里实验设置为<spanclass="math inline"><em>S</em> = 200, <em>η</em> = 0</span>，即论文所说的确定性<strong>DDIM采样</strong>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sample</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">           S,</span></span><br><span class="line"><span class="params">           batch_size,</span></span><br><span class="line"><span class="params">           shape,</span></span><br><span class="line"><span class="params">           conditioning=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">           callback=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">           normals_sequence=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">           img_callback=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">           quantize_x0=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">           eta=<span class="number">0.</span>,</span></span><br><span class="line"><span class="params">           mask=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">           x0=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">           temperature=<span class="number">1.</span>,</span></span><br><span class="line"><span class="params">           noise_dropout=<span class="number">0.</span>,</span></span><br><span class="line"><span class="params">           score_corrector=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">           corrector_kwargs=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">           verbose=<span class="literal">True</span>,</span></span><br><span class="line"><span class="params">           x_T=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">           log_every_t=<span class="number">100</span>,</span></span><br><span class="line"><span class="params">           unconditional_guidance_scale=<span class="number">1.</span>,</span></span><br><span class="line"><span class="params">           unconditional_conditioning=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">           <span class="comment"># this has to come in the same format as the conditioning, # e.g. as encoded tokens, ...</span></span></span><br><span class="line"><span class="params">           **kwargs</span></span><br><span class="line"><span class="params">           </span>):</span><br><span class="line">    <span class="keyword">if</span> conditioning <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(conditioning, <span class="built_in">dict</span>):</span><br><span class="line">            cbs = conditioning[<span class="built_in">list</span>(conditioning.keys())[<span class="number">0</span>]].shape[<span class="number">0</span>]</span><br><span class="line">            <span class="keyword">if</span> cbs != batch_size:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;Warning: Got <span class="subst">&#123;cbs&#125;</span> conditionings but batch-size is <span class="subst">&#123;batch_size&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> conditioning.shape[<span class="number">0</span>] != batch_size:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;Warning: Got <span class="subst">&#123;conditioning.shape[<span class="number">0</span>]&#125;</span> conditionings but batch-size is <span class="subst">&#123;batch_size&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="variable language_">self</span>.make_schedule(ddim_num_steps=S, ddim_eta=eta, verbose=verbose)</span><br><span class="line">    <span class="comment"># sampling</span></span><br><span class="line">    C, H, W = shape</span><br><span class="line">    size = (batch_size, C, H, W)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;Data shape for DDIM sampling is <span class="subst">&#123;size&#125;</span>, eta <span class="subst">&#123;eta&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    samples, intermediates = <span class="variable language_">self</span>.ddim_sampling(conditioning, size,</span><br><span class="line">                                                callback=callback,</span><br><span class="line">                                                 img_callback=img_callback,</span><br><span class="line">                                                 quantize_denoised=quantize_x0,</span><br><span class="line">                                                 mask=mask, x0=x0,</span><br><span class="line">                                                 ddim_use_original_steps=<span class="literal">False</span>,</span><br><span class="line">                                                 noise_dropout=noise_dropout,</span><br><span class="line">                                                 temperature=temperature,</span><br><span class="line">                                                 score_corrector=score_corrector,</span><br><span class="line">                                                 corrector_kwargs=corrector_kwargs ,</span><br><span class="line">                                                 x_T=x_T,</span><br><span class="line">                                                 log_every_t=log_every_t,</span><br><span class="line">                                                 unconditional_guidance_scale=unconditional_guidance_scale,</span><br><span class="line">                                                unconditional_conditioning=unconditional_conditioning,</span><br><span class="line">                                                )</span><br><span class="line">    <span class="keyword">return</span> samples, intermediates</span><br></pre></td></tr></table></figure><h2 id="参数计算">参数计算</h2><p>在<code>make_schedule</code>函数中，定义了绝大多数需要用到的中间计算参数。</p><p>首先是DDIM所用到的时间步 <spanclass="math inline">[<em>τ</em><sub>1</sub>, …, <em>τ</em><sub><em>S</em></sub>]</span>，可以以两种方式进行生成：<code>uniform</code>方法和<code>quad</code>方法。<code>uniform</code>方法用于生成均匀分布的时间步，时间间隔为<code>c = num_ddpm_timesteps // num_ddim_timesteps</code>，这也是常用的方式；而<code>quad</code>方法用于生成二次函数分布的非均匀时间步，首先在<span class="math inline">$[0,\sqrt{0.8*T}]$</span>中生成均匀分布的点，然后再对这些点进行平方。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">make_schedule</span>(<span class="params">self, ddim_num_steps, ddim_discretize=<span class="string">&quot;uniform&quot;</span>, ddim_eta=<span class="number">0.</span>, verbose=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="variable language_">self</span>.ddim_timesteps = make_ddim_timesteps(ddim_discr_method=ddim_discretize, num_ddim_timesteps=ddim_num_steps, num_ddpm_timesteps=<span class="variable language_">self</span>.ddpm_num_timesteps,verbose=verbose)</span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">make_ddim_timesteps</span>(<span class="params">ddim_discr_method, num_ddim_timesteps, num_ddpm_timesteps, verbose=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="keyword">if</span> ddim_discr_method == <span class="string">&#x27;uniform&#x27;</span>:</span><br><span class="line">        c = num_ddpm_timesteps // num_ddim_timesteps</span><br><span class="line">        ddim_timesteps = np.asarray(<span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">0</span>, num_ddpm_timesteps, c)))</span><br><span class="line">    <span class="keyword">elif</span> ddim_discr_method == <span class="string">&#x27;quad&#x27;</span>:</span><br><span class="line">        ddim_timesteps = ((np.linspace(<span class="number">0</span>, np.sqrt(num_ddpm_timesteps * <span class="number">.8</span>), num_ddim_timesteps)) ** <span class="number">2</span>).astype(<span class="built_in">int</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError(<span class="string">f&#x27;There is no ddim discretization method called &quot;<span class="subst">&#123;ddim_discr_method&#125;</span>&quot;&#x27;</span>)</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> steps_out</span><br></pre></td></tr></table></figure><p>其次主要是 <spanclass="math inline"><em>α</em><sub><em>τ</em><sub><em>i</em></sub></sub></span>和 <spanclass="math inline"><em>σ</em><sub><em>τ</em><sub><em>i</em></sub></sub></span>的计算，<spanclass="math inline"><em>α</em><sub><em>τ</em><sub><em>i</em></sub></sub></span>（<code>ddim_alphas</code>）相当于是在原来DDPM的 <spanclass="math inline"><em>α</em><sub><em>t</em></sub></span>上在新的子序列时间步上采样得到，这里为了方便后续计算也提前定义好了 <spanclass="math inline"><em>α</em><sub><em>τ</em><sub><em>i</em> − 1</sub></sub></span>（<code>ddim_alphas_prev</code>）和 <spanclass="math inline">$\sqrt{1-\alpha_{\tau_i}}$</span>（<code>ddim_sqrt_one_minus_alphas</code>），<spanclass="math inline"><em>σ</em><sub><em>τ</em><sub><em>i</em></sub></sub></span>（<code>ddim_sigmas</code>）的计算则是直接使用公式<spanclass="math inline">$\sigma_{\tau_i}(\eta)=\eta\sqrt{(1-\alpha_{\tau_{i-1}})/(1-\alpha_{\tau_i})}\sqrt{1-\alpha_{\tau_i}/\alpha_{\tau_{i-1}}}$</span>得到。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">make_schedule</span>(<span class="params">self, ddim_num_steps, ddim_discretize=<span class="string">&quot;uniform&quot;</span>, ddim_eta=<span class="number">0.</span>, verbose=<span class="literal">True</span></span>):</span><br><span class="line">    ...</span><br><span class="line">    ddim_sigmas, ddim_alphas, ddim_alphas_prev = make_ddim_sampling_parameters(alphacums=alphas_cumprod.cpu(), ddim_timesteps=<span class="variable language_">self</span>.ddim_timesteps, eta=ddim_eta,verbose=verbose)</span><br><span class="line">    <span class="variable language_">self</span>.register_buffer(<span class="string">&#x27;ddim_sigmas&#x27;</span>, ddim_sigmas)</span><br><span class="line">    <span class="variable language_">self</span>.register_buffer(<span class="string">&#x27;ddim_alphas&#x27;</span>, ddim_alphas)</span><br><span class="line">    <span class="variable language_">self</span>.register_buffer(<span class="string">&#x27;ddim_alphas_prev&#x27;</span>, ddim_alphas_prev)</span><br><span class="line">    <span class="variable language_">self</span>.register_buffer(<span class="string">&#x27;ddim_sqrt_one_minus_alphas&#x27;</span>, np.sqrt(<span class="number">1.</span> - ddim_alphas))</span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">make_ddim_sampling_parameters</span>(<span class="params">alphacums, ddim_timesteps, eta, verbose=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="comment"># alpha_t的计算</span></span><br><span class="line">    alphas = alphacums[ddim_timesteps]</span><br><span class="line">    <span class="comment"># alpha_&#123;t-1&#125;的计算</span></span><br><span class="line">    alphas_prev = np.asarray([alphacums[<span class="number">0</span>]] + alphacums[ddim_timesteps[:-<span class="number">1</span>]].tolist())</span><br><span class="line">    <span class="comment"># sigma_t的计算</span></span><br><span class="line">    sigmas = eta * np.sqrt((<span class="number">1</span> - alphas_prev) / (<span class="number">1</span> - alphas) * (<span class="number">1</span> - alphas / alphas_prev))</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> sigmas, alphas, alphas_prev</span><br></pre></td></tr></table></figure><h2 id="加速采样">加速采样</h2><p>已经用DDPM训好了噪声预测模型，并且也已经计算好了中间的参数，其实剩下的部分直接套公式就可以了：</p><p><span class="math display">$$\begin{split}x_{t-1}=\sqrt{\alpha_{t-1}}{\left(\frac{x_t-\sqrt{1-\alpha_t}\epsilon_\theta^{(t)}(x_t)}{\sqrt{\alpha_t}}\right)}+{\sqrt{1-\alpha_{t-1}-\sigma_t^2}\cdot\epsilon_\theta^{(t)}(\mathbf{x}_t)}+{\sigma_t\epsilon_t}\end{split}$$</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">p_sample_ddim</span>(<span class="params">self, x, c, t, ...</span>):</span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 得到模型预测噪声(Classifier-free Guidance)</span></span><br><span class="line">    <span class="keyword">if</span> unconditional_conditioning <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> unconditional_guidance_scale == <span class="number">1.</span>:</span><br><span class="line">        e_t = <span class="variable language_">self</span>.model.apply_model(x, t, c)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        x_in = torch.cat([x] * <span class="number">2</span>)</span><br><span class="line">        t_in = torch.cat([t] * <span class="number">2</span>)</span><br><span class="line">        c_in = torch.cat([unconditional_conditioning, c])</span><br><span class="line">        e_t_uncond, e_t = <span class="variable language_">self</span>.model.apply_model(x_in, t_in, c_in).chunk(<span class="number">2</span>)</span><br><span class="line">        e_t = e_t_uncond + unconditional_guidance_scale * (e_t - e_t_uncond)</span><br><span class="line"></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 预测的x_0(即上述公式的第一部分)</span></span><br><span class="line">    pred_x0 = (x - sqrt_one_minus_at * e_t) / a_t.sqrt()</span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    <span class="comment"># direction pointing to x_t(即上述公式的第二部分)</span></span><br><span class="line">    dir_xt = (<span class="number">1.</span> - a_prev - sigma_t**<span class="number">2</span>).sqrt() * e_t</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 不确定性部分（即上述公式的第三部分），由于实验设置eta=0，这部分相当于确定性采样</span></span><br><span class="line">    noise = sigma_t * noise_like(x.shape, device, repeat_noise) * temperature</span><br><span class="line">    <span class="keyword">if</span> noise_dropout &gt; <span class="number">0.</span>:</span><br><span class="line">        noise = torch.nn.functional.dropout(noise, p=noise_dropout)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 求解x_&#123;t-1&#125;</span></span><br><span class="line">    x_prev = a_prev.sqrt() * pred_x0 + dir_xt + noise</span><br><span class="line">    <span class="keyword">return</span> x_prev, pred_x0</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AIGC </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Davinci学习之初级学习笔记</title>
      <link href="/2025/04/16/photograph/Davinci/%E5%88%9D%E7%BA%A7%E6%95%99%E7%A8%8B/"/>
      <url>/2025/04/16/photograph/Davinci/%E5%88%9D%E7%BA%A7%E6%95%99%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<h1 id="davinci入门学习">Davinci入门学习</h1><p>主要参考影视飓风的达芬奇教程，这里是leo的学习笔记，我的主要需求其实是调色，其他的部分应该只是大致浏览一下。</p><h2 id="入门简介">入门简介</h2><p>创作视频的一般流程：</p><ul><li>导入素材进行分类：<code>媒体</code> 面板中进行</li><li>进行视频的粗剪、然后进行精细剪辑：<code>快编</code> 及<code>剪辑</code> 面板中进行</li><li>效果制作、调色、音频调整：分别对应<code>Fusion</code>、<code>调色</code>、<code>Fairlight</code> 面板</li><li>导出视频：<code>交付</code> 面板</li></ul><h3 id="媒体面板">媒体面板</h3><p>通常流程是我们通过<strong>媒体浏览器</strong>找到素材所在文件夹，在<strong>监视器</strong>中回看，选择需要的素材拖拽到<strong>素材池</strong>中，等待剪辑。</p><p>达芬奇素材池进行项目管理有一个很大的弊端：一旦导入素材，就无法更改项目的帧率，容易造成后期视频的跳帧或卡顿（Davinci17后可以更改了）。</p><blockquote><p>解决方法：需要在前期导入素材之前，进入 文件-项目设置-设置合适的帧率配置项目</p></blockquote><h3 id="快编和剪辑面板">快编和剪辑面板</h3><p>在 <code>剪辑</code> 面板中进行剪辑的一般流程：</p><ul><li>媒体池进行筛选</li><li>在素材监视器中使用 <code>I</code> 键和 <code>O</code>键打下出入点，选出有用的素材拖入时间线进行组装</li><li>通过特效库面板选择对应的音频或视频效果添加转场</li></ul><h3 id="fusion调色fairlight面板">Fusion、调色、Fairlight面板</h3><p>不推荐使用 <code>Fusion</code>制作复杂的效果及动画，节点式工作逻辑复杂。<code>调色</code>面板很强大（这其实也是我的主要需求），<code>Fairlight</code>面板则相当于音频的调色面板。</p><h3 id="交付面板">交付面板</h3><p>通常传播互联网选择 <code>mp4</code> 封装，<code>h264</code>编码，分辨率和帧率则根据需求调整。之后添加到渲染序列进行渲染即可。</p><h2 id="剪辑">剪辑</h2><p>一些Tips：</p><ul><li>建议在 <code>媒体</code>面板中一次性将所有素材放入素材池，然后再进行分类，这样剪辑时不用因为缺少素材又重新添加。</li><li><code>I</code> 键和 <code>O</code>键来打下视频的出入点，可以单独点击画面或音频拖入画面或音频轨道。</li><li>使用 <code>alt</code>+鼠标滚轮 来进行时间线的缩放</li></ul><h3 id="更多工具栏">更多工具栏</h3><table><thead><tr><th style="text-align: center;">工具</th><th style="text-align: center;">快捷键</th><th style="text-align: center;">描述</th></tr></thead><tbody><tr><td style="text-align: center;">鼠标模式</td><td style="text-align: center;">-</td><td style="text-align: center;">快速编辑</td></tr><tr><td style="text-align: center;">修建编辑模式</td><td style="text-align: center;"><code>T</code></td><td style="text-align: center;">波纹删除（适合精剪）</td></tr><tr><td style="text-align: center;">动态修剪模式</td><td style="text-align: center;">-</td><td style="text-align: center;">不经常用</td></tr><tr><td style="text-align: center;">剃刀工具</td><td style="text-align: center;"><code>B</code></td><td style="text-align: center;">素材切刀</td></tr><tr><td style="text-align: center;">素材导入方式</td><td style="text-align: center;">-</td><td style="text-align: center;">分为插入、覆盖以及替换</td></tr></tbody></table><h2 id="添加效果与关键帧">添加效果与关键帧</h2><ul><li>添加转场：打开特效库，拖拽一个转场效果到素材拼接处，可以在检查器中精细调节转场效果</li><li>片段调整：点击一个素材片段，打开检查器，进行缩放、位置、变速等，按住<code>alt</code> 再进行调整能更精细地控制</li><li>添加关键帧：打开检查器，起始帧选中需要变换的参数右侧红色菱形，接着选中结束帧改变参数即可自动加入关键帧。可以打开非线性变换选项，这样比线性的参数变化更有质感（缓动效果）</li><li>批处理：只需要 <code>ctrl+c</code>复制某个已处理好的片段，然后选中需要批处理的其他片段进行粘贴即可</li></ul><blockquote><p>插件推荐：</p><p><ahref="https://www.maxon.net/en/red-giant">红巨星宇宙</a>：很多效果和滤镜预设，一年订阅在1400元</p><p><ahref="https://www.filmconvert.com/">FilmConvert</a>：调色插件，有很多相机预设</p></blockquote><h2 id="快编界面">快编界面</h2><h3 id="预备知识之代理文件的生成">预备知识之代理文件的生成</h3><p>媒体优化：减少电脑负荷</p><p><strong>方案一</strong>（适用于比较简单的剪辑）：播放–代理模式–选择合适的分辨率</p><p><strong>方案二</strong>（适用于复杂的剪辑）：媒体池选中素材，右键生成优化媒体文件</p><p>设置在 文件–项目设置–主设置–优化的媒体和渲染缓存</p><blockquote><p>常用设置：4K视频 选择 分辨率–二分之一 编码–DNXHR SQ</p></blockquote><h3id="预备知识之智能媒体夹及双时间线剪辑">预备知识之智能媒体夹及双时间线剪辑</h3><p>Aroll 主要内容 Broll 辅助时间线</p>]]></content>
      
      
      <categories>
          
          <category> 摄影 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Davinci </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>调色不重要？调色很重要</title>
      <link href="/2025/04/16/photograph/Lightroom/%E4%B8%80%E4%BA%9B%E4%B8%AA%E4%BA%BA%E8%B0%83%E8%89%B2%E6%80%9D%E8%B7%AF/"/>
      <url>/2025/04/16/photograph/Lightroom/%E4%B8%80%E4%BA%9B%E4%B8%AA%E4%BA%BA%E8%B0%83%E8%89%B2%E6%80%9D%E8%B7%AF/</url>
      
        <content type="html"><![CDATA[<h1 id="总结贴">总结贴</h1><p>示例图片有点糊…为了加载速度只能压缩画质了</p><h2 id="日系蓝色胶片">日系蓝色胶片</h2><h3 id="基本">基本</h3><p>色温 - 色调 +</p><p>曝光适度 对比 - 白色色阶 - 黑色色阶 +</p><h3 id="曲线">曲线</h3><p>稍微下拉，亮部降暗部升</p><p>红色通道暗部下拉</p><h3 id="混色器">混色器</h3><p>绿色相 - 蓝色相 +</p><p>暖色饱和度 + 冷色饱和度（尤其蓝色） -</p><p>暖色明度 + 冷色明度（尤其蓝色） -</p><h3 id="颜色分级">颜色分级</h3><p>阴影偏青，高光偏黄绿</p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/lr1.png" width="50%" /></p><h2 id="亮调春花">亮调春花</h2><h3 id="基本-1">基本</h3><p>色温 + 色调 -</p><p>曝光适度 对比 - 高光 - 阴影 + 白色色阶 - 黑色色阶 +</p><p>纹理 - 清晰度 + 去朦胧 -</p><p>鲜艳度 + 饱和度 -</p><h3 id="曲线-1">曲线</h3><p>稍微下拉，暗部升</p><h3 id="混色器-1">混色器</h3><p>橙绿色相 - 红蓝色相 +</p><p>暖色及蓝色饱和度 + 绿色饱和度 -</p><p>冷色明度 -</p><h3 id="颜色分级-1">颜色分级</h3><p>中间色调及阴影偏青，高光偏黄绿</p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/lr2.png" width="50%" /></p><h2 id="暗调春花">暗调春花</h2><h3 id="基本-2">基本</h3><p>色温 - 色调 -</p><p>曝光适度 对比 - 高光 - 阴影 + 白色色阶 - 黑色色阶 +</p><p>纹理 + 清晰度 - 去朦胧 +</p><p>鲜艳度 + 饱和度 -</p><h3 id="曲线-2">曲线</h3><p>稍微下拉，暗部升</p><h3 id="混色器-2">混色器</h3><p>蓝橙色相 - 红绿色相 +</p><p>暖色饱和度 + 绿色饱和度 -</p><p>冷色明度 -</p><h3 id="颜色分级-2">颜色分级</h3><p>阴影偏青，中间色调偏黄绿，高光偏橙黄</p><h3 id="蒙版">蒙版</h3><p>径向蒙版模拟打光（曝光+色温+），反向蒙版加深对比</p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/lr3.png" width="50%" /></p>]]></content>
      
      
      <categories>
          
          <category> 摄影 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Lightroom </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Lightroom入门笔记</title>
      <link href="/2025/04/16/photograph/Lightroom/%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/"/>
      <url>/2025/04/16/photograph/Lightroom/%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<p>教程来自摄影师泰罗</p><h1 id="lr系统教程">LR系统教程</h1><h2id="第一期照片管理筛选与批量处理">第一期、照片管理、筛选与批量处理</h2><p>PS是重新分配空间来处理照片，而LR是建立软连接到导入照片路径。</p><ul><li>注意在导入时有几个选项：包括<strong>复制、移动、添加</strong></li><li>添加是不会改变数据存储路径的，只是建立了一个映射；</li><li>而移动则是将照片从硬盘中移动到本地。</li></ul><h2 id="第二期直方图与曝光补偿">第二期、直方图与曝光补偿</h2><h3 id="直方图的原理">直方图的原理</h3><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/image2.png" width="50%" /></p><p>直方图是关于亮度的统计报告图，横坐标是0-255的亮度级别，越右越亮；纵坐标表示该亮度级别下像素数量的多少。</p><h2 id="第三期曲线">第三期、曲线</h2><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/image3.png" width="30%" /></p><p>通过映射重塑直方图</p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/image4.png" width="50%" /></p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/image5.png" width="50%" /></p><p>胶片灰</p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/image6.png" width="30%" /></p><p><strong>曲线调色原理：</strong>分别在RGB三种模式下调色，对应模式下调高曝光会加重对应色调、反之加重对应补色色调</p><p>曲线调色案例：</p><ul><li>日系小清新：RGB稍过曝（变亮）、G阴影稍高（阴影偏绿）、B阴影稍低（阴影偏暖）、R整体调低（整张图偏青）</li><li>暖色调电影感：RGB稍过曝、R整体偏低、B整体调低</li></ul><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/image7.png" width="50%" /></p><h2 id="第四期色温色调以及分离色调">第四期、色温色调以及分离色调</h2><p>调曲线过于复杂，不如关注眼前的工具</p><pre><code>城市夜景人文调色思路之一：降低色温、降低色调、提亮阴影、二次构图（2.35：1）</code></pre><p>色温色调的缺陷：过于整体</p><p>分离色调：分别为高光和阴影分别调色相</p><pre><code>一个有意思的调色思路：阴影偏黄绿、高光偏冷青</code></pre><p><strong>曲线工具 VS 分离色调</strong></p><p>曲线工具精确选择区域不精确赋予色彩；分离色调不精确选择区域精确选择色彩</p><h2 id="第五期hsl调色">第五期、HSL调色</h2><p>更精细的局部调色</p><p>一个调色思路是各种色相都向对应高光或阴影色调调整，以达成整体和谐的效果</p><p>用于学习大佬调色思路的网站：<ahref="https://anvaka.github.io/pixchart/?d=4&amp;ignore=&amp;link=&amp;groupBy=hsl.h">https://anvaka.github.io/pixchart/?d=4&amp;ignore=&amp;link=&amp;groupBy=hsl.h</a></p><h2id="第六期锐化清晰度等局部调整工具">第六期、锐化、清晰度等局部调整工具</h2><p>锐化的本质：描边 + 数量：描边越明显 +半径：描边的粗细程度（一般保持默认） +细节：描边的黑白反差（一般保持默认） +蒙版：使用alt键可以查看，保护纯净的背景不被锐化</p><p>清晰度：没有描边、加大色块交界处的差异（黑色更黑、白色更白）</p><p>其实照片想要表现出清晰与高质，重点不在于锐化和清晰度，也不在于像素的多少，而在于<strong>信息的表达</strong></p><p>less ismore（删繁就简）、秩序（画面元素达到统一）、色彩对比虚实对比明亮对比</p><p>在<code>蒙版</code>中将主体部分提亮、将背景部分变暗</p><h2 id="第七期提升摄影后期水准">第七期、提升摄影后期水准</h2><p>降噪：高ISO会造成明度噪点和彩色噪点这两种，分别使用明亮度和颜色按钮调节，一般明亮度不能给太高、而颜色可以给高</p><p>镜头校正：消除镜头带来的偏移以及曲变</p><p>导出：质量可以选择60（默认）、如果要发朋友圈调整图像大小以适合短边、像素选择1080（可以避免微信的二次压缩）</p><p><strong>平面构成、立体构成、色彩构成</strong></p><p>简单矫正<span class="math inline">→</span>局部修饰<spanclass="math inline">→</span>艺术加工</p><h2 id="第八期让图片更干净">第八期、让图片更干净</h2><ul><li>压低曲线的高光、提高曝光值（关键步骤）</li><li>压缩曲线的阴影</li><li>提高橙色，追加一些饱和度，红色追加饱和度（针对图片中有人像的情况，让皮肤和嘴唇更好看）</li><li>绿色的色相往右滑动，明度和饱和度提高（让树木更精神）</li><li>蓝色色相向右滑动</li><li>曲线红色通道稍微下压，使整张图偏青</li></ul><h2 id="第九期油画感教程">第九期、油画感教程</h2><blockquote><p>油画风特征分析： + 饱和度高、对比度中 + 色系统一 +细节模糊、整体清晰</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/image8.png" width="50%" /></p>]]></content>
      
      
      <categories>
          
          <category> 摄影 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Lightroom </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PS入门笔记</title>
      <link href="/2025/04/16/photograph/Photoshop/%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/"/>
      <url>/2025/04/16/photograph/Photoshop/%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> 摄影 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Photoshop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DDPM总结</title>
      <link href="/2025/04/15/AIGC/DDPM/"/>
      <url>/2025/04/15/AIGC/DDPM/</url>
      
        <content type="html"><![CDATA[<p><strong>系列文章：</strong></p><ol class="series-items"><li><a href="/2025/03/28/AIGC/overview_DM/" title="扩散模型的发展（简略版）">扩散模型的发展（简略版）</a></li><li><a href="/2025/04/15/AIGC/DDPM/" title="DDPM总结">DDPM总结</a></li><li><a href="/2025/04/17/AIGC/DDIM/" title="DDIM总结">DDIM总结</a></li><li><a href="/2025/04/27/AIGC/LDM/" title="LDM的细节">LDM的细节</a></li><li><a href="/2025/05/08/AIGC/DIT/" title="DiT的细节">DiT的细节</a></li><li><a href="/2025/05/08/AIGC/iDDPM/" title="iDDPM总结">iDDPM总结</a></li><li><a href="/2026/02/27/AIGC/JiT/" title="PixelDiT & JiT总结">PixelDiT & JiT总结</a></li><li><a href="/2026/02/27/AIGC/Flow_Matching/" title="Flow Matching简要介绍">Flow Matching简要介绍</a></li></ol><p>这里分析的源码并非来自DDPM原论文，而是LDM（<ahref="https://github.com/CompVis/latent-diffusion">https://github.com/CompVis/latent-diffusion</a>）中的DDPM部分，因此和原论文可能有些出入。</p><h1 id="回顾">回顾</h1><p>首先在分析源码之前，可以先回顾一下DDPM的关键步骤。</p><p><strong>前向过程（Forward Process）</strong></p><p>对输入图像 <span class="math inline"><em>x</em><sub>0</sub></span>按照预定义的加噪操作，逐步加入高斯噪声，直到最终接近纯高斯噪声 <spanclass="math inline"><em>x</em><sub><em>T</em></sub></span>。</p><p><span class="math display">$$\begin{split}    q(x_t \mid x_{t-1})=\mathcal{N}\bigl(x_t;\;\sqrt{\alpha_t}\,x_{t-1},\;\beta_t I\bigr).\end{split}$$</span></p><p>对于单步加噪，代入连续求解可以得到上式的闭式形式，其中<spanclass="math inline">$\alpha_t = 1-\beta_t,\quad \bar\alpha_t =\prod_{i=1}^t \alpha_i$</span>。</p><p><span class="math display">$$\begin{split}    q(x_t \mid x_0) =\mathcal{N}\bigl(x_t;\;\sqrt{\bar\alpha_t}\,x_0,\;(1-\bar\alpha_t)\,I\bigr).\end{split}$$</span></p><p><strong>去噪过程（Reverse Process）</strong></p><p><span class="math display">$$\begin{split}    q(x_{t-1} \mid x_t,x_0)=\mathcal{N}(x_t;\frac{1}{\sqrt{\alpha_t}}(x_t-\frac{1-\alpha_t}{\sqrt{1-\bar\alpha_t}}\epsilon_t),\beta_t\cdot\frac{1-\bar\alpha_{t-1}}{1-\bar\alpha_t}I).\end{split}$$</span></p><p><strong>损失函数</strong></p><p>主要是对噪声预测的MSE损失：</p><p><span class="math display">$$\begin{split}    \mathcalL=\mathbb{E}_{t,x_0,\epsilon}\left[\|\epsilon-\varepsilon_\theta(x_t,t)\|^2\right].\end{split}$$</span></p><h1 id="源码分析">源码分析</h1><p>主要代码逻辑定义在<code>ldm\models\diffusion\ddpm.py</code>中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, *args, **kwargs</span>):</span><br><span class="line">    <span class="comment"># b, c, h, w, device, img_size, = *x.shape, x.device, self.image_size</span></span><br><span class="line">    <span class="comment"># assert h == img_size and w == img_size, f&#x27;height and width of image must be &#123;img_size&#125;&#x27;</span></span><br><span class="line">    t = torch.randint(</span><br><span class="line">        <span class="number">0</span>, <span class="variable language_">self</span>.num_timesteps, (x.shape[<span class="number">0</span>],), device=<span class="variable language_">self</span>.device</span><br><span class="line">    ).long()</span><br><span class="line">    <span class="keyword">return</span> <span class="variable language_">self</span>.p_losses(x, t, *args, **kwargs)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">p_losses</span>(<span class="params">self, x_start, t, noise=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="comment"># --------------------</span></span><br><span class="line">    noise = default(noise, <span class="keyword">lambda</span>: torch.randn_like(x_start))</span><br><span class="line">    x_noisy = <span class="variable language_">self</span>.q_sample(x_start=x_start, t=t, noise=noise)</span><br><span class="line">    model_out = <span class="variable language_">self</span>.model(x_noisy, t)</span><br><span class="line">    <span class="comment"># --------------------</span></span><br><span class="line"></span><br><span class="line">    loss_dict = &#123;&#125;</span><br><span class="line">    <span class="keyword">if</span> <span class="variable language_">self</span>.parameterization == <span class="string">&quot;eps&quot;</span>:</span><br><span class="line">        target = noise</span><br><span class="line">    <span class="keyword">elif</span> <span class="variable language_">self</span>.parameterization == <span class="string">&quot;x0&quot;</span>:</span><br><span class="line">        target = x_start</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError(</span><br><span class="line">            <span class="string">f&quot;Paramterization <span class="subst">&#123;self.parameterization&#125;</span> not yet supported&quot;</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    loss = <span class="variable language_">self</span>.get_loss(model_out, target, mean=<span class="literal">False</span>).mean(dim=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">    log_prefix = <span class="string">&#x27;train&#x27;</span> <span class="keyword">if</span> <span class="variable language_">self</span>.training <span class="keyword">else</span> <span class="string">&#x27;val&#x27;</span></span><br><span class="line"></span><br><span class="line">    loss_dict.update(&#123;<span class="string">f&#x27;<span class="subst">&#123;log_prefix&#125;</span>/loss_simple&#x27;</span>: loss.mean()&#125;)</span><br><span class="line">    loss_simple = loss.mean() * <span class="variable language_">self</span>.l_simple_weight</span><br><span class="line"></span><br><span class="line">    loss_vlb = (<span class="variable language_">self</span>.lvlb_weights[t] * loss).mean()</span><br><span class="line">    loss_dict.update(&#123;<span class="string">f&#x27;<span class="subst">&#123;log_prefix&#125;</span>/loss_vlb&#x27;</span>: loss_vlb&#125;)</span><br><span class="line"></span><br><span class="line">    loss = loss_simple + <span class="variable language_">self</span>.original_elbo_weight * loss_vlb</span><br><span class="line"></span><br><span class="line">    loss_dict.update(&#123;<span class="string">f&#x27;<span class="subst">&#123;log_prefix&#125;</span>/loss&#x27;</span>: loss&#125;)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss, loss_dict</span><br></pre></td></tr></table></figure><p>根据这部分代码可以理解，每次训练都是<strong>对每个batch随机从 <spanclass="math inline">[0, <em>T</em>)</span>中选择一个时刻进行训练</strong>，在该时间步内进行单独训练。而重点其实就在<code>p_losses</code>函数中的前三步，后续步骤主要是在计算损失。首先初始化高斯噪音<code>noise</code>，接着通过前向过程得到<span class="math inline"><em>x</em><sub><em>t</em></sub></span>，然后经过模型预测得到预测噪音 <spanclass="math inline"><em>ϵ</em><sub><em>θ</em></sub></span>或是预测输出图像 <span class="math inline"><em>x</em><sub>0</sub></span>。在这里 <code>parameterization</code> 被设置为 <code>eps</code>，所以这里与原论文保持一致是<strong>预测噪声</strong>而不是DPM中的预测<span class="math inline"><em>x</em><sub>0</sub></span> 。</p><p>还有这部分损失的计算还加上了<code>loss_vlb</code>，这其实是后续iDDPM的工作，可以先不用管。</p><h2 id="前向过程">前向过程</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">q_sample</span>(<span class="params">self, x_start, t, noise=<span class="literal">None</span></span>):</span><br><span class="line">    noise = default(noise, <span class="keyword">lambda</span>: torch.randn_like(x_start))</span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">        extract_into_tensor(<span class="variable language_">self</span>.sqrt_alphas_cumprod, t, x_start.shape) * x_start</span><br><span class="line">        + extract_into_tensor(<span class="variable language_">self</span>.sqrt_one_minus_alphas_cumprod, t, x_start.shape)</span><br><span class="line">        * noise</span><br><span class="line">    )</span><br></pre></td></tr></table></figure><p>很明显，这里的加噪部分和原论文的一步加噪的公式保持一致，其中的中间参数在源码中<code>register_schedule</code>函数中都有提前定义好：</p><p><span class="math display">$$\begin{split}    q(x_t \mid x_0) =\mathcal{N}\bigl(x_t;\;\sqrt{\bar\alpha_t}\,x_0,\;(1-\bar\alpha_t)\,I\bigr).\end{split}$$</span></p><h2 id="预测噪声">预测噪声</h2><p>原论文预测噪声是直接用UNet进行预测的，源码里也是保持一致，这里的<code>model</code>是<code>UNetModel</code>类的一个实例。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">model_out = <span class="variable language_">self</span>.model(x_noisy, t)</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>对于<code>UNetModel</code>类的前向过程，其实主要包含几个部分：对于时间步<code>t</code>的embedding、其他条件的处理（原论文里不包含，该部分源码来自于LDM）、得到最终预测的噪声或<spanclass="math inline"><em>x</em><sub>0</sub></span>。时间步的信息则是通过embedding层后通过attention嵌入到特征图中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">UNetModel</span>(nn.Module):</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, timesteps=<span class="literal">None</span>, context=<span class="literal">None</span>, y=<span class="literal">None</span>,**kwargs</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Apply the model to an input batch.</span></span><br><span class="line"><span class="string">        :param x: an [N x C x ...] Tensor of inputs.</span></span><br><span class="line"><span class="string">        :param timesteps: a 1-D batch of timesteps.</span></span><br><span class="line"><span class="string">        :param context: conditioning plugged in via crossattn</span></span><br><span class="line"><span class="string">        :param y: an [N] Tensor of labels, if class-conditional.</span></span><br><span class="line"><span class="string">        :return: an [N x C x ...] Tensor of outputs.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">assert</span> (y <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>) == (</span><br><span class="line">            <span class="variable language_">self</span>.num_classes <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line">        ), <span class="string">&quot;must specify y if and only if the model is class-conditional&quot;</span></span><br><span class="line">        hs = []</span><br><span class="line">        t_emb = timestep_embedding(timesteps, <span class="variable language_">self</span>.model_channels, repeat_only=<span class="literal">False</span>)</span><br><span class="line">        emb = <span class="variable language_">self</span>.time_embed(t_emb)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.num_classes <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">assert</span> y.shape == (x.shape[<span class="number">0</span>],)</span><br><span class="line">            emb = emb + <span class="variable language_">self</span>.label_emb(y)</span><br><span class="line"></span><br><span class="line">        h = x.<span class="built_in">type</span>(<span class="variable language_">self</span>.dtype)</span><br><span class="line">        <span class="keyword">for</span> module <span class="keyword">in</span> <span class="variable language_">self</span>.input_blocks:</span><br><span class="line">            h = module(h, emb, context)</span><br><span class="line">            hs.append(h)</span><br><span class="line">        h = <span class="variable language_">self</span>.middle_block(h, emb, context)</span><br><span class="line">        <span class="keyword">for</span> module <span class="keyword">in</span> <span class="variable language_">self</span>.output_blocks:</span><br><span class="line">            h = th.cat([h, hs.pop()], dim=<span class="number">1</span>)</span><br><span class="line">            h = module(h, emb, context)</span><br><span class="line">        h = h.<span class="built_in">type</span>(x.dtype)</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.predict_codebook_ids:</span><br><span class="line">            <span class="keyword">return</span> <span class="variable language_">self</span>.id_predictor(h)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="variable language_">self</span>.out(h)</span><br></pre></td></tr></table></figure><h2 id="采样过程">采样过程</h2><p>采样过程在DDPM中还是逐步逆向去噪的，一共迭代<code>t</code>次，每次单步执行<code>p_sample</code>函数进行单步去噪。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">p_sample_loop</span>(<span class="params">self, shape, return_intermediates=<span class="literal">False</span></span>):</span><br><span class="line">    device = <span class="variable language_">self</span>.betas.device</span><br><span class="line">    b = shape[<span class="number">0</span>]</span><br><span class="line">    img = torch.randn(shape, device=device)</span><br><span class="line">    intermediates = [img]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> tqdm(</span><br><span class="line">        <span class="built_in">reversed</span>(<span class="built_in">range</span>(<span class="number">0</span>, <span class="variable language_">self</span>.num_timesteps)),</span><br><span class="line">        desc=<span class="string">&#x27;Sampling t&#x27;</span>,</span><br><span class="line">        total=<span class="variable language_">self</span>.num_timesteps,</span><br><span class="line">    ):</span><br><span class="line">        img = <span class="variable language_">self</span>.p_sample(</span><br><span class="line">            img,</span><br><span class="line">            torch.full((b,), i, device=device, dtype=torch.long),</span><br><span class="line">            clip_denoised=<span class="variable language_">self</span>.clip_denoised,</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">if</span> i % <span class="variable language_">self</span>.log_every_t == <span class="number">0</span> <span class="keyword">or</span> i == <span class="variable language_">self</span>.num_timesteps - <span class="number">1</span>:</span><br><span class="line">            intermediates.append(img)</span><br><span class="line">    <span class="keyword">if</span> return_intermediates:</span><br><span class="line">        <span class="keyword">return</span> img, intermediates</span><br><span class="line">    <span class="keyword">return</span> img</span><br><span class="line"></span><br><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sample</span>(<span class="params">self, batch_size=<span class="number">16</span>, return_intermediates=<span class="literal">False</span></span>):</span><br><span class="line">    image_size = <span class="variable language_">self</span>.image_size</span><br><span class="line">    channels = <span class="variable language_">self</span>.channels</span><br><span class="line">    <span class="keyword">return</span> <span class="variable language_">self</span>.p_sample_loop(</span><br><span class="line">        (batch_size, channels, image_size, image_size),</span><br><span class="line">        return_intermediates=return_intermediates,</span><br><span class="line">    )</span><br></pre></td></tr></table></figure><p>下面我们分析单步去噪过程，回顾一下之前说的单步去噪：</p><p><span class="math display">$$\begin{split}    q(x_{t-1} \mid x_t,x_0)=\mathcal{N}(x_t;\frac{1}{\sqrt{\alpha_t}}(x_t-\frac{1-\alpha_t}{\sqrt{1-\bar\alpha_t}}\epsilon_t),\beta_t\cdot\frac{1-\bar\alpha_{t-1}}{1-\bar\alpha_t}I).\end{split}$$</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">p_sample</span>(<span class="params">self, x, t, clip_denoised=<span class="literal">True</span>, repeat_noise=<span class="literal">False</span></span>):</span><br><span class="line">    b, *_, device = *x.shape, x.device</span><br><span class="line">    model_mean, _, model_log_variance = <span class="variable language_">self</span>.p_mean_variance(</span><br><span class="line">        x=x, t=t, clip_denoised=clip_denoised</span><br><span class="line">    )</span><br><span class="line">    noise = noise_like(x.shape, device, repeat_noise)</span><br><span class="line">    <span class="comment"># no noise when t == 0</span></span><br><span class="line">    nonzero_mask = (<span class="number">1</span> - (t == <span class="number">0</span>).<span class="built_in">float</span>()).reshape(b, *((<span class="number">1</span>,) * (<span class="built_in">len</span>(x.shape) - <span class="number">1</span>)))</span><br><span class="line">    <span class="keyword">return</span> model_mean + nonzero_mask * (<span class="number">0.5</span> * model_log_variance).exp() * noise</span><br></pre></td></tr></table></figure><p>这里相当于是通过预测噪声 <spanclass="math inline"><em>ϵ</em><sub><em>θ</em></sub></span> 和 <spanclass="math inline"><em>x</em><sub><em>t</em></sub></span> 来预测 <spanclass="math inline"><em>x</em><sub><em>t</em> − 1</sub></span>，但是在代码实现上考虑到DPM每步直接预测<span class="math inline"><em>x</em><sub>0</sub></span>的做法，为了方便统一处理，这里在实现时即使是预测噪声 <spanclass="math inline"><em>ϵ</em><sub><em>θ</em></sub></span>，也会进一步得到预测 <span class="math inline">$\hat{x_0}$</span>，然后通过后验公式计算均值。即下面代码所展示的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">p_mean_variance</span>(<span class="params">self, x, t, clip_denoised: <span class="built_in">bool</span></span>):</span><br><span class="line">    model_out = <span class="variable language_">self</span>.model(x, t)</span><br><span class="line">    <span class="keyword">if</span> <span class="variable language_">self</span>.parameterization == <span class="string">&quot;eps&quot;</span>:</span><br><span class="line">        x_recon = <span class="variable language_">self</span>.predict_start_from_noise(x, t=t, noise=model_out)</span><br><span class="line">    <span class="keyword">elif</span> <span class="variable language_">self</span>.parameterization == <span class="string">&quot;x0&quot;</span>:</span><br><span class="line">        x_recon = model_out</span><br><span class="line">    <span class="keyword">if</span> clip_denoised:</span><br><span class="line">        x_recon.clamp_(-<span class="number">1.0</span>, <span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line">    model_mean, posterior_variance, posterior_log_variance = <span class="variable language_">self</span>.q_posterior(</span><br><span class="line">        x_start=x_recon, x_t=x, t=t</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> model_mean, posterior_variance, posterior_log_variance</span><br></pre></td></tr></table></figure><p>给定 <span class="math inline"><em>x</em><sub><em>t</em></sub></span>和 <span class="math inline"><em>ϵ</em><sub><em>θ</em></sub></span>，可以推导 <span class="math inline">$\hat{x_0}$</span> ：</p><p><span class="math display">$$\begin{split}    \bar{x_0} =\frac{1}{\sqrt{\bar\alpha_t}}x_t-\frac{\sqrt{1-\bar\alpha_t}}{\sqrt{\bar\alpha_t}}\epsilon_\theta(x_t,t)\end{split}$$</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">predict_start_from_noise</span>(<span class="params">self, x_t, t, noise</span>):</span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">        extract_into_tensor(<span class="variable language_">self</span>.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t</span><br><span class="line">        - extract_into_tensor(<span class="variable language_">self</span>.sqrt_recipm1_alphas_cumprod, t, x_t.shape)</span><br><span class="line">        * noise</span><br><span class="line">    )</span><br></pre></td></tr></table></figure><p>接着给定 <spanclass="math inline"><em>x</em><sub><em>t</em></sub></span> 和 <spanclass="math inline">$\hat{x_0}$</span> ，可以根据后验公式继续推导出<span class="math inline"><em>x</em><sub><em>t</em> − 1</sub></span>：</p><p><span class="math display">$$\begin{gather}\begin{split}    q(x_{t-1}\mid x_t,x_0)&amp;=\mathcal{N}(\mu_t,\sigma_t^2I) \\    \mu_t&amp;=\frac{\sqrt{\bar{\alpha}_{t-1}}\cdot\beta_t}{1-\bar{\alpha}_t}x_0+\frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t}x_t\\    \sigma_t^2&amp;=\beta_t\cdot\frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t}\end{split}\end{gather}$$</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">q_posterior</span>(<span class="params">self, x_start, x_t, t</span>):</span><br><span class="line">    posterior_mean = (</span><br><span class="line">        extract_into_tensor(<span class="variable language_">self</span>.posterior_mean_coef1, t, x_t.shape) * x_start</span><br><span class="line">        + extract_into_tensor(<span class="variable language_">self</span>.posterior_mean_coef2, t, x_t.shape) * x_t</span><br><span class="line">    )</span><br><span class="line">    posterior_variance = extract_into_tensor(<span class="variable language_">self</span>.posterior_variance, t, x_t.shape)</span><br><span class="line">    posterior_log_variance_clipped = extract_into_tensor(</span><br><span class="line">        <span class="variable language_">self</span>.posterior_log_variance_clipped, t, x_t.shape</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> posterior_mean, posterior_variance, posterior_log_variance_clipped</span><br></pre></td></tr></table></figure><p>其中用到的中间变量在<code>register_schedule</code>中提前定义好了，包括计算均值需要用到的<code>posterior_mean_coef1</code>和<code>posterior_mean_coef2</code>以及方差<code>posterior_variance</code>和log形态方差<code>posterior_log_variance_clipped</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># calculations for posterior q(x_&#123;t-1&#125; | x_t, x_0)</span></span><br><span class="line">posterior_variance = (<span class="number">1</span> - <span class="variable language_">self</span>.v_posterior) * betas * ( <span class="number">1.0</span> - alphas_cumprod_prev ) / (<span class="number">1.0</span> - alphas_cumprod) + <span class="variable language_">self</span>.v_posterior * betas</span><br><span class="line"><span class="comment"># above: equal to 1. / (1. / (1. - alpha_cumprod_tm1) + alpha_t / beta_t)</span></span><br><span class="line"><span class="variable language_">self</span>.register_buffer(<span class="string">&#x27;posterior_variance&#x27;</span>, to_torch(posterior_variance))</span><br><span class="line"><span class="comment"># below: log calculation clipped because the posterior variance is 0 at the beginning of the diffusion chain</span></span><br><span class="line"><span class="variable language_">self</span>.register_buffer(</span><br><span class="line">    <span class="string">&#x27;posterior_log_variance_clipped&#x27;</span>,</span><br><span class="line">    to_torch(np.log(np.maximum(posterior_variance, <span class="number">1e-20</span>))),</span><br><span class="line">)</span><br><span class="line"><span class="variable language_">self</span>.register_buffer(</span><br><span class="line">    <span class="string">&#x27;posterior_mean_coef1&#x27;</span>,</span><br><span class="line">    to_torch(betas * np.sqrt(alphas_cumprod_prev) / (<span class="number">1.0</span> - alphas_cumprod)),</span><br><span class="line">)</span><br><span class="line"><span class="variable language_">self</span>.register_buffer(</span><br><span class="line">    <span class="string">&#x27;posterior_mean_coef2&#x27;</span>,</span><br><span class="line">    to_torch(</span><br><span class="line">        (<span class="number">1.0</span> - alphas_cumprod_prev) * np.sqrt(alphas) / (<span class="number">1.0</span> - alphas_cumprod)</span><br><span class="line">    ),</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>这里细心的小伙伴就会注意到了，为什么要计算一个log形态的方差呢？事实上回看<code>p_sample</code>函数也会发现，计算<span class="math inline"><em>x</em><sub><em>t</em> − 1</sub></span>时用的也是<code>posterior_log_variance_clipped</code>而不是<code>posterior_variance</code>，然后最后一步还要还原成<code>posterior_variance</code>形态用于求解，这不是多此一举吗？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">p_sample</span>(<span class="params">self, x, t, clip_denoised=<span class="literal">True</span>, repeat_noise=<span class="literal">False</span></span>):</span><br><span class="line">    ...</span><br><span class="line">    model_mean, _, model_log_variance = <span class="variable language_">self</span>.p_mean_variance(</span><br><span class="line">        x=x, t=t, clip_denoised=clip_denoised</span><br><span class="line">    )</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> model_mean + nonzero_mask * (<span class="number">0.5</span> * model_log_variance).exp() * noise</span><br></pre></td></tr></table></figure><p>其实这里也是因为为了适配后续改进的原因，我们知道DDPM其实是固定方差的形式，即方差是像这里通过一系列参数直接计算得到的，但是后续iDDPM对这里进行了改进，模型直接预测得到log方差<spanclass="math inline"><em>l</em><em>o</em><em>g</em><em>σ</em><sup>2</sup></span>，将方差作为可学习表征，因此这里统一使用log形态的方差便于代码复用性。</p>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AIGC </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>扩散模型的发展（简略版）</title>
      <link href="/2025/03/28/AIGC/overview_DM/"/>
      <url>/2025/03/28/AIGC/overview_DM/</url>
      
        <content type="html"><![CDATA[<p><strong>系列文章：</strong></p><ol class="series-items"><li><a href="/2025/03/28/AIGC/overview_DM/" title="扩散模型的发展（简略版）">扩散模型的发展（简略版）</a></li><li><a href="/2025/04/15/AIGC/DDPM/" title="DDPM总结">DDPM总结</a></li><li><a href="/2025/04/17/AIGC/DDIM/" title="DDIM总结">DDIM总结</a></li><li><a href="/2025/04/27/AIGC/LDM/" title="LDM的细节">LDM的细节</a></li><li><a href="/2025/05/08/AIGC/DIT/" title="DiT的细节">DiT的细节</a></li><li><a href="/2025/05/08/AIGC/iDDPM/" title="iDDPM总结">iDDPM总结</a></li><li><a href="/2026/02/27/AIGC/JiT/" title="PixelDiT & JiT总结">PixelDiT & JiT总结</a></li><li><a href="/2026/02/27/AIGC/Flow_Matching/" title="Flow Matching简要介绍">Flow Matching简要介绍</a></li></ol><p>其实契机是因为要完成高级机器学习的综述作业（@^@），这里也顺便写到博客里。</p><h1 id="引言">引言</h1><p>自监督学习是一种特殊的无监督学习，它不需要人工标注数据，而是通过数据本身构造学习任务。而作为自监督学习中的一个重要分支方向，图像生成模型已有几十年的持续发展，其核心目标是学习数据的概率分布<span class="math inline"><em>P</em>(<em>x</em>)</span>，并从中采样出新的数据分布以生成新的图像。</p><figure><imgsrc="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%8F%91%E5%B1%95%E5%8E%86%E7%A8%8B_0.png"alt="图像生成模型的发展大致历程" /><figcaption aria-hidden="true">图像生成模型的发展大致历程</figcaption></figure><h2 id="图像生成模型">图像生成模型</h2><p>目前主流的图像生成架构可大致分为三类：</p><ul><li><strong>基于自回归（Autoregressive）的模型</strong>，使用逐像素或逐子块的方式生成图像，每一步依赖于前面生成的部分，生成速度较慢并难以适应高分辨率生成任务；</li><li><strong>基于生成对抗网络（GANs）的方法</strong>，使用生成器和判别器组成的对抗网络生成图像，生成速度较快但训练不稳定，并且难以控制生成细节；</li><li><strong>基于扩散（Diffusion）模型的方法</strong>，在前向过程中逐步向图像添加噪声，并在反向过程中训练模型学习如何去噪，使得模型能从噪声中恢复出图像，从而生成新图像。基于扩散模型的方法生成质量高，训练稳定并能引入精确的条件控制，虽然早期存在采样速度缓慢的问题，但后期的各种改进版本使其能逐渐适应各种生成任务，特别是文生图（Text-to-Image）任务，基本统治了各大模型，包括OpenAI的DALL·E系列<sub><spanstyle="color:blue;"><span class="citation"data-cites="ramesh2022hierarchical">[@ramesh2022hierarchical]</span></span></sub>和Google的IMAGEN<sub><spanstyle="color:blue;"><span class="citation"data-cites="saharia2022photorealistic">[@saharia2022photorealistic]</span></span></sub>系列背后的核心技术都是扩散模型。</li></ul><h2 id="扩散模型的发展">扩散模型的发展</h2><p>其实扩散模型的提出最初是受到了非平衡统计物理（nonequilibriumthermodynamics）中的扩散过程的启发，后续经过一系列的改进和发展，逐渐成为最主流的生成模型之一。</p><p>近期的扩散理论基础可以追溯到去噪自编码器<sub><spanstyle="color:blue;"><span class="citation"data-cites="vincent2011connection">[@vincent2011connection]</span></span></sub>的提出，该工作证明了去噪过程与分数匹配的相关性，为之后宋飏等人提出基于分数匹配的生成模型（Score-BasedGenerative Models，SGM）<sub><span style="color:blue;"><spanclass="citation"data-cites="song2019generative">[@song2019generative]</span></span></sub>奠定了基础。之后，DDPM（DenoisingDiffusion Probabilistic Models）<sub><span style="color:blue;"><spanclass="citation"data-cites="ho2020denoising">[@ho2020denoising]</span></span></sub>作为现代扩散模型的开山之作，在DPM（DenoisingProbabilistic Models）<sub><span style="color:blue;"><spanclass="citation"data-cites="sohl2015deep">[@sohl2015deep]</span></span></sub>的基础上建立了从高斯噪声逐步去噪生成图像的框架。2021年宋飏等人提出的随机微分方程（StochasticDiffusion Equation，SDE）<sub><span style="color:blue;"><spanclass="citation"data-cites="song2020score">[@song2020score]</span></span></sub>则通过数学证明将基于分数匹配和基于去噪的生成模型的两种范式统一了起来，由此进一步巩固了扩散模型的理论基础。</p><p>但是此时的扩散模型有一个严重的问题，采样速度极慢，无法适应各大场景的需求，DDIM<sub><spanstyle="color:blue;"><span class="citation"data-cites="song2020denoising">[@song2020denoising]</span></span></sub>提出了非马尔科夫链推理的想法，将采样步骤大幅减少，显著提升推理速度，却仍能保持与DDPM不相上下的生成质量。后续分类器引导（Classifier-Guidance）<sub><spanstyle="color:blue;"><span class="citation"data-cites="dhariwal2021diffusion">[@dhariwal2021diffusion]</span></span></sub>和无分类器引导（Classifier-FreeGuidance）<sub><span style="color:blue;"><span class="citation"data-cites="ho2022classifier">[@ho2022classifier]</span></span></sub>的提出推动了条件扩散模型的发展，并通过大量实验证明扩散模型在图像生成任务上首次超越GAN。之后GLIDE<sub><spanstyle="color:blue;"><span class="citation"data-cites="nichol2021glide">[@nichol2021glide]</span></span></sub>和DALL·E2<sub><span style="color:blue;"><span class="citation"data-cites="ramesh2022hierarchical">[@ramesh2022hierarchical]</span></span></sub>尝试使用Clip来进行文本引导的扩散模型，实现了高质量的文本到图像的生成。</p><p>潜在扩散模型（Latent Diffusion Model，LDM）<sub><spanstyle="color:blue;"><span class="citation"data-cites="rombach2022high">[@rombach2022high]</span></span></sub>提出以潜在隐空间代替原像素空间以大幅减少计算量，成为之后StableDiffusion的技术核心，支持高效的文本到图像生成。DiT（DiffusionTransformer）<sub><span style="color:blue;"><span class="citation"data-cites="peebles2023scalable">[@peebles2023scalable]</span></span></sub>则将原UNet结构替换为了纯Transformer架构，尽管计算量有所增加，但能适应更高分辨率的图像生成任务。除此之外，VideoDiffusion<sub><span style="color:blue;"><span class="citation"data-cites="ho2022video">[@ho2022video]</span></span></sub>等工作则探索了3DDiffusion的可能，Consistency Models<sub><span style="color:blue;"><spanclass="citation"data-cites="song2023consistency">[@song2023consistency]</span></span></sub>则尝试进一步加快采样速度的极限。</p><h1 id="相关工作">相关工作</h1><p>本节将根据扩散模型的发展介绍部分重要的相关工作。</p><h2 id="ncsn">NCSN</h2><p>首先本文将介绍宋飏老师的基于分数匹配的生成模型（Score-basedGenerative Model，SGM）的工作，原论文这个工作又叫作Noise ConditionalScore Networks（NCSN）<sub><span style="color:blue;"><spanclass="citation"data-cites="song2019generative">[@song2019generative]</span></span></sub>。NCSN通过估计数据分布的梯度，实现从噪声到数据的生成。</p><p>具体来说，假设有一个数据分布 <spanclass="math inline"><em>p</em><sub><em>d</em><em>a</em><em>t</em><em>a</em></sub>(<em>x</em>)</span>，我们需要估计该数据分布的对数梯度，又定义为<strong>分数函数</strong>（ScoreFuction）。但是直接估计数据分布的梯度很难，NCSN引入了多个噪声尺度来进行分数估计，类似于DDPM中加噪过程中的时间步，即：</p><p><span class="math display">$$  \begin{split}    s_\theta(x)\approx\nabla_x\log p_\mathrm{data}(x|\sigma)  \end{split}$$</span></p><p>由此可以得到分数匹配的目标函数为：</p><p><span class="math display">$$\begin{gather}\begin{split}    J(\theta) &amp;=\frac{1}{2}\intp_\mathrm{data}{(x)}\|s_{data}(x)-s_\theta(x)\|_2^2dx \\     &amp;=\frac{1}{2}\mathbb{E}_{p_{\mathrm{data}}(x)}\left[\left\|s_{data}(x)-s_\theta(x)\right\|_2^2\right]\end{split}\end{gather}$$</span></p><p>但是在实际求解时，<spanclass="math inline"><em>s</em><sub><em>d</em><em>a</em><em>t</em><em>a</em></sub>(<em>x</em>)</span>是无法计算的。经过一系列变换，可以得到原目标函数的一个等价表示：</p><p><span class="math display">$$\begin{gather}\begin{split}    J(\theta) &amp;=\int p_{\text {data}}(x)\left[\operatorname{tr}\left(\nabla_xs_\theta(x)\right)+\frac{1}{2}\left\|s_\theta(x)\right\|_2^2\right] dx\\    &amp;=\mathbb{E}_{p_{\text {data}}(x)}\left[\operatorname{tr}\left(\nabla_xs_\theta(x)\right)+\frac{1}{2}\left\|s_\theta(x)\right\|_2^2\right]\end{split}\end{gather}$$</span></p><p>其中涉及到了二阶偏导的计算，这在网络层次很深的时候开销是很大的，因此NCSN提出了分层分数匹配和降噪分数匹配两种方法来解决这个问题。分数估计训练完成后，便可以使用<strong>郎之万动力学（LangevinDynamics，LD）采样</strong>来生成数据分布：</p><p><span class="math display">$$    x_{t+1}=x_t+\frac{\sigma}{2}s_\theta(x_t)+\sqrt{\sigma}z_t$$</span></p><p>其中 <span class="math inline"><em>z</em><sub><em>t</em></sub></span>表示高斯噪声，这个过程模拟的是粒子在数据分布的梯度场中进行随机行走，最终收敛到真实数据分布。NCSN可以看作是扩散模型的前身，后续宋飏老师提出的SDE也从数学形式上统一了DDPM和NCSN，虽然它的采样较慢，训练也不稳定，但是它为后续扩散模型的发展提供了坚实的理论基础。</p><h2 id="ddpm">DDPM</h2><p>去噪扩散概率模型（Denoising Diffusion ProbabilisticModels）<sub><span style="color:blue;"><span class="citation"data-cites="ho2020denoising">[@ho2020denoising]</span></span></sub>是现代扩散模型的开山之作，它正式确立了扩散模型的数学框架，在图像生成任务上表现很出色。它的核心思想是前向扩散（ForwardDiffusion）和逆向去噪（ReverseDenoising）两部分，其推导基于马尔科夫链进行逐步加噪和去噪。</p><figure><imgsrc="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/DDPM_0.png"alt="DDPM的扩散及去噪过程" /><figcaption aria-hidden="true">DDPM的扩散及去噪过程</figcaption></figure><h3 id="前向扩散">前向扩散</h3><p>首先对于前向扩散过程，给定真实数据 <spanclass="math inline"><em>x</em><sub>0</sub> ∼ <em>q</em>(<em>x</em>)</span>，经过 <span class="math inline"><em>T</em></span>步的加噪过程，数据最终符合标准高斯分布。定义单步扩散过程为：</p><p><span class="math display">$$    q(x_t|x_{t-1})=\mathcal{N}(x_t;\sqrt{1-\beta_t}x_{t-1},\beta_tI)$$</span></p><p>令 <spanclass="math inline"><em>α</em><sub><em>t</em></sub> = 1 − <em>β</em><sub><em>t</em></sub></span>，由此可以发现 <spanclass="math inline"><em>q</em>(<em>x</em><sub><em>t</em></sub>|<em>x</em><sub><em>t</em> − 1</sub>)</span>就是一个以 <span class="math inline">$\sqrt{\alpha_t}x_{t-1}$</span>为均值，以 <spanclass="math inline">(1 − <em>α</em><sub><em>t</em></sub>)<em>I</em></span>为方差的高斯分布，加噪过程相当于是 <spanclass="math inline">$\sqrt{\alpha_t}x_{t-1}$</span> 基础上加上一个 <spanclass="math inline">𝒩(0, (1 − <em>α</em><sub><em>t</em></sub>)<em>I</em>)</span>的随机高斯噪声。进一步推导可以得到直接从 <spanclass="math inline"><em>x</em><sub>0</sub></span> 生成 <spanclass="math inline"><em>x</em><sub><em>t</em></sub></span>的公式为：</p><p><span class="math display">$$    q(x_t|x_0)=\mathcal{N}(x_t;\sqrt{\bar{\alpha}_t}x_0,(1-\bar{\alpha}_t)I)$$</span></p><p>其中 <spanclass="math inline">$\bar{\alpha}_t=\prod_{i=1}^t\alpha_i$</span>，表示累积噪声。最终当 <spanclass="math inline"><em>t</em> → <em>T</em></span> 时，<spanclass="math inline"><em>x</em><sub><em>T</em></sub></span>近似服从于标准高斯分布 <spanclass="math inline">𝒩(0, <em>I</em>)</span>。</p><h3 id="逆向去噪">逆向去噪</h3><p>逆向去噪的目标是从 <spanclass="math inline"><em>x</em><sub><em>T</em></sub></span>开始，逐步去噪恢复 <spanclass="math inline"><em>x</em><sub>0</sub></span> ，即 <spanclass="math inline">$p(x_{0:T})=p(x_T)\prod_{t=T-1}^0p(x_t|x_{t+1})$</span>，那么问题的关键在于如何求解 <spanclass="math inline"><em>p</em>(<em>x</em><sub><em>t</em></sub>|<em>x</em><sub><em>t</em> + 1</sub>)</span>。假设我们可以训练一个模型求解这个分布：</p><p><span class="math display">$$\begin{split}    p_{\theta}(x_{t-1} \mid x_t)=\mathcal{N}(x_{t-1} ; \mu_{\theta}(x_t,t),  \Sigma_{\theta}(x_t, t))\end{split}$$</span></p><p>不同于DPM<sub><span style="color:blue;"><span class="citation"data-cites="sohl2015deep">[@sohl2015deep]</span></span></sub>的直接预测<span class="math inline"><em>x</em><sub>0</sub></span>，DDPM使用UNet预测每一个时间步添加的噪声 <spanclass="math inline">$\hat{\epsilon_\theta}(x_t,t)\approx\epsilon$</span>，然后通过一系列数学推导得到：</p><p><span class="math display">$$\begin{split}    \mu_\theta\left(x_t,t\right)=\frac{1}{\sqrt{\alpha_t}}\left(x_t-\frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}_t}}\hat{\epsilon}_\theta\left(x_t, t\right)\right)\end{split}$$</span></p><p><spanclass="math inline"><em>p</em><sub><em>θ</em></sub>(<em>x</em><sub><em>t</em> − 1</sub> ∣ <em>x</em><sub><em>t</em></sub>)</span>的预测均值已经得到，而预测方差 <spanclass="math inline"><em>Σ</em><sub><em>θ</em></sub>(<em>x</em><sub><em>t</em></sub>, <em>t</em>)</span>在DDPM中被设为随机高斯噪声 <spanclass="math inline"><em>σ</em><sub><em>t</em></sub><em>z</em></span>，由此可以得到：</p><p><span class="math display">$$\begin{split}    x_{t-1}=\mu_\theta\left(x_t, t\right)+\sigma_t z\end{split}$$</span></p><p>可以看到，模型预测每一步的噪声 <spanclass="math inline"><em>ϵ</em><sub><em>t</em></sub></span>要比之前直接预测 <span class="math inline"><em>x</em><sub>0</sub></span>容易得多，DDPM的最终目标函数就是最小化去噪误差，即：</p><p><span class="math display">$$\begin{split}    \mathcalL(\theta)=\mathbb{E}_{x_0,\epsilon\sim\mathcal{N}(0,I),t}\left[\left\|\epsilon-\hat{\epsilon}_\theta(x_t,t)\right\|^2\right]\end{split}$$</span></p><p>这个损失函数就相当于训练模型预测噪声 <spanclass="math inline"><em>ϵ</em></span>的均方误差，学习难度大幅降低。不仅如此，在ImageNet256$$256任务上，DDPM取得了优于GAN的FID分数，这是扩散模型首次取得能和GAN竞争的分数，后续OpenAI发布了基于DDPM的DALL·E，也进一步提升了扩散模型的影响力。</p><h2 id="sde">SDE</h2><p>随机微分方程（Stochastic Differential Equations，SDE）<sub><spanstyle="color:blue;"><span class="citation"data-cites="song2020score">[@song2020score]</span></span></sub>是宋飏老师提出的一个连续时间扩散框架，它从数学上统一了NCSN和DDPM。如前文所介绍的，NCSN采用分数匹配的方法来预测概率分布的对数梯度，之后使用郎之万动力学采样以生成图像，它可以看作是连续噪声扰动；而DDPM则在有限的<span class="math inline"><em>T</em></span>步内加噪随后逆向去噪，采用预测噪声的方法来生成数据分布，基础理论依据是离散马尔可夫过程。</p><p><strong>随机微分方程</strong>是一类在经典微分方程基础上引入随机过程的数学方程，用于描述具有随机性或不确定性系统的演化，数据的随机扩散过程就可以用随机微分方程进行描述：</p><p><span class="math display">$$\begin{split}    dx=f(x,t)dt+g(t)dW\end{split}$$</span></p><p>其中 <spanclass="math inline"><em>f</em>(<em>x</em>, <em>t</em>)</span>是漂移项（drift term），用于描述数据的确定性过程，<spanclass="math inline"><em>g</em>(<em>t</em>)</span>是扩散项（diffusionterm），描述系统的随机性过程，<spanclass="math inline"><em>d</em><em>W</em></span>是标准布朗运动（WienerProcess）。经过一系列数学推导，可以分别得到NCSN和DDPM所对应的SDE，分别命名为<strong>VE-SDE</strong>（Variance Exploding SDE）和<strong>VP-SDE</strong>（Variance Preserving SDE）：</p><p><span class="math display">$$\begin{gather}\begin{split}        dx&amp;=\sqrt{d[\sigma^2(t)]}dW\quad\text{(VESDE)} \\        dx&amp;=-\frac{1}{2}\beta(t)xdt+\sqrt{\beta(t)}dW\quad\text{(VPSDE)}\end{split}\end{gather}$$</span></p><p>可以发现VESDE没有漂移项，噪声会随着 <spanclass="math inline"><em>t</em></span>的增加而爆炸性增长，因此叫作”VarianceExploding”，而VPSDE的噪声变化则较为平缓，因此叫作”VariancePreserving”。其实除了这两种方程外，原文还针对VPSDE提出了改进版本sub-VPSDE，为扩散项加上了一个额外的衰减因子让噪声水平增长得更慢，不会在早期就过度加噪，造成采样时需要更多的去噪步数。</p><p>使用SDE采样的关键是要确定逆向SDE，即从 <spanclass="math inline"><em>x</em><sub><em>T</em></sub></span> 逆推 <spanclass="math inline"><em>x</em><sub>0</sub></span>，由于扩散过程的反向过程也是一个扩散过程，因此我们可以得到逆向SDE的方程，其中的漂移系数和扩散系数和SDE保持一致：</p><p><span class="math display">$$\begin{split}    dx=[f(x,t)-g^2(t)\nabla_x\log p_t(x)]dt+g(t)d\bar{W}\end{split}$$</span></p><p>其中 <spanclass="math inline">∇<sub><em>x</em></sub>log <em>p</em><sub><em>t</em></sub>(<em>x</em>)</span>可由神经网络 <spanclass="math inline"><em>s</em><sub><em>θ</em></sub>(<em>x</em>, <em>t</em>)</span>预测得到，由此可以分别得到两个方程的逆向SDE离散表达：</p><p><span class="math display">$$\begin{gather}\begin{split}    x_i&amp;=x_{i+1}+(\sigma_{i+1}^2-\sigma_i^2)s_{\theta^*}(x_{i+1},i+1)+\sqrt{\sigma_{i+1}^2-\sigma_i^2}z_{i+1}\quad\text{(VESDE)} \\    x_i&amp;=(2-\sqrt{1-\beta_{i+1}})x_{i+1}+\beta_{i+1}s_{\theta^*}(x_{i+1},i+1)+\sqrt{\beta_{i+1}}z_{i+1}\quad\text{(VPSDE)}\end{split}\end{gather}$$</span></p><p>根据两个逆向SDE可以发现，它们的形式分别等效于NCSN所使用的郎之万采样和DDPM所使用的马尔科夫链，由此原文使用连续的SDE统一了两种生成框架。</p><p>除此之外，原文还提出了<strong>PC采样方法</strong>，这也是SDE数值求解常用的一种方法，因为SDE是对时间连续的方程，所以不同的离散化方案总是存在一定误差，可以使用score-basedMCMC采样方法进行进一步校正，即在每一步采样中额外添加一个修正步骤，让数据更快收敛。这里PC采样采用的修正器还是郎之万动力学方程，总结来说PC采样分两步：（1）Predictor，使用逆向SDE进行一步采样；（2）Corrector，使用郎之万动力学采样进一步调整。</p><p>去掉逆向SDE中的随机项（布朗运动 <spanclass="math inline"><em>d</em><em>w</em></span>项）后，可以得到概率流常微分方程（Probabilistic FlowODE），此时整个采样过程是一条确定性的轨迹，求解更快，因此适用于高效采样需求的任务。</p><h2 id="ddim">DDIM</h2><p>DDPM中，前向扩散过程定义为马尔科夫过程，为数据 <spanclass="math inline"><em>x</em><sub>0</sub></span>逐步添加噪声，逆向过程同样需要逐步去噪。但是为了保证最终前向过程的 <spanclass="math inline"><em>x</em><sub><em>T</em></sub></span>满足高斯噪声，这里的步数 <span class="math inline"><em>T</em></span>要设置得足够大（1000+），导致逆向过程的采样步数也非常大，推理的时间和计算开销巨大。DDIM<sub><spanstyle="color:blue;"><span class="citation"data-cites="song2020denoising">[@song2020denoising]</span></span></sub>提出了确定性采样方法，是扩散过程变为确定性过程（ODE），从而加速采样。</p><p>首先回顾一下定义在马尔科夫链的DDPM中单步加噪和单步去噪过程：</p><p><span class="math display">$$\begin{gather}\begin{split}        x_t &amp;= \sqrt{\alpha_t}x_{t-1}+\sqrt{1-\alpha_t}\epsilon_t \\        x_{t-1}&amp;=\frac{1}{\alpha_t}(x_t-\frac{\beta_t}{\sqrt{1-\bar{\alpha_t}}}\epsilon_\theta(x_t,t))+\sigma_tz\end{split}\end{gather}$$</span></p><p>基于此重新推导DDPM的优化目标，可以得到：</p><p><span class="math display">$$\begin{split}\mathcalL=\mathbb{E}_{x_0,\epsilon\sim\mathcal{N}(\mathbf{0},\mathbf{I})}\left[\|\epsilon-\epsilon_\theta\left(\sqrt{\bar{\alpha}_t}x_0+\sqrt{1-\bar{\alpha}_t}\epsilon,t\right)\|^2\right]\end{split}$$</span></p><p>可以发现，DDPM的优化目标其实仅仅依赖于边缘分布 <spanclass="math inline"><em>q</em>(<em>x</em><sub><em>t</em></sub>|<em>x</em><sub>0</sub>)</span>，而不依赖于联合分布 <spanclass="math inline"><em>q</em>(<em>x</em><sub>1 : <em>T</em></sub>|<em>x</em><sub>0</sub>)</span>，因此可以看出DDPM其实并不要求推理过程一定要是马尔科夫过程，只要推理分布满足边缘分布条件即可。经过一系列重新推导，可以得到非马尔科夫链下的单步逆向过程为：</p><p><span class="math display">$$\begin{split}\mathbf{x}_{t-1}=\sqrt{\alpha_{t-1}}\left(\underbrace{\frac{\mathbf{x}_t-\sqrt{1-\alpha_t}\epsilon_\theta(\mathbf{x}_t,t)}{\sqrt{\alpha_t}}}_{\mathrm{predicted~}\mathbf{x}_0}\right)+\underbrace{\sqrt{1-\alpha_{t-1}-\sigma_t^2}\cdot\epsilon_\theta(\mathbf{x}_t,t)}_{\text{directionpointing to }\mathbf{x}_t}+\underbrace{\sigma_t\epsilon_t}_{\text{randomnoise}}\end{split}$$</span></p><p>其中 <spanclass="math inline">$\sigma_t^2=\eta\cdot\sqrt{(1-\alpha_{t-1})/(1-\alpha_t)}\sqrt{(1-\alpha_t/\alpha_{t-1})}$</span>。</p><ul><li>当 <span class="math inline"><em>η</em> = 1</span>时，此时的生成过程和DDPM一致；</li><li>当 <span class="math inline"><em>η</em> = 0</span>时，此时的生成过程就没有随机噪声项了，是一个确定性的过程，这就是DDIM（DenoisingDiffusion ImplicModel），此时的样本生成就变成了确定的过程，有点类似于SDE中的概率流ODE。</li></ul><figure><imgsrc="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/DDIM_0.png"alt="DDIM的采样过程" /><figcaption aria-hidden="true">DDIM的采样过程</figcaption></figure><p>由于DDIM并没有明确前向过程，这意味这可以定义一个更短的步数的前向过程以减少采样步数。原文从原始序列<span class="math inline">[1, …, <em>T</em>]</span> 采样一个长度为 <spanclass="math inline"><em>S</em></span> 的子序列 <spanclass="math inline">[<em>τ</em><sub>1</sub>, …, <em>τ</em><sub><em>S</em></sub>]</span>，此时前向过程 <spanclass="math inline">[<em>x</em><sub><em>τ</em><sub>1</sub></sub>, …, <em>x</em><sub><em>τ</em><sub><em>S</em></sub></sub>]</span>同样满足 <spanclass="math inline">$q({x}_{\tau_i}|{x}_0)=\mathcal{N}({x}_t;\sqrt{\alpha_{\tau_i}}{x}_0,(1-\alpha_{\tau_i}){I})$</span>。由此该生成过程也可以用这个子序列进行采样，最终加速生成过程，能从DDPM的1000步采样减少为50步采样，大幅降低推理开销。</p><h2 id="cdm">CDM</h2><p>条件扩散模型（Conditional DiffusionModel，CDM）指使用条件控制生成的扩散模型，其中最常用的方法是引导（guidance）方法，可以用于增强生成质量或是让模型朝向某个目标分布进行采样。最主要的两种方法是：分类器引导（ClassifierGuidance）<sub><span style="color:blue;"><span class="citation"data-cites="dhariwal2021diffusion">[@dhariwal2021diffusion]</span></span></sub>和无分类器引导（Classifier-FreeGuidance）<sub><span style="color:blue;"><span class="citation"data-cites="ho2022classifier">[@ho2022classifier]</span></span></sub>，因此这里将分别简要介绍这两篇工作。</p><h3 id="分类器引导">分类器引导</h3><p>假设我们需要在扩散过程中引入条件信息 <spanclass="math inline"><em>y</em></span>，直觉上来说，条件信息不会影响前向过程，因为最终加噪都会变为高斯噪音，即<spanclass="math inline"><em>q</em>(<em>x</em><sub>1 : <em>T</em></sub>|<em>x</em><sub>0</sub>, <em>y</em>) = <em>q</em>(<em>x</em><sub>1 : <em>T</em></sub>|<em>x</em><sub>0</sub>)</span>。因此我们需要重点关注逆向过程的条件控制，不妨说我们需要重点关注分数函数<spanclass="math inline"><em>ŝ</em><sub><em>θ</em></sub>(<em>x</em><sub><em>t</em></sub>, <em>t</em>) ≈ ∇<sub><em>x</em><sub><em>t</em></sub></sub>log <em>p</em>(<em>x</em><sub><em>t</em></sub>)</span>的变化。引入条件信息后，我们可以使用贝叶斯公式进行几步变换：</p><p><span class="math display">$$\begin{gather}\begin{split}        \nabla_{x_t}\logp(x_t|y)&amp;=\nabla_{x_t}\log(\frac{p(x_t)p(y|x_t)}{p(y)}) \\        &amp;= \nabla_{x_t}\log p(x_t)+\nabla_{x_t}\logp(y|x_t)-\nabla_{x_t}\log p(y) \\        &amp;= \nabla_{x_t}\log p(x_t)+\nabla_{x_t}\log p(y|x_t)\end{split}\end{gather}$$</span></p><p>可以看到，展开后有两项，其中第一项相当于无条件生成时的分数函数，可以称为无条件分数（UnconditionalScore）；而第二项相当于一个分类器的对数梯度，称为对抗梯度（AdversarialGradient）。为了能更好的控制生成内容的方向，论文额外引入了一个超参数<span class="math inline"><em>λ</em></span> 作为引导强度：</p><p><span class="math display">$$\begin{gather}\nabla_{x_t}\log p(x_t|y)=\nabla_{x_t}\logp(x_t)+\lambda\nabla_{x_t}\log p(y|x_t)\end{gather}$$</span></p><h3 id="无分类器引导">无分类器引导</h3><p>自OpenAI发布分类器引导的条件生成范式后，GoogleBrain团队提出了无分类器引导方法，直接在扩散模型内部学习引导信息，无需额外的分类器。还是从分数估计的角度来进行推导，从条件引导的工作我们得知<spanclass="math inline">∇<sub><em>x</em><sub><em>t</em></sub></sub>log <em>p</em>(<em>x</em><sub><em>t</em></sub>|<em>y</em>) = ∇<sub><em>x</em><sub><em>t</em></sub></sub>log <em>p</em>(<em>x</em><sub><em>t</em></sub>) + ∇<sub><em>x</em><sub><em>t</em></sub></sub>log <em>p</em>(<em>y</em>|<em>x</em><sub><em>t</em></sub>)</span>，变换一下可以得到 <spanclass="math inline">∇<sub><em>x</em><sub><em>t</em></sub></sub>log <em>p</em>(<em>y</em>|<em>x</em><sub><em>t</em></sub>) = ∇<sub><em>x</em><sub><em>t</em></sub></sub>log <em>p</em>(<em>x</em><sub><em>t</em></sub>|<em>y</em>) − ∇<sub><em>x</em><sub><em>t</em></sub></sub>log <em>p</em>(<em>x</em><sub><em>t</em></sub>)</span>，由此可以得到：</p><p><span class="math display">$$\begin{gather}\begin{split}    \nabla_{x_t}\log p(x_t|y)&amp;=\nabla_{x_t}\logp(x_t)+\lambda\nabla_{x_t}\log p(y|x_t) \\    &amp;=\nabla_{x_t}\log p(x_t)+\lambda(\nabla_{x_t}\logp(x_t|y)-\nabla_{x_t}\log p(x_t)) \\    &amp;=\lambda\nabla_{x_t}\log p(x_t|y)+(1-\lambda)\log p(x_t)\end{split}\end{gather}$$</span></p><p>可以发现，此时分数仍然分为两部分：第一项可以看作是条件分数（ConditionalScore），第二项则是无条件分数（Unconditional Score）。并且 <spanclass="math inline"><em>λ</em></span> 的取值会影响到条件控制的强弱：</p><ul><li>当 <span class="math inline"><em>λ</em> = 0</span>时，此时相当于无条件生成；</li><li>当 <span class="math inline"><em>λ</em> &gt; 1</span>时，模型会优先考虑条件控制而远离无条件分数网络方向。</li></ul><p>这样看似乎还是要训练两个网络，但实际上无条件可看作是条件控制的特殊情况，即<span class="math inline"><em>y</em> = ⌀</span>，这样在训练时可以交替训练有条件和无条件的情况。</p><h2 id="ldm">LDM</h2><p>DDPM生成的图像质量已经非常好了，但是训练开销很大，一个问题在于中间的加噪状态<span class="math inline"><em>x</em><sub><em>t</em></sub></span>的尺寸是和输入保持一致的，这使得其训练开销随图像分辨率的增大而加重，无法适应高质量图像生成任务。因此潜在扩散模型（LatentDiffusion Model，LDM）<sub><span style="color:blue;"><spanclass="citation"data-cites="rombach2022high">[@rombach2022high]</span></span></sub>针对这个问题做了一些改进，<strong>将图像从像素空间表示（PixelSpace）转变为潜在空间表示（LatentSpace）实现高分辨率图像生成任务</strong>。</p><figure><imgsrc="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/LDM_0.png"alt="潜在扩散模型的模型架构" /><figcaption aria-hidden="true">潜在扩散模型的模型架构</figcaption></figure><p>LDM的主要架构由三部分组成：VAE编码器（Encoder）、扩散模型以及VAE解码器（Decoder）。其中VAE编码器将高维图像<span class="math inline"><em>x</em><sub>0</sub></span>编码表示为潜在表示 <spanclass="math inline"><em>z</em><sub>0</sub></span>，然后送入扩散模型中进行扩散和去噪，最终VAE解码器将去噪得到的潜在空间表示<span class="math inline">$\hat{z_0}$</span> 还原为像素空间表示 <spanclass="math inline">$\hat{x_0}$</span>，得到高质量的图像。这个VAE可以是预训练好的模型，在训练扩散模型时，其参数是被冻结的。</p><p>而对于条件生成处理上，LDM引入条件融合模块 <spanclass="math inline"><em>τ</em><sub><em>θ</em></sub></span>来处理多种模态的条件信息 <span class="math inline"><em>y</em></span>。比如对于文生图任务，这里的 <spanclass="math inline"><em>τ</em><sub><em>θ</em></sub></span>就是一个文本编码器，可以使用预训练好的CLIP模型<sub><spanstyle="color:blue;"><span class="citation"data-cites="radford2021learning">[@radford2021learning]</span></span></sub>中的文本编码器。同时引入条件融合开关：</p><ul><li>对于文本输入，这里在Unet网络中添加了Attention层将Embedding向量 <spanclass="math inline"><em>τ</em><sub><em>θ</em></sub>(<em>y</em>)</span>融合到每层特征中；</li><li>而对于其他空间的条件（语义图、修复图等），则直接通过拼接完成条件融合。</li></ul><p>由此我们可以得到LDM的目标函数为：</p><p><span class="math display">$$\begin{split}\mathcal L_{L D M}:=\mathbb{E}_{\mathcal{E}(x), y, \epsilon \sim\mathcal{N}(0,1), t}\left[\left\|\epsilon-\epsilon_\theta\left(z_t, t,\tau_\theta(y)\right)\right\|_2^2\right]\end{split}$$</span></p><p>后续爆火的StableDiffusion就是LDM的一个开源预训练模型，一度占据图像生成开源领域的主导地位。</p><h2 id="dit">DiT</h2><p>DiT（Diffusion Transformer）<sub><span style="color:blue;"><spanclass="citation"data-cites="peebles2023scalable">[@peebles2023scalable]</span></span></sub>是MetaAI提出的基于Transformer的扩散模型，它首次在扩散模型完全用Transformer替代了UNet，提升了扩散模型的可扩展性和生成质量。</p><figure><imgsrc="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/DiT_0.png"alt="DiT的模型架构" /><figcaption aria-hidden="true">DiT的模型架构</figcaption></figure><p>DiT也是一个作用在潜在空间上的模型，同样使用一个VQVAE将图像编码到潜在空间上，之后送入DiT模块中加工。不同的是，由于舍弃掉了CNN，这里使用了ViT进一步将潜在空间特征转换为一维序列特征（PatchToken），并将时间步 <span class="math inline"><em>t</em></span>和条件信息 <span class="math inline"><em>y</em></span>融合后嵌入到图像的Patch Token中。</p><p>为了选择融合条件特征效果最好的DiT模块，原文一共探索了四种不同的DiT模块：</p><ul><li>基于上下文条件（In-contextconditioning）的DiT模块，直接将条件特征嵌入到输入序列中；</li><li>基于交叉注意力（Cross Attention）的DiT模块，将时间步 <spanclass="math inline"><em>t</em></span> 和条件信息 <spanclass="math inline"><em>y</em></span>拼成长度为2的序列，然后输入到多头交叉注意力模块中和图像特征进行融合；</li><li>基于自适应层归一化（Adaptive LayerNormalization，AdaLN）的DiT模块，通过使用条件信息学习 <spanclass="math inline"><em>β</em></span> 和 <spanclass="math inline"><em>γ</em></span>两个归一化参数来调整中间特征；</li><li>基于Zero初始化的AdaLN的DiT模块，是AdaLN方案的改进版本，将AdaLN的线性层参数初始化为zero，并额外在每个残差模块结束之前引入回归缩放参数<span class="math inline"><em>α</em></span> 。</li></ul><p>通过对四种模块进行对比实验，发现AdaLN-Zero的效果是最好的，DiT模块默认采用这种方式来嵌入条件。</p><p>同时，需要注意的是DiT所使用的扩散模型沿用了OpenAI的改进版DDPM<sub><spanstyle="color:blue;"><span class="citation"data-cites="song2020improved">[@song2020improved]</span></span></sub>，不再采用固定的方差，而是采用另一个网络来预测方差<spanclass="math inline"><em>Σ</em><sub><em>θ</em></sub>(<em>x</em><sub><em>t</em></sub>, <em>t</em>) = exp (<em>v</em>log <em>β</em><sub><em>t</em></sub> + (1 − <em>v</em>)<em>β̃</em><sub><em>t</em></sub>)</span>，训练时采用无分类器引导的范式进行学习。该种方法对于视频生成等需要强可扩展性的任务来说很适用，OpenAI推出的Sora就是用了DiT作为模型架构。</p><h2 id="pixart">PixArt</h2><p>在DiT推出之后，华为诺亚方舟实验室又提出了PixArt-<spanclass="math inline"><em>α</em></span><sub><spanstyle="color:blue;"><span class="citation"data-cites="chen2023pixart">[@chen2023pixart]</span></span></sub>，这也是一种基于transformer的文本到图像的扩散模型，它在显著降低训练成本的同时，也实现了很不错的图像生成质量。</p><p>PixArt-<spanclass="math inline"><em>α</em></span>模型还是使用DiT作为基础架构，但是进行了一些改进。原DiT架构中的每个DiT模块中的<spanclass="math inline"><em>S</em><sup><em>i</em></sup></span>都是通过独立的MLP计算得到的，即<spanclass="math inline"><em>S</em><sup>(<em>i</em>)</sup> = <em>f</em><sup>(<em>i</em>)</sup>(<em>c</em> + <em>t</em>)</span>，其中 <span class="math inline"><em>c</em>, <em>t</em></span>分别表示类别条件和时间步信息，这会占据很高的开销。基于此，PixArt提出了AdaLN-single，定义一个全局<span class="math inline"><em>S̄</em> = <em>f</em>(<em>t</em>)</span>，只使用时间步信息生成 <span class="math inline"><em>S̄</em></span>，在第 <span class="math inline"><em>i</em></span> 个模块中，通过计算<spanclass="math inline"><em>S</em><sup>(<em>i</em>)</sup> = <em>g</em>(<em>S̄</em>, <em>E</em><sup>(<em>i</em>)</sup>)</span>得到每个模块的缩放和偏移参数，其中 <spanclass="math inline"><em>E</em><sup>(<em>i</em>)</sup></span>是可训练的嵌入表示；而文本条件 <spanclass="math inline"><em>c</em></span>则通过一个额外的多头交叉注意力嵌入到模块中。大量实验表明，通过引入全局MLP和逐层嵌入处理时间步<span class="math inline"><em>t</em></span>信息、使用交叉注意力层处理文本信息 <spanclass="math inline"><em>c</em></span>的改进，能在有效减小模型大小的同时保持原生成能力。</p><figure><imgsrc="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/PixArt_0.png"alt="PixArt-\alpha的模型架构" /><figcaption aria-hidden="true">PixArt-<spanclass="math inline"><em>α</em></span>的模型架构</figcaption></figure><p>除此之外，PixArt-<spanclass="math inline"><em>α</em></span>将复杂的文本到图像的生成任务分解为三个子任务以逐步训练：</p><ul><li>像素级依赖的学习（Pixel DependencyLearning），为了实现后续高质量的生成，PixArt先在ImageNet上预训练类引导生成模型，这一过程成本低廉并能帮助模型有效学习到图像的像素级依赖性；</li><li>文本到图像的精确对齐学习（Text-image AlignmentLearning），原论文构建了一个包含高概念密度的精确文本-图像对数据集，相较于以往的数据集，歧义显著减少，并能处理更多的名词；</li><li>高质量图像微调（High-resolution and Aesthetic ImageGeneration），为了生成高审美质量的图片，原论文最后使用高质量的图片对模型进行进一步微调。</li></ul><h1 id="总结与展望">总结与展望</h1><p>在扩散模型成为主流之前，基于能量的生成模型和分数匹配已经被研究了许多年，这些工作为扩散模型的出现奠定了理论基础，直到后来DDPM的提出正式标志着扩散模型的爆发。之后一系列的工作探讨了对原始模型的改进，体现在加速采样、降低训练开销、提升图像生成质量等，一度使扩散模型超越基于自回归和GAN的生成模型成为大规模生成任务的首选模型。后来进入大模型时代，包括DALL·E、IMAGEN、StableDiffusion的文生图大模型更是进一步引爆了Diffusion的影响力。总的来说，作为当下Aigc领域中热门研究领域之一，扩散模型的发展正值草长莺飞的时期，它开创了一种全新的生成模型范式，并被广泛应用于各类生成式任务以及当下视觉生成大模型中。</p><p>未来对于扩散模型的研究还在继续，比如如何进一步提升采样速度、创造更通用的多模态扩散模型、更精细的条件控制、量化优化以实现EdgeAI等等，还有很多课题值得探索...</p>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AIGC </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>建站日志以及记录</title>
      <link href="/2025/03/27/hello-world/"/>
      <url>/2025/03/27/hello-world/</url>
      
        <content type="html"><![CDATA[<h1 id="leo小破站的诞生日">Leo小破站的诞生日</h1><p><strong>2025.3.31</strong></p><h2 id="关于建站">关于建站</h2><p>使用Hexo框架 + Github Pages + Butterfly主题，这里要感谢<ahref="https://blog.csdn.net/m0_51269961/article/details/122575897">杰森的教程</a>，向大佬低头orz…</p><h2 id="关于契机">关于契机</h2><p>其实很早之前就想建站了，但是拖延症一直拖到现在…上本研共修课看到了大二学弟做的博客，觉得自愧不如，趁现在还有时间，随便建一个站吧</p><h1 id="hexo">Hexo</h1><p>关于hexo的一些常用命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 事实上已经修改json，调试时只需要hexo s即可</span></span><br><span class="line">$ hexo clean</span><br><span class="line">$ hexo generate</span><br><span class="line">$ hexo server</span><br><span class="line"></span><br><span class="line">$ hexo d</span><br></pre></td></tr></table></figure><p>More info about writing: <ahref="https://hexo.io/docs/writing.html">Writing</a></p><p>More info about server: <ahref="https://hexo.io/docs/server.html">Server</a></p><p>More info about deployment: <ahref="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p><h1 id="butterfly">Butterfly</h1><p>根据<a href="https://butterfly.js.org/">原主题配置文档</a>修改<code>_config.butterfly.yml</code> 文件自定义博客配置。</p><h2 id="发布文章">发布文章</h2><p>需要修改 <code>Post Fron-matter</code> ，于文章最上方以<code>---</code> 分隔的区域，常用配置参数如下。</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title: （required）文章标题</span><br><span class="line">date: （optional）创建时间</span><br><span class="line">updated: （optional）更新时间</span><br><span class="line">tags: （optional）文章标签</span><br><span class="line">categories: （optional）文章分类</span><br><span class="line">description: （optional）文章简介</span><br><span class="line">top<span class="emphasis">_img: （optional）文章头图</span></span><br><span class="line"><span class="emphasis">% 转载其他作品时放置</span></span><br><span class="line"><span class="emphasis">copyright: （optional）显示文章版权模块</span></span><br><span class="line"><span class="emphasis">copyright_</span>author: （optional）文章作者</span><br><span class="line">copyright<span class="emphasis">_author_</span>href: （optional）文章作者url</span><br><span class="line">copyright<span class="emphasis">_url: （optional）文章url</span></span><br><span class="line"><span class="emphasis">copyright_</span>info: （optional）版权声明</span><br><span class="line">---</span><br></pre></td></tr></table></figure><h2 id="发布页面">发布页面</h2><p>需要修改 <code>Page Front-matter</code>，于文章最上方以<code>---</code> 分隔的区域，常用配置参数如下。</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title: （required）文章标题</span><br><span class="line">date: （optional）创建时间</span><br><span class="line">updated: （optional）更新时间</span><br><span class="line">type: （required）标签、分类等页面必须标明</span><br><span class="line">description: 页面描述</span><br><span class="line"><span class="section">top<span class="emphasis">_img: 页面头图</span></span></span><br><span class="line"><span class="emphasis"><span class="section">---</span></span></span><br></pre></td></tr></table></figure><h2 id="关于公式">关于公式</h2><p>原公式渲染器无法正常渲染复杂latex公式，卸载了原公式渲染器：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm un hexo-renderer-marked</span><br></pre></td></tr></table></figure><p>并安装了 <code>pandoc</code> 对应的渲染器：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm i hexo-renderer-pandoc</span><br></pre></td></tr></table></figure><p>启用 <code>mathjax</code>设置，但是这样做行内公式其实不太好看、行间公式也还是无法正常渲染，调试发现需要额外插入<code>begin-end</code> 块才能正常渲染。以下为演示：</p><blockquote><p>行内公式： 引入缩放因子 <spanclass="math inline"><em>λ</em><sub><em>θ</em></sub></span></p></blockquote><blockquote><p>行间公式： <span class="math display">$$\begin{split}  s_\theta(x)\approx\nabla_x\log p_\mathrm{data}(x|\sigma)\end{split}$$</span></p><p><span class="math display">$$\begin{gather}\begin{split}   J(\theta) &amp;=\frac{1}{2}\intp_\mathrm{data}{(x)}\|s_{data}(x)-s_\theta(x)\|_2^2dx \\    &amp;=\frac{1}{2}\mathbb{E}_{p_{\mathrm{data}}(x)}\left[\left\|s_{data}(x)-s_\theta(x)\right\|_2^2\right]\end{split}\end{gather}$$</span></p></blockquote><p>以上行间公式对应的md写法为： <figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">$$</span><br><span class="line">  \begin&#123;split&#125; </span><br><span class="line"><span class="code">    s_\theta(x)\approx\nabla_x\log p_\mathrm&#123;data&#125;(x|\sigma) </span></span><br><span class="line"><span class="code">  \end&#123;split&#125;</span></span><br><span class="line"><span class="code">$$</span></span><br><span class="line"><span class="code"></span></span><br><span class="line">$$</span><br><span class="line">\begin&#123;gather&#125;</span><br><span class="line">\begin&#123;split&#125;</span><br><span class="line"><span class="code">    J(\theta) &amp;=\frac&#123;1&#125;&#123;2&#125;\int p_\mathrm&#123;data&#125;&#123;(x)&#125;\|s_&#123;data&#125;(x)-s_\theta(x)\|_2^2dx \\</span></span><br><span class="line"><span class="code">     &amp;=\frac&#123;1&#125;&#123;2&#125;\mathbb&#123;E&#125;_&#123;p_&#123;\mathrm&#123;data&#125;&#125;(x)&#125;\left[\left\|s_&#123;data&#125;(x)-s_\theta(x)\right\|_2^2\right]</span></span><br><span class="line"><span class="code">\end&#123;split&#125;</span></span><br><span class="line"><span class="code">\end&#123;gather&#125;</span></span><br><span class="line"><span class="code">$$</span></span><br></pre></td></tr></table></figure></p><div class="note warning simple"><p>暂时没有找到什么其他解决办法，留待解决…</p></div><h1 id="github-pages">Github Pages</h1><p>Leo的博客：<ahref="https://litchi-lee.github.io/">https://litchi-lee.github.io/</a></p><h2 id="图床">图床</h2><p>额外建库搭配 <code>PicGo</code> 生成图床，配合Github插件<ahref="https://github.com/marketplace/imgbot">Githubimgbot</a>压缩库的图片。</p><p>由于GithubRaw链接被大量匿名访问会被封禁，因此这里使用免费的jsDelivr加速GitHub资源：</p><pre><code>https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main</code></pre>]]></content>
      
      
      <categories>
          
          <category> 杂七杂八 </category>
          
      </categories>
      
      
    </entry>
    
    
  
  
</search>
