<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>DiT的细节</title>
      <link href="/2025/05/08/AIGC/DIT/"/>
      <url>/2025/05/08/AIGC/DIT/</url>
      
        <content type="html"><![CDATA[<p><strong>系列文章：</strong></p><ol class="series-items"><li><a href="/2025/04/15/AIGC/DDPM/" title="DDPM总结">DDPM总结</a></li><li><a href="/2025/04/17/AIGC/DDIM/" title="DDIM总结">DDIM总结</a></li><li><a href="/2025/05/08/AIGC/DIT/" title="DiT的细节">DiT的细节</a></li><li><a href="/2025/05/08/AIGC/iDDPM/" title="iDDPM总结">iDDPM总结</a></li></ol><blockquote><p>原论文： <a href="http://openaccess.thecvf.com/content/ICCV2023/html/Peebles_Scalable_Diffusion_Models_with_Transformers_ICCV_2023_paper.html">Scalable diffusion models with transformers</a></p><p>源码： <a href="https://github.com/facebookresearch/DiT">https://github.com/facebookresearch/DiT</a></p></blockquote><h1 id="背景介绍">背景介绍</h1>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AIGC </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>iDDPM总结</title>
      <link href="/2025/05/08/AIGC/iDDPM/"/>
      <url>/2025/05/08/AIGC/iDDPM/</url>
      
        <content type="html"><![CDATA[<p><strong>系列文章：</strong></p><ol class="series-items"><li><a href="/2025/04/15/AIGC/DDPM/" title="DDPM总结">DDPM总结</a></li><li><a href="/2025/04/17/AIGC/DDIM/" title="DDIM总结">DDIM总结</a></li><li><a href="/2025/05/08/AIGC/DIT/" title="DiT的细节">DiT的细节</a></li><li><a href="/2025/05/08/AIGC/iDDPM/" title="iDDPM总结">iDDPM总结</a></li></ol><blockquote><p>原论文： <a href="https://proceedings.mlr.press/v139/nichol21a.html">Improved denoising diffusion probabilistic models</a></p><p>源码： <a href="https://github.com/openai/improved-diffusion">https://github.com/openai/improved-diffusion</a></p></blockquote><h1 id="背景介绍">背景介绍</h1><p>虽然DDPMs可以生成很高质量的图像，但是在log似然指标上还是无法超过自回归模型（VAEs），DDPMs是否能真正学习到数据分布还有待验证。同时DDPM在采样效率上也存在大问题，该如何减少采样所需的步长也有待研究。</p><p>本文提出了结合原目标函数和variational lower-bound（VLB）目标函数的<strong>混合目标函数</strong>，能实现更好的log似然指标，并使用<strong>重要性采样</strong>实现更平滑的训练。并且本文发现使用预训练好的混合损失模型能使用更少的步数实现高质量的生成。除此之外，还提出了一些其他的改进，进一步提升DDPM的生成质量。</p><h1 id="一些改进ddpms的tricks">一些改进DDPMs的tricks</h1><h2 id="可学习方差">可学习方差</h2><p>在DDPM中，对于逆向过程的后验概率可以用神经网络估计为：</p><p><span class="math display">$$\begin{split}    p_\theta\left(x_{t-1} \mid x_t\right):=\mathcal{N}\left(x_{t-1} ; \mu_\theta\left(x_t, t\right), \Sigma_\theta\left(x_t, t\right)\right)\end{split}$$</span></p><p>其中方差部分DDPM设定为固定方差（不可学习），即 <span class="math inline"><em>Σ</em><sub><em>θ</em></sub>(<em>x</em><sub><em>t</em></sub>,<em>t</em>) = <em>σ</em><sub><em>t</em></sub><sup>2</sup><strong>I</strong></span> 。并且DDPM发现当 <span class="math inline"><em>σ</em><sub><em>t</em></sub><sup>2</sup></span> 设定为 <span class="math inline"><em>β</em><sub><em>t</em></sub></span> 和 <span class="math inline">$\tilde{\beta_t}$</span> 时效果其实相差不大，其中 <span class="math inline"><em>β</em><sub><em>t</em></sub> := 1 − <em>α</em><sub><em>t</em></sub></span>，<span class="math inline">$\tilde{\beta}_t:=\frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t} \beta_t$</span>。但实际这两个参数在时间步逐渐增长的过程中也在逐渐逼近，最终基本重合。</p>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AIGC </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LDM的细节</title>
      <link href="/2025/04/27/AIGC/LDM/"/>
      <url>/2025/04/27/AIGC/LDM/</url>
      
        <content type="html"><![CDATA[<p><strong>系列文章：</strong></p><ol class="series-items"><li><a href="/2025/03/28/AIGC/overview_DM/" title="扩散模型的发展（简略版）">扩散模型的发展（简略版）</a></li><li><a href="/2025/04/17/AIGC/DDIM/" title="DDIM的细节">DDIM的细节</a></li><li><a href="/2025/04/27/AIGC/LDM/" title="LDM的细节">LDM的细节</a></li><li><a href="/2025/04/27/AIGC/LDM/" title="LDM的细节">LDM的细节</a></li><li><a href="/2025/04/27/AIGC/LDM/" title="LDM的细节">LDM的细节</a></li></ol><blockquote><p>原论文：<a href="http://openaccess.thecvf.com/content/CVPR2022/html/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.html">High-resolution image synthesis with latent diffusion models</a></p><p>源码：<a href="https://github.com/CompVis/latent-diffusion">https://github.com/CompVis/latent-diffusion</a></p></blockquote><h1 id="背景介绍">背景介绍</h1><p>影像生成是当时计算需求最大的领域之一，尤其是复杂场景的高分辨率合成。这对于自回归架构（AR）来说可能需要数十亿参数，同时GANs所取得的成果大多局限于有限变化的数据，不容易扩展到复杂的多模态分布建模。扩散模型的进步在多个任务下取得了SOTA，却不会像GANs一样表现出模式崩溃或者训练不稳定性，也无需像AR模型那样涉及数十亿个参数。即便如此，DMs仍是计算高需求的。</p><blockquote><p>likelihood-based model学习过程可分为两个阶段：</p><ul><li>感知压缩阶段：消除高频细节，学习语义信息</li><li>生成模型阶段：学习该语义压缩信息</li></ul></blockquote><p>本文的目标就是找到一个合适的表示空间，保持相同的语义信息但是更计算高效。</p><h2 id="两阶段图像生成">两阶段图像生成</h2><h3 id="vqvae">VQVAE</h3><p><strong>VQVAE</strong>使用自回归模型学习离散潜空间的先验表达，它与AE最大的不同在于AE编码的特征向量是连续的，但是VQVAE编码的特征向量是离散的，即视觉码本（Visual Codebook）或视觉字典（Visual Dictionary）。</p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/屏幕截图 2025-04-27 142255.png" width="80%"/></p><h3 id="vqgan">VQGAN</h3><p><strong>VQGAN</strong>是一个改良版的VQVAE，相比于VQVAE，其有3点改进：</p><ul><li>将传统CNN改为Transformer来捕捉较远像素之间的依赖关系</li><li>额外增加了一个PatchGAN作为判别器，并在训练时加入判别损失</li><li>使用感知损失代替传统的L2损失</li></ul><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/屏幕截图 2025-04-27 143922.png" width="60%"/></p><h1 id="method">Method</h1><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/LDM_0.png" width="60%"/></p><h2 id="感知图像压缩">感知图像压缩</h2><p>Perceptual Image Compression模块使用一个预训练好的VQGAN将原始输入图像进行感知压缩。具体来说，对于给定的输入空间 <span class="math inline"><em>x</em> ∈ ℝ<sup><em>H</em> × <em>W</em> × 3</sup></span> ，编码器 <span class="math inline">ℰ</span> 将 <span class="math inline"><em>x</em></span> 编码到隐空间表示 <span class="math inline"><em>z</em> = ℰ(<em>x</em>)</span> ，其中 <span class="math inline"><em>z</em> ∈ ℝ<sup><em>h</em> × <em>w</em> × <em>c</em></sup></span>。</p><p>为了避免潜空间的方差过大，LDM使用了两种不同的正则化方法：KL-reg和VQ-reg。其中KL-reg相当于在潜空间上施加了KL惩罚，VQ-reg的作用则是使用向量量化层（Vector Quantization Layer）将特征归一化为码本中最近的那个特征。</p><p>对于VQ-reg的计算可以分解为以下步骤：</p><ol type="1"><li>特征reshape：将输入数据 <span class="math inline"><em>z</em><sub><em>e</em></sub> ∈ ℝ<sup><em>n</em> × <em>h</em> × <em>w</em> × <em>d</em></sup></span> 进行维度合并，得到 <span class="math inline"><em>n</em> × <em>h</em> × <em>w</em></span> 个长度为 <span class="math inline"><em>d</em></span> 的特征向量；</li><li>映射码本：对于每个特征向量，计算其与码本中<span class="math inline"><em>k</em></span>个向量的距离，选择距离最近的特征作为索引，最终得到经过码本查找的<span class="math inline"><em>n</em> × <em>h</em> × <em>w</em></span> 个长度为 <span class="math inline"><em>d</em></span> 的特征向量；</li><li>还原reshape：对特征再次进行reshape，得到 <span class="math inline"><em>z</em><sub><em>q</em></sub> ∈ ℝ<sup><em>n</em> × <em>h</em> × <em>w</em> × <em>d</em></sup></span>；</li><li><strong>复制梯度</strong>：由于在进行选择码本索引时使用了 <span class="math inline"><em>a</em><em>r</em><em>g</em><em>m</em><em>i</em><em>n</em></span>，因此造成了梯度回传不连续，因此这里直接将 <span class="math inline"><em>z</em><sub><em>q</em></sub></span> 的导数复制到 <span class="math inline"><em>z</em><sub><em>e</em></sub></span>。</li></ol><h2 id="ldm">LDM</h2><p>得到压缩后的图像潜空间表示后，可以直接在隐空间上进行前向过程和去噪过程了，由于隐空间特征的大小要比图像空间小很多，因此LDM的推理速度要快很多。</p><p><span class="math display">$$\begin{split}    L_{L D M}:=\mathbb{E}_{\mathcal{E}(x), \epsilon \sim \mathcal{N}(0,1), t}\left[\left\|\epsilon-\epsilon_\theta\left(z_t, t\right)\right\|_2^2\right]\end{split}$$</span></p><h2 id="条件控制">条件控制</h2><p>去噪过程的条件引入往往是将条件信息 <span class="math inline"><em>y</em></span> 和时间步条件 <span class="math inline"><em>t</em></span> 一同加入噪声预测器 <span class="math inline"><em>ϵ</em><sub><em>θ</em></sub>(<em>z</em><sub><em>t</em></sub>, <em>t</em>, <em>y</em>)</span>，针对不同的生成任务，条件<span class="math inline"><em>y</em></span>可能是文本信息、语义图等。</p><p>对于文本信息，LDM引入了一个条件编码器 <span class="math inline"><em>τ</em><sub><em>θ</em></sub></span> 来将条件信息 <span class="math inline"><em>y</em></span> 统一编码为中间特征 <span class="math inline"><em>τ</em><sub><em>θ</em></sub>(<em>y</em>) ∈ ℝ<sup><em>M</em> × <em>d</em><sub><em>r</em></sub></sup></span> ，之后通过交叉注意力映射到UNet的中间层。</p><p><span class="math display">$$\begin{split}    L_{L D M}:=\mathbb{E}_{\mathcal{E}(x), \epsilon \sim \mathcal{N}(0,1), t}\left[\left\|\epsilon-\epsilon_\theta\left(z_t, t, \tau_\theta(y)\right)\right\|_2^2\right]\end{split}$$</span></p><p>对于语义图等条件，则可直接将特征图拼接到UNet中完成。</p><h1 id="rdm">RDM</h1><blockquote><p>原论文：<a href="http://arxiv.org/abs/2204.11824">Semi-Parametric Neural Image Synthesis</a></p></blockquote><p>这里简单介绍了一下RDM训练范式，因为LDM提供的源码可通过RDM进行推理。</p><h2 id="背景">背景</h2><p>当时的训练范式需要巨量的计算资源和训练时间，受到<strong>检索增强NLP（RAG）</strong>发展的启发，RDM质疑目前将不同训练样例直接转化为巨量可训练参数的方式，并提出为一个小的生成模型配备一个大型图像数据库的方式。</p><pre><code>注：RAG（Retrieval-Augmented Generation）是一种使LLM在生成回答时读取外部信息库的技术，可以理解为在生成内容之前，先从外部数据库中检索出相关信息作为参考。</code></pre><p>在训练过程中，它会通过近邻查找访问数据库，无需从头学习数据，而是通过检索到的视觉特征合成新的场景。这一方法不仅提升了生成性能，也大幅度降低了参数数量和训练开销。并且这种方式也独立于所使用的生成模型，允许我们使用retrieval-augmented diffusion（RDM），也能使用retrieval-augmented autoregressive（RARM）。</p><h2 id="方法">方法</h2><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/RDM.png" width="80%"/></p><h3 id="总体介绍">总体介绍</h3><p>和传统全参数生成模型不同，我们定义了一个半参数生成模型 <span class="math inline"><em>p</em><sub><em>θ</em>, 𝒟, <em>ξ</em><sub><em>k</em></sub></sub>(<em>x</em>)</span> ，其中 <span class="math inline"><em>θ</em></span> 是可训练参数而 <span class="math inline">𝒟, <em>ξ</em><sub><em>k</em></sub></span> 是不可训练的模型模块，<span class="math inline">𝒟 = {<em>y</em><sub><em>i</em></sub>}<sub><em>i</em> = 1</sub><sup><em>N</em></sup></span> 是一个大小为 <span class="math inline"><em>N</em></span> 的固定图像数据库，与训练图像 <span class="math inline">𝒳</span>不相关， <span class="math inline"><em>ξ</em><sub><em>k</em></sub></span> 则表示基于查询 <span class="math inline"><em>x</em></span> 的采样策略，即 <span class="math inline"><em>ξ</em><sub><em>k</em></sub> : <em>x</em>, 𝒟 ↦ ℳ<sub>𝒟</sub><sup>(<em>k</em>)</sup></span>。</p><p>由此可以发现，<span class="math inline"><em>ξ</em><sub><em>k</em></sub>(<em>x</em>, 𝒟)</span> 的选择很重要，以便模型能从检索数据库中提取到有用的信息。通常的做法是，考虑一个查询图像 <span class="math inline"><em>x</em> ∈ ℝ<sup><em>H</em><sub><em>x</em></sub> × <em>W</em><sub><em>x</em></sub> × 3</sup></span> ，<span class="math inline"><em>ξ</em><sub><em>k</em></sub>(<em>x</em>, 𝒟)</span> 通过给定距离函数 <span class="math inline"><em>d</em>{<em>x</em>,  ⋅ }</span> ，返回 <span class="math inline"><em>k</em></span> 个最近邻的集合。得到检索的图像样本 <span class="math inline"><em>y</em> ∈ ℳ<sub>𝒟</sub><sup>(<em>k</em>)</sup></span> 后，再通过一个固定的预训练图像编码器 <span class="math inline"><em>ϕ</em></span> 将高维度图像投射到低维流形。即：</p><p><span class="math display">$$\begin{split}    p_{\theta,\mathcal D,\xi_k}(x) = p_\theta(x|\{\phi(y)|y\in\xi_k(x,\mathcal D)\}).\end{split}$$</span></p><h3 id="半参数图像生成模型的两种实例">半参数图像生成模型的两种实例</h3><p>在训练过程中，假设训练数据集 <span class="math inline">𝒳 = {<em>x</em><sub><em>i</em></sub>}<sub><em>i</em> = 1</sub><sup><em>M</em></sup></span> 满足分布 <span class="math inline"><em>p</em>(<em>x</em>)</span> ，我们使用的采样策略是使用CLIP在图像特征空间中计算余弦相似度来选取 <span class="math inline"><em>k</em></span> -近邻，同样后续也通过CLIP编码器 <span class="math inline"><em>ϕ</em> = <em>ϕ</em><sub><em>C</em><em>L</em><em>I</em><em>P</em></sub></span> 得到图像表征。</p><ol type="1"><li>Retrieval-Augmented Diffusion Models（RDM）使用LDMs作为底层图像生成模型，即：</li></ol><p><span class="math display">$$\begin{split}    \min_\theta\mathcal{L}=\mathbb{E}_{p(x),z\thicksim E(x),\epsilon\thicksim\mathcal{N}(0,1),t}\left[\|\epsilon-\epsilon_\theta(z_t,t,\left.\{\phi_{\mathbf{CLIP}}(y)\mid y\in\xi_k(x,\mathcal{D})\}\right)\|_2^2\right].\end{split}$$</span></p><ol start="2" type="1"><li>Retrieval-Augmented Autoregressive Models（RARM）使用VQGAN作为底层图像生成模型，即：</li></ol><p><span class="math display">$$\begin{split}    \min_\theta\mathcal{L}=-\mathbb{E}_{p(x),z_q\sim E(x)}{\left[\sum_i\log p(z_q^{(i)}\mid z_q^{(&lt;i)},\mathrm{~}\{\phi_{\mathbf{CLIP}}(y)\mid y\in\xi_k(x,\mathcal{D})\})\right]}\end{split}$$</span></p><h3 id="推理过程">推理过程</h3><p>在推理过程中，<span class="math inline">𝒟, <em>ξ</em><sub><em>k</em></sub></span> 是可以随着下游任务的要求而改变的，比如根据应用需求扩大或者缩减数据库 <span class="math inline">𝒟</span> 、或是直接跳过检索提供条件集 <span class="math inline"><em>ϕ</em><sub><strong>C</strong><strong>L</strong><strong>I</strong><strong>P</strong></sub>(<em>y</em>)</span>。这也允许模型完成训练过程中未涉及到的其他任务，比如text-prompt或是class-labeled。</p><p>比如对于text-to-image任务，既可以通过text prompt <span class="math inline"><em>c</em><sub><em>t</em><em>e</em><em>x</em><em>t</em></sub></span> 使用CLIP生成特征图，然后通过该特征图检索<span class="math inline"><em>k</em></span>-近邻作为条件生成；也可以直接通过 <span class="math inline"><em>c</em><sub><em>t</em><em>e</em><em>x</em><em>t</em></sub></span> 生成的特征图作为条件生成。</p><h1 id="源码">源码</h1><h2 id="dm模型">DM模型</h2><p>LDM定义的模型在 <code>ldm\models\diffusion\ddpm.py</code> 中，因为使用pytorch-lighting进行训练，所以看起来很复杂，其实只需要关注模型结构部分。其中在模型<code>forward</code>部分，先对每个batch随机选取一个时间步进行训练，然后对条件输入进行处理，将其转为对应的中间特征，最终返回的是<code>p_losses</code>得到的损失。</p><p>而在<code>p_losses</code>中，则先随机初始化一个高斯噪声，通过前向过程<code>q_sample</code>得到 <span class="math inline"><em>x</em><sub><em>t</em></sub></span> ，接着将 <span class="math inline"><em>x</em><sub><em>t</em></sub></span> 、时间步 <span class="math inline"><em>t</em></span> 、条件 <span class="math inline"><em>c</em></span> 一同输入噪声预测网络中，得到预测噪声，之后通过l2损失计算loss。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, c, *args, **kwargs</span>):</span><br><span class="line">    t = torch.randint(<span class="number">0</span>, <span class="variable language_">self</span>.num_timesteps, (x.shape[<span class="number">0</span>],), device=<span class="variable language_">self</span>.device).long()</span><br><span class="line">    <span class="keyword">if</span> <span class="variable language_">self</span>.model.conditioning_key <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">assert</span> c <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.cond_stage_trainable:</span><br><span class="line">            c = <span class="variable language_">self</span>.get_learned_conditioning(c)</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.shorten_cond_schedule:  <span class="comment"># <span class="doctag">TODO:</span> drop this option</span></span><br><span class="line">            tc = <span class="variable language_">self</span>.cond_ids[t].to(<span class="variable language_">self</span>.device)</span><br><span class="line">            c = <span class="variable language_">self</span>.q_sample(x_start=c, t=tc, noise=torch.randn_like(c.<span class="built_in">float</span>()))</span><br><span class="line">    <span class="keyword">return</span> <span class="variable language_">self</span>.p_losses(x, c, t, *args, **kwargs)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">p_losses</span>(<span class="params">self, x_start, cond, t, noise=<span class="literal">None</span></span>):</span><br><span class="line">    noise = default(noise, <span class="keyword">lambda</span>: torch.randn_like(x_start))</span><br><span class="line">    x_noisy = <span class="variable language_">self</span>.q_sample(x_start=x_start, t=t, noise=noise)</span><br><span class="line">    model_output = <span class="variable language_">self</span>.apply_model(x_noisy, t, cond)</span><br><span class="line"></span><br><span class="line">    ...</span><br><span class="line">    loss = <span class="variable language_">self</span>.get_loss(model_out, target, mean=<span class="literal">False</span>).mean(dim=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss, loss_dict</span><br></pre></td></tr></table></figure><h3 id="前向过程">前向过程</h3><p>这里的前向过程与DDPM是保持一致的：</p><p><span class="math display">$$\begin{split}    q(x_t \mid x_0) = \mathcal{N}\bigl(x_t;\;\sqrt{\bar\alpha_t}\,x_0,\;(1-\bar\alpha_t)\,I\bigr).\end{split}$$</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">q_sample</span>(<span class="params">self, x_start, t, noise=<span class="literal">None</span></span>):</span><br><span class="line">    noise = default(noise, <span class="keyword">lambda</span>: torch.randn_like(x_start))</span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">        extract_into_tensor(<span class="variable language_">self</span>.sqrt_alphas_cumprod, t, x_start.shape) * x_start</span><br><span class="line">        + extract_into_tensor(<span class="variable language_">self</span>.sqrt_one_minus_alphas_cumprod, t, x_start.shape)</span><br><span class="line">        * noise</span><br><span class="line">    )</span><br></pre></td></tr></table></figure><h3 id="条件控制噪声预测">条件控制噪声预测</h3><p>这里噪声预测和DDPM一样还是使用UNet，不同在于为了引入条件控制加入了交叉注意力来嵌入条件信息。需要注意的是，时间步信息和条件信息的嵌入方式是不同的，时间步信息 <span class="math inline"><em>t</em></span> 是直接拼接到UNet每一层中的特征图，而条件信息 <span class="math inline"><em>c</em></span> 则是通过每一层的交叉注意力进行嵌入的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, timesteps=<span class="literal">None</span>, context=<span class="literal">None</span>, y=<span class="literal">None</span>,**kwargs</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Apply the model to an input batch.</span></span><br><span class="line"><span class="string">    :param x: an [N x C x ...] Tensor of inputs.</span></span><br><span class="line"><span class="string">    :param timesteps: a 1-D batch of timesteps.</span></span><br><span class="line"><span class="string">    :param context: conditioning plugged in via crossattn</span></span><br><span class="line"><span class="string">    :param y: an [N] Tensor of labels, if class-conditional.</span></span><br><span class="line"><span class="string">    :return: an [N x C x ...] Tensor of outputs.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">assert</span> (y <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>) == (</span><br><span class="line">        <span class="variable language_">self</span>.num_classes <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line">    ), <span class="string">&quot;must specify y if and only if the model is class-conditional&quot;</span></span><br><span class="line">    hs = []</span><br><span class="line">    t_emb = timestep_embedding(timesteps, <span class="variable language_">self</span>.model_channels, repeat_only=<span class="literal">False</span>)</span><br><span class="line">    emb = <span class="variable language_">self</span>.time_embed(t_emb)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="variable language_">self</span>.num_classes <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">assert</span> y.shape == (x.shape[<span class="number">0</span>],)</span><br><span class="line">        emb = emb + <span class="variable language_">self</span>.label_emb(y)</span><br><span class="line"></span><br><span class="line">    h = x.<span class="built_in">type</span>(<span class="variable language_">self</span>.dtype)</span><br><span class="line">    <span class="keyword">for</span> module <span class="keyword">in</span> <span class="variable language_">self</span>.input_blocks:</span><br><span class="line">        h = module(h, emb, context)</span><br><span class="line">        hs.append(h)</span><br><span class="line">    h = <span class="variable language_">self</span>.middle_block(h, emb, context)</span><br><span class="line">    <span class="keyword">for</span> module <span class="keyword">in</span> <span class="variable language_">self</span>.output_blocks:</span><br><span class="line">        h = th.cat([h, hs.pop()], dim=<span class="number">1</span>)</span><br><span class="line">        h = module(h, emb, context)</span><br><span class="line">    h = h.<span class="built_in">type</span>(x.dtype)</span><br><span class="line">    <span class="keyword">if</span> <span class="variable language_">self</span>.predict_codebook_ids:</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.id_predictor(h)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.out(h)</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/unet_trans.png"/></p><h3 id="采样过程推理">采样过程（推理）</h3><p>可以直接用 DDIM 进行采样，大大减少推理所需时间。这部分源码可参考<a href="https://litchi-lee.github.io/2025/04/17/AIGC/DDIM/">DDIM的细节</a>。</p><h2 id="感知图像压缩-1">感知图像压缩</h2><p>在<code>get_input</code>函数中，输入batch，将返回压缩后的图像和处理后的条件信息，这里我们先关注输入是如何进行压缩的，条件信息的处理将在后续重点讲解。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LatentDiffusion</span>(<span class="title class_ inherited__">DDPM</span>):</span><br><span class="line">    ...</span><br><span class="line"><span class="meta">    @torch.no_grad()</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_input</span>(<span class="params">self, batch, k, return_first_stage_outputs=<span class="literal">False</span>, force_c_encode=<span class="literal">False</span>, cond_key=<span class="literal">None</span>, return_original_cond=<span class="literal">False</span>, bs=<span class="literal">None</span></span>):</span><br><span class="line">        x = <span class="built_in">super</span>().get_input(batch, k)</span><br><span class="line">        <span class="keyword">if</span> bs <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            x = x[:bs]</span><br><span class="line">        x = x.to(<span class="variable language_">self</span>.device)</span><br><span class="line">        <span class="comment"># 感知图像压缩</span></span><br><span class="line">        encoder_posterior = <span class="variable language_">self</span>.encode_first_stage(x)</span><br><span class="line">        z = <span class="variable language_">self</span>.get_first_stage_encoding(encoder_posterior).detach()</span><br><span class="line">        ...</span><br><span class="line">        out = [z, c]</span><br><span class="line">        ...</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line">    </span><br><span class="line"><span class="meta">    @torch.no_grad()</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">encode_first_stage</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">hasattr</span>(<span class="variable language_">self</span>, <span class="string">&quot;split_input_params&quot;</span>):</span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>.split_input_params[<span class="string">&quot;patch_distributed_vq&quot;</span>]:</span><br><span class="line">            ...</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">return</span> <span class="variable language_">self</span>.first_stage_model.encode(x)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="variable language_">self</span>.first_stage_model.encode(x)</span><br></pre></td></tr></table></figure><p>进一步深入，可以发现输入是通过<code>self.first_stage_model.encode(x)</code>这个接口进行压缩的，源码中实现了两种自编码器用于encode，分别是VQVAE和VAE（在<code>ldm\models\autoencoder.py</code>中），但是由于原论文使用VQVAE，因此这里主要扒一扒VQVAE部分的源码。</p><h3 id="encode部分">encode部分</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">VQModel</span>(pl.LightningModule):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,...</span>)</span><br><span class="line">        ...</span><br><span class="line">        <span class="variable language_">self</span>.encoder = Encoder(**ddconfig)</span><br><span class="line">        <span class="variable language_">self</span>.quant_conv = torch.nn.Conv2d(ddconfig[<span class="string">&quot;z_channels&quot;</span>], embed_dim, <span class="number">1</span>)</span><br><span class="line">        <span class="variable language_">self</span>.quantize = VectorQuantizer(n_embed, embed_dim, beta=<span class="number">0.25</span>, remap=remap, sane_index_shape=sane_index_shape)</span><br><span class="line">    </span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">encode</span>(<span class="params">self, x</span>):</span><br><span class="line">        h = <span class="variable language_">self</span>.encoder(x)</span><br><span class="line">        h = <span class="variable language_">self</span>.quant_conv(h)</span><br><span class="line">        quant, emb_loss, info = <span class="variable language_">self</span>.quantize(h)</span><br><span class="line">        <span class="keyword">return</span> quant, emb_loss, info</span><br></pre></td></tr></table></figure><p>首先是第一部分Encoder部分，这一部分主要是一系列的下采样，最终得到中间特征图<span class="math inline"><em>h</em></span>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Encoder</span>(nn.Module):</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">    <span class="comment"># timestep embedding</span></span><br><span class="line">    temb = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># downsampling</span></span><br><span class="line">    hs = [<span class="variable language_">self</span>.conv_in(x)]</span><br><span class="line">    <span class="keyword">for</span> i_level <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.num_resolutions):</span><br><span class="line">        <span class="keyword">for</span> i_block <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.num_res_blocks):</span><br><span class="line">            h = <span class="variable language_">self</span>.down[i_level].block[i_block](hs[-<span class="number">1</span>], temb)</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.down[i_level].attn) &gt; <span class="number">0</span>:</span><br><span class="line">                h = <span class="variable language_">self</span>.down[i_level].attn[i_block](h)</span><br><span class="line">            hs.append(h)</span><br><span class="line">        <span class="keyword">if</span> i_level != <span class="variable language_">self</span>.num_resolutions-<span class="number">1</span>:</span><br><span class="line">            hs.append(<span class="variable language_">self</span>.down[i_level].downsample(hs[-<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># middle</span></span><br><span class="line">    h = hs[-<span class="number">1</span>]</span><br><span class="line">    h = <span class="variable language_">self</span>.mid.block_1(h, temb)</span><br><span class="line">    h = <span class="variable language_">self</span>.mid.attn_1(h)</span><br><span class="line">    h = <span class="variable language_">self</span>.mid.block_2(h, temb)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># end</span></span><br><span class="line">    h = <span class="variable language_">self</span>.norm_out(h)</span><br><span class="line">    h = nonlinearity(h)</span><br><span class="line">    h = <span class="variable language_">self</span>.conv_out(h)</span><br><span class="line">    <span class="keyword">return</span> h</span><br></pre></td></tr></table></figure><p>之后经过<code>quant_conv</code>将<span class="math inline"><em>h</em></span>的维度转为<code>embed_dim</code>，经过VectorQuantizer转为离散化量化向量。具体来说，先将原特征图reshape为<span class="math inline">(<em>b</em> * <em>h</em> * <em>w</em> * <em>c</em>, <em>d</em>)</span>的向量，方便与码本中的向量计算欧氏距离，然后选择最小距离的向量作为量化向量。之后计算量化损失（codebook loss+commitment loss）：</p><p><span class="math display">$$\begin{split}    \mathcal L_{vq} = \beta\|z_q - z.detach\| + \|z_q.detach - z\|\end{split}$$</span></p><p>由于中间操作涉及到取最小值，会阻断梯度的回传，因此额外加了一步<code>z_q = z + (z_q - z).detach()</code>用于保留梯度。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">VectorQuantizer2</span>(nn.Module):</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, z, temp=<span class="literal">None</span>, rescale_logits=<span class="literal">False</span>, return_logits=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="keyword">assert</span> temp <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> temp==<span class="number">1.0</span>, <span class="string">&quot;Only for interface compatible with Gumbel&quot;</span></span><br><span class="line">        <span class="keyword">assert</span> rescale_logits==<span class="literal">False</span>, <span class="string">&quot;Only for interface compatible with Gumbel&quot;</span></span><br><span class="line">        <span class="keyword">assert</span> return_logits==<span class="literal">False</span>, <span class="string">&quot;Only for interface compatible with Gumbel&quot;</span></span><br><span class="line">        <span class="comment"># reshape z -&gt; (batch, height, width, channel) and flatten</span></span><br><span class="line">        z = rearrange(z, <span class="string">&#x27;b c h w d -&gt; b h w d c&#x27;</span>).contiguous()</span><br><span class="line">        <span class="comment"># 展平为 (b * h * w * c, d) 的二维矩阵</span></span><br><span class="line">        z_flattened = z.view(-<span class="number">1</span>, <span class="variable language_">self</span>.e_dim)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算每个向量与codebook中所有向量的欧式距离平方 </span></span><br><span class="line">        <span class="comment"># (z - e)^2 = z^2 + e^2 - 2ze</span></span><br><span class="line">        <span class="comment"># 输出形状为 (b * h * w * c, n_codes)</span></span><br><span class="line">        d = torch.<span class="built_in">sum</span>(z_flattened ** <span class="number">2</span>, dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>) + \</span><br><span class="line">            torch.<span class="built_in">sum</span>(<span class="variable language_">self</span>.embedding.weight**<span class="number">2</span>, dim=<span class="number">1</span>) - <span class="number">2</span> * \</span><br><span class="line">            torch.einsum(<span class="string">&#x27;bd,dn-&gt;bn&#x27;</span>, z_flattened, rearrange(<span class="variable language_">self</span>.embedding.weight, <span class="string">&#x27;n d -&gt; d n&#x27;</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 为每个输入向量选出距离最小的codebook向量的下标</span></span><br><span class="line">        min_encoding_indices = torch.argmin(d, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 得到量化后的向量</span></span><br><span class="line">        z_q = <span class="variable language_">self</span>.embedding(min_encoding_indices).view(z.shape)</span><br><span class="line">        perplexity = <span class="literal">None</span></span><br><span class="line">        min_encodings = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 量化损失 = commit loss + codebook loss</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="variable language_">self</span>.legacy:</span><br><span class="line">            loss = <span class="variable language_">self</span>.beta * torch.mean((z_q.detach()-z)**<span class="number">2</span>) + \</span><br><span class="line">                   torch.mean((z_q - z.detach()) ** <span class="number">2</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> z_q.<span class="built_in">min</span>() &lt; <span class="variable language_">self</span>.lower <span class="keyword">or</span> z_q.<span class="built_in">max</span>() &gt; <span class="variable language_">self</span>.upper:</span><br><span class="line">                loss = torch.mean((z_q.detach() - z) ** <span class="number">2</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                loss = torch.mean((z_q.detach()-z)**<span class="number">2</span>) + <span class="variable language_">self</span>.beta * \</span><br><span class="line">                           torch.mean((z_q - z.detach()) ** <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># preserve gradients</span></span><br><span class="line">        z_q = z + (z_q - z).detach()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># reshape back to match original input shape</span></span><br><span class="line">        z_q = rearrange(z_q, <span class="string">&#x27;b h w d c -&gt; b c h w d&#x27;</span>).contiguous()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.remap <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            min_encoding_indices = min_encoding_indices.reshape(z.shape[<span class="number">0</span>],-<span class="number">1</span>) <span class="comment"># add batch axis</span></span><br><span class="line">            min_encoding_indices = <span class="variable language_">self</span>.remap_to_used(min_encoding_indices)</span><br><span class="line">            min_encoding_indices = min_encoding_indices.reshape(-<span class="number">1</span>,<span class="number">1</span>) <span class="comment"># flatten</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.sane_index_shape:</span><br><span class="line">            min_encoding_indices = min_encoding_indices.reshape(</span><br><span class="line">                z_q.shape[<span class="number">0</span>], z_q.shape[<span class="number">2</span>], z_q.shape[<span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> z_q, loss, (perplexity, min_encodings, min_encoding_indices)</span><br></pre></td></tr></table></figure><h3 id="decode部分">decode部分</h3><p>decode部分和encode一样，也是直接调用定义好的自编码器类内部的<code>decode</code>方法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LatentDiffusion</span>(<span class="title class_ inherited__">DDPM</span>):</span><br><span class="line">    ...</span><br><span class="line"><span class="meta">    @torch.no_grad()</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">decode_first_stage</span>(<span class="params">self, z, predict_cids=<span class="literal">False</span>, force_not_quantize=<span class="literal">False</span></span>):</span><br><span class="line">        ...</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(<span class="variable language_">self</span>.first_stage_model, VQModelInterface):</span><br><span class="line">                <span class="keyword">return</span> <span class="variable language_">self</span>.first_stage_model.decode(</span><br><span class="line">                    z, force_not_quantize=predict_cids <span class="keyword">or</span> force_not_quantize</span><br><span class="line">                )</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">return</span> <span class="variable language_">self</span>.first_stage_model.decode(z)</span><br></pre></td></tr></table></figure><p>和encode过程类似，不过相当于是反过来了，先过一层<code>post_quant_conv</code>恢复原特征维度，然后通过若干层上采样恢复原输入图像的尺寸。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">VQModel</span>(pl.LightningModule):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,...</span>)</span><br><span class="line">        ...</span><br><span class="line">        <span class="variable language_">self</span>.post_quant_conv = torch.nn.Conv2d(embed_dim, ddconfig[<span class="string">&quot;z_channels&quot;</span>], <span class="number">1</span>)</span><br><span class="line">        <span class="variable language_">self</span>.decoder = Decoder(**ddconfig)</span><br><span class="line"></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">decode</span>(<span class="params">self, quant</span>):</span><br><span class="line">        quant = <span class="variable language_">self</span>.post_quant_conv(quant)</span><br><span class="line">        dec = <span class="variable language_">self</span>.decoder(quant)</span><br><span class="line">        <span class="keyword">return</span> dec</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Decoder</span>(nn.Module):</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, z</span>):</span><br><span class="line">        <span class="comment">#assert z.shape[1:] == self.z_shape[1:]</span></span><br><span class="line">        <span class="variable language_">self</span>.last_z_shape = z.shape</span><br><span class="line"></span><br><span class="line">        <span class="comment"># timestep embedding</span></span><br><span class="line">        temb = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># z to block_in</span></span><br><span class="line">        h = <span class="variable language_">self</span>.conv_in(z)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># middle</span></span><br><span class="line">        h = <span class="variable language_">self</span>.mid.block_1(h, temb)</span><br><span class="line">        h = <span class="variable language_">self</span>.mid.attn_1(h)</span><br><span class="line">        h = <span class="variable language_">self</span>.mid.block_2(h, temb)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># upsampling</span></span><br><span class="line">        <span class="keyword">for</span> i_level <span class="keyword">in</span> <span class="built_in">reversed</span>(<span class="built_in">range</span>(<span class="variable language_">self</span>.num_resolutions)):</span><br><span class="line">            <span class="keyword">for</span> i_block <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.num_res_blocks+<span class="number">1</span>):</span><br><span class="line">                h = <span class="variable language_">self</span>.up[i_level].block[i_block](h, temb)</span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.up[i_level].attn) &gt; <span class="number">0</span>:</span><br><span class="line">                    h = <span class="variable language_">self</span>.up[i_level].attn[i_block](h)</span><br><span class="line">            <span class="keyword">if</span> i_level != <span class="number">0</span>:</span><br><span class="line">                h = <span class="variable language_">self</span>.up[i_level].upsample(h)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># end</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.give_pre_end:</span><br><span class="line">            <span class="keyword">return</span> h</span><br><span class="line"></span><br><span class="line">        h = <span class="variable language_">self</span>.norm_out(h)</span><br><span class="line">        h = nonlinearity(h)</span><br><span class="line">        h = <span class="variable language_">self</span>.conv_out(h)</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.tanh_out:</span><br><span class="line">            h = torch.tanh(h)</span><br><span class="line">        <span class="keyword">return</span> h</span><br></pre></td></tr></table></figure><h2 id="条件信息控制">条件信息控制</h2><p>之前在DM部分其实已经讲过条件特征是如何嵌入到扩散模型中的，这里来扒一扒在源码中是如何处理各类条件信息的，以及如何将其编码为统一的条件特征。可以看到条件信息都是通过<code>self.get_learned_conditioning</code>这个方法进行编码的，而在<code>self.get_learned_conditioning</code>中条件信息又是统一调用<code>self.cond_stage_model</code>中的方法进行编码的，因此我们将重点看一看各类条件信息编码器模型（定义在<code>ldm\modules\encoders\modules.py</code>中）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LatentDiffusion</span>(<span class="title class_ inherited__">DDPM</span>):</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, c, *args, **kwargs</span>):</span><br><span class="line">        ...</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.model.conditioning_key <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">assert</span> c <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>.cond_stage_trainable:</span><br><span class="line">                c = <span class="variable language_">self</span>.get_learned_conditioning(c)</span><br><span class="line">            ...</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.p_losses(x, c, t, *args, **kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_learned_conditioning</span>(<span class="params">self, c</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.cond_stage_forward <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">hasattr</span>(<span class="variable language_">self</span>.cond_stage_model, <span class="string">&#x27;encode&#x27;</span>) <span class="keyword">and</span> <span class="built_in">callable</span>(</span><br><span class="line">                <span class="variable language_">self</span>.cond_stage_model.encode</span><br><span class="line">            ):</span><br><span class="line">                c = <span class="variable language_">self</span>.cond_stage_model.encode(c)</span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">isinstance</span>(c, DiagonalGaussianDistribution):</span><br><span class="line">                    c = c.mode()</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                c = <span class="variable language_">self</span>.cond_stage_model(c)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">assert</span> <span class="built_in">hasattr</span>(<span class="variable language_">self</span>.cond_stage_model, <span class="variable language_">self</span>.cond_stage_forward)</span><br><span class="line">            c = <span class="built_in">getattr</span>(<span class="variable language_">self</span>.cond_stage_model, <span class="variable language_">self</span>.cond_stage_forward)(c)</span><br><span class="line">        <span class="keyword">return</span> c</span><br></pre></td></tr></table></figure><h3 id="类别编码器">类别编码器</h3><p>用于将<strong>类别信息</strong>转换为embedding向量，适用于分类条件生成任务。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ClassEmbedder</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, embed_dim, n_classes=<span class="number">1000</span>, key=<span class="string">&#x27;class&#x27;</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.key = key</span><br><span class="line">        <span class="variable language_">self</span>.embedding = nn.Embedding(n_classes, embed_dim)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, batch, key=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="keyword">if</span> key <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            key = <span class="variable language_">self</span>.key</span><br><span class="line">        <span class="comment"># this is for use in crossattn</span></span><br><span class="line">        c = batch[key][:, <span class="literal">None</span>]</span><br><span class="line">        c = <span class="variable language_">self</span>.embedding(c)</span><br><span class="line">        <span class="keyword">return</span> c</span><br></pre></td></tr></table></figure><h3 id="文本编码器">文本编码器</h3><h4 id="transformer">transformer</h4><p>普通的transformer编码器，输入<strong>Token IDs</strong>，输出其embedding向量表示。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TransformerEmbedder</span>(<span class="title class_ inherited__">AbstractEncoder</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Some transformer encoder layers&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n_embed, n_layer, vocab_size, max_seq_len=<span class="number">77</span>, device=<span class="string">&quot;cuda&quot;</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.device = device</span><br><span class="line">        <span class="variable language_">self</span>.transformer = TransformerWrapper(num_tokens=vocab_size, max_seq_len=max_seq_len,</span><br><span class="line">                                              attn_layers=Encoder(dim=n_embed, depth=n_layer))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, tokens</span>):</span><br><span class="line">        tokens = tokens.to(<span class="variable language_">self</span>.device)  <span class="comment"># meh</span></span><br><span class="line">        z = <span class="variable language_">self</span>.transformer(tokens, return_embeddings=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> z</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">encode</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>(x)</span><br></pre></td></tr></table></figure><h4 id="bert">Bert</h4><p>输入<strong>原始文本</strong>，先使用BERT的Tokenizer将其转为Token IDs，然后再调用<code>BertEmbedder</code>将其转为embedding向量。其中Tokenizer来自hugging face无法训练，后者可训练。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BERTTokenizer</span>(<span class="title class_ inherited__">AbstractEncoder</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; Uses a pretrained BERT tokenizer by huggingface. Vocab size: 30522 (?)&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, device=<span class="string">&quot;cuda&quot;</span>, vq_interface=<span class="literal">True</span>, max_length=<span class="number">77</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizerFast  <span class="comment"># <span class="doctag">TODO:</span> add to reuquirements</span></span><br><span class="line">        <span class="variable language_">self</span>.tokenizer = BertTokenizerFast.from_pretrained(<span class="string">&quot;bert-base-uncased&quot;</span>)</span><br><span class="line">        <span class="variable language_">self</span>.device = device</span><br><span class="line">        <span class="variable language_">self</span>.vq_interface = vq_interface</span><br><span class="line">        <span class="variable language_">self</span>.max_length = max_length</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, text</span>):</span><br><span class="line">        batch_encoding = <span class="variable language_">self</span>.tokenizer(text, truncation=<span class="literal">True</span>, max_length=<span class="variable language_">self</span>.max_length, return_length=<span class="literal">True</span>,</span><br><span class="line">                                        return_overflowing_tokens=<span class="literal">False</span>, padding=<span class="string">&quot;max_length&quot;</span>, return_tensors=<span class="string">&quot;pt&quot;</span>)</span><br><span class="line">        tokens = batch_encoding[<span class="string">&quot;input_ids&quot;</span>].to(<span class="variable language_">self</span>.device)</span><br><span class="line">        <span class="keyword">return</span> tokens</span><br><span class="line"></span><br><span class="line"><span class="meta">    @torch.no_grad()</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">encode</span>(<span class="params">self, text</span>):</span><br><span class="line">        tokens = <span class="variable language_">self</span>(text)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="variable language_">self</span>.vq_interface:</span><br><span class="line">            <span class="keyword">return</span> tokens</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span>, <span class="literal">None</span>, [<span class="literal">None</span>, <span class="literal">None</span>, tokens]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">decode</span>(<span class="params">self, text</span>):</span><br><span class="line">        <span class="keyword">return</span> text</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BERTEmbedder</span>(<span class="title class_ inherited__">AbstractEncoder</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Uses the BERT tokenizr model and add some transformer encoder layers&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n_embed, n_layer, vocab_size=<span class="number">30522</span>, max_seq_len=<span class="number">77</span>,</span></span><br><span class="line"><span class="params">                 device=<span class="string">&quot;cuda&quot;</span>,use_tokenizer=<span class="literal">True</span>, embedding_dropout=<span class="number">0.0</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.use_tknz_fn = use_tokenizer</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.use_tknz_fn:</span><br><span class="line">            <span class="variable language_">self</span>.tknz_fn = BERTTokenizer(vq_interface=<span class="literal">False</span>, max_length=max_seq_len)</span><br><span class="line">        <span class="variable language_">self</span>.device = device</span><br><span class="line">        <span class="variable language_">self</span>.transformer = TransformerWrapper(num_tokens=vocab_size, max_seq_len=max_seq_len,</span><br><span class="line">                                              attn_layers=Encoder(dim=n_embed, depth=n_layer),</span><br><span class="line">                                              emb_dropout=embedding_dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, text</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.use_tknz_fn:</span><br><span class="line">            tokens = <span class="variable language_">self</span>.tknz_fn(text)<span class="comment">#.to(self.device)</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            tokens = text</span><br><span class="line">        z = <span class="variable language_">self</span>.transformer(tokens, return_embeddings=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> z</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">encode</span>(<span class="params">self, text</span>):</span><br><span class="line">        <span class="comment"># output of length 77</span></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>(text)</span><br></pre></td></tr></table></figure><h4 id="clip">CLIP</h4><p>输入<strong>原始文本</strong>，使用OpenAI CLIP的tokenizer以及文本编码器，生成文本的embedding表示，参数冻结无法训练。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">FrozenCLIPTextEmbedder</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Uses the CLIP transformer encoder for text.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, version=<span class="string">&#x27;ViT-L/14&#x27;</span>, device=<span class="string">&quot;cuda&quot;</span>, max_length=<span class="number">77</span>, n_repeat=<span class="number">1</span>, normalize=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.model, _ = clip.load(version, jit=<span class="literal">False</span>, device=<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">        <span class="variable language_">self</span>.device = device</span><br><span class="line">        <span class="variable language_">self</span>.max_length = max_length</span><br><span class="line">        <span class="variable language_">self</span>.n_repeat = n_repeat</span><br><span class="line">        <span class="variable language_">self</span>.normalize = normalize</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">freeze</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="variable language_">self</span>.model = <span class="variable language_">self</span>.model.<span class="built_in">eval</span>()</span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> <span class="variable language_">self</span>.parameters():</span><br><span class="line">            param.requires_grad = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, text</span>):</span><br><span class="line">        tokens = clip.tokenize(text).to(<span class="variable language_">self</span>.device)</span><br><span class="line">        z = <span class="variable language_">self</span>.model.encode_text(tokens)</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.normalize:</span><br><span class="line">            z = z / torch.linalg.norm(z, dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> z</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">encode</span>(<span class="params">self, text</span>):</span><br><span class="line">        z = <span class="variable language_">self</span>(text)</span><br><span class="line">        <span class="keyword">if</span> z.ndim==<span class="number">2</span>:</span><br><span class="line">            z = z[:, <span class="literal">None</span>, :]</span><br><span class="line">        z = repeat(z, <span class="string">&#x27;b 1 d -&gt; b k d&#x27;</span>, k=<span class="variable language_">self</span>.n_repeat)</span><br><span class="line">        <span class="keyword">return</span> z</span><br></pre></td></tr></table></figure><h3 id="图像编码器">图像编码器</h3><h4 id="图像缩放器">图像缩放器</h4><p>输入<strong>图像特征图</strong>，对其进行空间尺寸的调整（下采样或上采样以及维度映射），输出调整尺寸后的图像特征图。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SpatialRescaler</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                 n_stages=<span class="number">1</span>,</span></span><br><span class="line"><span class="params">                 method=<span class="string">&#x27;bilinear&#x27;</span>,</span></span><br><span class="line"><span class="params">                 multiplier=<span class="number">0.5</span>,</span></span><br><span class="line"><span class="params">                 in_channels=<span class="number">3</span>,</span></span><br><span class="line"><span class="params">                 out_channels=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 bias=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.n_stages = n_stages</span><br><span class="line">        <span class="keyword">assert</span> <span class="variable language_">self</span>.n_stages &gt;= <span class="number">0</span></span><br><span class="line">        <span class="keyword">assert</span> method <span class="keyword">in</span> [<span class="string">&#x27;nearest&#x27;</span>,<span class="string">&#x27;linear&#x27;</span>,<span class="string">&#x27;bilinear&#x27;</span>,<span class="string">&#x27;trilinear&#x27;</span>,<span class="string">&#x27;bicubic&#x27;</span>,<span class="string">&#x27;area&#x27;</span>]</span><br><span class="line">        <span class="variable language_">self</span>.multiplier = multiplier</span><br><span class="line">        <span class="variable language_">self</span>.interpolator = partial(torch.nn.functional.interpolate, mode=method)</span><br><span class="line">        <span class="variable language_">self</span>.remap_output = out_channels <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.remap_output:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&#x27;Spatial Rescaler mapping from <span class="subst">&#123;in_channels&#125;</span> to <span class="subst">&#123;out_channels&#125;</span> channels after resizing.&#x27;</span>)</span><br><span class="line">            <span class="variable language_">self</span>.channel_mapper = nn.Conv2d(in_channels,out_channels,<span class="number">1</span>,bias=bias)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        <span class="keyword">for</span> stage <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.n_stages):</span><br><span class="line">            x = <span class="variable language_">self</span>.interpolator(x, scale_factor=<span class="variable language_">self</span>.multiplier)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.remap_output:</span><br><span class="line">            x = <span class="variable language_">self</span>.channel_mapper(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">encode</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>(x)</span><br></pre></td></tr></table></figure><h4 id="clip-1">CLIP</h4><p>输入<strong>图像特征</strong>，使用OpenAI CLIP图像编码器，将其转为图像对应的embedding向量，参数冻结同样无法训练。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">FrozenClipImageEmbedder</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Uses the CLIP image encoder.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">            self,</span></span><br><span class="line"><span class="params">            model,</span></span><br><span class="line"><span class="params">            jit=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">            device=<span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available(<span class="params"></span>) <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>,</span></span><br><span class="line"><span class="params">            antialias=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">        </span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.model, _ = clip.load(name=model, device=device, jit=jit)</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.antialias = antialias</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.register_buffer(<span class="string">&#x27;mean&#x27;</span>, torch.Tensor([<span class="number">0.48145466</span>, <span class="number">0.4578275</span>, <span class="number">0.40821073</span>]), persistent=<span class="literal">False</span>)</span><br><span class="line">        <span class="variable language_">self</span>.register_buffer(<span class="string">&#x27;std&#x27;</span>, torch.Tensor([<span class="number">0.26862954</span>, <span class="number">0.26130258</span>, <span class="number">0.27577711</span>]), persistent=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">preprocess</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># normalize to [0,1]</span></span><br><span class="line">        x = kornia.geometry.resize(x, (<span class="number">224</span>, <span class="number">224</span>),</span><br><span class="line">                                   interpolation=<span class="string">&#x27;bicubic&#x27;</span>,align_corners=<span class="literal">True</span>,</span><br><span class="line">                                   antialias=<span class="variable language_">self</span>.antialias)</span><br><span class="line">        x = (x + <span class="number">1.</span>) / <span class="number">2.</span></span><br><span class="line">        <span class="comment"># renormalize according to clip</span></span><br><span class="line">        x = kornia.enhance.normalize(x, <span class="variable language_">self</span>.mean, <span class="variable language_">self</span>.std)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># x is assumed to be in range [-1,1]</span></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.model.encode_image(<span class="variable language_">self</span>.preprocess(x))</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AIGC </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>重建任务常用指标</title>
      <link href="/2025/04/26/AIGC/metrics/"/>
      <url>/2025/04/26/AIGC/metrics/</url>
      
        <content type="html"><![CDATA[<h1 id="psnr">PSNR</h1><p>峰值信噪比（Peak Signal-to-Noise Ratio，PSNR）是<strong>像素级误差指标</strong>，用于衡量重建图像与参考图像之间的像素误差，值越大表示图像越接近参考图像，单位dB。</p><p><span class="math display">$$  \begin{split}     PSNR = 10 \cdot log_{10}(\frac{MAX^2}{MSE})  \end{split}$$</span></p><p>其中 <span class="math inline">$MSE = \frac{1}{n} \sum (x-\hat{x})^2$</span> 。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">psnr = <span class="number">20</span> * np.log10(PIXEL_MAX / np.sqrt(np.power(img1 - img2, <span class="number">2</span>)))</span><br></pre></td></tr></table></figure><p>或者在pytorch中，可以写作：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_psnr</span>(<span class="params">pred, target, max_val=<span class="number">1.0</span></span>):</span><br><span class="line">    mse = F.mse_loss(pred, target)</span><br><span class="line">    psnr = <span class="number">10</span> * torch.log10(max_val ** <span class="number">2</span> / mse)</span><br><span class="line">    <span class="keyword">return</span> psnr.item()</span><br></pre></td></tr></table></figure><h1 id="ssim">SSIM</h1><p>结构相似性（Structure Similarity Index，SSIM），衡量图像在亮度、对比度、结构上的相似性，范围为 <span class="math inline">[ − 1, 1]</span> ，越接近1表示越相似。</p><p><span class="math display">$$\begin{split}    \mathrm{SSIM}(x,y)=\frac{(2\mu_x\mu_y+C_1)(2\sigma_{xy}+C_2)}{(\mu_x^2+\mu_y^2+C_1)(\sigma_x^2+\sigma_y^2+C_2)}\end{split}$$</span></p><p>在pytorch中，直接调用：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> piq <span class="keyword">import</span> ssim</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_ssim</span>(<span class="params">pred, target</span>):</span><br><span class="line">    <span class="keyword">return</span> ssim(pred, target, data_range=<span class="number">1.0</span>).item()</span><br></pre></td></tr></table></figure><p><strong>MS-SSIM</strong>（Multi-Scale SSIM）是SSIM的扩展版本，更鲁棒细致。在pytorch中同样可以直接调包：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> piq <span class="keyword">import</span> multi_scale_ssim</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_ms_ssim</span>(<span class="params">pred, target</span>):</span><br><span class="line">    <span class="keyword">return</span> multi_scale_ssim(pred, target, data_range=<span class="number">1.0</span>).item()</span><br></pre></td></tr></table></figure><h1 id="lpips">LPIPS</h1><p><strong>LPIPS</strong>(Learned Perceptual Image Patch Similarity)基于深度网络（如Alexnet或者VGG）提取特征后计算相似性，更强调感知上的差距，范围为 <span class="math inline">[ − 1, 1]</span> ，越小越好。在pytorch中可直接调用：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> lpips</span><br><span class="line">loss_fn = lpips.LPIPS(net=<span class="string">&#x27;alex&#x27;</span>)  <span class="comment"># 可选: alex / vgg / squeeze</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_lpips</span>(<span class="params">pred, target</span>):</span><br><span class="line">    <span class="keyword">return</span> loss_fn(pred, target).item()</span><br></pre></td></tr></table></figure><h1 id="hfen">HFEN</h1><p><strong>HFEN</strong>（High-Frequency Error Norm）用于评估图像中的高频信息是否被重建出来。具体来说，它将图像通过<strong>LoG</strong>（Laplacian of Gaussian）滤波器提取高频信息，然后计算误差。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">hfen</span>(<span class="params">pred, target</span>):</span><br><span class="line">    <span class="comment"># 使用 LoG 近似（可以用更精确的方法替代）</span></span><br><span class="line">    laplacian_kernel = torch.tensor([[[[<span class="number">0</span>,  <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">                                       [<span class="number">1</span>, -<span class="number">4</span>, <span class="number">1</span>],</span><br><span class="line">                                       [<span class="number">0</span>,  <span class="number">1</span>, <span class="number">0</span>]]]], dtype=pred.dtype, device=pred.device)</span><br><span class="line">    pred_hf = F.conv2d(pred, laplacian_kernel, padding=<span class="number">1</span>)</span><br><span class="line">    target_hf = F.conv2d(target, laplacian_kernel, padding=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> F.mse_loss(pred_hf, target_hf).item()</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AIGC </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DDIM总结</title>
      <link href="/2025/04/17/AIGC/DDIM/"/>
      <url>/2025/04/17/AIGC/DDIM/</url>
      
        <content type="html"><![CDATA[<p><strong>系列文章：</strong></p><ol class="series-items"><li><a href="/2025/04/15/AIGC/DDPM/" title="DDPM总结">DDPM总结</a></li><li><a href="/2025/04/17/AIGC/DDIM/" title="DDIM总结">DDIM总结</a></li><li><a href="/2025/05/08/AIGC/DIT/" title="DiT的细节">DiT的细节</a></li><li><a href="/2025/05/08/AIGC/iDDPM/" title="iDDPM总结">iDDPM总结</a></li></ol><h1 id="论文回顾">论文回顾</h1><blockquote><p>原论文：<a href="http://arxiv.org/abs/2010.02502">Denoising Diffusion Implicit Models</a></p></blockquote><h2 id="背景">背景</h2><p>近期的降噪过程要么是基于朗之万动力学（NCSN）或是基于加噪过程的逆过程（DDPM），但是这些方法都有很严重的缺陷–需要很多次迭代来生成高质量的图像。本文提出了DDIM（Denoising Diffusion Implicit Models），将<strong>原基于马尔可夫假设的DDPM推广到了非马尔科夫过程的DDPM</strong>，加速采样过程。</p><h2 id="非马尔可夫前向过程的变分推理">非马尔可夫前向过程的变分推理</h2><p>回顾DDPM中的目标函数，可以将其写作（其中<span class="math inline"><em>γ</em><sub><em>t</em></sub></span>是一些常数项）：</p><p><span class="math display">$$    \begin{split}        L_{\gamma} :=  \sum_{t=1}^T \gamma_t \mathbb E_{q(x_t|x_0)}[ \left\lVert\epsilon_t  - {\hat\epsilon}_{\theta}(x_t, t)\right\rVert_2^2 ], \epsilon_t \sim \mathcal{N}(0,\textit{I})    \end{split}$$</span></p><p>可以发现，其实<span class="math inline"><em>L</em><sub><em>γ</em></sub></span>只依赖于边缘分布<span class="math inline"><em>q</em>(<em>x</em><sub><em>t</em></sub>|<em>x</em><sub>0</sub>)</span>而不直接依赖于联合分布<span class="math inline"><em>q</em>(<em>x</em><sub>1 : <em>T</em></sub>|<em>x</em><sub>0</sub>)</span>，换句话说只要<span class="math inline"><em>q</em>(<em>x</em><sub><em>t</em></sub>|<em>x</em><sub>0</sub>)</span>已知并且满足高斯分布的形式，那么就可以用DDPM预测噪音的目标函数训练模型。进一步说，只要<strong>我们保证<span class="math inline"><em>q</em>(<em>x</em><sub><em>t</em></sub>|<em>x</em><sub>0</sub>)</span>的形式不变，那么我们就可以复用训好的DDPM，然后定义新的采样过程</strong>。</p><h3 id="非马尔可夫前向过程">非马尔可夫前向过程</h3><p>（1）考虑一个非马尔可夫过程，定义为（与DDPM不同）：</p><p><span class="math display">$$\begin{split}q_\sigma(\mathbf{x}_{1:T}|\mathbf{x}_0):=q_\sigma(\mathbf{x}_T|\mathbf{x}_0)\prod_{t=2}^Tq_\sigma(\mathbf{x}_{t-1}|\mathbf{x}_t,\mathbf{x}_0)\end{split}$$</span></p><blockquote><p>注意在DDPM中，马尔可夫前向过程为： <span class="math display">$$\begin{split}q(\mathbf{x}_{1:T}|\mathbf{x}_0):=\prod_{t=1}^Tq(\mathbf{x}_{t}|\mathbf{x}_{t-1})\end{split}$$</span></p></blockquote><p>（2）并且定义（与DDPM一致）：</p><p><span class="math display">$$\begin{split}q_\sigma(\mathbf{x}_T|\mathbf{x}_0)=\mathcal{N}(\sqrt{\alpha_T}\mathbf{x}_0,(1-\alpha_T)\mathbf{I})\end{split}$$</span></p><div class="note warning flat"><p>要注意这里的符号和DDPM中不太一样，这里的<span class="math inline"><em>α</em><sub><em>T</em></sub></span>相当于DDPM中的<span class="math inline"><em>ᾱ</em><sub><em>T</em></sub></span></p></div><p>（3）对<span class="math inline"><em>t</em> &gt; 1</span>定义：</p><p><span class="math display">$$\begin{split}q_\sigma(\mathbf{x}_{t-1}|\mathbf{x}_t,\mathbf{x}_0)=\mathcal{N}\left(\sqrt{\alpha_{t-1}}\mathbf{x}_0+\sqrt{1-\alpha_{t-1}-\sigma_t^2}\cdot\frac{\mathbf{x}_t-\sqrt{\alpha_t}\mathbf{x}_0}{\sqrt{1-\alpha_t}},\sigma_t^2\mathbf{I}\right).\end{split}$$</span></p><p>只要满足上述三个定义，就可以确保之前所说的<span class="math inline"><em>q</em>(<em>x</em><sub><em>t</em></sub>|<em>x</em><sub>0</sub>)</span>的形式不改变（原论文附录证明），即对任意<span class="math inline"><em>t</em></span>，满足：</p><p><span class="math display">$$\begin{split}q_\sigma(\mathbf{x}_{t}|\mathbf{x}_0)=\mathcal{N}\left(\sqrt{\alpha_t}\mathbf{x}_0,(1-\alpha_t)\mathbf{I}\right).\end{split}$$</span></p><p>由此可以根据贝叶斯公式得到：</p><p><span class="math display">$$\begin{split}q_\sigma(\mathbf{x}_t|\mathbf{x}_{t-1},\mathbf{x}_0)=\frac{q_\sigma(\mathbf{x}_{t-1}|\mathbf{x}_t,\mathbf{x}_0)q_\sigma(\mathbf{x}_t|\mathbf{x}_0)}{q_\sigma(\mathbf{x}_{t-1}|\mathbf{x}_0)}\end{split}$$</span></p><p>此时这里的前向过程不再是马尔可夫过程，<span class="math inline"><em>σ</em></span>的大小控制着前向过程的随机性，当<span class="math inline"><em>σ</em> → 0</span>时，就是一个确定性过程，即对<span class="math inline"><em>t</em></span>一旦确定<span class="math inline"><em>x</em><sub>0</sub></span>和<span class="math inline"><em>x</em><sub><em>t</em></sub></span>，<span class="math inline"><em>x</em><sub><em>t</em> − 1</sub></span>也随之确定。</p><h3 id="采样过程">采样过程</h3><p>回顾DDPM的采样过程，我们使用一个噪声预测模型<span class="math inline"><em>ϵ</em><sub><em>θ</em></sub></span>来预测<span class="math inline"><em>ϵ</em><sub><em>t</em></sub></span>，即<span class="math inline">$\mathbf{x}_t=\sqrt{\alpha_t}\mathbf{x}_0+\sqrt{1-\alpha_t}\epsilon_\theta$</span>，由此我们可以根据<span class="math inline"><em>x</em><sub><em>t</em></sub></span>预测<span class="math inline"><em>x</em><sub>0</sub></span>：</p><p><span class="math display">$$\begin{split}f_\theta^{(t)}(\mathbf{x}_t):=(\mathbf{x}_t-\sqrt{1-\alpha_t}\cdot\epsilon_\theta^{(t)}(\mathbf{x}_t))/\sqrt{\alpha_t}.\end{split}$$</span></p><p>进一步得到采样过程的后验公式：</p><p><span class="math display">$$p_\theta^{(t)}(\mathbf{x}_{t-1}|\mathbf{x}_t)=\begin{cases}\mathcal{N}(f_\theta^{(1)}(\mathbf{x}_1),\sigma_1^2\mathbf{I}) &amp; \mathrm{if~}t=1 \\q_\sigma(\mathbf{x}_{t-1}|\mathbf{x}_t,f_\theta^{(t)}(\mathbf{x}_t)) &amp; \text{otherwise,}\end{cases}$$</span></p><p>由此可以得到非马尔可夫过程的扩散过程。</p><h2 id="从广义生成过程中采样">从广义生成过程中采样</h2><h3 id="扩散隐模型的降噪过程">扩散隐模型的降噪过程</h3><p>进一步带入公式求解，可以<strong>从<span class="math inline"><em>x</em><sub><em>t</em></sub></span>得到<span class="math inline"><em>x</em><sub><em>t</em> − 1</sub></span>的求解</strong>：</p><p><span class="math display">$$\begin{split}x_{t-1}=\sqrt{\alpha_{t-1}}\underbrace{\left(\frac{x_t-\sqrt{1-\alpha_t}\epsilon_\theta^{(t)}(x_t)}{\sqrt{\alpha_t}}\right)}_{\text{“predicted }\mathbf{x}_0\text{”}}+\underbrace{\sqrt{1-\alpha_{t-1}-\sigma_t^2}\cdot\epsilon_\theta^{(t)}(\mathbf{x}_t)}_{\text{“direction}\,\text{pointing}\,\text{to}\,\mathbf{x}_t\text{”}}+\underbrace{\sigma_t\epsilon_t}_{\text{random}\,\text{noise}}\end{split}$$</span></p><p>由上述公式可以发现，在给定<span class="math inline"><em>x</em><sub><em>t</em></sub></span>和<span class="math inline"><em>ϵ</em><sub><em>θ</em></sub></span>时，此时只有<span class="math inline"><em>σ</em><sub><em>t</em></sub></span>会导致随机性，构成不同的生成过程。我们不妨讨论一下这里<span class="math inline"><em>σ</em><sub><em>t</em></sub></span>的取值，原文考虑了2种情况：</p><ol type="1"><li>当<span class="math inline">$\sigma_t=\sqrt{(1-\alpha_{t-1})/(1-\alpha_t)}\sqrt{1-\alpha_t/\alpha_{t-1}}$</span>时，此时生成过程退化为<strong>DDPM</strong>（推导略）；</li><li>当<span class="math inline"><em>σ</em><sub><em>t</em></sub> = 0</span>时，此时该生成过程为确定性过程，我们将这个生成过程命名为<strong>DDIM</strong>。</li></ol><p>在实验过程中，使用参数<span class="math inline"><em>η</em></span>来进行控制随机性生成：</p><p><span class="math display">$$\begin{split}\sigma_{\tau_i}(\eta)=\eta\sqrt{(1-\alpha_{\tau_{i-1}})/(1-\alpha_{\tau_i})}\sqrt{1-\alpha_{\tau_i}/\alpha_{\tau_{i-1}}}\end{split}$$</span></p><h3 id="加速采样过程">加速采样过程</h3><p>由于只要满足<span class="math inline"><em>q</em>(<em>x</em><sub><em>t</em></sub>|<em>x</em><sub>0</sub>)</span>的形式不改变，我们就可以复用DDPM训好的模型，而DDPM在前向和反向过程中都需要迭代<span class="math inline"><em>T</em></span>步，我们不妨来考虑一下步长小于<span class="math inline"><em>T</em></span>的前向过程。</p><p>考虑一个从原始序列 <span class="math inline">[1, …, <em>T</em>]</span> 采样得到的长度为 <span class="math inline"><em>S</em></span> 的子序列 <span class="math inline">[<em>τ</em><sub>1</sub>, …, <em>τ</em><sub><em>S</em></sub>]</span> ，此时前向过程 <span class="math inline">[<em>x</em><sub><em>τ</em><sub>1</sub></sub>, …, <em>x</em><sub><em>τ</em><sub><em>S</em></sub></sub>]</span> 同样满足 <span class="math inline">$q({x}_{\tau_i}|{x}_0)=\mathcal{N}({x}_t;\sqrt{\alpha_{\tau_i}}{x}_0,(1-\alpha_{\tau_i}){I})$</span> 。此时对于采样过程便可以同样使用子序列进行采样达到加速采样的效果。</p><h1 id="源码">源码</h1><p>来源于LDM（<a href="https://github.com/CompVis/latent-diffusion">https://github.com/CompVis/latent-diffusion</a>），和原论文可能有出入，但是核心实现是一样的。</p><p>DDIM代码主要逻辑在<code>ldm\models\diffusion\ddim.py</code>中，在训练时使用DDPM进行训练，推理时使用DDIM进行采样来加速。在DDIM中使用<code>sample</code>函数作为接口进行采样，其中比较重要的参数包括<strong>DDIM采样步数<span class="math inline"><em>S</em></span>、随机化生成参数<span class="math inline"><em>η</em></span></strong>、条件信息<span class="math inline"><em>c</em></span>（适配条件生成）、无条件引导参数（适配Classifier-free guidance）等，我们其实主要关注前两个参数即可，这里实验设置为<span class="math inline"><em>S</em> = 200, <em>η</em> = 0</span>，即论文所说的确定性<strong>DDIM采样</strong>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sample</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">           S,</span></span><br><span class="line"><span class="params">           batch_size,</span></span><br><span class="line"><span class="params">           shape,</span></span><br><span class="line"><span class="params">           conditioning=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">           callback=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">           normals_sequence=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">           img_callback=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">           quantize_x0=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">           eta=<span class="number">0.</span>,</span></span><br><span class="line"><span class="params">           mask=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">           x0=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">           temperature=<span class="number">1.</span>,</span></span><br><span class="line"><span class="params">           noise_dropout=<span class="number">0.</span>,</span></span><br><span class="line"><span class="params">           score_corrector=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">           corrector_kwargs=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">           verbose=<span class="literal">True</span>,</span></span><br><span class="line"><span class="params">           x_T=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">           log_every_t=<span class="number">100</span>,</span></span><br><span class="line"><span class="params">           unconditional_guidance_scale=<span class="number">1.</span>,</span></span><br><span class="line"><span class="params">           unconditional_conditioning=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">           <span class="comment"># this has to come in the same format as the conditioning, # e.g. as encoded tokens, ...</span></span></span><br><span class="line"><span class="params">           **kwargs</span></span><br><span class="line"><span class="params">           </span>):</span><br><span class="line">    <span class="keyword">if</span> conditioning <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(conditioning, <span class="built_in">dict</span>):</span><br><span class="line">            cbs = conditioning[<span class="built_in">list</span>(conditioning.keys())[<span class="number">0</span>]].shape[<span class="number">0</span>]</span><br><span class="line">            <span class="keyword">if</span> cbs != batch_size:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;Warning: Got <span class="subst">&#123;cbs&#125;</span> conditionings but batch-size is <span class="subst">&#123;batch_size&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> conditioning.shape[<span class="number">0</span>] != batch_size:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;Warning: Got <span class="subst">&#123;conditioning.shape[<span class="number">0</span>]&#125;</span> conditionings but batch-size is <span class="subst">&#123;batch_size&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="variable language_">self</span>.make_schedule(ddim_num_steps=S, ddim_eta=eta, verbose=verbose)</span><br><span class="line">    <span class="comment"># sampling</span></span><br><span class="line">    C, H, W = shape</span><br><span class="line">    size = (batch_size, C, H, W)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;Data shape for DDIM sampling is <span class="subst">&#123;size&#125;</span>, eta <span class="subst">&#123;eta&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    samples, intermediates = <span class="variable language_">self</span>.ddim_sampling(conditioning, size,</span><br><span class="line">                                                callback=callback,</span><br><span class="line">                                                 img_callback=img_callback,</span><br><span class="line">                                                 quantize_denoised=quantize_x0,</span><br><span class="line">                                                 mask=mask, x0=x0,</span><br><span class="line">                                                 ddim_use_original_steps=<span class="literal">False</span>,</span><br><span class="line">                                                 noise_dropout=noise_dropout,</span><br><span class="line">                                                 temperature=temperature,</span><br><span class="line">                                                 score_corrector=score_corrector,</span><br><span class="line">                                                 corrector_kwargs=corrector_kwargs ,</span><br><span class="line">                                                 x_T=x_T,</span><br><span class="line">                                                 log_every_t=log_every_t,</span><br><span class="line">                                                 unconditional_guidance_scale=unconditional_guidance_scale,</span><br><span class="line">                                                unconditional_conditioning=unconditional_conditioning,</span><br><span class="line">                                                )</span><br><span class="line">    <span class="keyword">return</span> samples, intermediates</span><br></pre></td></tr></table></figure><h2 id="参数计算">参数计算</h2><p>在<code>make_schedule</code>函数中，定义了绝大多数需要用到的中间计算参数。</p><p>首先是DDIM所用到的时间步 <span class="math inline">[<em>τ</em><sub>1</sub>, …, <em>τ</em><sub><em>S</em></sub>]</span> ，可以以两种方式进行生成：<code>uniform</code>方法和<code>quad</code>方法。<code>uniform</code>方法用于生成均匀分布的时间步，时间间隔为<code>c = num_ddpm_timesteps // num_ddim_timesteps</code>，这也是常用的方式；而<code>quad</code>方法用于生成二次函数分布的非均匀时间步，首先在 <span class="math inline">$[0,\sqrt{0.8*T}]$</span> 中生成均匀分布的点，然后再对这些点进行平方。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">make_schedule</span>(<span class="params">self, ddim_num_steps, ddim_discretize=<span class="string">&quot;uniform&quot;</span>, ddim_eta=<span class="number">0.</span>, verbose=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="variable language_">self</span>.ddim_timesteps = make_ddim_timesteps(ddim_discr_method=ddim_discretize, num_ddim_timesteps=ddim_num_steps, num_ddpm_timesteps=<span class="variable language_">self</span>.ddpm_num_timesteps,verbose=verbose)</span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">make_ddim_timesteps</span>(<span class="params">ddim_discr_method, num_ddim_timesteps, num_ddpm_timesteps, verbose=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="keyword">if</span> ddim_discr_method == <span class="string">&#x27;uniform&#x27;</span>:</span><br><span class="line">        c = num_ddpm_timesteps // num_ddim_timesteps</span><br><span class="line">        ddim_timesteps = np.asarray(<span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">0</span>, num_ddpm_timesteps, c)))</span><br><span class="line">    <span class="keyword">elif</span> ddim_discr_method == <span class="string">&#x27;quad&#x27;</span>:</span><br><span class="line">        ddim_timesteps = ((np.linspace(<span class="number">0</span>, np.sqrt(num_ddpm_timesteps * <span class="number">.8</span>), num_ddim_timesteps)) ** <span class="number">2</span>).astype(<span class="built_in">int</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError(<span class="string">f&#x27;There is no ddim discretization method called &quot;<span class="subst">&#123;ddim_discr_method&#125;</span>&quot;&#x27;</span>)</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> steps_out</span><br></pre></td></tr></table></figure><p>其次主要是 <span class="math inline"><em>α</em><sub><em>τ</em><sub><em>i</em></sub></sub></span> 和 <span class="math inline"><em>σ</em><sub><em>τ</em><sub><em>i</em></sub></sub></span> 的计算，<span class="math inline"><em>α</em><sub><em>τ</em><sub><em>i</em></sub></sub></span>（<code>ddim_alphas</code>） 相当于是在原来DDPM的 <span class="math inline"><em>α</em><sub><em>t</em></sub></span> 上在新的子序列时间步上采样得到，这里为了方便后续计算也提前定义好了 <span class="math inline"><em>α</em><sub><em>τ</em><sub><em>i</em> − 1</sub></sub></span>（<code>ddim_alphas_prev</code>） 和 <span class="math inline">$\sqrt{1-\alpha_{\tau_i}}$</span>（<code>ddim_sqrt_one_minus_alphas</code>）， <span class="math inline"><em>σ</em><sub><em>τ</em><sub><em>i</em></sub></sub></span>（<code>ddim_sigmas</code>）的计算则是直接使用公式 <span class="math inline">$\sigma_{\tau_i}(\eta)=\eta\sqrt{(1-\alpha_{\tau_{i-1}})/(1-\alpha_{\tau_i})}\sqrt{1-\alpha_{\tau_i}/\alpha_{\tau_{i-1}}}$</span> 得到。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">make_schedule</span>(<span class="params">self, ddim_num_steps, ddim_discretize=<span class="string">&quot;uniform&quot;</span>, ddim_eta=<span class="number">0.</span>, verbose=<span class="literal">True</span></span>):</span><br><span class="line">    ...</span><br><span class="line">    ddim_sigmas, ddim_alphas, ddim_alphas_prev = make_ddim_sampling_parameters(alphacums=alphas_cumprod.cpu(), ddim_timesteps=<span class="variable language_">self</span>.ddim_timesteps, eta=ddim_eta,verbose=verbose)</span><br><span class="line">    <span class="variable language_">self</span>.register_buffer(<span class="string">&#x27;ddim_sigmas&#x27;</span>, ddim_sigmas)</span><br><span class="line">    <span class="variable language_">self</span>.register_buffer(<span class="string">&#x27;ddim_alphas&#x27;</span>, ddim_alphas)</span><br><span class="line">    <span class="variable language_">self</span>.register_buffer(<span class="string">&#x27;ddim_alphas_prev&#x27;</span>, ddim_alphas_prev)</span><br><span class="line">    <span class="variable language_">self</span>.register_buffer(<span class="string">&#x27;ddim_sqrt_one_minus_alphas&#x27;</span>, np.sqrt(<span class="number">1.</span> - ddim_alphas))</span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">make_ddim_sampling_parameters</span>(<span class="params">alphacums, ddim_timesteps, eta, verbose=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="comment"># alpha_t的计算</span></span><br><span class="line">    alphas = alphacums[ddim_timesteps]</span><br><span class="line">    <span class="comment"># alpha_&#123;t-1&#125;的计算</span></span><br><span class="line">    alphas_prev = np.asarray([alphacums[<span class="number">0</span>]] + alphacums[ddim_timesteps[:-<span class="number">1</span>]].tolist())</span><br><span class="line">    <span class="comment"># sigma_t的计算</span></span><br><span class="line">    sigmas = eta * np.sqrt((<span class="number">1</span> - alphas_prev) / (<span class="number">1</span> - alphas) * (<span class="number">1</span> - alphas / alphas_prev))</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> sigmas, alphas, alphas_prev</span><br></pre></td></tr></table></figure><h2 id="加速采样">加速采样</h2><p>已经用DDPM训好了噪声预测模型，并且也已经计算好了中间的参数，其实剩下的部分直接套公式就可以了：</p><p><span class="math display">$$\begin{split}x_{t-1}=\sqrt{\alpha_{t-1}}{\left(\frac{x_t-\sqrt{1-\alpha_t}\epsilon_\theta^{(t)}(x_t)}{\sqrt{\alpha_t}}\right)}+{\sqrt{1-\alpha_{t-1}-\sigma_t^2}\cdot\epsilon_\theta^{(t)}(\mathbf{x}_t)}+{\sigma_t\epsilon_t}\end{split}$$</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">p_sample_ddim</span>(<span class="params">self, x, c, t, ...</span>):</span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 得到模型预测噪声(Classifier-free Guidance)</span></span><br><span class="line">    <span class="keyword">if</span> unconditional_conditioning <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> unconditional_guidance_scale == <span class="number">1.</span>:</span><br><span class="line">        e_t = <span class="variable language_">self</span>.model.apply_model(x, t, c)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        x_in = torch.cat([x] * <span class="number">2</span>)</span><br><span class="line">        t_in = torch.cat([t] * <span class="number">2</span>)</span><br><span class="line">        c_in = torch.cat([unconditional_conditioning, c])</span><br><span class="line">        e_t_uncond, e_t = <span class="variable language_">self</span>.model.apply_model(x_in, t_in, c_in).chunk(<span class="number">2</span>)</span><br><span class="line">        e_t = e_t_uncond + unconditional_guidance_scale * (e_t - e_t_uncond)</span><br><span class="line"></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 预测的x_0(即上述公式的第一部分)</span></span><br><span class="line">    pred_x0 = (x - sqrt_one_minus_at * e_t) / a_t.sqrt()</span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    <span class="comment"># direction pointing to x_t(即上述公式的第二部分)</span></span><br><span class="line">    dir_xt = (<span class="number">1.</span> - a_prev - sigma_t**<span class="number">2</span>).sqrt() * e_t</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 不确定性部分（即上述公式的第三部分），由于实验设置eta=0，这部分相当于确定性采样</span></span><br><span class="line">    noise = sigma_t * noise_like(x.shape, device, repeat_noise) * temperature</span><br><span class="line">    <span class="keyword">if</span> noise_dropout &gt; <span class="number">0.</span>:</span><br><span class="line">        noise = torch.nn.functional.dropout(noise, p=noise_dropout)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 求解x_&#123;t-1&#125;</span></span><br><span class="line">    x_prev = a_prev.sqrt() * pred_x0 + dir_xt + noise</span><br><span class="line">    <span class="keyword">return</span> x_prev, pred_x0</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AIGC </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Davinci学习之初级学习笔记</title>
      <link href="/2025/04/16/photograph/Davinci/%E5%88%9D%E7%BA%A7%E6%95%99%E7%A8%8B/"/>
      <url>/2025/04/16/photograph/Davinci/%E5%88%9D%E7%BA%A7%E6%95%99%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<h1 id="davinci入门学习">Davinci入门学习</h1><p>主要参考影视飓风的达芬奇教程，这里是leo的学习笔记，我的主要需求其实是调色，其他的部分应该只是大致浏览一下。</p><h2 id="入门简介">入门简介</h2><p>创作视频的一般流程：</p><ul><li>导入素材进行分类：<code>媒体</code> 面板中进行</li><li>进行视频的粗剪、然后进行精细剪辑：<code>快编</code> 及 <code>剪辑</code> 面板中进行</li><li>效果制作、调色、音频调整：分别对应 <code>Fusion</code>、<code>调色</code>、<code>Fairlight</code> 面板</li><li>导出视频：<code>交付</code> 面板</li></ul><h3 id="媒体面板">媒体面板</h3><p>通常流程是我们通过<strong>媒体浏览器</strong>找到素材所在文件夹，在<strong>监视器</strong>中回看，选择需要的素材拖拽到<strong>素材池</strong>中，等待剪辑。</p><p>达芬奇素材池进行项目管理有一个很大的弊端：一旦导入素材，就无法更改项目的帧率，容易造成后期视频的跳帧或卡顿（Davinci 17后可以更改了）。</p><blockquote><p>解决方法：需要在前期导入素材之前，进入 文件-项目设置-设置合适的帧率 配置项目</p></blockquote><h3 id="快编和剪辑面板">快编和剪辑面板</h3><p>在 <code>剪辑</code> 面板中进行剪辑的一般流程：</p><ul><li>媒体池进行筛选</li><li>在素材监视器中使用 <code>I</code> 键和 <code>O</code> 键打下出入点，选出有用的素材拖入时间线进行组装</li><li>通过特效库面板选择对应的音频或视频效果添加转场</li></ul><h3 id="fusion调色fairlight面板">Fusion、调色、Fairlight面板</h3><p>不推荐使用 <code>Fusion</code> 制作复杂的效果及动画，节点式工作逻辑复杂。<code>调色</code> 面板很强大（这其实也是我的主要需求），<code>Fairlight</code> 面板则相当于音频的调色面板。</p><h3 id="交付面板">交付面板</h3><p>通常传播互联网选择 <code>mp4</code> 封装，<code>h264</code> 编码，分辨率和帧率则根据需求调整。之后添加到渲染序列进行渲染即可。</p><h2 id="剪辑">剪辑</h2><p>一些Tips：</p><ul><li>建议在 <code>媒体</code> 面板中一次性将所有素材放入素材池，然后再进行分类，这样剪辑时不用因为缺少素材又重新添加。</li><li><code>I</code> 键和 <code>O</code> 键来打下视频的出入点，可以单独点击画面或音频拖入画面或音频轨道。</li><li>使用 <code>alt</code>+鼠标滚轮 来进行时间线的缩放</li></ul><h3 id="更多工具栏">更多工具栏</h3><table><thead><tr class="header"><th style="text-align: center;">工具</th><th style="text-align: center;">快捷键</th><th style="text-align: center;">描述</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">鼠标模式</td><td style="text-align: center;">-</td><td style="text-align: center;">快速编辑</td></tr><tr class="even"><td style="text-align: center;">修建编辑模式</td><td style="text-align: center;"><code>T</code></td><td style="text-align: center;">波纹删除（适合精剪）</td></tr><tr class="odd"><td style="text-align: center;">动态修剪模式</td><td style="text-align: center;">-</td><td style="text-align: center;">不经常用</td></tr><tr class="even"><td style="text-align: center;">剃刀工具</td><td style="text-align: center;"><code>B</code></td><td style="text-align: center;">素材切刀</td></tr><tr class="odd"><td style="text-align: center;">素材导入方式</td><td style="text-align: center;">-</td><td style="text-align: center;">分为插入、覆盖以及替换</td></tr></tbody></table><h2 id="添加效果与关键帧">添加效果与关键帧</h2><ul><li>添加转场：打开特效库，拖拽一个转场效果到素材拼接处，可以在检查器中精细调节转场效果</li><li>片段调整：点击一个素材片段，打开检查器，进行缩放、位置、变速等，按住 <code>alt</code> 再进行调整能更精细地控制</li><li>添加关键帧：打开检查器，起始帧选中需要变换的参数右侧红色菱形，接着选中结束帧改变参数即可自动加入关键帧。可以打开非线性变换选项，这样比线性的参数变化更有质感（缓动效果）</li><li>批处理：只需要 <code>ctrl+c</code> 复制某个已处理好的片段，然后选中需要批处理的其他片段进行粘贴即可</li></ul><blockquote><p>插件推荐：</p><p><a href="https://www.maxon.net/en/red-giant">红巨星宇宙</a>：很多效果和滤镜预设，一年订阅在1400元</p><p><a href="https://www.filmconvert.com/">FilmConvert</a>：调色插件，有很多相机预设</p></blockquote><h2 id="快编界面">快编界面</h2><h3 id="预备知识之代理文件的生成">预备知识之代理文件的生成</h3><p>媒体优化：减少电脑负荷</p><p><strong>方案一</strong>（适用于比较简单的剪辑）：播放–代理模式–选择合适的分辨率</p><p><strong>方案二</strong>（适用于复杂的剪辑）：媒体池选中素材，右键生成优化媒体文件</p><p>设置在 文件–项目设置–主设置–优化的媒体和渲染缓存</p><blockquote><p>常用设置：4K视频 选择 分辨率–二分之一 编码–DNXHR SQ</p></blockquote><h3 id="预备知识之智能媒体夹及双时间线剪辑">预备知识之智能媒体夹及双时间线剪辑</h3><p>Aroll 主要内容 Broll 辅助时间线</p>]]></content>
      
      
      <categories>
          
          <category> 摄影 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Davinci </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Lightroom入门笔记</title>
      <link href="/2025/04/16/photograph/Lightroom/%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/"/>
      <url>/2025/04/16/photograph/Lightroom/%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<p>教程来自摄影师泰罗</p><h1 id="lr系统教程">LR系统教程</h1><h2 id="第一期照片管理筛选与批量处理">第一期、照片管理、筛选与批量处理</h2><p>PS是重新分配空间来处理照片，而LR是建立软连接到导入照片路径。</p><ul><li>注意在导入时有几个选项：包括<strong>复制、移动、添加</strong></li><li>添加是不会改变数据存储路径的，只是建立了一个映射；</li><li>而移动则是将照片从硬盘中移动到本地。</li></ul><h2 id="第二期直方图与曝光补偿">第二期、直方图与曝光补偿</h2><h3 id="直方图的原理">直方图的原理</h3><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/image2.png" width="50%" /></p><p>直方图是关于亮度的统计报告图，横坐标是0-255的亮度级别，越右越亮；纵坐标表示该亮度级别下像素数量的多少。</p><h2 id="第三期曲线">第三期、曲线</h2><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/image3.png" width="30%" /></p><p>通过映射重塑直方图</p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/image4.png" width="50%" /></p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/image5.png" width="50%" /></p><p>胶片灰</p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/image6.png" width="30%" /></p><p><strong>曲线调色原理：</strong> 分别在RGB三种模式下调色，对应模式下调高曝光会加重对应色调、反之加重对应补色色调</p><p>曲线调色案例：</p><ul><li>日系小清新：RGB稍过曝（变亮）、G阴影稍高（阴影偏绿）、B阴影稍低（阴影偏暖）、R整体调低（整张图偏青）</li><li>暖色调电影感：RGB稍过曝、R整体偏低、B整体调低</li></ul><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/image7.png" width="50%" /></p><h2 id="第四期色温色调以及分离色调">第四期、色温色调以及分离色调</h2><p>调曲线过于复杂，不如关注眼前的工具</p><pre><code>城市夜景人文调色思路之一：降低色温、降低色调、提亮阴影、二次构图（2.35：1）</code></pre><p>色温色调的缺陷：过于整体</p><p>分离色调：分别为高光和阴影分别调色相</p><pre><code>一个有意思的调色思路：阴影偏黄绿、高光偏冷青</code></pre><p><strong>曲线工具 VS 分离色调</strong></p><p>曲线工具精确选择区域不精确赋予色彩；分离色调不精确选择区域精确选择色彩</p><h2 id="第五期hsl调色">第五期、HSL调色</h2><p>更精细的局部调色</p><p>一个调色思路是各种色相都向对应高光或阴影色调调整，以达成整体和谐的效果</p><p>用于学习大佬调色思路的网站：<a href="https://anvaka.github.io/pixchart/?d=4&amp;ignore=&amp;link=&amp;groupBy=hsl.h">https://anvaka.github.io/pixchart/?d=4&amp;ignore=&amp;link=&amp;groupBy=hsl.h</a></p><h2 id="第六期锐化清晰度等局部调整工具">第六期、锐化、清晰度等局部调整工具</h2><p>锐化的本质：描边 + 数量：描边越明显 + 半径：描边的粗细程度（一般保持默认） + 细节：描边的黑白反差（一般保持默认） + 蒙版：使用alt键可以查看，保护纯净的背景不被锐化</p><p>清晰度：没有描边、加大色块交界处的差异（黑色更黑、白色更白）</p><p>其实照片想要表现出清晰与高质，重点不在于锐化和清晰度，也不在于像素的多少，而在于<strong>信息的表达</strong></p><p>less is more（删繁就简）、秩序（画面元素达到统一）、色彩对比虚实对比明亮对比</p><p>在<code>蒙版</code>中将主体部分提亮、将背景部分变暗</p><h2 id="第七期提升摄影后期水准">第七期、提升摄影后期水准</h2><p>降噪：高ISO会造成明度噪点和彩色噪点这两种，分别使用明亮度和颜色按钮调节，一般明亮度不能给太高、而颜色可以给高</p><p>镜头校正：消除镜头带来的偏移以及曲变</p><p>导出：质量可以选择60（默认）、如果要发朋友圈调整图像大小以适合短边、像素选择1080（可以避免微信的二次压缩）</p><p><strong>平面构成、立体构成、色彩构成</strong></p><p>简单矫正<span class="math inline">→</span>局部修饰<span class="math inline">→</span>艺术加工</p><h2 id="第八期让图片更干净">第八期、让图片更干净</h2><ul><li>压低曲线的高光、提高曝光值（关键步骤）</li><li>压缩曲线的阴影</li><li>提高橙色，追加一些饱和度，红色追加饱和度（针对图片中有人像的情况，让皮肤和嘴唇更好看）</li><li>绿色的色相往右滑动，明度和饱和度提高（让树木更精神）</li><li>蓝色色相向右滑动</li><li>曲线红色通道稍微下压，使整张图偏青</li></ul><h2 id="第九期油画感教程">第九期、油画感教程</h2><blockquote><p>油画风特征分析： + 饱和度高、对比度中 + 色系统一 + 细节模糊、整体清晰</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/image8.png" width="50%" /></p>]]></content>
      
      
      <categories>
          
          <category> 摄影 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Lightroom </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>调色不重要？调色很重要</title>
      <link href="/2025/04/16/photograph/Lightroom/%E4%B8%80%E4%BA%9B%E4%B8%AA%E4%BA%BA%E8%B0%83%E8%89%B2%E6%80%9D%E8%B7%AF/"/>
      <url>/2025/04/16/photograph/Lightroom/%E4%B8%80%E4%BA%9B%E4%B8%AA%E4%BA%BA%E8%B0%83%E8%89%B2%E6%80%9D%E8%B7%AF/</url>
      
        <content type="html"><![CDATA[<h1 id="总结贴">总结贴</h1><p>示例图片有点糊…为了加载速度只能压缩画质了</p><h2 id="日系蓝色胶片">日系蓝色胶片</h2><h3 id="基本">基本</h3><p>色温 - 色调 +</p><p>曝光适度 对比 - 白色色阶 - 黑色色阶 +</p><h3 id="曲线">曲线</h3><p>稍微下拉，亮部降暗部升</p><p>红色通道暗部下拉</p><h3 id="混色器">混色器</h3><p>绿色相 - 蓝色相 +</p><p>暖色饱和度 + 冷色饱和度（尤其蓝色） -</p><p>暖色明度 + 冷色明度（尤其蓝色） -</p><h3 id="颜色分级">颜色分级</h3><p>阴影偏青，高光偏黄绿</p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/lr1.png" width="50%" /></p><h2 id="亮调春花">亮调春花</h2><h3 id="基本-1">基本</h3><p>色温 + 色调 -</p><p>曝光适度 对比 - 高光 - 阴影 + 白色色阶 - 黑色色阶 +</p><p>纹理 - 清晰度 + 去朦胧 -</p><p>鲜艳度 + 饱和度 -</p><h3 id="曲线-1">曲线</h3><p>稍微下拉，暗部升</p><h3 id="混色器-1">混色器</h3><p>橙绿色相 - 红蓝色相 +</p><p>暖色及蓝色饱和度 + 绿色饱和度 -</p><p>冷色明度 -</p><h3 id="颜色分级-1">颜色分级</h3><p>中间色调及阴影偏青，高光偏黄绿</p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/lr2.png" width="50%" /></p><h2 id="暗调春花">暗调春花</h2><h3 id="基本-2">基本</h3><p>色温 - 色调 -</p><p>曝光适度 对比 - 高光 - 阴影 + 白色色阶 - 黑色色阶 +</p><p>纹理 + 清晰度 - 去朦胧 +</p><p>鲜艳度 + 饱和度 -</p><h3 id="曲线-2">曲线</h3><p>稍微下拉，暗部升</p><h3 id="混色器-2">混色器</h3><p>蓝橙色相 - 红绿色相 +</p><p>暖色饱和度 + 绿色饱和度 -</p><p>冷色明度 -</p><h3 id="颜色分级-2">颜色分级</h3><p>阴影偏青，中间色调偏黄绿，高光偏橙黄</p><h3 id="蒙版">蒙版</h3><p>径向蒙版模拟打光（曝光+色温+），反向蒙版加深对比</p><p><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/lr3.png" width="50%" /></p>]]></content>
      
      
      <categories>
          
          <category> 摄影 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Lightroom </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PS入门笔记</title>
      <link href="/2025/04/16/photograph/Photoshop/%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/"/>
      <url>/2025/04/16/photograph/Photoshop/%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> 摄影 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Photoshop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DDPM总结</title>
      <link href="/2025/04/15/AIGC/DDPM/"/>
      <url>/2025/04/15/AIGC/DDPM/</url>
      
        <content type="html"><![CDATA[<p><strong>系列文章：</strong></p><ol class="series-items"><li><a href="/2025/04/15/AIGC/DDPM/" title="DDPM总结">DDPM总结</a></li><li><a href="/2025/04/17/AIGC/DDIM/" title="DDIM总结">DDIM总结</a></li><li><a href="/2025/05/08/AIGC/DIT/" title="DiT的细节">DiT的细节</a></li><li><a href="/2025/05/08/AIGC/iDDPM/" title="iDDPM总结">iDDPM总结</a></li></ol><p>这里分析的源码并非来自DDPM原论文，而是LDM（<a href="https://github.com/CompVis/latent-diffusion">https://github.com/CompVis/latent-diffusion</a>）中的DDPM部分，因此和原论文可能有些出入。</p><h1 id="回顾">回顾</h1><p>首先在分析源码之前，可以先回顾一下DDPM的关键步骤。</p><p><strong>前向过程（Forward Process）</strong></p><p>对输入图像 <span class="math inline"><em>x</em><sub>0</sub></span> 按照预定义的加噪操作，逐步加入高斯噪声，直到最终接近纯高斯噪声 <span class="math inline"><em>x</em><sub><em>T</em></sub></span>。</p><p><span class="math display">$$\begin{split}    q(x_t \mid x_{t-1})= \mathcal{N}\bigl(x_t;\;\sqrt{\alpha_t}\,x_{t-1},\;\beta_t I\bigr).\end{split}$$</span></p><p>对于单步加噪，代入连续求解可以得到上式的闭式形式，其中<span class="math inline">$\alpha_t = 1-\beta_t,\quad \bar\alpha_t = \prod_{i=1}^t \alpha_i$</span>。</p><p><span class="math display">$$\begin{split}    q(x_t \mid x_0) = \mathcal{N}\bigl(x_t;\;\sqrt{\bar\alpha_t}\,x_0,\;(1-\bar\alpha_t)\,I\bigr).\end{split}$$</span></p><p><strong>去噪过程（Reverse Process）</strong></p><p><span class="math display">$$\begin{split}    q(x_{t-1} \mid x_t, x_0)=\mathcal{N}(x_t;\frac{1}{\sqrt{\alpha_t}}(x_t-\frac{1-\alpha_t}{\sqrt{1-\bar\alpha_t}}\epsilon_t), \beta_t\cdot\frac{1-\bar\alpha_{t-1}}{1-\bar\alpha_t}I).\end{split}$$</span></p><p><strong>损失函数</strong></p><p>主要是对噪声预测的MSE损失：</p><p><span class="math display">$$\begin{split}    \mathcal L=\mathbb{E}_{t,x_0,\epsilon}\left[\|\epsilon-\varepsilon_\theta(x_t,t)\|^2\right].\end{split}$$</span></p><h1 id="源码分析">源码分析</h1><p>主要代码逻辑定义在<code>ldm\models\diffusion\ddpm.py</code>中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, *args, **kwargs</span>):</span><br><span class="line">    <span class="comment"># b, c, h, w, device, img_size, = *x.shape, x.device, self.image_size</span></span><br><span class="line">    <span class="comment"># assert h == img_size and w == img_size, f&#x27;height and width of image must be &#123;img_size&#125;&#x27;</span></span><br><span class="line">    t = torch.randint(</span><br><span class="line">        <span class="number">0</span>, <span class="variable language_">self</span>.num_timesteps, (x.shape[<span class="number">0</span>],), device=<span class="variable language_">self</span>.device</span><br><span class="line">    ).long()</span><br><span class="line">    <span class="keyword">return</span> <span class="variable language_">self</span>.p_losses(x, t, *args, **kwargs)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">p_losses</span>(<span class="params">self, x_start, t, noise=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="comment"># --------------------</span></span><br><span class="line">    noise = default(noise, <span class="keyword">lambda</span>: torch.randn_like(x_start))</span><br><span class="line">    x_noisy = <span class="variable language_">self</span>.q_sample(x_start=x_start, t=t, noise=noise)</span><br><span class="line">    model_out = <span class="variable language_">self</span>.model(x_noisy, t)</span><br><span class="line">    <span class="comment"># --------------------</span></span><br><span class="line"></span><br><span class="line">    loss_dict = &#123;&#125;</span><br><span class="line">    <span class="keyword">if</span> <span class="variable language_">self</span>.parameterization == <span class="string">&quot;eps&quot;</span>:</span><br><span class="line">        target = noise</span><br><span class="line">    <span class="keyword">elif</span> <span class="variable language_">self</span>.parameterization == <span class="string">&quot;x0&quot;</span>:</span><br><span class="line">        target = x_start</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError(</span><br><span class="line">            <span class="string">f&quot;Paramterization <span class="subst">&#123;self.parameterization&#125;</span> not yet supported&quot;</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    loss = <span class="variable language_">self</span>.get_loss(model_out, target, mean=<span class="literal">False</span>).mean(dim=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">    log_prefix = <span class="string">&#x27;train&#x27;</span> <span class="keyword">if</span> <span class="variable language_">self</span>.training <span class="keyword">else</span> <span class="string">&#x27;val&#x27;</span></span><br><span class="line"></span><br><span class="line">    loss_dict.update(&#123;<span class="string">f&#x27;<span class="subst">&#123;log_prefix&#125;</span>/loss_simple&#x27;</span>: loss.mean()&#125;)</span><br><span class="line">    loss_simple = loss.mean() * <span class="variable language_">self</span>.l_simple_weight</span><br><span class="line"></span><br><span class="line">    loss_vlb = (<span class="variable language_">self</span>.lvlb_weights[t] * loss).mean()</span><br><span class="line">    loss_dict.update(&#123;<span class="string">f&#x27;<span class="subst">&#123;log_prefix&#125;</span>/loss_vlb&#x27;</span>: loss_vlb&#125;)</span><br><span class="line"></span><br><span class="line">    loss = loss_simple + <span class="variable language_">self</span>.original_elbo_weight * loss_vlb</span><br><span class="line"></span><br><span class="line">    loss_dict.update(&#123;<span class="string">f&#x27;<span class="subst">&#123;log_prefix&#125;</span>/loss&#x27;</span>: loss&#125;)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss, loss_dict</span><br></pre></td></tr></table></figure><p>根据这部分代码可以理解，每次训练都是<strong>对每个batch随机从 <span class="math inline">[0, <em>T</em>)</span> 中选择一个时刻进行训练</strong>，在该时间步内进行单独训练。而重点其实就在<code>p_losses</code>函数中的前三步，后续步骤主要是在计算损失。首先初始化高斯噪音<code>noise</code>，接着通过前向过程得到 <span class="math inline"><em>x</em><sub><em>t</em></sub></span> ，然后经过模型预测得到预测噪音 <span class="math inline"><em>ϵ</em><sub><em>θ</em></sub></span> 或是预测输出图像 <span class="math inline"><em>x</em><sub>0</sub></span> 。在这里 <code>parameterization</code> 被设置为 <code>eps</code> ，所以这里与原论文保持一致是<strong>预测噪声</strong>而不是DPM中的预测 <span class="math inline"><em>x</em><sub>0</sub></span> 。</p><p>还有这部分损失的计算还加上了<code>loss_vlb</code>，这其实是后续iDDPM的工作，可以先不用管。</p><h2 id="前向过程">前向过程</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">q_sample</span>(<span class="params">self, x_start, t, noise=<span class="literal">None</span></span>):</span><br><span class="line">    noise = default(noise, <span class="keyword">lambda</span>: torch.randn_like(x_start))</span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">        extract_into_tensor(<span class="variable language_">self</span>.sqrt_alphas_cumprod, t, x_start.shape) * x_start</span><br><span class="line">        + extract_into_tensor(<span class="variable language_">self</span>.sqrt_one_minus_alphas_cumprod, t, x_start.shape)</span><br><span class="line">        * noise</span><br><span class="line">    )</span><br></pre></td></tr></table></figure><p>很明显，这里的加噪部分和原论文的一步加噪的公式保持一致，其中的中间参数在源码中<code>register_schedule</code> 函数中都有提前定义好：</p><p><span class="math display">$$\begin{split}    q(x_t \mid x_0) = \mathcal{N}\bigl(x_t;\;\sqrt{\bar\alpha_t}\,x_0,\;(1-\bar\alpha_t)\,I\bigr).\end{split}$$</span></p><h2 id="预测噪声">预测噪声</h2><p>原论文预测噪声是直接用UNet进行预测的，源码里也是保持一致，这里的<code>model</code>是<code>UNetModel</code>类的一个实例。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">model_out = <span class="variable language_">self</span>.model(x_noisy, t)</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>对于<code>UNetModel</code>类的前向过程，其实主要包含几个部分：对于时间步<code>t</code>的embedding、其他条件的处理（原论文里不包含，该部分源码来自于LDM）、得到最终预测的噪声或 <span class="math inline"><em>x</em><sub>0</sub></span>。时间步的信息则是通过embedding层后通过attention嵌入到特征图中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">UNetModel</span>(nn.Module):</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, timesteps=<span class="literal">None</span>, context=<span class="literal">None</span>, y=<span class="literal">None</span>,**kwargs</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Apply the model to an input batch.</span></span><br><span class="line"><span class="string">        :param x: an [N x C x ...] Tensor of inputs.</span></span><br><span class="line"><span class="string">        :param timesteps: a 1-D batch of timesteps.</span></span><br><span class="line"><span class="string">        :param context: conditioning plugged in via crossattn</span></span><br><span class="line"><span class="string">        :param y: an [N] Tensor of labels, if class-conditional.</span></span><br><span class="line"><span class="string">        :return: an [N x C x ...] Tensor of outputs.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">assert</span> (y <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>) == (</span><br><span class="line">            <span class="variable language_">self</span>.num_classes <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line">        ), <span class="string">&quot;must specify y if and only if the model is class-conditional&quot;</span></span><br><span class="line">        hs = []</span><br><span class="line">        t_emb = timestep_embedding(timesteps, <span class="variable language_">self</span>.model_channels, repeat_only=<span class="literal">False</span>)</span><br><span class="line">        emb = <span class="variable language_">self</span>.time_embed(t_emb)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.num_classes <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">assert</span> y.shape == (x.shape[<span class="number">0</span>],)</span><br><span class="line">            emb = emb + <span class="variable language_">self</span>.label_emb(y)</span><br><span class="line"></span><br><span class="line">        h = x.<span class="built_in">type</span>(<span class="variable language_">self</span>.dtype)</span><br><span class="line">        <span class="keyword">for</span> module <span class="keyword">in</span> <span class="variable language_">self</span>.input_blocks:</span><br><span class="line">            h = module(h, emb, context)</span><br><span class="line">            hs.append(h)</span><br><span class="line">        h = <span class="variable language_">self</span>.middle_block(h, emb, context)</span><br><span class="line">        <span class="keyword">for</span> module <span class="keyword">in</span> <span class="variable language_">self</span>.output_blocks:</span><br><span class="line">            h = th.cat([h, hs.pop()], dim=<span class="number">1</span>)</span><br><span class="line">            h = module(h, emb, context)</span><br><span class="line">        h = h.<span class="built_in">type</span>(x.dtype)</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.predict_codebook_ids:</span><br><span class="line">            <span class="keyword">return</span> <span class="variable language_">self</span>.id_predictor(h)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="variable language_">self</span>.out(h)</span><br></pre></td></tr></table></figure><h2 id="采样过程">采样过程</h2><p>采样过程在DDPM中还是逐步逆向去噪的，一共迭代<code>t</code>次，每次单步执行<code>p_sample</code>函数进行单步去噪。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">p_sample_loop</span>(<span class="params">self, shape, return_intermediates=<span class="literal">False</span></span>):</span><br><span class="line">    device = <span class="variable language_">self</span>.betas.device</span><br><span class="line">    b = shape[<span class="number">0</span>]</span><br><span class="line">    img = torch.randn(shape, device=device)</span><br><span class="line">    intermediates = [img]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> tqdm(</span><br><span class="line">        <span class="built_in">reversed</span>(<span class="built_in">range</span>(<span class="number">0</span>, <span class="variable language_">self</span>.num_timesteps)),</span><br><span class="line">        desc=<span class="string">&#x27;Sampling t&#x27;</span>,</span><br><span class="line">        total=<span class="variable language_">self</span>.num_timesteps,</span><br><span class="line">    ):</span><br><span class="line">        img = <span class="variable language_">self</span>.p_sample(</span><br><span class="line">            img,</span><br><span class="line">            torch.full((b,), i, device=device, dtype=torch.long),</span><br><span class="line">            clip_denoised=<span class="variable language_">self</span>.clip_denoised,</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">if</span> i % <span class="variable language_">self</span>.log_every_t == <span class="number">0</span> <span class="keyword">or</span> i == <span class="variable language_">self</span>.num_timesteps - <span class="number">1</span>:</span><br><span class="line">            intermediates.append(img)</span><br><span class="line">    <span class="keyword">if</span> return_intermediates:</span><br><span class="line">        <span class="keyword">return</span> img, intermediates</span><br><span class="line">    <span class="keyword">return</span> img</span><br><span class="line"></span><br><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sample</span>(<span class="params">self, batch_size=<span class="number">16</span>, return_intermediates=<span class="literal">False</span></span>):</span><br><span class="line">    image_size = <span class="variable language_">self</span>.image_size</span><br><span class="line">    channels = <span class="variable language_">self</span>.channels</span><br><span class="line">    <span class="keyword">return</span> <span class="variable language_">self</span>.p_sample_loop(</span><br><span class="line">        (batch_size, channels, image_size, image_size),</span><br><span class="line">        return_intermediates=return_intermediates,</span><br><span class="line">    )</span><br></pre></td></tr></table></figure><p>下面我们分析单步去噪过程，回顾一下之前说的单步去噪：</p><p><span class="math display">$$\begin{split}    q(x_{t-1} \mid x_t, x_0)=\mathcal{N}(x_t;\frac{1}{\sqrt{\alpha_t}}(x_t-\frac{1-\alpha_t}{\sqrt{1-\bar\alpha_t}}\epsilon_t), \beta_t\cdot\frac{1-\bar\alpha_{t-1}}{1-\bar\alpha_t}I).\end{split}$$</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">p_sample</span>(<span class="params">self, x, t, clip_denoised=<span class="literal">True</span>, repeat_noise=<span class="literal">False</span></span>):</span><br><span class="line">    b, *_, device = *x.shape, x.device</span><br><span class="line">    model_mean, _, model_log_variance = <span class="variable language_">self</span>.p_mean_variance(</span><br><span class="line">        x=x, t=t, clip_denoised=clip_denoised</span><br><span class="line">    )</span><br><span class="line">    noise = noise_like(x.shape, device, repeat_noise)</span><br><span class="line">    <span class="comment"># no noise when t == 0</span></span><br><span class="line">    nonzero_mask = (<span class="number">1</span> - (t == <span class="number">0</span>).<span class="built_in">float</span>()).reshape(b, *((<span class="number">1</span>,) * (<span class="built_in">len</span>(x.shape) - <span class="number">1</span>)))</span><br><span class="line">    <span class="keyword">return</span> model_mean + nonzero_mask * (<span class="number">0.5</span> * model_log_variance).exp() * noise</span><br></pre></td></tr></table></figure><p>这里相当于是通过预测噪声 <span class="math inline"><em>ϵ</em><sub><em>θ</em></sub></span> 和 <span class="math inline"><em>x</em><sub><em>t</em></sub></span> 来预测 <span class="math inline"><em>x</em><sub><em>t</em> − 1</sub></span>，但是在代码实现上考虑到DPM每步直接预测 <span class="math inline"><em>x</em><sub>0</sub></span> 的做法，为了方便统一处理，这里在实现时即使是预测噪声 <span class="math inline"><em>ϵ</em><sub><em>θ</em></sub></span> ，也会进一步得到预测 <span class="math inline">$\hat{x_0}$</span> ，然后通过后验公式计算均值。即下面代码所展示的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">p_mean_variance</span>(<span class="params">self, x, t, clip_denoised: <span class="built_in">bool</span></span>):</span><br><span class="line">    model_out = <span class="variable language_">self</span>.model(x, t)</span><br><span class="line">    <span class="keyword">if</span> <span class="variable language_">self</span>.parameterization == <span class="string">&quot;eps&quot;</span>:</span><br><span class="line">        x_recon = <span class="variable language_">self</span>.predict_start_from_noise(x, t=t, noise=model_out)</span><br><span class="line">    <span class="keyword">elif</span> <span class="variable language_">self</span>.parameterization == <span class="string">&quot;x0&quot;</span>:</span><br><span class="line">        x_recon = model_out</span><br><span class="line">    <span class="keyword">if</span> clip_denoised:</span><br><span class="line">        x_recon.clamp_(-<span class="number">1.0</span>, <span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line">    model_mean, posterior_variance, posterior_log_variance = <span class="variable language_">self</span>.q_posterior(</span><br><span class="line">        x_start=x_recon, x_t=x, t=t</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> model_mean, posterior_variance, posterior_log_variance</span><br></pre></td></tr></table></figure><p>给定 <span class="math inline"><em>x</em><sub><em>t</em></sub></span> 和 <span class="math inline"><em>ϵ</em><sub><em>θ</em></sub></span> ，可以推导 <span class="math inline">$\hat{x_0}$</span> ：</p><p><span class="math display">$$\begin{split}    \bar{x_0} = \frac{1}{\sqrt{\bar\alpha_t}}x_t-\frac{\sqrt{1-\bar\alpha_t}}{\sqrt{\bar\alpha_t}}\epsilon_\theta(x_t,t)\end{split}$$</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">predict_start_from_noise</span>(<span class="params">self, x_t, t, noise</span>):</span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">        extract_into_tensor(<span class="variable language_">self</span>.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t</span><br><span class="line">        - extract_into_tensor(<span class="variable language_">self</span>.sqrt_recipm1_alphas_cumprod, t, x_t.shape)</span><br><span class="line">        * noise</span><br><span class="line">    )</span><br></pre></td></tr></table></figure><p>接着给定 <span class="math inline"><em>x</em><sub><em>t</em></sub></span> 和 <span class="math inline">$\hat{x_0}$</span> ，可以根据后验公式继续推导出 <span class="math inline"><em>x</em><sub><em>t</em> − 1</sub></span> ：</p><p><span class="math display">$$\begin{gather}\begin{split}    q(x_{t-1}\mid x_t,x_0)&amp;=\mathcal{N}(\mu_t,\sigma_t^2I) \\    \mu_t&amp;=\frac{\sqrt{\bar{\alpha}_{t-1}}\cdot\beta_t}{1-\bar{\alpha}_t}x_0+\frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t}x_t \\    \sigma_t^2&amp;=\beta_t\cdot\frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t}\end{split}\end{gather}$$</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">q_posterior</span>(<span class="params">self, x_start, x_t, t</span>):</span><br><span class="line">    posterior_mean = (</span><br><span class="line">        extract_into_tensor(<span class="variable language_">self</span>.posterior_mean_coef1, t, x_t.shape) * x_start</span><br><span class="line">        + extract_into_tensor(<span class="variable language_">self</span>.posterior_mean_coef2, t, x_t.shape) * x_t</span><br><span class="line">    )</span><br><span class="line">    posterior_variance = extract_into_tensor(<span class="variable language_">self</span>.posterior_variance, t, x_t.shape)</span><br><span class="line">    posterior_log_variance_clipped = extract_into_tensor(</span><br><span class="line">        <span class="variable language_">self</span>.posterior_log_variance_clipped, t, x_t.shape</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> posterior_mean, posterior_variance, posterior_log_variance_clipped</span><br></pre></td></tr></table></figure><p>其中用到的中间变量在<code>register_schedule</code>中提前定义好了，包括计算均值需要用到的<code>posterior_mean_coef1</code>和<code>posterior_mean_coef2</code>以及方差<code>posterior_variance</code>和log形态方差<code>posterior_log_variance_clipped</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># calculations for posterior q(x_&#123;t-1&#125; | x_t, x_0)</span></span><br><span class="line">posterior_variance = (<span class="number">1</span> - <span class="variable language_">self</span>.v_posterior) * betas * ( <span class="number">1.0</span> - alphas_cumprod_prev ) / (<span class="number">1.0</span> - alphas_cumprod) + <span class="variable language_">self</span>.v_posterior * betas</span><br><span class="line"><span class="comment"># above: equal to 1. / (1. / (1. - alpha_cumprod_tm1) + alpha_t / beta_t)</span></span><br><span class="line"><span class="variable language_">self</span>.register_buffer(<span class="string">&#x27;posterior_variance&#x27;</span>, to_torch(posterior_variance))</span><br><span class="line"><span class="comment"># below: log calculation clipped because the posterior variance is 0 at the beginning of the diffusion chain</span></span><br><span class="line"><span class="variable language_">self</span>.register_buffer(</span><br><span class="line">    <span class="string">&#x27;posterior_log_variance_clipped&#x27;</span>,</span><br><span class="line">    to_torch(np.log(np.maximum(posterior_variance, <span class="number">1e-20</span>))),</span><br><span class="line">)</span><br><span class="line"><span class="variable language_">self</span>.register_buffer(</span><br><span class="line">    <span class="string">&#x27;posterior_mean_coef1&#x27;</span>,</span><br><span class="line">    to_torch(betas * np.sqrt(alphas_cumprod_prev) / (<span class="number">1.0</span> - alphas_cumprod)),</span><br><span class="line">)</span><br><span class="line"><span class="variable language_">self</span>.register_buffer(</span><br><span class="line">    <span class="string">&#x27;posterior_mean_coef2&#x27;</span>,</span><br><span class="line">    to_torch(</span><br><span class="line">        (<span class="number">1.0</span> - alphas_cumprod_prev) * np.sqrt(alphas) / (<span class="number">1.0</span> - alphas_cumprod)</span><br><span class="line">    ),</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>这里细心的小伙伴就会注意到了，为什么要计算一个log形态的方差呢？事实上回看<code>p_sample</code>函数也会发现，计算 <span class="math inline"><em>x</em><sub><em>t</em> − 1</sub></span> 时用的也是<code>posterior_log_variance_clipped</code>而不是<code>posterior_variance</code>，然后最后一步还要还原成<code>posterior_variance</code>形态用于求解，这不是多此一举吗？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">p_sample</span>(<span class="params">self, x, t, clip_denoised=<span class="literal">True</span>, repeat_noise=<span class="literal">False</span></span>):</span><br><span class="line">    ...</span><br><span class="line">    model_mean, _, model_log_variance = <span class="variable language_">self</span>.p_mean_variance(</span><br><span class="line">        x=x, t=t, clip_denoised=clip_denoised</span><br><span class="line">    )</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> model_mean + nonzero_mask * (<span class="number">0.5</span> * model_log_variance).exp() * noise</span><br></pre></td></tr></table></figure><p>其实这里也是因为为了适配后续改进的原因，我们知道DDPM其实是固定方差的形式，即方差是像这里通过一系列参数直接计算得到的，但是后续iDDPM对这里进行了改进，模型直接预测得到log方差 <span class="math inline"><em>l</em><em>o</em><em>g</em><em>σ</em><sup>2</sup></span> ，将方差作为可学习表征，因此这里统一使用log形态的方差便于代码复用性。</p>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AIGC </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>扩散模型的发展（简略版）</title>
      <link href="/2025/03/28/AIGC/overview_DM/"/>
      <url>/2025/03/28/AIGC/overview_DM/</url>
      
        <content type="html"><![CDATA[<p><strong>系列文章：</strong></p><ol class="series-items"><li><a href="/2025/03/28/AIGC/overview_DM/" title="扩散模型的发展（简略版）">扩散模型的发展（简略版）</a></li><li><a href="/2025/04/27/AIGC/LDM/" title="LDM的细节">LDM的细节</a></li></ol><p>其实契机是因为要完成高级机器学习的综述作业（@^@），这里也顺便写到博客里。</p><h1 id="引言">引言</h1><p>自监督学习是一种特殊的无监督学习，它不需要人工标注数据，而是通过数据本身构造学习任务。而作为自监督学习中的一个重要分支方向，图像生成模型已有几十年的持续发展，其核心目标是学习数据的概率分布 <span class="math inline"><em>P</em>(<em>x</em>)</span> ，并从中采样出新的数据分布以生成新的图像。</p><figure><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%8F%91%E5%B1%95%E5%8E%86%E7%A8%8B_0.png" alt="" /><figcaption>图像生成模型的发展大致历程</figcaption></figure><h2 id="图像生成模型">图像生成模型</h2><p>目前主流的图像生成架构可大致分为三类：</p><ul><li><strong>基于自回归（Autoregressive）的模型</strong>，使用逐像素或逐子块的方式生成图像，每一步依赖于前面生成的部分，生成速度较慢并难以适应高分辨率生成任务；</li><li><strong>基于生成对抗网络（GANs）的方法</strong>，使用生成器和判别器组成的对抗网络生成图像，生成速度较快但训练不稳定，并且难以控制生成细节；</li><li><strong>基于扩散（Diffusion）模型的方法</strong>，在前向过程中逐步向图像添加噪声，并在反向过程中训练模型学习如何去噪，使得模型能从噪声中恢复出图像，从而生成新图像。基于扩散模型的方法生成质量高，训练稳定并能引入精确的条件控制，虽然早期存在采样速度缓慢的问题，但后期的各种改进版本使其能逐渐适应各种生成任务，特别是文生图（Text-to-Image）任务，基本统治了各大模型，包括OpenAI的DALL·E系列<sub><span style="color:blue;"><span class="citation" data-cites="ramesh2022hierarchical">[@ramesh2022hierarchical]</span></span></sub>和Google的IMAGEN<sub><span style="color:blue;"><span class="citation" data-cites="saharia2022photorealistic">[@saharia2022photorealistic]</span></span></sub>系列背后的核心技术都是扩散模型。</li></ul><h2 id="扩散模型的发展">扩散模型的发展</h2><p>其实扩散模型的提出最初是受到了非平衡统计物理（nonequilibrium thermodynamics）中的扩散过程的启发，后续经过一系列的改进和发展，逐渐成为最主流的生成模型之一。</p><p>近期的扩散理论基础可以追溯到去噪自编码器<sub><span style="color:blue;"><span class="citation" data-cites="vincent2011connection">[@vincent2011connection]</span></span></sub>的提出，该工作证明了去噪过程与分数匹配的相关性，为之后宋飏等人提出基于分数匹配的生成模型（Score-Based Generative Models，SGM）<sub><span style="color:blue;"><span class="citation" data-cites="song2019generative">[@song2019generative]</span></span></sub>奠定了基础。之后，DDPM（Denoising Diffusion Probabilistic Models）<sub><span style="color:blue;"><span class="citation" data-cites="ho2020denoising">[@ho2020denoising]</span></span></sub>作为现代扩散模型的开山之作，在DPM（Denoising Probabilistic Models）<sub><span style="color:blue;"><span class="citation" data-cites="sohl2015deep">[@sohl2015deep]</span></span></sub>的基础上建立了从高斯噪声逐步去噪生成图像的框架。2021年宋飏等人提出的随机微分方程（Stochastic Diffusion Equation，SDE）<sub><span style="color:blue;"><span class="citation" data-cites="song2020score">[@song2020score]</span></span></sub>则通过数学证明将基于分数匹配和基于去噪的生成模型的两种范式统一了起来，由此进一步巩固了扩散模型的理论基础。</p><p>但是此时的扩散模型有一个严重的问题，采样速度极慢，无法适应各大场景的需求，DDIM<sub><span style="color:blue;"><span class="citation" data-cites="song2020denoising">[@song2020denoising]</span></span></sub>提出了非马尔科夫链推理的想法，将采样步骤大幅减少，显著提升推理速度，却仍能保持与DDPM不相上下的生成质量。后续分类器引导（Classifier-Guidance）<sub><span style="color:blue;"><span class="citation" data-cites="dhariwal2021diffusion">[@dhariwal2021diffusion]</span></span></sub>和无分类器引导（Classifier-Free Guidance）<sub><span style="color:blue;"><span class="citation" data-cites="ho2022classifier">[@ho2022classifier]</span></span></sub>的提出推动了条件扩散模型的发展，并通过大量实验证明扩散模型在图像生成任务上首次超越GAN。之后GLIDE<sub><span style="color:blue;"><span class="citation" data-cites="nichol2021glide">[@nichol2021glide]</span></span></sub>和DALL·E 2<sub><span style="color:blue;"><span class="citation" data-cites="ramesh2022hierarchical">[@ramesh2022hierarchical]</span></span></sub>尝试使用Clip来进行文本引导的扩散模型，实现了高质量的文本到图像的生成。</p><p>潜在扩散模型（Latent Diffusion Model，LDM）<sub><span style="color:blue;"><span class="citation" data-cites="rombach2022high">[@rombach2022high]</span></span></sub>提出以潜在隐空间代替原像素空间以大幅减少计算量，成为之后Stable Diffusion的技术核心，支持高效的文本到图像生成。DiT（Diffusion Transformer）<sub><span style="color:blue;"><span class="citation" data-cites="peebles2023scalable">[@peebles2023scalable]</span></span></sub>则将原UNet结构替换为了纯Transformer架构，尽管计算量有所增加，但能适应更高分辨率的图像生成任务。除此之外，Video Diffusion<sub><span style="color:blue;"><span class="citation" data-cites="ho2022video">[@ho2022video]</span></span></sub>等工作则探索了3D Diffusion的可能，Consistency Models<sub><span style="color:blue;"><span class="citation" data-cites="song2023consistency">[@song2023consistency]</span></span></sub>则尝试进一步加快采样速度的极限。</p><h1 id="相关工作">相关工作</h1><p>本节将根据扩散模型的发展介绍部分重要的相关工作。</p><h2 id="ncsn">NCSN</h2><p>首先本文将介绍宋飏老师的基于分数匹配的生成模型（Score-based Generative Model，SGM）的工作，原论文这个工作又叫作Noise Conditional Score Networks（NCSN）<sub><span style="color:blue;"><span class="citation" data-cites="song2019generative">[@song2019generative]</span></span></sub>。NCSN通过估计数据分布的梯度，实现从噪声到数据的生成。</p><p>具体来说，假设有一个数据分布 <span class="math inline"><em>p</em><sub><em>d</em><em>a</em><em>t</em><em>a</em></sub>(<em>x</em>)</span> ，我们需要估计该数据分布的对数梯度，又定义为<strong>分数函数</strong>（Score Fuction）。但是直接估计数据分布的梯度很难，NCSN引入了多个噪声尺度来进行分数估计，类似于DDPM中加噪过程中的时间步，即：</p><p><span class="math display">$$  \begin{split}     s_\theta(x)\approx\nabla_x\log p_\mathrm{data}(x|\sigma)   \end{split}$$</span></p><p>由此可以得到分数匹配的目标函数为：</p><p><span class="math display">$$\begin{gather}\begin{split}    J(\theta) &amp;=\frac{1}{2}\int p_\mathrm{data}{(x)}\|s_{data}(x)-s_\theta(x)\|_2^2dx \\     &amp;=\frac{1}{2}\mathbb{E}_{p_{\mathrm{data}}(x)}\left[\left\|s_{data}(x)-s_\theta(x)\right\|_2^2\right]\end{split}\end{gather}$$</span></p><p>但是在实际求解时，<span class="math inline"><em>s</em><sub><em>d</em><em>a</em><em>t</em><em>a</em></sub>(<em>x</em>)</span> 是无法计算的。经过一系列变换，可以得到原目标函数的一个等价表示：</p><p><span class="math display">$$\begin{gather}\begin{split}    J(\theta) &amp;=\int p_{\text {data }}(x)\left[\operatorname{tr}\left(\nabla_x s_\theta(x)\right)+\frac{1}{2}\left\|s_\theta(x)\right\|_2^2\right] dx \\    &amp;=\mathbb{E}_{p_{\text {data }}(x)}\left[\operatorname{tr}\left(\nabla_x s_\theta(x)\right)+\frac{1}{2}\left\|s_\theta(x)\right\|_2^2\right]\end{split}\end{gather}$$</span></p><p>其中涉及到了二阶偏导的计算，这在网络层次很深的时候开销是很大的，因此NCSN提出了分层分数匹配和降噪分数匹配两种方法来解决这个问题。分数估计训练完成后，便可以使用<strong>郎之万动力学（Langevin Dynamics，LD）采样</strong>来生成数据分布：</p><p><span class="math display">$$     x_{t+1}=x_t+\frac{\sigma}{2}s_\theta(x_t)+\sqrt{\sigma}z_t $$</span></p><p>其中 <span class="math inline"><em>z</em><sub><em>t</em></sub></span> 表示高斯噪声，这个过程模拟的是粒子在数据分布的梯度场中进行随机行走，最终收敛到真实数据分布。NCSN可以看作是扩散模型的前身，后续宋飏老师提出的SDE也从数学形式上统一了DDPM和NCSN，虽然它的采样较慢，训练也不稳定，但是它为后续扩散模型的发展提供了坚实的理论基础。</p><h2 id="ddpm">DDPM</h2><p>去噪扩散概率模型（Denoising Diffusion Probabilistic Models）<sub><span style="color:blue;"><span class="citation" data-cites="ho2020denoising">[@ho2020denoising]</span></span></sub>是现代扩散模型的开山之作，它正式确立了扩散模型的数学框架，在图像生成任务上表现很出色。它的核心思想是前向扩散（Forward Diffusion）和逆向去噪（Reverse Denoising）两部分，其推导基于马尔科夫链进行逐步加噪和去噪。</p><figure><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/DDPM_0.png" alt="" /><figcaption>DDPM的扩散及去噪过程</figcaption></figure><h3 id="前向扩散">前向扩散</h3><p>首先对于前向扩散过程，给定真实数据 <span class="math inline"><em>x</em><sub>0</sub> ∼ <em>q</em>(<em>x</em>)</span> ，经过 <span class="math inline"><em>T</em></span> 步的加噪过程，数据最终符合标准高斯分布。定义单步扩散过程为：</p><p><span class="math display">$$    q(x_t|x_{t-1})=\mathcal{N}(x_t;\sqrt{1-\beta_t}x_{t-1},\beta_tI)$$</span></p><p>令 <span class="math inline"><em>α</em><sub><em>t</em></sub> = 1 − <em>β</em><sub><em>t</em></sub></span> ，由此可以发现 <span class="math inline"><em>q</em>(<em>x</em><sub><em>t</em></sub>|<em>x</em><sub><em>t</em> − 1</sub>)</span> 就是一个以 <span class="math inline">$\sqrt{\alpha_t}x_{t-1}$</span> 为均值，以 <span class="math inline">(1 − <em>α</em><sub><em>t</em></sub>)<em>I</em></span> 为方差的高斯分布，加噪过程相当于是 <span class="math inline">$\sqrt{\alpha_t}x_{t-1}$</span> 基础上加上一个 <span class="math inline">𝒩(0, (1 − <em>α</em><sub><em>t</em></sub>)<em>I</em>)</span> 的随机高斯噪声。进一步推导可以得到直接从 <span class="math inline"><em>x</em><sub>0</sub></span> 生成 <span class="math inline"><em>x</em><sub><em>t</em></sub></span> 的公式为：</p><p><span class="math display">$$    q(x_t|x_0)=\mathcal{N}(x_t;\sqrt{\bar{\alpha}_t}x_0,(1-\bar{\alpha}_t)I)$$</span></p><p>其中 <span class="math inline">$\bar{\alpha}_t=\prod_{i=1}^t\alpha_i$</span> ，表示累积噪声。最终当 <span class="math inline"><em>t</em> → <em>T</em></span> 时，<span class="math inline"><em>x</em><sub><em>T</em></sub></span> 近似服从于标准高斯分布 <span class="math inline">𝒩(0, <em>I</em>)</span>。</p><h3 id="逆向去噪">逆向去噪</h3><p>逆向去噪的目标是从 <span class="math inline"><em>x</em><sub><em>T</em></sub></span> 开始，逐步去噪恢复 <span class="math inline"><em>x</em><sub>0</sub></span> ，即 <span class="math inline">$p(x_{0:T})=p(x_T)\prod_{t=T-1}^0p(x_t|x_{t+1})$</span> ，那么问题的关键在于如何求解 <span class="math inline"><em>p</em>(<em>x</em><sub><em>t</em></sub>|<em>x</em><sub><em>t</em> + 1</sub>)</span> 。假设我们可以训练一个模型求解这个分布：</p><p><span class="math display">$$\begin{split}    p_{\theta}(x_{t-1} \mid x_t)=\mathcal{N}(x_{t-1} ; \mu_{\theta}(x_t, t),  \Sigma_{\theta}(x_t, t))\end{split}$$</span></p><p>不同于DPM<sub><span style="color:blue;"><span class="citation" data-cites="sohl2015deep">[@sohl2015deep]</span></span></sub>的直接预测 <span class="math inline"><em>x</em><sub>0</sub></span> ，DDPM使用UNet预测每一个时间步添加的噪声 <span class="math inline">$\hat{\epsilon_\theta}(x_t,t)\approx\epsilon$</span> ，然后通过一系列数学推导得到：</p><p><span class="math display">$$\begin{split}    \mu_\theta\left(x_t, t\right)=\frac{1}{\sqrt{\alpha_t}}\left(x_t-\frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}_t}} \hat{\epsilon}_\theta\left(x_t, t\right)\right)\end{split}$$</span></p><p><span class="math inline"><em>p</em><sub><em>θ</em></sub>(<em>x</em><sub><em>t</em> − 1</sub>∣<em>x</em><sub><em>t</em></sub>)</span> 的预测均值已经得到，而预测方差 <span class="math inline"><em>Σ</em><sub><em>θ</em></sub>(<em>x</em><sub><em>t</em></sub>,<em>t</em>)</span> 在DDPM中被设为随机高斯噪声 <span class="math inline"><em>σ</em><sub><em>t</em></sub><em>z</em></span> ，由此可以得到：</p><p><span class="math display">$$\begin{split}    x_{t-1}=\mu_\theta\left(x_t, t\right)+\sigma_t z\end{split}$$</span></p><p>可以看到，模型预测每一步的噪声 <span class="math inline"><em>ϵ</em><sub><em>t</em></sub></span> 要比之前直接预测 <span class="math inline"><em>x</em><sub>0</sub></span> 容易得多，DDPM的最终目标函数就是最小化去噪误差，即：</p><p><span class="math display">$$\begin{split}    \mathcal L(\theta)=\mathbb{E}_{x_0,\epsilon\sim\mathcal{N}(0,I),t}\left[\left\|\epsilon-\hat{\epsilon}_\theta(x_t,t)\right\|^2\right]\end{split}$$</span></p><p>这个损失函数就相当于训练模型预测噪声 <span class="math inline"><em>ϵ</em></span> 的均方误差，学习难度大幅降低。不仅如此，在ImageNet 256$$256任务上，DDPM取得了优于GAN的FID分数，这是扩散模型首次取得能和GAN竞争的分数，后续OpenAI发布了基于DDPM的DALL·E，也进一步提升了扩散模型的影响力。</p><h2 id="sde">SDE</h2><p>随机微分方程（Stochastic Differential Equations，SDE）<sub><span style="color:blue;"><span class="citation" data-cites="song2020score">[@song2020score]</span></span></sub>是宋飏老师提出的一个连续时间扩散框架，它从数学上统一了NCSN和DDPM。如前文所介绍的，NCSN采用分数匹配的方法来预测概率分布的对数梯度，之后使用郎之万动力学采样以生成图像，它可以看作是连续噪声扰动；而DDPM则在有限的 <span class="math inline"><em>T</em></span> 步内加噪随后逆向去噪，采用预测噪声的方法来生成数据分布，基础理论依据是离散马尔可夫过程。</p><p><strong>随机微分方程</strong>是一类在经典微分方程基础上引入随机过程的数学方程，用于描述具有随机性或不确定性系统的演化，数据的随机扩散过程就可以用随机微分方程进行描述：</p><p><span class="math display">$$\begin{split}    dx=f(x,t)dt+g(t)dW\end{split}$$</span></p><p>其中 <span class="math inline"><em>f</em>(<em>x</em>, <em>t</em>)</span> 是漂移项（drift term），用于描述数据的确定性过程，<span class="math inline"><em>g</em>(<em>t</em>)</span>是扩散项（diffusion term），描述系统的随机性过程，<span class="math inline"><em>d</em><em>W</em></span>是标准布朗运动（Wiener Process）。经过一系列数学推导，可以分别得到NCSN和DDPM所对应的SDE，分别命名为 <strong>VE-SDE</strong>（Variance Exploding SDE）和 <strong>VP-SDE</strong>（Variance Preserving SDE）：</p><p><span class="math display">$$\begin{gather}\begin{split}        dx&amp;=\sqrt{d[\sigma^2(t)]}dW\quad\text{(VESDE)} \\        dx&amp;=-\frac{1}{2}\beta(t)xdt+\sqrt{\beta(t)}dW\quad\text{(VPSDE)} \end{split}\end{gather}$$</span></p><p>可以发现VESDE没有漂移项，噪声会随着 <span class="math inline"><em>t</em></span> 的增加而爆炸性增长，因此叫作“Variance Exploding”，而VPSDE的噪声变化则较为平缓，因此叫作“Variance Preserving”。其实除了这两种方程外，原文还针对VPSDE提出了改进版本sub-VPSDE，为扩散项加上了一个额外的衰减因子让噪声水平增长得更慢，不会在早期就过度加噪，造成采样时需要更多的去噪步数。</p><p>使用SDE采样的关键是要确定逆向SDE，即从 <span class="math inline"><em>x</em><sub><em>T</em></sub></span> 逆推 <span class="math inline"><em>x</em><sub>0</sub></span>，由于扩散过程的反向过程也是一个扩散过程，因此我们可以得到逆向SDE的方程，其中的漂移系数和扩散系数和SDE保持一致：</p><p><span class="math display">$$\begin{split}    dx=[f(x,t)-g^2(t)\nabla_x\log p_t(x)]dt+g(t)d\bar{W}\end{split}$$</span></p><p>其中 <span class="math inline">∇<sub><em>x</em></sub>log <em>p</em><sub><em>t</em></sub>(<em>x</em>)</span> 可由神经网络 <span class="math inline"><em>s</em><sub><em>θ</em></sub>(<em>x</em>, <em>t</em>)</span> 预测得到，由此可以分别得到两个方程的逆向SDE离散表达：</p><p><span class="math display">$$\begin{gather}\begin{split}    x_i&amp;=x_{i+1}+(\sigma_{i+1}^2-\sigma_i^2)s_{\theta^*}(x_{i+1},i+1) +\sqrt{\sigma_{i+1}^2-\sigma_i^2}z_{i+1}\quad\text{(VESDE)} \\    x_i&amp;=(2-\sqrt{1-\beta_{i+1}})x_{i+1}+\beta_{i+1}s_{\theta^*}(x_{i+1},i+1)+\sqrt{\beta_{i+1}}z_{i+1}\quad\text{(VPSDE)} \end{split}\end{gather}$$</span></p><p>根据两个逆向SDE可以发现，它们的形式分别等效于NCSN所使用的郎之万采样和DDPM所使用的马尔科夫链，由此原文使用连续的SDE统一了两种生成框架。</p><p>除此之外，原文还提出了<strong>PC采样方法</strong>，这也是SDE数值求解常用的一种方法，因为SDE是对时间连续的方程，所以不同的离散化方案总是存在一定误差，可以使用score-based MCMC采样方法进行进一步校正，即在每一步采样中额外添加一个修正步骤，让数据更快收敛。这里PC采样采用的修正器还是郎之万动力学方程，总结来说PC采样分两步：（1）Predictor，使用逆向SDE进行一步采样；（2）Corrector，使用郎之万动力学采样进一步调整。</p><p>去掉逆向SDE中的随机项（布朗运动 <span class="math inline"><em>d</em><em>w</em></span> 项）后，可以得到概率流常微分方程（Probabilistic Flow ODE），此时整个采样过程是一条确定性的轨迹，求解更快，因此适用于高效采样需求的任务。</p><h2 id="ddim">DDIM</h2><p>DDPM中，前向扩散过程定义为马尔科夫过程，为数据 <span class="math inline"><em>x</em><sub>0</sub></span> 逐步添加噪声，逆向过程同样需要逐步去噪。但是为了保证最终前向过程的 <span class="math inline"><em>x</em><sub><em>T</em></sub></span> 满足高斯噪声，这里的步数 <span class="math inline"><em>T</em></span> 要设置得足够大（1000+），导致逆向过程的采样步数也非常大，推理的时间和计算开销巨大。DDIM<sub><span style="color:blue;"><span class="citation" data-cites="song2020denoising">[@song2020denoising]</span></span></sub>提出了确定性采样方法，是扩散过程变为确定性过程（ODE），从而加速采样。</p><p>首先回顾一下定义在马尔科夫链的DDPM中单步加噪和单步去噪过程：</p><p><span class="math display">$$\begin{gather}\begin{split}        x_t &amp;= \sqrt{\alpha_t}x_{t-1}+\sqrt{1-\alpha_t}\epsilon_t \\        x_{t-1}&amp;=\frac{1}{\alpha_t}(x_t-\frac{\beta_t}{\sqrt{1-\bar{\alpha_t}}}\epsilon_\theta(x_t,t))+\sigma_t z\end{split}\end{gather}$$</span></p><p>基于此重新推导DDPM的优化目标，可以得到：</p><p><span class="math display">$$ \begin{split}\mathcal L=\mathbb{E}_{x_0,\epsilon\sim\mathcal{N}(\mathbf{0},\mathbf{I})}\left[\|\epsilon-\epsilon_\theta\left(\sqrt{\bar{\alpha}_t}x_0+\sqrt{1-\bar{\alpha}_t}\epsilon,t\right)\|^2\right] \end{split}$$</span></p><p>可以发现，DDPM的优化目标其实仅仅依赖于边缘分布 <span class="math inline"><em>q</em>(<em>x</em><sub><em>t</em></sub>|<em>x</em><sub>0</sub>)</span> ，而不依赖于联合分布 <span class="math inline"><em>q</em>(<em>x</em><sub>1 : <em>T</em></sub>|<em>x</em><sub>0</sub>)</span> ，因此可以看出DDPM其实并不要求推理过程一定要是马尔科夫过程，只要推理分布满足边缘分布条件即可。经过一系列重新推导，可以得到非马尔科夫链下的单步逆向过程为：</p><p><span class="math display">$$\begin{split}\mathbf{x}_{t-1}=\sqrt{\alpha_{t-1}}\left(\underbrace{\frac{\mathbf{x}_t-\sqrt{1-\alpha_t}\epsilon_\theta(\mathbf{x}_t,t)}{\sqrt{\alpha_t}}}_{\mathrm{predicted~}\mathbf{x}_0}\right) +\underbrace{\sqrt{1-\alpha_{t-1}-\sigma_t^2}\cdot\epsilon_\theta(\mathbf{x}_t,t)}_{\text{direction pointing to }\mathbf{x}_t}+\underbrace{\sigma_t\epsilon_t}_{\text{random noise}}\end{split}$$</span></p><p>其中 <span class="math inline">$\sigma_t^2=\eta\cdot\sqrt{(1-\alpha_{t-1})/(1-\alpha_t)}\sqrt{(1-\alpha_t/\alpha_{t-1})}$</span> 。</p><ul><li>当 <span class="math inline"><em>η</em> = 1</span> 时，此时的生成过程和DDPM一致；</li><li>当 <span class="math inline"><em>η</em> = 0</span> 时，此时的生成过程就没有随机噪声项了，是一个确定性的过程，这就是DDIM（Denoising Diffusion Implic Model），此时的样本生成就变成了确定的过程，有点类似于SDE中的概率流ODE。</li></ul><figure><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/DDIM_0.png" alt="" /><figcaption>DDIM的采样过程</figcaption></figure><p>由于DDIM并没有明确前向过程，这意味这可以定义一个更短的步数的前向过程以减少采样步数。原文从原始序列 <span class="math inline">[1, …, <em>T</em>]</span> 采样一个长度为 <span class="math inline"><em>S</em></span> 的子序列 <span class="math inline">[<em>τ</em><sub>1</sub>, …, <em>τ</em><sub><em>S</em></sub>]</span> ，此时前向过程 <span class="math inline">[<em>x</em><sub><em>τ</em><sub>1</sub></sub>, …, <em>x</em><sub><em>τ</em><sub><em>S</em></sub></sub>]</span> 同样满足 <span class="math inline">$q({x}_{\tau_i}|{x}_0)=\mathcal{N}({x}_t;\sqrt{\alpha_{\tau_i}}{x}_0,(1-\alpha_{\tau_i}){I})$</span> 。由此该生成过程也可以用这个子序列进行采样，最终加速生成过程，能从DDPM的1000步采样减少为50步采样，大幅降低推理开销。</p><h2 id="cdm">CDM</h2><p>条件扩散模型（Conditional Diffusion Model，CDM）指使用条件控制生成的扩散模型，其中最常用的方法是引导（guidance）方法，可以用于增强生成质量或是让模型朝向某个目标分布进行采样。最主要的两种方法是：分类器引导（Classifier Guidance）<sub><span style="color:blue;"><span class="citation" data-cites="dhariwal2021diffusion">[@dhariwal2021diffusion]</span></span></sub>和无分类器引导（Classifier-Free Guidance）<sub><span style="color:blue;"><span class="citation" data-cites="ho2022classifier">[@ho2022classifier]</span></span></sub>，因此这里将分别简要介绍这两篇工作。</p><h3 id="分类器引导">分类器引导</h3><p>假设我们需要在扩散过程中引入条件信息 <span class="math inline"><em>y</em></span> ，直觉上来说，条件信息不会影响前向过程，因为最终加噪都会变为高斯噪音，即 <span class="math inline"><em>q</em>(<em>x</em><sub>1 : <em>T</em></sub>|<em>x</em><sub>0</sub>, <em>y</em>) = <em>q</em>(<em>x</em><sub>1 : <em>T</em></sub>|<em>x</em><sub>0</sub>)</span> 。因此我们需要重点关注逆向过程的条件控制，不妨说我们需要重点关注分数函数 <span class="math inline"><em>ŝ</em><sub><em>θ</em></sub>(<em>x</em><sub><em>t</em></sub>, <em>t</em>) ≈ ∇<sub><em>x</em><sub><em>t</em></sub></sub>log <em>p</em>(<em>x</em><sub><em>t</em></sub>)</span> 的变化。引入条件信息后，我们可以使用贝叶斯公式进行几步变换：</p><p><span class="math display">$$\begin{gather}\begin{split}        \nabla_{x_t}\log p(x_t|y)&amp;=\nabla_{x_t}\log(\frac{p(x_t)p(y|x_t)}{p(y)}) \\        &amp;= \nabla_{x_t}\log p(x_t)+\nabla_{x_t}\log p(y|x_t)-\nabla_{x_t}\log p(y) \\         &amp;= \nabla_{x_t}\log p(x_t)+\nabla_{x_t}\log p(y|x_t)\end{split}\end{gather}$$</span></p><p>可以看到，展开后有两项，其中第一项相当于无条件生成时的分数函数，可以称为无条件分数（Unconditional Score）；而第二项相当于一个分类器的对数梯度，称为对抗梯度（Adversarial Gradient）。为了能更好的控制生成内容的方向，论文额外引入了一个超参数 <span class="math inline"><em>λ</em></span> 作为引导强度：</p><p><span class="math display">$$\begin{gather}\nabla_{x_t}\log p(x_t|y)=\nabla_{x_t}\log p(x_t)+\lambda\nabla_{x_t}\log p(y|x_t)\end{gather}$$</span></p><h3 id="无分类器引导">无分类器引导</h3><p>自OpenAI发布分类器引导的条件生成范式后，GoogleBrain团队提出了无分类器引导方法，直接在扩散模型内部学习引导信息，无需额外的分类器。还是从分数估计的角度来进行推导，从条件引导的工作我们得知 <span class="math inline">∇<sub><em>x</em><sub><em>t</em></sub></sub>log <em>p</em>(<em>x</em><sub><em>t</em></sub>|<em>y</em>) = ∇<sub><em>x</em><sub><em>t</em></sub></sub>log <em>p</em>(<em>x</em><sub><em>t</em></sub>) + ∇<sub><em>x</em><sub><em>t</em></sub></sub>log <em>p</em>(<em>y</em>|<em>x</em><sub><em>t</em></sub>)</span> ，变换一下可以得到 <span class="math inline">∇<sub><em>x</em><sub><em>t</em></sub></sub>log <em>p</em>(<em>y</em>|<em>x</em><sub><em>t</em></sub>) = ∇<sub><em>x</em><sub><em>t</em></sub></sub>log <em>p</em>(<em>x</em><sub><em>t</em></sub>|<em>y</em>) − ∇<sub><em>x</em><sub><em>t</em></sub></sub>log <em>p</em>(<em>x</em><sub><em>t</em></sub>)</span> ，由此可以得到：</p><p><span class="math display">$$\begin{gather}\begin{split}    \nabla_{x_t}\log p(x_t|y)&amp;=\nabla_{x_t}\log p(x_t)+\lambda\nabla_{x_t}\log p(y|x_t) \\    &amp;=\nabla_{x_t}\log p(x_t)+\lambda(\nabla_{x_t}\log p(x_t|y)-\nabla_{x_t}\log p(x_t)) \\    &amp;=\lambda\nabla_{x_t}\log p(x_t|y)+(1-\lambda)\log p(x_t)\end{split}\end{gather}$$</span></p><p>可以发现，此时分数仍然分为两部分：第一项可以看作是条件分数（Conditional Score），第二项则是无条件分数（Unconditional Score）。并且 <span class="math inline"><em>λ</em></span> 的取值会影响到条件控制的强弱：</p><ul><li>当 <span class="math inline"><em>λ</em> = 0</span> 时，此时相当于无条件生成；</li><li>当 <span class="math inline"><em>λ</em> &gt; 1</span> 时，模型会优先考虑条件控制而远离无条件分数网络方向。</li></ul><p>这样看似乎还是要训练两个网络，但实际上无条件可看作是条件控制的特殊情况，即 <span class="math inline"><em>y</em> = ⌀</span> ，这样在训练时可以交替训练有条件和无条件的情况。</p><h2 id="ldm">LDM</h2><p>DDPM生成的图像质量已经非常好了，但是训练开销很大，一个问题在于中间的加噪状态 <span class="math inline"><em>x</em><sub><em>t</em></sub></span> 的尺寸是和输入保持一致的，这使得其训练开销随图像分辨率的增大而加重，无法适应高质量图像生成任务。因此潜在扩散模型（Latent Diffusion Model，LDM）<sub><span style="color:blue;"><span class="citation" data-cites="rombach2022high">[@rombach2022high]</span></span></sub>针对这个问题做了一些改进，<strong>将图像从像素空间表示（Pixel Space）转变为潜在空间表示（Latent Space）实现高分辨率图像生成任务</strong>。</p><figure><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/LDM_0.png" alt="" /><figcaption>潜在扩散模型的模型架构</figcaption></figure><p>LDM的主要架构由三部分组成：VAE编码器（Encoder）、扩散模型以及VAE解码器（Decoder）。其中VAE编码器将高维图像 <span class="math inline"><em>x</em><sub>0</sub></span> 编码表示为潜在表示 <span class="math inline"><em>z</em><sub>0</sub></span> ，然后送入扩散模型中进行扩散和去噪，最终VAE解码器将去噪得到的潜在空间表示 <span class="math inline">$\hat{z_0}$</span> 还原为像素空间表示 <span class="math inline">$\hat{x_0}$</span> ，得到高质量的图像。这个VAE可以是预训练好的模型，在训练扩散模型时，其参数是被冻结的。</p><p>而对于条件生成处理上，LDM引入条件融合模块 <span class="math inline"><em>τ</em><sub><em>θ</em></sub></span> 来处理多种模态的条件信息 <span class="math inline"><em>y</em></span> 。比如对于文生图任务，这里的 <span class="math inline"><em>τ</em><sub><em>θ</em></sub></span> 就是一个文本编码器，可以使用预训练好的CLIP模型<sub><span style="color:blue;"><span class="citation" data-cites="radford2021learning">[@radford2021learning]</span></span></sub>中的文本编码器。同时引入条件融合开关：</p><ul><li>对于文本输入，这里在Unet网络中添加了Attention层将Embedding向量 <span class="math inline"><em>τ</em><sub><em>θ</em></sub>(<em>y</em>)</span> 融合到每层特征中；</li><li>而对于其他空间的条件（语义图、修复图等），则直接通过拼接完成条件融合。</li></ul><p>由此我们可以得到LDM的目标函数为：</p><p><span class="math display">$$\begin{split}\mathcal L_{L D M}:=\mathbb{E}_{\mathcal{E}(x), y, \epsilon \sim \mathcal{N}(0,1), t}\left[\left\|\epsilon-\epsilon_\theta\left(z_t, t, \tau_\theta(y)\right)\right\|_2^2\right]\end{split}$$</span></p><p>后续爆火的Stable Diffusion就是LDM的一个开源预训练模型，一度占据图像生成开源领域的主导地位。</p><h2 id="dit">DiT</h2><p>DiT（Diffusion Transformer）<sub><span style="color:blue;"><span class="citation" data-cites="peebles2023scalable">[@peebles2023scalable]</span></span></sub>是Meta AI提出的基于Transformer的扩散模型，它首次在扩散模型完全用Transformer替代了UNet，提升了扩散模型的可扩展性和生成质量。</p><figure><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/DiT_0.png" alt="" /><figcaption>DiT的模型架构</figcaption></figure><p>DiT也是一个作用在潜在空间上的模型，同样使用一个VQVAE将图像编码到潜在空间上，之后送入DiT模块中加工。不同的是，由于舍弃掉了CNN，这里使用了ViT进一步将潜在空间特征转换为一维序列特征（Patch Token），并将时间步 <span class="math inline"><em>t</em></span> 和条件信息 <span class="math inline"><em>y</em></span> 融合后嵌入到图像的Patch Token中。</p><p>为了选择融合条件特征效果最好的DiT模块，原文一共探索了四种不同的DiT模块：</p><ul><li>基于上下文条件（In-context conditioning）的DiT模块，直接将条件特征嵌入到输入序列中；</li><li>基于交叉注意力（Cross Attention）的DiT模块，将时间步 <span class="math inline"><em>t</em></span> 和条件信息 <span class="math inline"><em>y</em></span> 拼成长度为2的序列，然后输入到多头交叉注意力模块中和图像特征进行融合；</li><li>基于自适应层归一化（Adaptive Layer Normalization，AdaLN）的DiT模块，通过使用条件信息学习 <span class="math inline"><em>β</em></span> 和 <span class="math inline"><em>γ</em></span> 两个归一化参数来调整中间特征；</li><li>基于Zero初始化的AdaLN的DiT模块，是AdaLN方案的改进版本，将AdaLN的线性层参数初始化为zero，并额外在每个残差模块结束之前引入回归缩放参数 <span class="math inline"><em>α</em></span> 。</li></ul><p>通过对四种模块进行对比实验，发现AdaLN-Zero的效果是最好的，DiT模块默认采用这种方式来嵌入条件。</p><p>同时，需要注意的是DiT所使用的扩散模型沿用了OpenAI的改进版DDPM<sub><span style="color:blue;"><span class="citation" data-cites="song2020improved">[@song2020improved]</span></span></sub>，不再采用固定的方差，而是采用另一个网络来预测方差 <span class="math inline"><em>Σ</em><sub><em>θ</em></sub>(<em>x</em><sub><em>t</em></sub>,<em>t</em>) = exp (<em>v</em>log<em>β</em><sub><em>t</em></sub>+(1−<em>v</em>)<em>β̃</em><sub><em>t</em></sub>)</span> ，训练时采用无分类器引导的范式进行学习。该种方法对于视频生成等需要强可扩展性的任务来说很适用，OpenAI推出的Sora就是用了DiT作为模型架构。</p><h2 id="pixart">PixArt</h2><p>在DiT推出之后，华为诺亚方舟实验室又提出了PixArt-<span class="math inline"><em>α</em></span><sub><span style="color:blue;"><span class="citation" data-cites="chen2023pixart">[@chen2023pixart]</span></span></sub>，这也是一种基于transformer的文本到图像的扩散模型，它在显著降低训练成本的同时，也实现了很不错的图像生成质量。</p><p>PixArt-<span class="math inline"><em>α</em></span>模型还是使用DiT作为基础架构，但是进行了一些改进。原DiT架构中的每个DiT模块中的<span class="math inline"><em>S</em><sup><em>i</em></sup></span>都是通过独立的MLP计算得到的，即 <span class="math inline"><em>S</em><sup>(<em>i</em>)</sup> = <em>f</em><sup>(<em>i</em>)</sup>(<em>c</em> + <em>t</em>)</span> ，其中 <span class="math inline"><em>c</em>, <em>t</em></span> 分别表示类别条件和时间步信息，这会占据很高的开销。基于此，PixArt提出了AdaLN-single，定义一个全局 <span class="math inline"><em>S̄</em> = <em>f</em>(<em>t</em>)</span> ，只使用时间步信息生成 <span class="math inline"><em>S̄</em></span> ，在第 <span class="math inline"><em>i</em></span> 个模块中，通过计算 <span class="math inline"><em>S</em><sup>(<em>i</em>)</sup> = <em>g</em>(<em>S̄</em>, <em>E</em><sup>(<em>i</em>)</sup>)</span> 得到每个模块的缩放和偏移参数，其中 <span class="math inline"><em>E</em><sup>(<em>i</em>)</sup></span> 是可训练的嵌入表示；而文本条件 <span class="math inline"><em>c</em></span> 则通过一个额外的多头交叉注意力嵌入到模块中。大量实验表明，通过引入全局MLP和逐层嵌入处理时间步 <span class="math inline"><em>t</em></span> 信息、使用交叉注意力层处理文本信息 <span class="math inline"><em>c</em></span> 的改进，能在有效减小模型大小的同时保持原生成能力。</p><figure><img src="https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main/images/PixArt_0.png" alt="" /><figcaption>PixArt-<span class="math inline"><em>α</em></span>的模型架构</figcaption></figure><p>除此之外，PixArt-<span class="math inline"><em>α</em></span>将复杂的文本到图像的生成任务分解为三个子任务以逐步训练：</p><ul><li>像素级依赖的学习（Pixel Dependency Learning），为了实现后续高质量的生成，PixArt先在ImageNet上预训练类引导生成模型，这一过程成本低廉并能帮助模型有效学习到图像的像素级依赖性；</li><li>文本到图像的精确对齐学习（Text-image Alignment Learning），原论文构建了一个包含高概念密度的精确文本-图像对数据集，相较于以往的数据集，歧义显著减少，并能处理更多的名词；</li><li>高质量图像微调（High-resolution and Aesthetic Image Generation），为了生成高审美质量的图片，原论文最后使用高质量的图片对模型进行进一步微调。</li></ul><h1 id="总结与展望">总结与展望</h1><p>在扩散模型成为主流之前，基于能量的生成模型和分数匹配已经被研究了许多年，这些工作为扩散模型的出现奠定了理论基础，直到后来DDPM的提出正式标志着扩散模型的爆发。之后一系列的工作探讨了对原始模型的改进，体现在加速采样、降低训练开销、提升图像生成质量等，一度使扩散模型超越基于自回归和GAN的生成模型成为大规模生成任务的首选模型。后来进入大模型时代，包括DALL·E、IMAGEN、Stable Diffusion的文生图大模型更是进一步引爆了Diffusion的影响力。总的来说，作为当下Aigc领域中热门研究领域之一，扩散模型的发展正值草长莺飞的时期，它开创了一种全新的生成模型范式，并被广泛应用于各类生成式任务以及当下视觉生成大模型中。</p><p>未来对于扩散模型的研究还在继续，比如如何进一步提升采样速度、创造更通用的多模态扩散模型、更精细的条件控制、量化优化以实现Edge AI等等，还有很多课题值得探索...</p>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AIGC </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>建站日志以及记录</title>
      <link href="/2025/03/27/hello-world/"/>
      <url>/2025/03/27/hello-world/</url>
      
        <content type="html"><![CDATA[<h1 id="leo小破站的诞生日">Leo小破站的诞生日</h1><p><strong>2025.3.31</strong></p><h2 id="关于建站">关于建站</h2><p>使用Hexo框架 + Github Pages + Butterfly主题，这里要感谢<a href="https://blog.csdn.net/m0_51269961/article/details/122575897">杰森的教程</a>，向大佬低头orz…</p><h2 id="关于契机">关于契机</h2><p>其实很早之前就想建站了，但是拖延症一直拖到现在…上本研共修课看到了大二学弟做的博客，觉得自愧不如，趁现在还有时间，随便建一个站吧</p><h1 id="hexo">Hexo</h1><p>关于hexo的一些常用命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 事实上已经修改json，调试时只需要hexo s即可</span></span><br><span class="line">$ hexo clean</span><br><span class="line">$ hexo generate</span><br><span class="line">$ hexo server</span><br><span class="line"></span><br><span class="line">$ hexo d</span><br></pre></td></tr></table></figure><p>More info about writing: <a href="https://hexo.io/docs/writing.html">Writing</a></p><p>More info about server: <a href="https://hexo.io/docs/server.html">Server</a></p><p>More info about deployment: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p><h1 id="butterfly">Butterfly</h1><p>根据<a href="https://butterfly.js.org/">原主题配置文档</a>修改 <code>_config.butterfly.yml</code> 文件自定义博客配置。</p><h2 id="发布文章">发布文章</h2><p>需要修改 <code>Post Fron-matter</code> ，于文章最上方以 <code>---</code> 分隔的区域，常用配置参数如下。</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title: （required）文章标题</span><br><span class="line">date: （optional）创建时间</span><br><span class="line">updated: （optional）更新时间</span><br><span class="line">tags: （optional）文章标签</span><br><span class="line">categories: （optional）文章分类</span><br><span class="line">description: （optional）文章简介</span><br><span class="line">top<span class="emphasis">_img: （optional）文章头图</span></span><br><span class="line"><span class="emphasis">% 转载其他作品时放置</span></span><br><span class="line"><span class="emphasis">copyright: （optional）显示文章版权模块</span></span><br><span class="line"><span class="emphasis">copyright_</span>author: （optional）文章作者</span><br><span class="line">copyright<span class="emphasis">_author_</span>href: （optional）文章作者url</span><br><span class="line">copyright<span class="emphasis">_url: （optional）文章url</span></span><br><span class="line"><span class="emphasis">copyright_</span>info: （optional）版权声明</span><br><span class="line">---</span><br></pre></td></tr></table></figure><h2 id="发布页面">发布页面</h2><p>需要修改 <code>Page Front-matter</code>，于文章最上方以 <code>---</code> 分隔的区域，常用配置参数如下。</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title: （required）文章标题</span><br><span class="line">date: （optional）创建时间</span><br><span class="line">updated: （optional）更新时间</span><br><span class="line">type: （required）标签、分类等页面必须标明</span><br><span class="line">description: 页面描述</span><br><span class="line"><span class="section">top<span class="emphasis">_img: 页面头图</span></span></span><br><span class="line"><span class="emphasis"><span class="section">---</span></span></span><br></pre></td></tr></table></figure><h2 id="关于公式">关于公式</h2><p>原公式渲染器无法正常渲染复杂latex公式，卸载了原公式渲染器：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm un hexo-renderer-marked</span><br></pre></td></tr></table></figure><p>并安装了 <code>pandoc</code> 对应的渲染器：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm i hexo-renderer-pandoc</span><br></pre></td></tr></table></figure><p>启用 <code>mathjax</code> 设置，但是这样做行内公式其实不太好看、行间公式也还是无法正常渲染，调试发现需要额外插入 <code>begin-end</code> 块才能正常渲染。以下为演示：</p><blockquote><p>行内公式： 引入缩放因子 <span class="math inline"><em>λ</em><sub><em>θ</em></sub></span></p></blockquote><blockquote><p>行间公式： <span class="math display">$$\begin{split}   s_\theta(x)\approx\nabla_x\log p_\mathrm{data}(x|\sigma) \end{split}$$</span></p><p><span class="math display">$$\begin{gather}\begin{split}   J(\theta) &amp;=\frac{1}{2}\int p_\mathrm{data}{(x)}\|s_{data}(x)-s_\theta(x)\|_2^2dx \\    &amp;=\frac{1}{2}\mathbb{E}_{p_{\mathrm{data}}(x)}\left[\left\|s_{data}(x)-s_\theta(x)\right\|_2^2\right]\end{split}\end{gather}$$</span></p></blockquote><p>以上行间公式对应的md写法为： <figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">$$</span><br><span class="line">  \begin&#123;split&#125; </span><br><span class="line"><span class="code">    s_\theta(x)\approx\nabla_x\log p_\mathrm&#123;data&#125;(x|\sigma) </span></span><br><span class="line"><span class="code">  \end&#123;split&#125;</span></span><br><span class="line"><span class="code">$$</span></span><br><span class="line"><span class="code"></span></span><br><span class="line">$$</span><br><span class="line">\begin&#123;gather&#125;</span><br><span class="line">\begin&#123;split&#125;</span><br><span class="line"><span class="code">    J(\theta) &amp;=\frac&#123;1&#125;&#123;2&#125;\int p_\mathrm&#123;data&#125;&#123;(x)&#125;\|s_&#123;data&#125;(x)-s_\theta(x)\|_2^2dx \\</span></span><br><span class="line"><span class="code">     &amp;=\frac&#123;1&#125;&#123;2&#125;\mathbb&#123;E&#125;_&#123;p_&#123;\mathrm&#123;data&#125;&#125;(x)&#125;\left[\left\|s_&#123;data&#125;(x)-s_\theta(x)\right\|_2^2\right]</span></span><br><span class="line"><span class="code">\end&#123;split&#125;</span></span><br><span class="line"><span class="code">\end&#123;gather&#125;</span></span><br><span class="line"><span class="code">$$</span></span><br></pre></td></tr></table></figure></p><div class="note warning simple"><p>暂时没有找到什么其他解决办法，留待解决…</p></div><h1 id="github-pages">Github Pages</h1><p>Leo的博客：<a href="https://litchi-lee.github.io/">https://litchi-lee.github.io/</a></p><h2 id="图床">图床</h2><p>额外建库搭配 <code>PicGo</code> 生成图床，配合Github插件<a href="https://github.com/marketplace/imgbot">Github imgbot</a>压缩库的图片。</p><p>由于Github Raw链接被大量匿名访问会被封禁，因此这里使用免费的jsDelivr加速GitHub资源：</p><pre><code>https://cdn.jsdelivr.net/gh/litchi-lee/Images_shop@main</code></pre>]]></content>
      
      
      <categories>
          
          <category> 杂七杂八 </category>
          
      </categories>
      
      
    </entry>
    
    
  
  
</search>
